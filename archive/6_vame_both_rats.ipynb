{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy.lib.recfunctions as rfn\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000, only_rat1 = False):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    if not only_rat1:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    else:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_1_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_1_vel_labels])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, splits= (0.7, 0.2, 0.1), separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = data_len // chunk_size\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = splits[0]*num_chunks\n",
    "    val_chunk_idx_bound = (splits[0]+splits[1])*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    features_val = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    targets_val = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        elif i < val_chunk_idx_bound:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('val')\n",
    "            features_val.append(curr_chunk)\n",
    "            targets_val.append(curr_chunk_t)\n",
    "\n",
    "#     print(len(features_separate))\n",
    "#     print(len(targets_separate))\n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    features_val = n.concatenate(features_val, axis=0)\n",
    "    \n",
    "    targets_val = n.concatenate(targets_val)\n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, features_val, targets_train, targets_test, targets_val\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_val, features_separate,\\\n",
    "                targets_train, targets_test, targets_val, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / n.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return (n.arccos(n.clip(n.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = ['Female1']#list(all_data.keys())\n",
    "datas = []\n",
    "# for key in all_data.keys():\n",
    "#     all_data[key] = calculate_velocity(all_data[key])\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels, None, False)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 40)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_points = n.zeros((features_all.shape[0], int(features_all.shape[1]/2), 2))\n",
    "# t = 1000\n",
    "angles = []\n",
    "for t in range(features_all.shape[0]):\n",
    "    points = features_all[t]\n",
    "    # x, y format\n",
    "    anchor_point = n.array([points[1], points[0]])\n",
    "    second_point = n.array([points[3], points[2]])\n",
    "    anchor_vector = second_point - anchor_point\n",
    "    angle_to_rotate = -angle_between(anchor_vector, n.array((1,0)))\n",
    "    if n.all(anchor_vector[1] < 0): angle_to_rotate *= -1\n",
    "    angles.append(angle_to_rotate)\n",
    "#     if angle_to_rotate < 0: angle_to_rotate += 2*n.pi\n",
    "    # given vector (x1, y1), rotating it by A around origin gives:\n",
    "    # x2 = cosA x1 - sinA y1\n",
    "    # y2 = sinA x1 + cosA y1\n",
    "    num_points = int(len(points)/2)\n",
    "    new_points = n.zeros((num_points,2))\n",
    "    for points_idx in range(1,num_points):\n",
    "        second_point = n.array([points[points_idx*2+1], points[points_idx*2]])\n",
    "        vector = second_point - anchor_point\n",
    "        new_x = n.cos(angle_to_rotate) * vector[0] - n.sin(angle_to_rotate) * vector[1]\n",
    "        new_y = n.sin(angle_to_rotate) * vector[0] + n.cos(angle_to_rotate) * vector[1]\n",
    "\n",
    "        ego_points[t][points_idx] = [new_x, new_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 18000\n",
    "scale = 100\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(ego_points[t,i,0], ego_points[t,i,1])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-scale,scale)\n",
    "plt.xlim(-scale,scale)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(features_all[t][i*2+1], features_all[t][i*2])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-scale,scale)\n",
    "plt.xlim(-scale,scale)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\both-rats-f1-Nov23-2020\\videos\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\both-rats-f1-Nov23-2020\\data\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\both-rats-f1-Nov23-2020\\results\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\both-rats-f1-Nov23-2020\\model\"\n",
      "1  videos from the directory C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1 were added to the project.\n",
      "Copying the videos \n",
      "\n",
      "A VAME project has been created. \n",
      "\n",
      "Next use vame.create_trainset(config) to split your data into a train and test set. \n",
      "Afterwards you can use vame.rnn_model() to train the model on your data.\n"
     ]
    }
   ],
   "source": [
    "config = vame.init_new_project(project='both-rats-f1', videos = ['C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1'], \n",
    "                      working_directory='C:/Users/Neuropixel/AH-EN/swc-en-social/vame', videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\both-rats-f1-Nov23-2020\\\\config.yaml'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 20, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(, 'w+') as f:\n",
    "f = \"C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\both-rats-f1-Nov23-2020\\\\data\\\\Female1\\\\Female1-PE-seq.npy\"\n",
    "n.save(f, ego_points.reshape(-1, 40).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset.\n",
      "Lenght of train data: 24216\n",
      "Lenght of test data: 6054\n"
     ]
    }
   ],
   "source": [
    "trainset = vame.create_trainset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RNN model!\n",
      "Using CUDA\n",
      "GPU active: True\n",
      "GPU used: Quadro P620\n",
      "Latent Dimensions: 30, Beta: 1, lr: 0.0005\n",
      "Initialize train data. Datapoints 24216\n",
      "Initialize test data. Datapoints 6054\n",
      "Epoch: 1\n",
      "Train: \n",
      "Epoch: 1.  loss: 484490.1250\n",
      "Average Train loss: 95658.1016, MSE-Loss: 61378.1494, MSE-Future-Loss 34279.9516, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 4792.9370, MSE-Loss: 4792.9370, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 2\n",
      "Train: \n",
      "Epoch: 2.  loss: 41458.3945\n",
      "Average Train loss: 38760.8641, MSE-Loss: 22793.2892, MSE-Future-Loss 15967.5751, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 3045.3324, MSE-Loss: 3045.3324, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 3\n",
      "Train: \n",
      "Epoch: 3.  loss: 29246.6855\n",
      "Average Train loss: 29710.0811, MSE-Loss: 17073.0507, MSE-Future-Loss 12637.0304, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 2576.7392, MSE-Loss: 2576.7392, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 4\n",
      "Train: \n",
      "Epoch: 4.  loss: 27883.8398\n",
      "Average Train loss: 25700.9885, MSE-Loss: 14225.6746, MSE-Future-Loss 11465.4960, KL-Loss: 6.1346,  Kmeans-Loss: 3.6834, weigt: 0.5000\n",
      "Test: \n",
      "Average Test loss: 2258.4916, MSE-Loss: 2249.8767, KL-Loss: 6.1458, Kmeans-Loss: 2.4691\n",
      "lr: 0.0005\n",
      "Epoch: 5\n",
      "Train: \n",
      "Epoch: 5.  loss: 25325.3008\n",
      "Average Train loss: 23311.1830, MSE-Loss: 12193.8824, MSE-Future-Loss 11102.8946, KL-Loss: 9.3814,  Kmeans-Loss: 5.0247, weigt: 0.6250\n",
      "Test: \n",
      "Average Test loss: 1942.2500, MSE-Loss: 1929.1108, KL-Loss: 9.6317, Kmeans-Loss: 3.5075\n",
      "lr: 0.0005\n",
      "Epoch: 6\n",
      "Train: \n",
      "Epoch: 6.  loss: 22030.4961\n",
      "Average Train loss: 21237.6236, MSE-Loss: 10661.9751, MSE-Future-Loss 10555.6736, KL-Loss: 13.4682,  Kmeans-Loss: 6.5067, weigt: 0.7500\n",
      "Test: \n",
      "Average Test loss: 1675.5945, MSE-Loss: 1657.7682, KL-Loss: 13.2168, Kmeans-Loss: 4.6096\n",
      "lr: 0.0005\n",
      "Epoch: 7\n",
      "Train: \n",
      "Epoch: 7.  loss: 19896.4336\n",
      "Average Train loss: 19944.4617, MSE-Loss: 9640.8557, MSE-Future-Loss 10277.8715, KL-Loss: 17.6886,  Kmeans-Loss: 8.0458, weigt: 0.8750\n",
      "Test: \n",
      "Average Test loss: 1615.0047, MSE-Loss: 1591.8460, KL-Loss: 17.3005, Kmeans-Loss: 5.8582\n",
      "lr: 0.0005\n",
      "Epoch: 8\n",
      "Train: \n",
      "Epoch: 8.  loss: 20836.3027\n",
      "Average Train loss: 18981.7497, MSE-Loss: 8885.6735, MSE-Future-Loss 10064.2222, KL-Loss: 22.2111,  Kmeans-Loss: 9.6428, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1514.3186, MSE-Loss: 1485.3575, KL-Loss: 21.8177, Kmeans-Loss: 7.1435\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 9\n",
      "Train: \n",
      "Epoch: 9.  loss: 19725.2949\n",
      "Average Train loss: 17864.5111, MSE-Loss: 8184.2867, MSE-Future-Loss 9646.2484, KL-Loss: 23.9220,  Kmeans-Loss: 10.0539, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1354.8478, MSE-Loss: 1324.3065, KL-Loss: 23.0312, Kmeans-Loss: 7.5102\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 10\n",
      "Train: \n",
      "Epoch: 10.  loss: 16423.7793\n",
      "Average Train loss: 16634.2336, MSE-Loss: 7467.2071, MSE-Future-Loss 9130.8720, KL-Loss: 25.7121,  Kmeans-Loss: 10.4424, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1304.7020, MSE-Loss: 1272.2046, KL-Loss: 24.6712, Kmeans-Loss: 7.8262\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 11\n",
      "Train: \n",
      "Epoch: 11.  loss: 15736.1426\n",
      "Average Train loss: 16331.3311, MSE-Loss: 7177.1590, MSE-Future-Loss 9116.1540, KL-Loss: 27.2257,  Kmeans-Loss: 10.7924, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1143.2608, MSE-Loss: 1109.0000, KL-Loss: 26.1526, Kmeans-Loss: 8.1083\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 12\n",
      "Train: \n",
      "Epoch: 12.  loss: 16425.7227\n",
      "Average Train loss: 15800.3889, MSE-Loss: 6894.3823, MSE-Future-Loss 8866.1831, KL-Loss: 28.7188,  Kmeans-Loss: 11.1046, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1093.8071, MSE-Loss: 1058.5789, KL-Loss: 26.9261, Kmeans-Loss: 8.3022\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 13\n",
      "Train: \n",
      "Epoch: 13.  loss: 15259.9326\n",
      "Average Train loss: 15253.5829, MSE-Loss: 6588.0226, MSE-Future-Loss 8624.1459, KL-Loss: 30.0057,  Kmeans-Loss: 11.4087, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1124.1372, MSE-Loss: 1087.2966, KL-Loss: 28.1792, Kmeans-Loss: 8.6613\n",
      "lr: 0.0005\n",
      "Epoch: 14\n",
      "Train: \n",
      "Epoch: 14.  loss: 13536.7539\n",
      "Average Train loss: 14686.1520, MSE-Loss: 6283.5249, MSE-Future-Loss 8359.7009, KL-Loss: 31.2491,  Kmeans-Loss: 11.6771, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1063.1049, MSE-Loss: 1024.7919, KL-Loss: 29.3987, Kmeans-Loss: 8.9142\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 15\n",
      "Train: \n",
      "Epoch: 15.  loss: 14166.7881\n",
      "Average Train loss: 14200.3752, MSE-Loss: 6118.6888, MSE-Future-Loss 8037.0115, KL-Loss: 32.7028,  Kmeans-Loss: 11.9722, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1046.8876, MSE-Loss: 1007.1603, KL-Loss: 30.5549, Kmeans-Loss: 9.1723\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 16\n",
      "Train: \n",
      "Epoch: 16.  loss: 14019.7490\n",
      "Average Train loss: 13746.5966, MSE-Loss: 5916.1652, MSE-Future-Loss 7784.4911, KL-Loss: 33.7321,  Kmeans-Loss: 12.2083, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 974.9549, MSE-Loss: 933.4420, KL-Loss: 32.0979, Kmeans-Loss: 9.4151\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 17\n",
      "Train: \n",
      "Epoch: 17.  loss: 11697.7646\n",
      "Average Train loss: 13417.3043, MSE-Loss: 5796.3413, MSE-Future-Loss 7573.6531, KL-Loss: 34.8555,  Kmeans-Loss: 12.4544, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1067.3965, MSE-Loss: 1024.8934, KL-Loss: 32.7825, Kmeans-Loss: 9.7206\n",
      "lr: 0.0005\n",
      "Epoch: 18\n",
      "Train: \n",
      "Epoch: 18.  loss: 14116.7979\n",
      "Average Train loss: 12972.3965, MSE-Loss: 5665.3225, MSE-Future-Loss 7258.4261, KL-Loss: 35.9517,  Kmeans-Loss: 12.6962, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 1008.3651, MSE-Loss: 965.2630, KL-Loss: 33.3074, Kmeans-Loss: 9.7947\n",
      "lr: 0.0005\n",
      "Epoch: 19\n",
      "Train: \n",
      "Epoch: 19.  loss: 12412.3896\n",
      "Average Train loss: 12896.8186, MSE-Loss: 5677.8010, MSE-Future-Loss 7169.1866, KL-Loss: 36.9229,  Kmeans-Loss: 12.9081, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 968.5984, MSE-Loss: 924.4562, KL-Loss: 34.1211, Kmeans-Loss: 10.0210\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 20\n",
      "Train: \n",
      "Epoch: 20.  loss: 11926.0781\n",
      "Average Train loss: 12184.8938, MSE-Loss: 5306.9011, MSE-Future-Loss 6827.4195, KL-Loss: 37.4967,  Kmeans-Loss: 13.0765, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 962.1727, MSE-Loss: 917.6512, KL-Loss: 34.3949, Kmeans-Loss: 10.1267\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 21\n",
      "Train: \n",
      "Epoch: 21.  loss: 12751.4443\n",
      "Average Train loss: 12117.8791, MSE-Loss: 5390.0256, MSE-Future-Loss 6676.0630, KL-Loss: 38.4864,  Kmeans-Loss: 13.3041, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 988.9899, MSE-Loss: 942.5087, KL-Loss: 36.0156, Kmeans-Loss: 10.4656\n",
      "lr: 0.0005\n",
      "Epoch: 22\n",
      "Train: \n",
      "Epoch: 22.  loss: 12418.8438\n",
      "Average Train loss: 11574.0749, MSE-Loss: 5163.6712, MSE-Future-Loss 6357.7007, KL-Loss: 39.2076,  Kmeans-Loss: 13.4952, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 963.9521, MSE-Loss: 917.2317, KL-Loss: 36.2127, Kmeans-Loss: 10.5077\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 23\n",
      "Train: \n",
      "Epoch: 23.  loss: 11545.7549\n",
      "Average Train loss: 11142.2174, MSE-Loss: 4936.5350, MSE-Future-Loss 6152.0713, KL-Loss: 39.9508,  Kmeans-Loss: 13.6603, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 949.4286, MSE-Loss: 901.2278, KL-Loss: 37.4759, Kmeans-Loss: 10.7249\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 24\n",
      "Train: \n",
      "Epoch: 24.  loss: 12794.0771\n",
      "Average Train loss: 10997.4750, MSE-Loss: 5009.8926, MSE-Future-Loss 5932.9862, KL-Loss: 40.7298,  Kmeans-Loss: 13.8665, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 982.7843, MSE-Loss: 934.6428, KL-Loss: 37.3287, Kmeans-Loss: 10.8127\n",
      "lr: 0.0005\n",
      "Epoch: 25\n",
      "Train: \n",
      "Epoch: 25.  loss: 11435.6406\n",
      "Average Train loss: 10658.3668, MSE-Loss: 4795.7101, MSE-Future-Loss 5807.0882, KL-Loss: 41.5268,  Kmeans-Loss: 14.0417, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 928.6153, MSE-Loss: 878.9161, KL-Loss: 38.6100, Kmeans-Loss: 11.0892\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 26\n",
      "Train: \n",
      "Epoch: 26.  loss: 9779.8750\n",
      "Average Train loss: 10465.0655, MSE-Loss: 4771.6887, MSE-Future-Loss 5636.6761, KL-Loss: 42.4466,  Kmeans-Loss: 14.2540, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 902.3457, MSE-Loss: 852.1127, KL-Loss: 39.0534, Kmeans-Loss: 11.1796\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 27\n",
      "Train: \n",
      "Epoch: 27.  loss: 10005.8555\n",
      "Average Train loss: 10049.0941, MSE-Loss: 4603.1162, MSE-Future-Loss 5388.3391, KL-Loss: 43.2295,  Kmeans-Loss: 14.4095, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 868.8560, MSE-Loss: 817.2103, KL-Loss: 40.2759, Kmeans-Loss: 11.3698\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 28\n",
      "Train: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28.  loss: 10505.3203\n",
      "Average Train loss: 9829.5180, MSE-Loss: 4586.8628, MSE-Future-Loss 5184.1989, KL-Loss: 43.8851,  Kmeans-Loss: 14.5711, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 859.4783, MSE-Loss: 806.5592, KL-Loss: 41.3599, Kmeans-Loss: 11.5592\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 29\n",
      "Train: \n",
      "Epoch: 29.  loss: 9527.3965\n",
      "Average Train loss: 9559.4375, MSE-Loss: 4443.3920, MSE-Future-Loss 5056.6622, KL-Loss: 44.6292,  Kmeans-Loss: 14.7541, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 880.8533, MSE-Loss: 827.9871, KL-Loss: 41.1356, Kmeans-Loss: 11.7305\n",
      "lr: 0.0005\n",
      "Epoch: 30\n",
      "Train: \n",
      "Epoch: 30.  loss: 9422.9131\n",
      "Average Train loss: 9364.3051, MSE-Loss: 4464.6555, MSE-Future-Loss 4839.5168, KL-Loss: 45.2313,  Kmeans-Loss: 14.9015, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 862.3351, MSE-Loss: 807.8751, KL-Loss: 42.5376, Kmeans-Loss: 11.9224\n",
      "lr: 0.0005\n",
      "Epoch: 31\n",
      "Train: \n",
      "Epoch: 31.  loss: 8957.0479\n",
      "Average Train loss: 9139.8763, MSE-Loss: 4383.6733, MSE-Future-Loss 4695.1151, KL-Loss: 46.0132,  Kmeans-Loss: 15.0746, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 886.7639, MSE-Loss: 831.6873, KL-Loss: 42.9948, Kmeans-Loss: 12.0818\n",
      "lr: 0.0005\n",
      "Epoch: 32\n",
      "Train: \n",
      "Epoch: 32.  loss: 7618.7192\n",
      "Average Train loss: 9057.5233, MSE-Loss: 4367.7633, MSE-Future-Loss 4627.7785, KL-Loss: 46.7673,  Kmeans-Loss: 15.2142, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 841.7079, MSE-Loss: 786.5197, KL-Loss: 43.0677, Kmeans-Loss: 12.1204\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 33\n",
      "Train: \n",
      "Epoch: 33.  loss: 9435.6611\n",
      "Average Train loss: 8854.1650, MSE-Loss: 4303.9968, MSE-Future-Loss 4487.7110, KL-Loss: 47.0930,  Kmeans-Loss: 15.3643, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 845.3267, MSE-Loss: 788.9281, KL-Loss: 44.1429, Kmeans-Loss: 12.2557\n",
      "lr: 0.0005\n",
      "Epoch: 34\n",
      "Train: \n",
      "Epoch: 34.  loss: 9426.9160\n",
      "Average Train loss: 8575.9769, MSE-Loss: 4190.4001, MSE-Future-Loss 4322.4382, KL-Loss: 47.6536,  Kmeans-Loss: 15.4849, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 850.4151, MSE-Loss: 793.4804, KL-Loss: 44.5340, Kmeans-Loss: 12.4006\n",
      "lr: 0.0005\n",
      "Epoch: 35\n",
      "Train: \n",
      "Epoch: 35.  loss: 8289.1172\n",
      "Average Train loss: 8350.9582, MSE-Loss: 4117.6116, MSE-Future-Loss 4169.5888, KL-Loss: 48.1589,  Kmeans-Loss: 15.5989, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 843.6438, MSE-Loss: 785.7844, KL-Loss: 45.2930, Kmeans-Loss: 12.5665\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 36\n",
      "Train: \n",
      "Epoch: 36.  loss: 8930.8047\n",
      "Average Train loss: 8454.7737, MSE-Loss: 4186.5234, MSE-Future-Loss 4203.8486, KL-Loss: 48.6863,  Kmeans-Loss: 15.7153, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 904.4014, MSE-Loss: 846.7507, KL-Loss: 45.0536, Kmeans-Loss: 12.5971\n",
      "lr: 0.0005\n",
      "Epoch: 37\n",
      "Train: \n",
      "Epoch: 37.  loss: 8099.8672\n",
      "Average Train loss: 7954.3067, MSE-Loss: 4002.6974, MSE-Future-Loss 3886.5933, KL-Loss: 49.1743,  Kmeans-Loss: 15.8417, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 810.2345, MSE-Loss: 751.6253, KL-Loss: 45.9144, Kmeans-Loss: 12.6949\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 38\n",
      "Train: \n",
      "Epoch: 38.  loss: 7319.5112\n",
      "Average Train loss: 7962.8173, MSE-Loss: 3993.4866, MSE-Future-Loss 3903.5561, KL-Loss: 49.7961,  Kmeans-Loss: 15.9786, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 809.4315, MSE-Loss: 749.7173, KL-Loss: 46.8997, Kmeans-Loss: 12.8145\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 39\n",
      "Train: \n",
      "Epoch: 39.  loss: 8068.1597\n",
      "Average Train loss: 7963.9765, MSE-Loss: 4106.2162, MSE-Future-Loss 3791.6450, KL-Loss: 50.0457,  Kmeans-Loss: 16.0695, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 822.4066, MSE-Loss: 762.6800, KL-Loss: 46.7617, Kmeans-Loss: 12.9649\n",
      "lr: 0.0005\n",
      "Epoch: 40\n",
      "Train: \n",
      "Epoch: 40.  loss: 7170.3560\n",
      "Average Train loss: 7621.4961, MSE-Loss: 3879.1348, MSE-Future-Loss 3675.6825, KL-Loss: 50.4933,  Kmeans-Loss: 16.1856, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 818.6148, MSE-Loss: 758.4566, KL-Loss: 47.1612, Kmeans-Loss: 12.9970\n",
      "lr: 0.0005\n",
      "Epoch: 41\n",
      "Train: \n",
      "Epoch: 41.  loss: 7091.5005\n",
      "Average Train loss: 7481.4475, MSE-Loss: 3771.9712, MSE-Future-Loss 3642.3597, KL-Loss: 50.8509,  Kmeans-Loss: 16.2657, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 806.4205, MSE-Loss: 745.5397, KL-Loss: 47.7827, Kmeans-Loss: 13.0981\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 42\n",
      "Train: \n",
      "Epoch: 42.  loss: 7043.4663\n",
      "Average Train loss: 7483.8131, MSE-Loss: 3814.3993, MSE-Future-Loss 3601.6645, KL-Loss: 51.3614,  Kmeans-Loss: 16.3879, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 812.0231, MSE-Loss: 749.9153, KL-Loss: 48.8334, Kmeans-Loss: 13.2744\n",
      "lr: 0.0005\n",
      "Epoch: 43\n",
      "Train: \n",
      "Epoch: 43.  loss: 7265.8154\n",
      "Average Train loss: 7284.9098, MSE-Loss: 3709.9558, MSE-Future-Loss 3506.5257, KL-Loss: 51.9415,  Kmeans-Loss: 16.4868, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 789.8874, MSE-Loss: 728.1712, KL-Loss: 48.4263, Kmeans-Loss: 13.2899\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 44\n",
      "Train: \n",
      "Epoch: 44.  loss: 6804.2812\n",
      "Average Train loss: 7054.7182, MSE-Loss: 3637.4086, MSE-Future-Loss 3348.3725, KL-Loss: 52.3338,  Kmeans-Loss: 16.6031, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 777.5770, MSE-Loss: 715.7181, KL-Loss: 48.4905, Kmeans-Loss: 13.3684\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 45\n",
      "Train: \n",
      "Epoch: 45.  loss: 6329.7173\n",
      "Average Train loss: 6986.5212, MSE-Loss: 3602.4861, MSE-Future-Loss 3314.6431, KL-Loss: 52.7286,  Kmeans-Loss: 16.6634, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 788.0392, MSE-Loss: 725.2465, KL-Loss: 49.3177, Kmeans-Loss: 13.4751\n",
      "lr: 0.0005\n",
      "Epoch: 46\n",
      "Train: \n",
      "Epoch: 46.  loss: 7012.3066\n",
      "Average Train loss: 7039.2766, MSE-Loss: 3637.6843, MSE-Future-Loss 3331.5746, KL-Loss: 53.2314,  Kmeans-Loss: 16.7862, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 790.6991, MSE-Loss: 727.2073, KL-Loss: 49.8907, Kmeans-Loss: 13.6010\n",
      "lr: 0.0005\n",
      "Epoch: 47\n",
      "Train: \n",
      "Epoch: 47.  loss: 7804.6196\n",
      "Average Train loss: 6819.3993, MSE-Loss: 3569.8250, MSE-Future-Loss 3179.0007, KL-Loss: 53.6884,  Kmeans-Loss: 16.8853, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 785.0071, MSE-Loss: 720.9638, KL-Loss: 50.3232, Kmeans-Loss: 13.7201\n",
      "lr: 0.0005\n",
      "Epoch: 48\n",
      "Train: \n",
      "Epoch: 48.  loss: 6756.0674\n",
      "Average Train loss: 6715.9529, MSE-Loss: 3511.2429, MSE-Future-Loss 3133.5923, KL-Loss: 54.1284,  Kmeans-Loss: 16.9893, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 797.9636, MSE-Loss: 733.1909, KL-Loss: 50.9947, Kmeans-Loss: 13.7781\n",
      "lr: 0.0005\n",
      "Epoch: 49\n",
      "Train: \n",
      "Epoch: 49.  loss: 7016.2031\n",
      "Average Train loss: 6700.1335, MSE-Loss: 3539.1289, MSE-Future-Loss 3089.4955, KL-Loss: 54.4447,  Kmeans-Loss: 17.0643, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 786.9197, MSE-Loss: 721.5159, KL-Loss: 51.4731, Kmeans-Loss: 13.9307\n",
      "lr: 0.0005\n",
      "Epoch: 50\n",
      "Train: \n",
      "Epoch: 50.  loss: 6145.6748\n",
      "Average Train loss: 6546.5993, MSE-Loss: 3459.7914, MSE-Future-Loss 3014.8514, KL-Loss: 54.8333,  Kmeans-Loss: 17.1232, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 775.2814, MSE-Loss: 710.5966, KL-Loss: 50.7862, Kmeans-Loss: 13.8986\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Saving model snapshot!\n",
      "\n",
      "Epoch: 51\n",
      "Train: \n",
      "Epoch: 51.  loss: 5527.6416\n",
      "Average Train loss: 6563.2842, MSE-Loss: 3460.6016, MSE-Future-Loss 3030.2927, KL-Loss: 55.1826,  Kmeans-Loss: 17.2073, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 844.7193, MSE-Loss: 778.9280, KL-Loss: 51.7685, Kmeans-Loss: 14.0228\n",
      "lr: 0.0005\n",
      "Epoch: 52\n",
      "Train: \n",
      "Epoch: 52.  loss: 6842.1685\n",
      "Average Train loss: 6562.4050, MSE-Loss: 3451.9390, MSE-Future-Loss 3037.8150, KL-Loss: 55.3595,  Kmeans-Loss: 17.2915, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 758.1237, MSE-Loss: 692.6229, KL-Loss: 51.4702, Kmeans-Loss: 14.0306\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 53\n",
      "Train: \n",
      "Epoch: 53.  loss: 6178.3022\n",
      "Average Train loss: 6408.2548, MSE-Loss: 3366.1097, MSE-Future-Loss 2968.9868, KL-Loss: 55.7861,  Kmeans-Loss: 17.3721, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 757.6903, MSE-Loss: 691.4565, KL-Loss: 52.0665, Kmeans-Loss: 14.1673\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 54\n",
      "Train: \n",
      "Epoch: 54.  loss: 6124.3594\n",
      "Average Train loss: 6470.4134, MSE-Loss: 3411.1087, MSE-Future-Loss 2986.1229, KL-Loss: 55.7799,  Kmeans-Loss: 17.4019, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 768.2565, MSE-Loss: 701.9259, KL-Loss: 52.1652, Kmeans-Loss: 14.1655\n",
      "lr: 0.0005\n",
      "Epoch: 55\n",
      "Train: \n",
      "Epoch: 55.  loss: 6422.3354\n",
      "Average Train loss: 6255.3387, MSE-Loss: 3300.8193, MSE-Future-Loss 2880.9292, KL-Loss: 56.1110,  Kmeans-Loss: 17.4791, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 763.8349, MSE-Loss: 697.3275, KL-Loss: 52.3530, Kmeans-Loss: 14.1544\n",
      "lr: 0.0005\n",
      "Epoch: 56\n",
      "Train: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56.  loss: 6301.7822\n",
      "Average Train loss: 6293.9264, MSE-Loss: 3284.5660, MSE-Future-Loss 2935.1504, KL-Loss: 56.6254,  Kmeans-Loss: 17.5846, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 765.9828, MSE-Loss: 698.9287, KL-Loss: 52.7489, Kmeans-Loss: 14.3052\n",
      "lr: 0.0005\n",
      "Epoch: 57\n",
      "Train: \n",
      "Epoch: 57.  loss: 7056.9253\n",
      "Average Train loss: 6164.7044, MSE-Loss: 3267.7532, MSE-Future-Loss 2822.5681, KL-Loss: 56.7628,  Kmeans-Loss: 17.6202, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 759.4842, MSE-Loss: 691.7253, KL-Loss: 53.4300, Kmeans-Loss: 14.3289\n",
      "lr: 0.0005\n",
      "Epoch: 58\n",
      "Train: \n",
      "Epoch: 58.  loss: 6417.9922\n",
      "Average Train loss: 6042.5558, MSE-Loss: 3244.5510, MSE-Future-Loss 2722.9227, KL-Loss: 57.3516,  Kmeans-Loss: 17.7305, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 745.2313, MSE-Loss: 677.6790, KL-Loss: 53.1776, Kmeans-Loss: 14.3747\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 59\n",
      "Train: \n",
      "Epoch: 59.  loss: 6540.9961\n",
      "Average Train loss: 5982.1958, MSE-Loss: 3187.6713, MSE-Future-Loss 2719.2707, KL-Loss: 57.4682,  Kmeans-Loss: 17.7856, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 778.9155, MSE-Loss: 710.8528, KL-Loss: 53.5365, Kmeans-Loss: 14.5262\n",
      "lr: 0.0005\n",
      "Epoch: 60\n",
      "Train: \n",
      "Epoch: 60.  loss: 5851.5503\n",
      "Average Train loss: 5870.9076, MSE-Loss: 3150.4406, MSE-Future-Loss 2644.7314, KL-Loss: 57.8995,  Kmeans-Loss: 17.8362, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 736.3222, MSE-Loss: 668.2129, KL-Loss: 53.6553, Kmeans-Loss: 14.4540\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 61\n",
      "Train: \n",
      "Epoch: 61.  loss: 5807.1074\n",
      "Average Train loss: 5816.4628, MSE-Loss: 3139.6111, MSE-Future-Loss 2600.6523, KL-Loss: 58.2818,  Kmeans-Loss: 17.9177, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 747.4727, MSE-Loss: 678.6824, KL-Loss: 54.1570, Kmeans-Loss: 14.6332\n",
      "lr: 0.0005\n",
      "Epoch: 62\n",
      "Train: \n",
      "Epoch: 62.  loss: 5156.0493\n",
      "Average Train loss: 5776.6890, MSE-Loss: 3107.8276, MSE-Future-Loss 2592.2980, KL-Loss: 58.5865,  Kmeans-Loss: 17.9769, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 747.6705, MSE-Loss: 678.3273, KL-Loss: 54.6487, Kmeans-Loss: 14.6946\n",
      "lr: 0.0005\n",
      "Epoch: 63\n",
      "Train: \n",
      "Epoch: 63.  loss: 5083.9971\n",
      "Average Train loss: 5691.9262, MSE-Loss: 3065.9645, MSE-Future-Loss 2548.9901, KL-Loss: 58.8990,  Kmeans-Loss: 18.0727, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 737.1768, MSE-Loss: 667.8255, KL-Loss: 54.6499, Kmeans-Loss: 14.7014\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 64\n",
      "Train: \n",
      "Epoch: 64.  loss: 5332.1665\n",
      "Average Train loss: 5804.4425, MSE-Loss: 3105.1796, MSE-Future-Loss 2622.0309, KL-Loss: 59.1084,  Kmeans-Loss: 18.1235, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 727.6529, MSE-Loss: 657.8507, KL-Loss: 55.0356, Kmeans-Loss: 14.7666\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 65\n",
      "Train: \n",
      "Epoch: 65.  loss: 5176.1963\n",
      "Average Train loss: 5742.4504, MSE-Loss: 3136.7086, MSE-Future-Loss 2528.2100, KL-Loss: 59.3663,  Kmeans-Loss: 18.1655, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 752.0770, MSE-Loss: 681.6907, KL-Loss: 55.5158, Kmeans-Loss: 14.8705\n",
      "lr: 0.0005\n",
      "Epoch: 66\n",
      "Train: \n",
      "Epoch: 66.  loss: 4671.6299\n",
      "Average Train loss: 5727.3108, MSE-Loss: 3122.0602, MSE-Future-Loss 2527.5500, KL-Loss: 59.4660,  Kmeans-Loss: 18.2347, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 744.8350, MSE-Loss: 674.1384, KL-Loss: 55.7968, Kmeans-Loss: 14.8998\n",
      "lr: 0.0005\n",
      "Epoch: 67\n",
      "Train: \n",
      "Epoch: 67.  loss: 5125.4487\n",
      "Average Train loss: 5571.6068, MSE-Loss: 3057.9851, MSE-Future-Loss 2435.3723, KL-Loss: 59.9201,  Kmeans-Loss: 18.3293, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 742.9546, MSE-Loss: 672.4525, KL-Loss: 55.6108, Kmeans-Loss: 14.8913\n",
      "lr: 0.0005\n",
      "Epoch: 68\n",
      "Train: \n",
      "Epoch: 68.  loss: 5630.3657\n",
      "Average Train loss: 5498.1552, MSE-Loss: 2990.3967, MSE-Future-Loss 2429.3048, KL-Loss: 60.0932,  Kmeans-Loss: 18.3604, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 756.2340, MSE-Loss: 684.7985, KL-Loss: 56.4316, Kmeans-Loss: 15.0039\n",
      "lr: 0.0005\n",
      "Epoch: 69\n",
      "Train: \n",
      "Epoch: 69.  loss: 5262.5996\n",
      "Average Train loss: 5593.3540, MSE-Loss: 3001.0205, MSE-Future-Loss 2513.7314, KL-Loss: 60.2283,  Kmeans-Loss: 18.3737, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 744.2587, MSE-Loss: 673.3835, KL-Loss: 55.8496, Kmeans-Loss: 15.0256\n",
      "lr: 0.0005\n",
      "Epoch: 70\n",
      "Train: \n",
      "Epoch: 70.  loss: 5688.8730\n",
      "Average Train loss: 5588.6585, MSE-Loss: 3038.8202, MSE-Future-Loss 2471.1100, KL-Loss: 60.3023,  Kmeans-Loss: 18.4260, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 772.5040, MSE-Loss: 700.7331, KL-Loss: 56.6737, Kmeans-Loss: 15.0972\n",
      "lr: 0.0005\n",
      "Epoch: 71\n",
      "Train: \n",
      "Epoch: 71.  loss: 5649.9111\n"
     ]
    }
   ],
   "source": [
    "vame.rnn_model(config, model_name='VAME', pretrained_weights=False, pretrained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
