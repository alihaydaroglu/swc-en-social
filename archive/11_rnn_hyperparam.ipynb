{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as n\n",
    "import numpy.lib.recfunctions as rfn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, split = 0.8, separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = int(data_len // chunk_size)\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = split*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        \n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    \n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, targets_train, targets_test\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_separate,\\\n",
    "                targets_train, targets_test, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, architecture):\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.D_in = self.architecture['D_in']\n",
    "        self.D_out = self.architecture['D_out']\n",
    "        self.hidden_dims = self.architecture['hidden_dims']\n",
    "        \n",
    "        self.layers = []\n",
    "        prev_dim = self.architecture['D_in']\n",
    "        for dim in self.architecture['hidden_dims']:\n",
    "            self.layers += [torch.nn.Linear(prev_dim, dim),\n",
    "                           torch.nn.ReLU()]\n",
    "            prev_dim = dim\n",
    "        self.layers += [torch.nn.Linear(prev_dim, self.D_out)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.trackers = {}\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.model.apply(init_weights)\n",
    "   \n",
    "    def start_trackers(self,track, reset_trackers=True):\n",
    "        if reset_trackers: self.trackers = {}\n",
    "        for t in track:\n",
    "            self.trackers[t] = []\n",
    "    \n",
    "    def track(self):\n",
    "        for variable in self.trackers.keys():\n",
    "            self.trackers[variable].append(copy.deepcopy(getattr(self, variable)))\n",
    "        \n",
    "    def learn(self,learning, training_set, test_set, reset_trackers=True, verbose=True):\n",
    "        \n",
    "        #self.init_weights()\n",
    "        \n",
    "        # set up variables\n",
    "        N_batch = learning['N_batch']\n",
    "        N_epochs = learning['N_epochs']\n",
    "        loss_fn = learning['loss_fn']\n",
    "        print_interval = learning['print_interval']\n",
    "        track = learning['track']\n",
    "        learning_rate = learning['learning_rate']\n",
    "        \n",
    "        self.learning = learning\n",
    "        \n",
    "        optimizer = learning['optimizer'](self.model.parameters(), learning_rate)\n",
    "        self.start_trackers(track, reset_trackers)\n",
    "        \n",
    "        # load data\n",
    "        x_train, y_train = training_set\n",
    "        x_test, y_test = test_set\n",
    "\n",
    "        N_training = len(y_train)\n",
    "        N_test = len(y_test)\n",
    "    \n",
    "        self.t = 0\n",
    "        tic = time.time()\n",
    "        end = False\n",
    "        for self.epoch_idx in range(N_epochs):\n",
    "            if verbose: print(\"### EPOCH {:2d} ###\".format(self.epoch_idx))\n",
    "                \n",
    "            # randomize batches\n",
    "            indices = n.random.choice(range(N_training), N_training, False)\n",
    "            num_batches = len(indices) // N_batch + 1\n",
    "\n",
    "            for self.batch_idx in range(num_batches):\n",
    "                # load batch\n",
    "                b_idx = self.batch_idx\n",
    "                x_train_batch = x_train[indices[b_idx*N_batch :(b_idx+1)*N_batch]]\n",
    "                y_train_batch = y_train[indices[b_idx*N_batch : (b_idx+1)*N_batch]]\n",
    "\n",
    "                \n",
    "                # predict, loss and learn\n",
    "                y_train_batch_pred = self.model(x_train_batch)\n",
    "                loss = loss_fn(y_train_batch_pred, y_train_batch)\n",
    "                self.train_loss = loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.t += 1\n",
    "\n",
    "                if self.batch_idx % print_interval == 0:\n",
    "                    \n",
    "#                     y_train_pred = self.model(x_train)\n",
    "#                     self.train_loss = loss_fn(y_train_pred, y_train).item()\n",
    "#                     pred_labels = n.argmax(y_train_pred.detach().numpy(),axis=1)\n",
    "#                     true_labels = y_train.detach().numpy()\n",
    "#                     correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.train_frac_correct = 0# n.mean(correct_preds)\n",
    "                    \n",
    "                    y_test_pred = self.model(x_test)\n",
    "                    self.test_loss = loss_fn(y_test_pred, y_test).item()\n",
    "                    pred_labels = n.argmax(y_test_pred.detach().numpy(),axis=1)\n",
    "                    true_labels = y_test.detach().numpy()\n",
    "                    correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.test_frac_correct = n.mean(correct_preds)\n",
    "\n",
    "                    toc = time.time()\n",
    "                    delta = toc - tic\n",
    "                    tic = toc\n",
    "                    print(\"Time: {:4.2f}, Batch {:3d}, Train Loss (for batch): {:4.2f}, Test Loss: {:4.2f}, Test Correct Frac: {:.3f}\".format(\\\n",
    "                                       delta, self.batch_idx, self.train_loss, self.test_loss, self.test_frac_correct))\n",
    "            \n",
    "                    self.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_net_results(net, x_test, y_test):\n",
    "    plt.plot(net.trackers['t'],net.trackers['train_loss'], label='train')\n",
    "    plt.plot(net.trackers['t'],net.trackers['test_loss'], label='test')\n",
    "    plt.title(\"Cross Entropy Loss through training\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,10)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(net.trackers['t'], net.trackers['test_frac_correct'])\n",
    "    plt.title(\"Fraction of correct labels on test set through training\")\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.show()\n",
    "    \n",
    "    max_perf_ind = n.argmax(net.trackers['test_frac_correct'])\n",
    "    min_loss_ind = n.argmin(net.trackers['test_loss'])\n",
    "    max_perf_model = net.trackers['model'][max_perf_ind]\n",
    "    \n",
    "    prediction_test = max_perf_model(x_test)\n",
    "    pred = n.argmax(prediction_test.detach().numpy(),axis=1)\n",
    "    true = y_test.detach().numpy()\n",
    "    confmat = confusion_matrix(true, pred, normalize='true')\n",
    "    f, ax = plt.subplots(figsize=(10,10))\n",
    "    m = ax.matshow(confmat, cmap='Blues', vmin=0,  vmax=1)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_xticks(list(range(len(categories))))\n",
    "    ax.set_xticklabels(categories, rotation=45)\n",
    "    ax.set_yticks(list(range(len(categories))))\n",
    "    ax.set_yticklabels(categories, rotation=45)\n",
    "    f.colorbar(m)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,output_dim, n_layers_LSTM, dropout_prob):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers_LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, \n",
    "                            num_layers  = n_layers_LSTM, dropout = dropout_prob,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.out_func = nn.Softmax(dim=1)\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "#         x = x.long()\n",
    "#         print(x.shape)\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(x)\n",
    "        else: \n",
    "            lstm_out, hidden = self.lstm(x,hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(batch_size, self.output_dim, -1)[:,:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(datas, n_window, n_stride, test_vids=[3,4],batch_size=32, shuffle=True):\n",
    "    datapoints = []\n",
    "    labels = []\n",
    "\n",
    "    trial_bounds = []\n",
    "\n",
    "    trial_idx = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for data in datas:  \n",
    "        trial_bounds.append(idx)\n",
    "        for i in range(int(data.shape[1]/n_stride)):\n",
    "            if (i*n_stride + n_window) < data.shape[1]:\n",
    "                datapoints.append(data[:,i*n_stride : i*n_stride + n_window])\n",
    "                new_labels = [all_data[trial_keys[trial_idx]]['ann'][i*n_stride + int(n_window/2)]]\n",
    "                new_labels = [target_map[l] for l in new_labels]\n",
    "                labels.append(new_labels)\n",
    "                idx += 1\n",
    "        trial_idx += 1\n",
    "    trial_bounds.append(idx)  \n",
    "    \n",
    "    Xs = n.stack(datapoints)\n",
    "    Ys = n.stack(labels)\n",
    "    num_features = Xs.shape[1]\n",
    "    \n",
    "    Xs_train = []; Ys_train = []; Xs_test = []; Ys_test = []\n",
    "    for i in range(len(trial_bounds)-1):\n",
    "        if i in test_vids:\n",
    "            Xs_test.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_test.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "        else:\n",
    "            Xs_train.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_train.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "#     print(Xs_train[0].shape)\n",
    "    Xs_train = n.concatenate(Xs_train,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_train = n.concatenate(Ys_train,axis=0).reshape(-1)\n",
    "    Xs_test = n.concatenate(Xs_test,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_test = n.concatenate(Ys_test,axis=0).reshape(-1)\n",
    "    \n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_train), torch.from_numpy(Ys_train))\n",
    "    test_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_test), torch.from_numpy(Ys_test))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    \n",
    "    return train_data, train_loader, test_data, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(params, datas, device, random_val):\n",
    "    train_data, train_loader, val_data, val_loader \\\n",
    "            = extract_data(datas, params['n_windows'], params['n_stride'],\n",
    "                        batch_size=params['batch_size'],shuffle=params['shuffle'])\n",
    "        \n",
    "    model = LSTM_net(params['input_dim'], params['hidden_dim'], params['output_dim'], params['n_layers_LSTM'], params['dropout_prob'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    params['val_losses'] = []\n",
    "    params['val_fracs'] = []\n",
    "    \n",
    "    counter = 0 \n",
    "    tic = time.time()\n",
    "    for i in range(params['N_epochs']):\n",
    "        h = model.init_hidden(params['batch_size'])\n",
    "        for inputs, labels in train_loader:\n",
    "            if inputs.shape[0] != params['batch_size']: continue\n",
    "            counter += 1\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs.float(), (h[0].float(), h[1].float()))\n",
    "            loss = criterion(output.squeeze(), labels[:].long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), params['clip'])\n",
    "            optimizer.step()\n",
    "                        \n",
    "        toc = time.time() - tic\n",
    "        tic = time.time()\n",
    "        \n",
    "        if counter%params['print_every'] == 0:\n",
    "            val_h = model.init_hidden(params['batch_size'])\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            fracs = []\n",
    "            for inp, lab in val_loader:\n",
    "                if inp.shape[0] != params['batch_size']: continue\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), [val_h[0].float(), val_h[1].float()])\n",
    "                val_loss = criterion(out.squeeze(), lab[:].long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred_labels = n.argmax(out.cpu().detach().numpy(),axis=1)\n",
    "                true_labels = lab.cpu().detach().numpy()\n",
    "                correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                fracs.append(n.mean(correct_preds))\n",
    "                \n",
    "            frac_correct_val = (n.mean(fracs))\n",
    "            params['val_losses'].append(n.mean(val_losses))\n",
    "            params['val_fracs'].append(frac_correct_val)\n",
    "            model.train()\n",
    "            print(\"\\tEpoch: {}/{}; {:.2f} sec...\".format(i+1, params['N_epochs'],toc),\n",
    "                  \"\\tLoss: {:.3f}...\".format(loss.item()),\n",
    "                  \"\\tVal Loss: {:.3f}\".format(n.mean(val_losses)),\n",
    "                  \"\\tVal Frac: {:.4f}\".format(frac_correct_val))\n",
    "            if n.mean(val_losses) <= params['valid_loss_min']:\n",
    "                torch.save(model.state_dict(), './state_dict' + str(random_val) + '.pt')\n",
    "                print('\\tValidation loss decreased ({:.3f} --> {:.3f}).  Saving model ...'.format(params['valid_loss_min'],n.mean(val_losses)))\n",
    "                params['valid_loss_min'] = n.mean(val_losses)\n",
    "            if frac_correct_val >= params['valid_frac_max']:\n",
    "                params['valid_frac_max'] = frac_correct_val\n",
    "                params['pred_labels_best'] = pred_labels.copy()\n",
    "                params['true_labels_best'] = true_labels.copy()\n",
    "\n",
    "                \n",
    "    return params, model\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n",
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = list(all_data.keys())\n",
    "datas = []\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)\n",
    "input_dim = datas[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_original = {\n",
    "    'n_windows'      : 50,\n",
    "    'n_stride'       : 20,\n",
    "    'shuffle'        : True,\n",
    "    'dropout_prob'   : 0.3,\n",
    "    'batch_size'     : 32,\n",
    "    'hidden_dim'     : 50,\n",
    "    'n_layers_LSTM'  : 1, \n",
    "    'output_dim'     : N_categories,\n",
    "    'input_dim'      : input_dim,\n",
    "    'lr'             : 0.005,\n",
    "    'N_epochs'       : 1000,\n",
    "    'print_every'    : 5,\n",
    "    'clip'           : 5,\n",
    "    'valid_loss_min' : n.Inf,\n",
    "    'valid_frac_max' : 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweeps = {\n",
    "#     'n_stride'        : [5,50],\n",
    "#     'dropout_prob'    : [0, 0.1, 0.5],\n",
    "#     'batch_size'      : [8, 64, 128],\n",
    "    'hidden_dim'      : [10, 20, 100],\n",
    "    'n_layers_LSTM'   : [2, 3, 5],\n",
    "    'lr'              : [0.001, 0.01],\n",
    "    'n_windows'       : [1, 20, 50,100, 250],\n",
    "}\n",
    "# add shuffle\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###########\n",
      "Testing with  hidden_dim :  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neuropixel\\.conda\\envs\\en\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 5/1000; 1.50 sec... \tStep: 1215... \tLoss: 1.600... \tVal Loss: 1.037 \tVal Frac: 0.7426\n",
      "\tValidation loss decreased (inf --> 1.037).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.39 sec... \tStep: 2430... \tLoss: 1.135... \tVal Loss: 0.987 \tVal Frac: 0.7268\n",
      "\tValidation loss decreased (1.037 --> 0.987).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.52 sec... \tStep: 3645... \tLoss: 0.879... \tVal Loss: 0.819 \tVal Frac: 0.7515\n",
      "\tValidation loss decreased (0.987 --> 0.819).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.40 sec... \tStep: 4860... \tLoss: 1.326... \tVal Loss: 1.235 \tVal Frac: 0.7096\n",
      "\tEpoch: 25/1000; 1.54 sec... \tStep: 6075... \tLoss: 1.077... \tVal Loss: 0.841 \tVal Frac: 0.7386\n",
      "\tEpoch: 30/1000; 1.47 sec... \tStep: 7290... \tLoss: 1.358... \tVal Loss: 1.212 \tVal Frac: 0.7228\n",
      "\tEpoch: 35/1000; 1.70 sec... \tStep: 8505... \tLoss: 1.428... \tVal Loss: 0.904 \tVal Frac: 0.7121\n",
      "\tEpoch: 40/1000; 1.54 sec... \tStep: 9720... \tLoss: 0.861... \tVal Loss: 0.770 \tVal Frac: 0.7390\n",
      "\tValidation loss decreased (0.819 --> 0.770).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.50 sec... \tStep: 10935... \tLoss: 1.156... \tVal Loss: 1.025 \tVal Frac: 0.7092\n",
      "\tEpoch: 50/1000; 1.56 sec... \tStep: 12150... \tLoss: 0.890... \tVal Loss: 1.077 \tVal Frac: 0.7294\n",
      "\tEpoch: 55/1000; 1.50 sec... \tStep: 13365... \tLoss: 1.332... \tVal Loss: 0.924 \tVal Frac: 0.7004\n",
      "\tEpoch: 60/1000; 1.49 sec... \tStep: 14580... \tLoss: 1.480... \tVal Loss: 0.949 \tVal Frac: 0.7371\n",
      "\tEpoch: 65/1000; 1.55 sec... \tStep: 15795... \tLoss: 0.866... \tVal Loss: 0.809 \tVal Frac: 0.7761\n",
      "\tEpoch: 70/1000; 1.82 sec... \tStep: 17010... \tLoss: 0.756... \tVal Loss: 1.170 \tVal Frac: 0.4243\n",
      "\tEpoch: 75/1000; 1.86 sec... \tStep: 18225... \tLoss: 1.573... \tVal Loss: 1.481 \tVal Frac: 0.3426\n",
      "\tEpoch: 80/1000; 1.62 sec... \tStep: 19440... \tLoss: 0.916... \tVal Loss: 0.830 \tVal Frac: 0.7243\n",
      "\tEpoch: 85/1000; 1.57 sec... \tStep: 20655... \tLoss: 1.255... \tVal Loss: 0.974 \tVal Frac: 0.6757\n",
      "\tEpoch: 90/1000; 1.37 sec... \tStep: 21870... \tLoss: 0.827... \tVal Loss: 0.782 \tVal Frac: 0.7901\n",
      "\tEpoch: 95/1000; 1.50 sec... \tStep: 23085... \tLoss: 0.887... \tVal Loss: 0.744 \tVal Frac: 0.7816\n",
      "\tValidation loss decreased (0.770 --> 0.744).  Saving model ...\n",
      "\tEpoch: 100/1000; 1.62 sec... \tStep: 24300... \tLoss: 1.163... \tVal Loss: 0.977 \tVal Frac: 0.7290\n",
      "\tEpoch: 105/1000; 1.57 sec... \tStep: 25515... \tLoss: 0.857... \tVal Loss: 0.777 \tVal Frac: 0.7581\n",
      "\tEpoch: 110/1000; 1.64 sec... \tStep: 26730... \tLoss: 0.835... \tVal Loss: 0.920 \tVal Frac: 0.7287\n",
      "\tEpoch: 115/1000; 1.48 sec... \tStep: 27945... \tLoss: 1.324... \tVal Loss: 1.027 \tVal Frac: 0.7026\n",
      "\tEpoch: 120/1000; 1.53 sec... \tStep: 29160... \tLoss: 1.085... \tVal Loss: 0.847 \tVal Frac: 0.7401\n",
      "\tEpoch: 125/1000; 1.48 sec... \tStep: 30375... \tLoss: 0.910... \tVal Loss: 0.734 \tVal Frac: 0.7757\n",
      "\tValidation loss decreased (0.744 --> 0.734).  Saving model ...\n",
      "\tEpoch: 130/1000; 1.49 sec... \tStep: 31590... \tLoss: 0.993... \tVal Loss: 0.732 \tVal Frac: 0.7551\n",
      "\tValidation loss decreased (0.734 --> 0.732).  Saving model ...\n",
      "\tEpoch: 135/1000; 1.62 sec... \tStep: 32805... \tLoss: 0.962... \tVal Loss: 0.847 \tVal Frac: 0.7375\n",
      "\tEpoch: 140/1000; 1.63 sec... \tStep: 34020... \tLoss: 0.956... \tVal Loss: 0.759 \tVal Frac: 0.7540\n",
      "\tEpoch: 145/1000; 1.50 sec... \tStep: 35235... \tLoss: 0.811... \tVal Loss: 0.821 \tVal Frac: 0.7522\n",
      "\tEpoch: 150/1000; 1.43 sec... \tStep: 36450... \tLoss: 0.745... \tVal Loss: 0.787 \tVal Frac: 0.7471\n",
      "\tEpoch: 155/1000; 1.30 sec... \tStep: 37665... \tLoss: 1.163... \tVal Loss: 0.918 \tVal Frac: 0.7445\n",
      "\tEpoch: 160/1000; 1.63 sec... \tStep: 38880... \tLoss: 1.159... \tVal Loss: 0.716 \tVal Frac: 0.7772\n",
      "\tValidation loss decreased (0.732 --> 0.716).  Saving model ...\n",
      "\tEpoch: 165/1000; 1.56 sec... \tStep: 40095... \tLoss: 0.565... \tVal Loss: 0.890 \tVal Frac: 0.7118\n",
      "\tEpoch: 170/1000; 1.52 sec... \tStep: 41310... \tLoss: 0.835... \tVal Loss: 0.789 \tVal Frac: 0.7544\n",
      "\tEpoch: 175/1000; 1.50 sec... \tStep: 42525... \tLoss: 1.082... \tVal Loss: 0.843 \tVal Frac: 0.7375\n",
      "\tEpoch: 180/1000; 1.52 sec... \tStep: 43740... \tLoss: 0.948... \tVal Loss: 0.738 \tVal Frac: 0.7820\n",
      "\tEpoch: 185/1000; 1.44 sec... \tStep: 44955... \tLoss: 0.779... \tVal Loss: 0.741 \tVal Frac: 0.7717\n",
      "\tEpoch: 190/1000; 1.37 sec... \tStep: 46170... \tLoss: 0.925... \tVal Loss: 0.835 \tVal Frac: 0.7555\n",
      "\tEpoch: 195/1000; 1.64 sec... \tStep: 47385... \tLoss: 1.341... \tVal Loss: 1.219 \tVal Frac: 0.4882\n",
      "\tEpoch: 200/1000; 1.74 sec... \tStep: 48600... \tLoss: 0.742... \tVal Loss: 0.741 \tVal Frac: 0.7522\n",
      "\tEpoch: 205/1000; 1.50 sec... \tStep: 49815... \tLoss: 0.882... \tVal Loss: 0.720 \tVal Frac: 0.7838\n",
      "\tEpoch: 210/1000; 1.57 sec... \tStep: 51030... \tLoss: 1.076... \tVal Loss: 0.732 \tVal Frac: 0.7618\n",
      "\tEpoch: 215/1000; 1.60 sec... \tStep: 52245... \tLoss: 1.007... \tVal Loss: 0.746 \tVal Frac: 0.7419\n",
      "\tEpoch: 220/1000; 1.44 sec... \tStep: 53460... \tLoss: 0.738... \tVal Loss: 0.622 \tVal Frac: 0.8018\n",
      "\tValidation loss decreased (0.716 --> 0.622).  Saving model ...\n",
      "\tEpoch: 225/1000; 1.57 sec... \tStep: 54675... \tLoss: 0.893... \tVal Loss: 0.840 \tVal Frac: 0.7482\n",
      "\tEpoch: 230/1000; 1.60 sec... \tStep: 55890... \tLoss: 1.376... \tVal Loss: 0.844 \tVal Frac: 0.7544\n",
      "\tEpoch: 235/1000; 1.60 sec... \tStep: 57105... \tLoss: 1.150... \tVal Loss: 0.836 \tVal Frac: 0.7812\n",
      "\tEpoch: 240/1000; 1.47 sec... \tStep: 58320... \tLoss: 1.168... \tVal Loss: 0.847 \tVal Frac: 0.7835\n",
      "\tEpoch: 245/1000; 1.70 sec... \tStep: 59535... \tLoss: 1.016... \tVal Loss: 0.714 \tVal Frac: 0.7886\n",
      "\tEpoch: 250/1000; 1.49 sec... \tStep: 60750... \tLoss: 1.411... \tVal Loss: 0.743 \tVal Frac: 0.7864\n",
      "\tEpoch: 255/1000; 1.47 sec... \tStep: 61965... \tLoss: 0.957... \tVal Loss: 0.865 \tVal Frac: 0.7434\n",
      "\tEpoch: 260/1000; 1.56 sec... \tStep: 63180... \tLoss: 1.431... \tVal Loss: 0.700 \tVal Frac: 0.7842\n",
      "\tEpoch: 265/1000; 1.37 sec... \tStep: 64395... \tLoss: 0.702... \tVal Loss: 0.716 \tVal Frac: 0.7805\n",
      "\tEpoch: 270/1000; 1.52 sec... \tStep: 65610... \tLoss: 0.901... \tVal Loss: 0.744 \tVal Frac: 0.7879\n",
      "\tEpoch: 275/1000; 1.39 sec... \tStep: 66825... \tLoss: 0.999... \tVal Loss: 0.710 \tVal Frac: 0.7904\n",
      "\tEpoch: 280/1000; 1.45 sec... \tStep: 68040... \tLoss: 1.096... \tVal Loss: 0.816 \tVal Frac: 0.7489\n",
      "\tEpoch: 285/1000; 1.43 sec... \tStep: 69255... \tLoss: 0.582... \tVal Loss: 0.710 \tVal Frac: 0.7871\n",
      "\tEpoch: 290/1000; 1.60 sec... \tStep: 70470... \tLoss: 0.955... \tVal Loss: 0.822 \tVal Frac: 0.7647\n",
      "\tEpoch: 295/1000; 1.49 sec... \tStep: 71685... \tLoss: 0.708... \tVal Loss: 0.846 \tVal Frac: 0.7449\n",
      "\tEpoch: 300/1000; 1.45 sec... \tStep: 72900... \tLoss: 1.102... \tVal Loss: 0.727 \tVal Frac: 0.7787\n",
      "\tEpoch: 305/1000; 1.67 sec... \tStep: 74115... \tLoss: 1.006... \tVal Loss: 0.758 \tVal Frac: 0.7835\n",
      "\tEpoch: 310/1000; 1.53 sec... \tStep: 75330... \tLoss: 0.998... \tVal Loss: 0.715 \tVal Frac: 0.7463\n",
      "\tEpoch: 315/1000; 1.45 sec... \tStep: 76545... \tLoss: 1.036... \tVal Loss: 0.682 \tVal Frac: 0.7743\n",
      "\tEpoch: 320/1000; 1.59 sec... \tStep: 77760... \tLoss: 0.794... \tVal Loss: 0.716 \tVal Frac: 0.7629\n",
      "\tEpoch: 325/1000; 1.57 sec... \tStep: 78975... \tLoss: 1.044... \tVal Loss: 0.797 \tVal Frac: 0.7489\n",
      "\tEpoch: 330/1000; 1.63 sec... \tStep: 80190... \tLoss: 0.738... \tVal Loss: 0.744 \tVal Frac: 0.7526\n",
      "\tEpoch: 335/1000; 1.53 sec... \tStep: 81405... \tLoss: 0.866... \tVal Loss: 0.690 \tVal Frac: 0.7721\n",
      "\tEpoch: 340/1000; 1.65 sec... \tStep: 82620... \tLoss: 1.327... \tVal Loss: 0.839 \tVal Frac: 0.7735\n",
      "\tEpoch: 345/1000; 1.42 sec... \tStep: 83835... \tLoss: 1.041... \tVal Loss: 0.727 \tVal Frac: 0.7724\n",
      "\tEpoch: 350/1000; 1.57 sec... \tStep: 85050... \tLoss: 1.111... \tVal Loss: 0.790 \tVal Frac: 0.7173\n",
      "\tEpoch: 355/1000; 1.51 sec... \tStep: 86265... \tLoss: 0.788... \tVal Loss: 0.787 \tVal Frac: 0.7434\n",
      "\tEpoch: 360/1000; 1.53 sec... \tStep: 87480... \tLoss: 0.858... \tVal Loss: 0.848 \tVal Frac: 0.7482\n",
      "\tEpoch: 365/1000; 1.63 sec... \tStep: 88695... \tLoss: 1.126... \tVal Loss: 0.918 \tVal Frac: 0.7496\n",
      "\tEpoch: 370/1000; 1.66 sec... \tStep: 89910... \tLoss: 0.972... \tVal Loss: 0.857 \tVal Frac: 0.7485\n",
      "\tEpoch: 375/1000; 1.64 sec... \tStep: 91125... \tLoss: 0.832... \tVal Loss: 0.717 \tVal Frac: 0.7849\n",
      "\tEpoch: 380/1000; 1.50 sec... \tStep: 92340... \tLoss: 0.351... \tVal Loss: 0.735 \tVal Frac: 0.7625\n",
      "\tEpoch: 385/1000; 1.48 sec... \tStep: 93555... \tLoss: 1.011... \tVal Loss: 1.034 \tVal Frac: 0.7158\n",
      "\tEpoch: 390/1000; 1.56 sec... \tStep: 94770... \tLoss: 1.069... \tVal Loss: 0.739 \tVal Frac: 0.7629\n",
      "\tEpoch: 395/1000; 1.42 sec... \tStep: 95985... \tLoss: 1.303... \tVal Loss: 0.928 \tVal Frac: 0.7551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 400/1000; 1.50 sec... \tStep: 97200... \tLoss: 0.907... \tVal Loss: 0.718 \tVal Frac: 0.7669\n",
      "\tEpoch: 405/1000; 1.57 sec... \tStep: 98415... \tLoss: 0.749... \tVal Loss: 0.710 \tVal Frac: 0.7507\n",
      "\tEpoch: 410/1000; 1.42 sec... \tStep: 99630... \tLoss: 1.492... \tVal Loss: 0.747 \tVal Frac: 0.7456\n",
      "\tEpoch: 415/1000; 1.46 sec... \tStep: 100845... \tLoss: 0.908... \tVal Loss: 0.689 \tVal Frac: 0.7853\n",
      "\tEpoch: 420/1000; 1.53 sec... \tStep: 102060... \tLoss: 0.854... \tVal Loss: 0.718 \tVal Frac: 0.7665\n",
      "\tEpoch: 425/1000; 1.52 sec... \tStep: 103275... \tLoss: 0.666... \tVal Loss: 0.712 \tVal Frac: 0.7835\n",
      "\tEpoch: 430/1000; 1.50 sec... \tStep: 104490... \tLoss: 1.398... \tVal Loss: 0.789 \tVal Frac: 0.7471\n",
      "\tEpoch: 435/1000; 1.55 sec... \tStep: 105705... \tLoss: 1.179... \tVal Loss: 0.738 \tVal Frac: 0.7743\n",
      "\tEpoch: 440/1000; 1.60 sec... \tStep: 106920... \tLoss: 1.161... \tVal Loss: 0.688 \tVal Frac: 0.7871\n",
      "\tEpoch: 445/1000; 1.52 sec... \tStep: 108135... \tLoss: 0.967... \tVal Loss: 0.704 \tVal Frac: 0.7761\n",
      "\tEpoch: 450/1000; 1.55 sec... \tStep: 109350... \tLoss: 1.050... \tVal Loss: 0.761 \tVal Frac: 0.7658\n",
      "\tEpoch: 455/1000; 1.52 sec... \tStep: 110565... \tLoss: 0.601... \tVal Loss: 0.671 \tVal Frac: 0.7864\n",
      "\tEpoch: 460/1000; 1.59 sec... \tStep: 111780... \tLoss: 0.740... \tVal Loss: 0.711 \tVal Frac: 0.7945\n",
      "\tEpoch: 465/1000; 1.57 sec... \tStep: 112995... \tLoss: 0.910... \tVal Loss: 0.687 \tVal Frac: 0.7864\n",
      "\tEpoch: 470/1000; 1.37 sec... \tStep: 114210... \tLoss: 0.816... \tVal Loss: 0.685 \tVal Frac: 0.7930\n",
      "\tEpoch: 475/1000; 1.73 sec... \tStep: 115425... \tLoss: 0.890... \tVal Loss: 0.698 \tVal Frac: 0.7949\n",
      "\tEpoch: 480/1000; 1.57 sec... \tStep: 116640... \tLoss: 0.804... \tVal Loss: 0.740 \tVal Frac: 0.7816\n",
      "\tEpoch: 485/1000; 1.52 sec... \tStep: 117855... \tLoss: 1.643... \tVal Loss: 0.826 \tVal Frac: 0.7897\n",
      "\tEpoch: 490/1000; 1.52 sec... \tStep: 119070... \tLoss: 0.816... \tVal Loss: 0.794 \tVal Frac: 0.7401\n",
      "\tEpoch: 495/1000; 1.67 sec... \tStep: 120285... \tLoss: 0.620... \tVal Loss: 0.696 \tVal Frac: 0.7857\n",
      "\tEpoch: 500/1000; 1.52 sec... \tStep: 121500... \tLoss: 0.742... \tVal Loss: 0.735 \tVal Frac: 0.7890\n",
      "\tEpoch: 505/1000; 1.46 sec... \tStep: 122715... \tLoss: 0.996... \tVal Loss: 0.732 \tVal Frac: 0.7783\n",
      "\tEpoch: 510/1000; 1.43 sec... \tStep: 123930... \tLoss: 0.811... \tVal Loss: 0.771 \tVal Frac: 0.7290\n",
      "\tEpoch: 515/1000; 1.62 sec... \tStep: 125145... \tLoss: 1.059... \tVal Loss: 0.670 \tVal Frac: 0.7640\n",
      "\tEpoch: 520/1000; 1.55 sec... \tStep: 126360... \tLoss: 0.595... \tVal Loss: 0.684 \tVal Frac: 0.7643\n",
      "\tEpoch: 525/1000; 1.70 sec... \tStep: 127575... \tLoss: 0.987... \tVal Loss: 0.796 \tVal Frac: 0.7798\n",
      "\tEpoch: 530/1000; 1.42 sec... \tStep: 128790... \tLoss: 0.531... \tVal Loss: 0.642 \tVal Frac: 0.7989\n",
      "\tEpoch: 535/1000; 1.42 sec... \tStep: 130005... \tLoss: 0.924... \tVal Loss: 0.762 \tVal Frac: 0.7537\n",
      "\tEpoch: 540/1000; 1.49 sec... \tStep: 131220... \tLoss: 1.772... \tVal Loss: 0.961 \tVal Frac: 0.6945\n",
      "\tEpoch: 545/1000; 1.47 sec... \tStep: 132435... \tLoss: 0.995... \tVal Loss: 0.764 \tVal Frac: 0.7919\n",
      "\tEpoch: 550/1000; 1.53 sec... \tStep: 133650... \tLoss: 0.934... \tVal Loss: 0.685 \tVal Frac: 0.7926\n",
      "\tEpoch: 555/1000; 1.53 sec... \tStep: 134865... \tLoss: 0.798... \tVal Loss: 0.697 \tVal Frac: 0.7915\n",
      "\tEpoch: 560/1000; 1.52 sec... \tStep: 136080... \tLoss: 0.715... \tVal Loss: 0.688 \tVal Frac: 0.7886\n",
      "\tEpoch: 565/1000; 1.67 sec... \tStep: 137295... \tLoss: 1.088... \tVal Loss: 0.711 \tVal Frac: 0.7599\n",
      "\tEpoch: 570/1000; 1.53 sec... \tStep: 138510... \tLoss: 1.029... \tVal Loss: 0.690 \tVal Frac: 0.7842\n",
      "\tEpoch: 575/1000; 1.60 sec... \tStep: 139725... \tLoss: 0.790... \tVal Loss: 0.680 \tVal Frac: 0.7926\n",
      "\tEpoch: 580/1000; 1.43 sec... \tStep: 140940... \tLoss: 1.140... \tVal Loss: 0.764 \tVal Frac: 0.7408\n",
      "\tEpoch: 585/1000; 1.50 sec... \tStep: 142155... \tLoss: 1.172... \tVal Loss: 0.667 \tVal Frac: 0.7982\n",
      "\tEpoch: 590/1000; 1.62 sec... \tStep: 143370... \tLoss: 0.733... \tVal Loss: 0.714 \tVal Frac: 0.7577\n",
      "\tEpoch: 595/1000; 1.77 sec... \tStep: 144585... \tLoss: 0.934... \tVal Loss: 0.765 \tVal Frac: 0.7500\n",
      "\tEpoch: 600/1000; 1.42 sec... \tStep: 145800... \tLoss: 1.179... \tVal Loss: 0.895 \tVal Frac: 0.6765\n",
      "\tEpoch: 605/1000; 1.66 sec... \tStep: 147015... \tLoss: 1.123... \tVal Loss: 0.680 \tVal Frac: 0.7790\n",
      "\tEpoch: 610/1000; 1.85 sec... \tStep: 148230... \tLoss: 0.828... \tVal Loss: 0.718 \tVal Frac: 0.7485\n",
      "\tEpoch: 615/1000; 1.62 sec... \tStep: 149445... \tLoss: 0.981... \tVal Loss: 0.731 \tVal Frac: 0.7460\n",
      "\tEpoch: 620/1000; 1.50 sec... \tStep: 150660... \tLoss: 0.879... \tVal Loss: 0.725 \tVal Frac: 0.7746\n",
      "\tEpoch: 625/1000; 1.55 sec... \tStep: 151875... \tLoss: 0.704... \tVal Loss: 0.655 \tVal Frac: 0.7937\n",
      "\tEpoch: 630/1000; 1.43 sec... \tStep: 153090... \tLoss: 0.629... \tVal Loss: 0.673 \tVal Frac: 0.7890\n",
      "\tEpoch: 635/1000; 1.63 sec... \tStep: 154305... \tLoss: 0.749... \tVal Loss: 0.652 \tVal Frac: 0.7831\n",
      "\tEpoch: 640/1000; 1.60 sec... \tStep: 155520... \tLoss: 0.634... \tVal Loss: 0.706 \tVal Frac: 0.7842\n",
      "\tEpoch: 645/1000; 1.59 sec... \tStep: 156735... \tLoss: 0.975... \tVal Loss: 0.668 \tVal Frac: 0.7897\n",
      "\tEpoch: 650/1000; 1.54 sec... \tStep: 157950... \tLoss: 0.933... \tVal Loss: 0.783 \tVal Frac: 0.7596\n",
      "\tEpoch: 655/1000; 1.58 sec... \tStep: 159165... \tLoss: 1.050... \tVal Loss: 0.660 \tVal Frac: 0.7838\n",
      "\tEpoch: 660/1000; 1.47 sec... \tStep: 160380... \tLoss: 1.015... \tVal Loss: 0.675 \tVal Frac: 0.7489\n",
      "\tEpoch: 665/1000; 1.48 sec... \tStep: 161595... \tLoss: 0.984... \tVal Loss: 0.801 \tVal Frac: 0.7691\n",
      "\tEpoch: 670/1000; 1.45 sec... \tStep: 162810... \tLoss: 1.052... \tVal Loss: 0.890 \tVal Frac: 0.7415\n",
      "\tEpoch: 675/1000; 1.57 sec... \tStep: 164025... \tLoss: 1.223... \tVal Loss: 1.148 \tVal Frac: 0.4077\n",
      "\tEpoch: 680/1000; 1.53 sec... \tStep: 165240... \tLoss: 0.998... \tVal Loss: 0.786 \tVal Frac: 0.7474\n",
      "\tEpoch: 685/1000; 1.45 sec... \tStep: 166455... \tLoss: 0.796... \tVal Loss: 0.682 \tVal Frac: 0.7820\n",
      "\tEpoch: 690/1000; 1.64 sec... \tStep: 167670... \tLoss: 0.909... \tVal Loss: 0.929 \tVal Frac: 0.7500\n",
      "\tEpoch: 695/1000; 1.75 sec... \tStep: 168885... \tLoss: 1.014... \tVal Loss: 0.753 \tVal Frac: 0.7893\n",
      "\tEpoch: 700/1000; 1.86 sec... \tStep: 170100... \tLoss: 0.491... \tVal Loss: 0.822 \tVal Frac: 0.7673\n",
      "\tEpoch: 705/1000; 1.74 sec... \tStep: 171315... \tLoss: 1.078... \tVal Loss: 0.852 \tVal Frac: 0.7636\n",
      "\tEpoch: 710/1000; 1.53 sec... \tStep: 172530... \tLoss: 0.727... \tVal Loss: 0.728 \tVal Frac: 0.7732\n",
      "\tEpoch: 715/1000; 1.44 sec... \tStep: 173745... \tLoss: 1.145... \tVal Loss: 0.767 \tVal Frac: 0.7838\n",
      "\tEpoch: 720/1000; 1.52 sec... \tStep: 174960... \tLoss: 0.965... \tVal Loss: 0.896 \tVal Frac: 0.7254\n",
      "\tEpoch: 725/1000; 1.45 sec... \tStep: 176175... \tLoss: 1.078... \tVal Loss: 0.833 \tVal Frac: 0.7812\n",
      "\tEpoch: 730/1000; 1.49 sec... \tStep: 177390... \tLoss: 0.563... \tVal Loss: 0.757 \tVal Frac: 0.7868\n",
      "\tEpoch: 735/1000; 1.52 sec... \tStep: 178605... \tLoss: 1.047... \tVal Loss: 0.730 \tVal Frac: 0.7949\n",
      "\tEpoch: 740/1000; 1.65 sec... \tStep: 179820... \tLoss: 0.566... \tVal Loss: 0.723 \tVal Frac: 0.7923\n",
      "\tEpoch: 745/1000; 1.60 sec... \tStep: 181035... \tLoss: 0.848... \tVal Loss: 0.767 \tVal Frac: 0.7930\n",
      "\tEpoch: 750/1000; 1.57 sec... \tStep: 182250... \tLoss: 1.167... \tVal Loss: 0.767 \tVal Frac: 0.7618\n",
      "\tEpoch: 755/1000; 1.60 sec... \tStep: 183465... \tLoss: 1.319... \tVal Loss: 0.952 \tVal Frac: 0.7937\n",
      "\tEpoch: 760/1000; 1.40 sec... \tStep: 184680... \tLoss: 0.753... \tVal Loss: 0.660 \tVal Frac: 0.7941\n",
      "\tEpoch: 765/1000; 1.50 sec... \tStep: 185895... \tLoss: 0.557... \tVal Loss: 0.679 \tVal Frac: 0.7901\n",
      "\tEpoch: 770/1000; 1.70 sec... \tStep: 187110... \tLoss: 0.580... \tVal Loss: 0.646 \tVal Frac: 0.7963\n",
      "\tEpoch: 775/1000; 1.47 sec... \tStep: 188325... \tLoss: 0.808... \tVal Loss: 0.643 \tVal Frac: 0.7868\n",
      "\tEpoch: 780/1000; 1.45 sec... \tStep: 189540... \tLoss: 0.822... \tVal Loss: 0.653 \tVal Frac: 0.7923\n",
      "\tEpoch: 785/1000; 1.57 sec... \tStep: 190755... \tLoss: 0.821... \tVal Loss: 0.717 \tVal Frac: 0.7860\n",
      "\tEpoch: 790/1000; 1.61 sec... \tStep: 191970... \tLoss: 1.332... \tVal Loss: 0.772 \tVal Frac: 0.7728\n",
      "\tEpoch: 795/1000; 1.54 sec... \tStep: 193185... \tLoss: 0.636... \tVal Loss: 0.670 \tVal Frac: 0.7548\n",
      "\tEpoch: 800/1000; 1.57 sec... \tStep: 194400... \tLoss: 0.470... \tVal Loss: 0.621 \tVal Frac: 0.7960\n",
      "\tValidation loss decreased (0.622 --> 0.621).  Saving model ...\n",
      "\tEpoch: 805/1000; 1.64 sec... \tStep: 195615... \tLoss: 0.803... \tVal Loss: 0.676 \tVal Frac: 0.7915\n",
      "\tEpoch: 810/1000; 1.67 sec... \tStep: 196830... \tLoss: 0.706... \tVal Loss: 0.677 \tVal Frac: 0.7875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 815/1000; 1.62 sec... \tStep: 198045... \tLoss: 0.793... \tVal Loss: 0.670 \tVal Frac: 0.7820\n",
      "\tEpoch: 820/1000; 1.70 sec... \tStep: 199260... \tLoss: 0.701... \tVal Loss: 0.661 \tVal Frac: 0.7912\n",
      "\tEpoch: 825/1000; 1.45 sec... \tStep: 200475... \tLoss: 0.794... \tVal Loss: 0.646 \tVal Frac: 0.7912\n",
      "\tEpoch: 830/1000; 1.52 sec... \tStep: 201690... \tLoss: 0.533... \tVal Loss: 0.617 \tVal Frac: 0.8000\n",
      "\tValidation loss decreased (0.621 --> 0.617).  Saving model ...\n",
      "\tEpoch: 835/1000; 1.59 sec... \tStep: 202905... \tLoss: 0.958... \tVal Loss: 0.648 \tVal Frac: 0.7974\n",
      "\tEpoch: 840/1000; 1.47 sec... \tStep: 204120... \tLoss: 0.942... \tVal Loss: 0.697 \tVal Frac: 0.7831\n",
      "\tEpoch: 845/1000; 1.37 sec... \tStep: 205335... \tLoss: 0.849... \tVal Loss: 0.725 \tVal Frac: 0.7820\n",
      "\tEpoch: 850/1000; 1.69 sec... \tStep: 206550... \tLoss: 0.930... \tVal Loss: 0.642 \tVal Frac: 0.8004\n",
      "\tEpoch: 855/1000; 1.45 sec... \tStep: 207765... \tLoss: 0.921... \tVal Loss: 0.833 \tVal Frac: 0.7669\n",
      "\tEpoch: 860/1000; 1.43 sec... \tStep: 208980... \tLoss: 0.773... \tVal Loss: 0.732 \tVal Frac: 0.7779\n",
      "\tEpoch: 865/1000; 1.37 sec... \tStep: 210195... \tLoss: 0.625... \tVal Loss: 0.758 \tVal Frac: 0.7699\n",
      "\tEpoch: 870/1000; 1.43 sec... \tStep: 211410... \tLoss: 1.140... \tVal Loss: 0.701 \tVal Frac: 0.7765\n",
      "\tEpoch: 875/1000; 1.37 sec... \tStep: 212625... \tLoss: 0.791... \tVal Loss: 0.678 \tVal Frac: 0.7901\n",
      "\tEpoch: 880/1000; 1.37 sec... \tStep: 213840... \tLoss: 0.624... \tVal Loss: 0.620 \tVal Frac: 0.7963\n",
      "\tEpoch: 885/1000; 1.46 sec... \tStep: 215055... \tLoss: 0.557... \tVal Loss: 0.682 \tVal Frac: 0.7901\n",
      "\tEpoch: 890/1000; 1.40 sec... \tStep: 216270... \tLoss: 0.994... \tVal Loss: 0.713 \tVal Frac: 0.7919\n",
      "\tEpoch: 895/1000; 1.27 sec... \tStep: 217485... \tLoss: 1.261... \tVal Loss: 0.818 \tVal Frac: 0.7327\n",
      "\tEpoch: 900/1000; 1.39 sec... \tStep: 218700... \tLoss: 0.727... \tVal Loss: 0.708 \tVal Frac: 0.7518\n",
      "\tEpoch: 905/1000; 1.29 sec... \tStep: 219915... \tLoss: 0.993... \tVal Loss: 0.730 \tVal Frac: 0.7680\n",
      "\tEpoch: 910/1000; 1.35 sec... \tStep: 221130... \tLoss: 1.009... \tVal Loss: 0.679 \tVal Frac: 0.7912\n",
      "\tEpoch: 915/1000; 1.34 sec... \tStep: 222345... \tLoss: 0.738... \tVal Loss: 0.674 \tVal Frac: 0.7746\n",
      "\tEpoch: 920/1000; 1.30 sec... \tStep: 223560... \tLoss: 0.749... \tVal Loss: 0.698 \tVal Frac: 0.7750\n",
      "\tEpoch: 925/1000; 1.35 sec... \tStep: 224775... \tLoss: 0.535... \tVal Loss: 0.676 \tVal Frac: 0.7699\n",
      "\tEpoch: 930/1000; 1.34 sec... \tStep: 225990... \tLoss: 0.921... \tVal Loss: 0.673 \tVal Frac: 0.7724\n",
      "\tEpoch: 935/1000; 1.42 sec... \tStep: 227205... \tLoss: 0.589... \tVal Loss: 0.703 \tVal Frac: 0.7842\n",
      "\tEpoch: 940/1000; 1.30 sec... \tStep: 228420... \tLoss: 0.869... \tVal Loss: 0.768 \tVal Frac: 0.7574\n",
      "\tEpoch: 945/1000; 1.29 sec... \tStep: 229635... \tLoss: 0.920... \tVal Loss: 0.688 \tVal Frac: 0.7857\n",
      "\tEpoch: 950/1000; 1.50 sec... \tStep: 230850... \tLoss: 0.639... \tVal Loss: 0.680 \tVal Frac: 0.7879\n",
      "\tEpoch: 955/1000; 1.59 sec... \tStep: 232065... \tLoss: 0.888... \tVal Loss: 0.680 \tVal Frac: 0.7901\n",
      "\tEpoch: 960/1000; 1.22 sec... \tStep: 233280... \tLoss: 0.708... \tVal Loss: 0.739 \tVal Frac: 0.7680\n",
      "\tEpoch: 965/1000; 1.38 sec... \tStep: 234495... \tLoss: 0.731... \tVal Loss: 0.696 \tVal Frac: 0.7754\n",
      "\tEpoch: 970/1000; 1.30 sec... \tStep: 235710... \tLoss: 1.444... \tVal Loss: 0.772 \tVal Frac: 0.7798\n",
      "\tEpoch: 975/1000; 1.35 sec... \tStep: 236925... \tLoss: 0.987... \tVal Loss: 0.963 \tVal Frac: 0.7629\n",
      "\tEpoch: 980/1000; 1.34 sec... \tStep: 238140... \tLoss: 0.687... \tVal Loss: 0.666 \tVal Frac: 0.7934\n",
      "\tEpoch: 985/1000; 1.29 sec... \tStep: 239355... \tLoss: 0.932... \tVal Loss: 0.684 \tVal Frac: 0.7728\n",
      "\tEpoch: 990/1000; 1.32 sec... \tStep: 240570... \tLoss: 0.739... \tVal Loss: 0.703 \tVal Frac: 0.7654\n",
      "\tEpoch: 995/1000; 1.52 sec... \tStep: 241785... \tLoss: 0.725... \tVal Loss: 0.691 \tVal Frac: 0.7665\n",
      "\tEpoch: 1000/1000; 1.43 sec... \tStep: 243000... \tLoss: 0.567... \tVal Loss: 0.673 \tVal Frac: 0.7857\n",
      "Completed:  hidden_dim :  10 \n",
      "\tloss: 0.617172 \n",
      "\tfrac:0.801838\n",
      "\n",
      "###########\n",
      "Testing with  hidden_dim :  20\n",
      "\tEpoch: 5/1000; 1.33 sec... \tStep: 1215... \tLoss: 1.281... \tVal Loss: 1.062 \tVal Frac: 0.7346\n",
      "\tValidation loss decreased (inf --> 1.062).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.43 sec... \tStep: 2430... \tLoss: 1.335... \tVal Loss: 0.909 \tVal Frac: 0.7290\n",
      "\tValidation loss decreased (1.062 --> 0.909).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.36 sec... \tStep: 3645... \tLoss: 0.934... \tVal Loss: 1.058 \tVal Frac: 0.7290\n",
      "\tEpoch: 20/1000; 1.46 sec... \tStep: 4860... \tLoss: 0.981... \tVal Loss: 0.719 \tVal Frac: 0.7570\n",
      "\tValidation loss decreased (0.909 --> 0.719).  Saving model ...\n",
      "\tEpoch: 25/1000; 1.42 sec... \tStep: 6075... \tLoss: 0.900... \tVal Loss: 0.902 \tVal Frac: 0.7276\n",
      "\tEpoch: 30/1000; 1.42 sec... \tStep: 7290... \tLoss: 0.762... \tVal Loss: 0.763 \tVal Frac: 0.7772\n",
      "\tEpoch: 35/1000; 1.30 sec... \tStep: 8505... \tLoss: 0.903... \tVal Loss: 0.885 \tVal Frac: 0.7250\n",
      "\tEpoch: 40/1000; 1.26 sec... \tStep: 9720... \tLoss: 0.552... \tVal Loss: 0.771 \tVal Frac: 0.7673\n",
      "\tEpoch: 45/1000; 1.33 sec... \tStep: 10935... \tLoss: 1.388... \tVal Loss: 0.880 \tVal Frac: 0.7544\n",
      "\tEpoch: 50/1000; 1.26 sec... \tStep: 12150... \tLoss: 0.897... \tVal Loss: 0.911 \tVal Frac: 0.7250\n",
      "\tEpoch: 55/1000; 1.37 sec... \tStep: 13365... \tLoss: 0.679... \tVal Loss: 0.713 \tVal Frac: 0.7820\n",
      "\tValidation loss decreased (0.719 --> 0.713).  Saving model ...\n",
      "\tEpoch: 60/1000; 1.39 sec... \tStep: 14580... \tLoss: 0.753... \tVal Loss: 0.709 \tVal Frac: 0.7684\n",
      "\tValidation loss decreased (0.713 --> 0.709).  Saving model ...\n",
      "\tEpoch: 65/1000; 1.40 sec... \tStep: 15795... \tLoss: 1.036... \tVal Loss: 0.687 \tVal Frac: 0.7912\n",
      "\tValidation loss decreased (0.709 --> 0.687).  Saving model ...\n",
      "\tEpoch: 70/1000; 1.23 sec... \tStep: 17010... \tLoss: 1.234... \tVal Loss: 0.745 \tVal Frac: 0.7555\n",
      "\tEpoch: 75/1000; 1.35 sec... \tStep: 18225... \tLoss: 0.638... \tVal Loss: 0.719 \tVal Frac: 0.7761\n",
      "\tEpoch: 80/1000; 1.37 sec... \tStep: 19440... \tLoss: 0.757... \tVal Loss: 0.768 \tVal Frac: 0.7511\n",
      "\tEpoch: 85/1000; 1.32 sec... \tStep: 20655... \tLoss: 1.683... \tVal Loss: 0.756 \tVal Frac: 0.7610\n",
      "\tEpoch: 90/1000; 1.33 sec... \tStep: 21870... \tLoss: 0.781... \tVal Loss: 0.713 \tVal Frac: 0.7710\n",
      "\tEpoch: 95/1000; 1.33 sec... \tStep: 23085... \tLoss: 0.872... \tVal Loss: 0.646 \tVal Frac: 0.7893\n",
      "\tValidation loss decreased (0.687 --> 0.646).  Saving model ...\n",
      "\tEpoch: 100/1000; 1.33 sec... \tStep: 24300... \tLoss: 0.892... \tVal Loss: 0.672 \tVal Frac: 0.7629\n",
      "\tEpoch: 105/1000; 1.43 sec... \tStep: 25515... \tLoss: 0.893... \tVal Loss: 0.730 \tVal Frac: 0.7750\n",
      "\tEpoch: 110/1000; 1.44 sec... \tStep: 26730... \tLoss: 0.537... \tVal Loss: 0.777 \tVal Frac: 0.7489\n",
      "\tEpoch: 115/1000; 1.32 sec... \tStep: 27945... \tLoss: 1.255... \tVal Loss: 0.934 \tVal Frac: 0.7445\n",
      "\tEpoch: 120/1000; 1.36 sec... \tStep: 29160... \tLoss: 0.660... \tVal Loss: 0.705 \tVal Frac: 0.7890\n",
      "\tEpoch: 125/1000; 1.37 sec... \tStep: 30375... \tLoss: 1.077... \tVal Loss: 0.771 \tVal Frac: 0.7585\n",
      "\tEpoch: 130/1000; 1.37 sec... \tStep: 31590... \tLoss: 0.904... \tVal Loss: 0.941 \tVal Frac: 0.7526\n",
      "\tEpoch: 135/1000; 1.78 sec... \tStep: 32805... \tLoss: 1.025... \tVal Loss: 0.717 \tVal Frac: 0.7801\n",
      "\tEpoch: 140/1000; 1.52 sec... \tStep: 34020... \tLoss: 0.949... \tVal Loss: 0.709 \tVal Frac: 0.7783\n",
      "\tEpoch: 145/1000; 1.54 sec... \tStep: 35235... \tLoss: 0.617... \tVal Loss: 0.635 \tVal Frac: 0.7945\n",
      "\tValidation loss decreased (0.646 --> 0.635).  Saving model ...\n",
      "\tEpoch: 150/1000; 1.37 sec... \tStep: 36450... \tLoss: 0.794... \tVal Loss: 0.730 \tVal Frac: 0.7860\n",
      "\tEpoch: 155/1000; 1.43 sec... \tStep: 37665... \tLoss: 0.999... \tVal Loss: 0.668 \tVal Frac: 0.7732\n",
      "\tEpoch: 160/1000; 1.33 sec... \tStep: 38880... \tLoss: 0.709... \tVal Loss: 0.694 \tVal Frac: 0.7901\n",
      "\tEpoch: 165/1000; 1.30 sec... \tStep: 40095... \tLoss: 0.910... \tVal Loss: 0.692 \tVal Frac: 0.7960\n",
      "\tEpoch: 170/1000; 1.39 sec... \tStep: 41310... \tLoss: 0.609... \tVal Loss: 0.669 \tVal Frac: 0.7960\n",
      "\tEpoch: 175/1000; 1.35 sec... \tStep: 42525... \tLoss: 0.958... \tVal Loss: 0.823 \tVal Frac: 0.7757\n",
      "\tEpoch: 180/1000; 1.33 sec... \tStep: 43740... \tLoss: 1.270... \tVal Loss: 0.691 \tVal Frac: 0.7676\n",
      "\tEpoch: 185/1000; 1.32 sec... \tStep: 44955... \tLoss: 0.889... \tVal Loss: 0.748 \tVal Frac: 0.7562\n",
      "\tEpoch: 190/1000; 1.42 sec... \tStep: 46170... \tLoss: 0.706... \tVal Loss: 0.701 \tVal Frac: 0.7555\n",
      "\tEpoch: 195/1000; 1.30 sec... \tStep: 47385... \tLoss: 1.277... \tVal Loss: 0.817 \tVal Frac: 0.7581\n",
      "\tEpoch: 200/1000; 1.37 sec... \tStep: 48600... \tLoss: 0.606... \tVal Loss: 0.691 \tVal Frac: 0.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 205/1000; 1.35 sec... \tStep: 49815... \tLoss: 0.769... \tVal Loss: 0.679 \tVal Frac: 0.7676\n",
      "\tEpoch: 210/1000; 1.36 sec... \tStep: 51030... \tLoss: 0.990... \tVal Loss: 0.743 \tVal Frac: 0.7772\n",
      "\tEpoch: 215/1000; 1.85 sec... \tStep: 52245... \tLoss: 1.284... \tVal Loss: 0.786 \tVal Frac: 0.7621\n",
      "\tEpoch: 220/1000; 1.30 sec... \tStep: 53460... \tLoss: 0.580... \tVal Loss: 0.683 \tVal Frac: 0.7849\n",
      "\tEpoch: 225/1000; 1.37 sec... \tStep: 54675... \tLoss: 0.925... \tVal Loss: 0.737 \tVal Frac: 0.7794\n",
      "\tEpoch: 230/1000; 1.42 sec... \tStep: 55890... \tLoss: 0.718... \tVal Loss: 0.687 \tVal Frac: 0.7996\n",
      "\tEpoch: 235/1000; 1.39 sec... \tStep: 57105... \tLoss: 1.165... \tVal Loss: 0.683 \tVal Frac: 0.7570\n",
      "\tEpoch: 240/1000; 1.34 sec... \tStep: 58320... \tLoss: 0.931... \tVal Loss: 0.645 \tVal Frac: 0.7754\n",
      "\tEpoch: 245/1000; 1.44 sec... \tStep: 59535... \tLoss: 0.824... \tVal Loss: 0.685 \tVal Frac: 0.7482\n",
      "\tEpoch: 250/1000; 1.45 sec... \tStep: 60750... \tLoss: 0.664... \tVal Loss: 0.660 \tVal Frac: 0.7886\n",
      "\tEpoch: 255/1000; 1.47 sec... \tStep: 61965... \tLoss: 0.988... \tVal Loss: 0.726 \tVal Frac: 0.7625\n",
      "\tEpoch: 260/1000; 1.39 sec... \tStep: 63180... \tLoss: 0.681... \tVal Loss: 0.625 \tVal Frac: 0.8029\n",
      "\tValidation loss decreased (0.635 --> 0.625).  Saving model ...\n",
      "\tEpoch: 265/1000; 1.36 sec... \tStep: 64395... \tLoss: 0.715... \tVal Loss: 0.669 \tVal Frac: 0.7577\n",
      "\tEpoch: 270/1000; 1.39 sec... \tStep: 65610... \tLoss: 1.053... \tVal Loss: 0.774 \tVal Frac: 0.7787\n",
      "\tEpoch: 275/1000; 1.40 sec... \tStep: 66825... \tLoss: 0.692... \tVal Loss: 0.643 \tVal Frac: 0.7849\n",
      "\tEpoch: 280/1000; 1.42 sec... \tStep: 68040... \tLoss: 0.804... \tVal Loss: 0.643 \tVal Frac: 0.7812\n",
      "\tEpoch: 285/1000; 1.42 sec... \tStep: 69255... \tLoss: 0.443... \tVal Loss: 0.683 \tVal Frac: 0.7783\n",
      "\tEpoch: 290/1000; 1.29 sec... \tStep: 70470... \tLoss: 1.118... \tVal Loss: 0.659 \tVal Frac: 0.7732\n",
      "\tEpoch: 295/1000; 1.36 sec... \tStep: 71685... \tLoss: 1.117... \tVal Loss: 0.627 \tVal Frac: 0.7941\n",
      "\tEpoch: 300/1000; 1.30 sec... \tStep: 72900... \tLoss: 0.754... \tVal Loss: 0.612 \tVal Frac: 0.8018\n",
      "\tValidation loss decreased (0.625 --> 0.612).  Saving model ...\n",
      "\tEpoch: 305/1000; 1.32 sec... \tStep: 74115... \tLoss: 0.601... \tVal Loss: 0.648 \tVal Frac: 0.7816\n",
      "\tEpoch: 310/1000; 1.37 sec... \tStep: 75330... \tLoss: 0.619... \tVal Loss: 0.659 \tVal Frac: 0.7798\n",
      "\tEpoch: 315/1000; 1.43 sec... \tStep: 76545... \tLoss: 0.785... \tVal Loss: 0.624 \tVal Frac: 0.7934\n",
      "\tEpoch: 320/1000; 1.30 sec... \tStep: 77760... \tLoss: 1.034... \tVal Loss: 0.675 \tVal Frac: 0.7904\n",
      "\tEpoch: 325/1000; 1.41 sec... \tStep: 78975... \tLoss: 0.812... \tVal Loss: 0.650 \tVal Frac: 0.7779\n",
      "\tEpoch: 330/1000; 1.38 sec... \tStep: 80190... \tLoss: 1.133... \tVal Loss: 0.671 \tVal Frac: 0.7790\n",
      "\tEpoch: 335/1000; 1.39 sec... \tStep: 81405... \tLoss: 0.635... \tVal Loss: 0.668 \tVal Frac: 0.7893\n",
      "\tEpoch: 340/1000; 1.29 sec... \tStep: 82620... \tLoss: 0.484... \tVal Loss: 0.681 \tVal Frac: 0.7926\n",
      "\tEpoch: 345/1000; 1.37 sec... \tStep: 83835... \tLoss: 0.555... \tVal Loss: 0.645 \tVal Frac: 0.7919\n",
      "\tEpoch: 350/1000; 1.33 sec... \tStep: 85050... \tLoss: 0.693... \tVal Loss: 0.779 \tVal Frac: 0.7526\n",
      "\tEpoch: 355/1000; 1.39 sec... \tStep: 86265... \tLoss: 0.851... \tVal Loss: 0.828 \tVal Frac: 0.7754\n",
      "\tEpoch: 360/1000; 1.29 sec... \tStep: 87480... \tLoss: 0.790... \tVal Loss: 0.674 \tVal Frac: 0.7651\n",
      "\tEpoch: 365/1000; 1.44 sec... \tStep: 88695... \tLoss: 0.615... \tVal Loss: 0.651 \tVal Frac: 0.7724\n",
      "\tEpoch: 370/1000; 1.26 sec... \tStep: 89910... \tLoss: 0.667... \tVal Loss: 0.616 \tVal Frac: 0.8066\n",
      "\tEpoch: 375/1000; 1.42 sec... \tStep: 91125... \tLoss: 0.410... \tVal Loss: 0.633 \tVal Frac: 0.8026\n",
      "\tEpoch: 380/1000; 1.29 sec... \tStep: 92340... \tLoss: 0.626... \tVal Loss: 0.723 \tVal Frac: 0.7857\n",
      "\tEpoch: 385/1000; 1.30 sec... \tStep: 93555... \tLoss: 0.577... \tVal Loss: 0.634 \tVal Frac: 0.7713\n",
      "\tEpoch: 390/1000; 1.42 sec... \tStep: 94770... \tLoss: 0.434... \tVal Loss: 0.650 \tVal Frac: 0.7897\n",
      "\tEpoch: 395/1000; 1.36 sec... \tStep: 95985... \tLoss: 0.636... \tVal Loss: 0.632 \tVal Frac: 0.7912\n",
      "\tEpoch: 400/1000; 1.37 sec... \tStep: 97200... \tLoss: 1.243... \tVal Loss: 0.684 \tVal Frac: 0.7684\n",
      "\tEpoch: 405/1000; 1.40 sec... \tStep: 98415... \tLoss: 0.738... \tVal Loss: 0.680 \tVal Frac: 0.7824\n",
      "\tEpoch: 410/1000; 1.39 sec... \tStep: 99630... \tLoss: 1.002... \tVal Loss: 0.837 \tVal Frac: 0.7577\n",
      "\tEpoch: 415/1000; 1.50 sec... \tStep: 100845... \tLoss: 0.579... \tVal Loss: 0.711 \tVal Frac: 0.7985\n",
      "\tEpoch: 420/1000; 1.37 sec... \tStep: 102060... \tLoss: 0.724... \tVal Loss: 0.673 \tVal Frac: 0.7783\n",
      "\tEpoch: 425/1000; 1.38 sec... \tStep: 103275... \tLoss: 0.756... \tVal Loss: 0.719 \tVal Frac: 0.7732\n",
      "\tEpoch: 430/1000; 1.30 sec... \tStep: 104490... \tLoss: 0.638... \tVal Loss: 0.692 \tVal Frac: 0.7816\n",
      "\tEpoch: 435/1000; 1.33 sec... \tStep: 105705... \tLoss: 1.103... \tVal Loss: 0.659 \tVal Frac: 0.7908\n",
      "\tEpoch: 440/1000; 1.40 sec... \tStep: 106920... \tLoss: 1.014... \tVal Loss: 0.685 \tVal Frac: 0.7934\n",
      "\tEpoch: 445/1000; 1.40 sec... \tStep: 108135... \tLoss: 0.957... \tVal Loss: 0.656 \tVal Frac: 0.7614\n",
      "\tEpoch: 450/1000; 1.46 sec... \tStep: 109350... \tLoss: 0.710... \tVal Loss: 0.715 \tVal Frac: 0.7728\n",
      "\tEpoch: 455/1000; 1.35 sec... \tStep: 110565... \tLoss: 0.717... \tVal Loss: 0.737 \tVal Frac: 0.7386\n",
      "\tEpoch: 460/1000; 1.33 sec... \tStep: 111780... \tLoss: 0.970... \tVal Loss: 0.729 \tVal Frac: 0.7588\n",
      "\tEpoch: 465/1000; 1.29 sec... \tStep: 112995... \tLoss: 0.494... \tVal Loss: 0.671 \tVal Frac: 0.7779\n",
      "\tEpoch: 470/1000; 1.36 sec... \tStep: 114210... \tLoss: 1.241... \tVal Loss: 0.670 \tVal Frac: 0.7768\n",
      "\tEpoch: 475/1000; 1.30 sec... \tStep: 115425... \tLoss: 0.641... \tVal Loss: 0.645 \tVal Frac: 0.7842\n",
      "\tEpoch: 480/1000; 1.37 sec... \tStep: 116640... \tLoss: 1.317... \tVal Loss: 0.723 \tVal Frac: 0.7408\n",
      "\tEpoch: 485/1000; 1.37 sec... \tStep: 117855... \tLoss: 0.726... \tVal Loss: 0.701 \tVal Frac: 0.7607\n",
      "\tEpoch: 490/1000; 1.34 sec... \tStep: 119070... \tLoss: 0.747... \tVal Loss: 0.675 \tVal Frac: 0.7632\n",
      "\tEpoch: 495/1000; 1.50 sec... \tStep: 120285... \tLoss: 0.860... \tVal Loss: 0.614 \tVal Frac: 0.7897\n",
      "\tEpoch: 500/1000; 1.40 sec... \tStep: 121500... \tLoss: 0.733... \tVal Loss: 0.640 \tVal Frac: 0.7849\n",
      "\tEpoch: 505/1000; 1.34 sec... \tStep: 122715... \tLoss: 0.593... \tVal Loss: 0.663 \tVal Frac: 0.7776\n",
      "\tEpoch: 510/1000; 1.33 sec... \tStep: 123930... \tLoss: 0.712... \tVal Loss: 0.685 \tVal Frac: 0.7676\n",
      "\tEpoch: 515/1000; 1.45 sec... \tStep: 125145... \tLoss: 0.965... \tVal Loss: 0.819 \tVal Frac: 0.7772\n",
      "\tEpoch: 520/1000; 1.30 sec... \tStep: 126360... \tLoss: 0.478... \tVal Loss: 0.656 \tVal Frac: 0.7952\n",
      "\tEpoch: 525/1000; 1.30 sec... \tStep: 127575... \tLoss: 0.669... \tVal Loss: 0.668 \tVal Frac: 0.7511\n",
      "\tEpoch: 530/1000; 1.39 sec... \tStep: 128790... \tLoss: 0.707... \tVal Loss: 0.652 \tVal Frac: 0.7912\n",
      "\tEpoch: 535/1000; 1.27 sec... \tStep: 130005... \tLoss: 0.921... \tVal Loss: 0.645 \tVal Frac: 0.7713\n",
      "\tEpoch: 540/1000; 1.33 sec... \tStep: 131220... \tLoss: 0.476... \tVal Loss: 0.731 \tVal Frac: 0.7779\n",
      "\tEpoch: 545/1000; 1.37 sec... \tStep: 132435... \tLoss: 0.608... \tVal Loss: 0.684 \tVal Frac: 0.7831\n",
      "\tEpoch: 550/1000; 1.34 sec... \tStep: 133650... \tLoss: 1.050... \tVal Loss: 0.666 \tVal Frac: 0.7816\n",
      "\tEpoch: 555/1000; 1.26 sec... \tStep: 134865... \tLoss: 1.133... \tVal Loss: 0.684 \tVal Frac: 0.7787\n",
      "\tEpoch: 560/1000; 1.64 sec... \tStep: 136080... \tLoss: 0.779... \tVal Loss: 0.683 \tVal Frac: 0.7739\n",
      "\tEpoch: 565/1000; 1.54 sec... \tStep: 137295... \tLoss: 0.533... \tVal Loss: 0.649 \tVal Frac: 0.7897\n",
      "\tEpoch: 570/1000; 1.66 sec... \tStep: 138510... \tLoss: 0.555... \tVal Loss: 0.700 \tVal Frac: 0.7500\n",
      "\tEpoch: 575/1000; 1.29 sec... \tStep: 139725... \tLoss: 0.764... \tVal Loss: 0.649 \tVal Frac: 0.7783\n",
      "\tEpoch: 580/1000; 1.27 sec... \tStep: 140940... \tLoss: 0.739... \tVal Loss: 0.668 \tVal Frac: 0.7879\n",
      "\tEpoch: 585/1000; 1.40 sec... \tStep: 142155... \tLoss: 0.811... \tVal Loss: 0.646 \tVal Frac: 0.7915\n",
      "\tEpoch: 590/1000; 1.27 sec... \tStep: 143370... \tLoss: 1.096... \tVal Loss: 0.679 \tVal Frac: 0.7890\n",
      "\tEpoch: 595/1000; 1.38 sec... \tStep: 144585... \tLoss: 0.934... \tVal Loss: 0.637 \tVal Frac: 0.7908\n",
      "\tEpoch: 600/1000; 1.49 sec... \tStep: 145800... \tLoss: 1.112... \tVal Loss: 0.639 \tVal Frac: 0.7779\n",
      "\tEpoch: 605/1000; 1.29 sec... \tStep: 147015... \tLoss: 0.940... \tVal Loss: 0.686 \tVal Frac: 0.7739\n",
      "\tEpoch: 610/1000; 1.37 sec... \tStep: 148230... \tLoss: 0.390... \tVal Loss: 0.653 \tVal Frac: 0.7904\n",
      "\tEpoch: 615/1000; 1.34 sec... \tStep: 149445... \tLoss: 0.704... \tVal Loss: 0.645 \tVal Frac: 0.7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 620/1000; 1.35 sec... \tStep: 150660... \tLoss: 1.134... \tVal Loss: 0.673 \tVal Frac: 0.7585\n",
      "\tEpoch: 625/1000; 1.26 sec... \tStep: 151875... \tLoss: 0.568... \tVal Loss: 0.663 \tVal Frac: 0.7868\n",
      "\tEpoch: 630/1000; 1.43 sec... \tStep: 153090... \tLoss: 0.556... \tVal Loss: 0.655 \tVal Frac: 0.7930\n",
      "\tEpoch: 635/1000; 1.23 sec... \tStep: 154305... \tLoss: 1.479... \tVal Loss: 0.701 \tVal Frac: 0.7324\n",
      "\tEpoch: 640/1000; 1.37 sec... \tStep: 155520... \tLoss: 1.187... \tVal Loss: 0.641 \tVal Frac: 0.7989\n",
      "\tEpoch: 645/1000; 1.34 sec... \tStep: 156735... \tLoss: 0.595... \tVal Loss: 0.639 \tVal Frac: 0.7971\n",
      "\tEpoch: 650/1000; 1.29 sec... \tStep: 157950... \tLoss: 0.444... \tVal Loss: 0.620 \tVal Frac: 0.8099\n",
      "\tEpoch: 655/1000; 1.36 sec... \tStep: 159165... \tLoss: 0.953... \tVal Loss: 0.700 \tVal Frac: 0.7669\n",
      "\tEpoch: 660/1000; 1.35 sec... \tStep: 160380... \tLoss: 0.298... \tVal Loss: 0.630 \tVal Frac: 0.8011\n",
      "\tEpoch: 665/1000; 1.38 sec... \tStep: 161595... \tLoss: 0.682... \tVal Loss: 0.639 \tVal Frac: 0.7842\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 1.131... \tVal Loss: 0.623 \tVal Frac: 0.8000\n",
      "\tEpoch: 675/1000; 1.40 sec... \tStep: 164025... \tLoss: 0.780... \tVal Loss: 0.635 \tVal Frac: 0.7963\n",
      "\tEpoch: 680/1000; 1.54 sec... \tStep: 165240... \tLoss: 0.957... \tVal Loss: 0.652 \tVal Frac: 0.7849\n",
      "\tEpoch: 685/1000; 1.38 sec... \tStep: 166455... \tLoss: 0.746... \tVal Loss: 0.655 \tVal Frac: 0.7974\n",
      "\tEpoch: 690/1000; 1.39 sec... \tStep: 167670... \tLoss: 0.649... \tVal Loss: 0.630 \tVal Frac: 0.7897\n",
      "\tEpoch: 695/1000; 1.39 sec... \tStep: 168885... \tLoss: 0.564... \tVal Loss: 0.617 \tVal Frac: 0.8048\n",
      "\tEpoch: 700/1000; 1.32 sec... \tStep: 170100... \tLoss: 1.109... \tVal Loss: 0.669 \tVal Frac: 0.7735\n",
      "\tEpoch: 705/1000; 1.43 sec... \tStep: 171315... \tLoss: 1.068... \tVal Loss: 0.647 \tVal Frac: 0.7629\n",
      "\tEpoch: 710/1000; 1.35 sec... \tStep: 172530... \tLoss: 0.839... \tVal Loss: 0.626 \tVal Frac: 0.7647\n",
      "\tEpoch: 715/1000; 1.24 sec... \tStep: 173745... \tLoss: 0.677... \tVal Loss: 0.622 \tVal Frac: 0.7868\n",
      "\tEpoch: 720/1000; 1.35 sec... \tStep: 174960... \tLoss: 0.880... \tVal Loss: 0.638 \tVal Frac: 0.7956\n",
      "\tEpoch: 725/1000; 1.37 sec... \tStep: 176175... \tLoss: 0.438... \tVal Loss: 0.648 \tVal Frac: 0.7625\n",
      "\tEpoch: 730/1000; 1.47 sec... \tStep: 177390... \tLoss: 0.726... \tVal Loss: 0.644 \tVal Frac: 0.7893\n",
      "\tEpoch: 735/1000; 1.30 sec... \tStep: 178605... \tLoss: 0.352... \tVal Loss: 0.622 \tVal Frac: 0.7871\n",
      "\tEpoch: 740/1000; 1.30 sec... \tStep: 179820... \tLoss: 0.675... \tVal Loss: 0.633 \tVal Frac: 0.7971\n",
      "\tEpoch: 745/1000; 1.32 sec... \tStep: 181035... \tLoss: 0.937... \tVal Loss: 0.644 \tVal Frac: 0.7890\n",
      "\tEpoch: 750/1000; 1.37 sec... \tStep: 182250... \tLoss: 0.919... \tVal Loss: 0.641 \tVal Frac: 0.7746\n",
      "\tEpoch: 755/1000; 1.37 sec... \tStep: 183465... \tLoss: 0.598... \tVal Loss: 0.635 \tVal Frac: 0.7695\n",
      "\tEpoch: 760/1000; 1.45 sec... \tStep: 184680... \tLoss: 1.130... \tVal Loss: 0.683 \tVal Frac: 0.7824\n",
      "\tEpoch: 765/1000; 1.43 sec... \tStep: 185895... \tLoss: 0.451... \tVal Loss: 0.687 \tVal Frac: 0.7607\n",
      "\tEpoch: 770/1000; 1.27 sec... \tStep: 187110... \tLoss: 0.352... \tVal Loss: 0.641 \tVal Frac: 0.7890\n",
      "\tEpoch: 775/1000; 1.36 sec... \tStep: 188325... \tLoss: 0.733... \tVal Loss: 0.642 \tVal Frac: 0.7971\n",
      "\tEpoch: 780/1000; 1.33 sec... \tStep: 189540... \tLoss: 0.396... \tVal Loss: 0.627 \tVal Frac: 0.7971\n",
      "\tEpoch: 785/1000; 1.40 sec... \tStep: 190755... \tLoss: 0.553... \tVal Loss: 0.626 \tVal Frac: 0.8040\n",
      "\tEpoch: 790/1000; 1.37 sec... \tStep: 191970... \tLoss: 0.944... \tVal Loss: 0.630 \tVal Frac: 0.8070\n",
      "\tEpoch: 795/1000; 1.19 sec... \tStep: 193185... \tLoss: 1.047... \tVal Loss: 0.716 \tVal Frac: 0.7721\n",
      "\tEpoch: 800/1000; 1.43 sec... \tStep: 194400... \tLoss: 0.772... \tVal Loss: 0.649 \tVal Frac: 0.8029\n",
      "\tEpoch: 805/1000; 1.39 sec... \tStep: 195615... \tLoss: 0.716... \tVal Loss: 0.638 \tVal Frac: 0.7945\n",
      "\tEpoch: 810/1000; 1.39 sec... \tStep: 196830... \tLoss: 1.091... \tVal Loss: 0.665 \tVal Frac: 0.7846\n",
      "\tEpoch: 815/1000; 1.37 sec... \tStep: 198045... \tLoss: 1.064... \tVal Loss: 0.661 \tVal Frac: 0.7805\n",
      "\tEpoch: 820/1000; 1.28 sec... \tStep: 199260... \tLoss: 0.655... \tVal Loss: 0.671 \tVal Frac: 0.7739\n",
      "\tEpoch: 825/1000; 1.39 sec... \tStep: 200475... \tLoss: 0.777... \tVal Loss: 0.689 \tVal Frac: 0.7636\n",
      "\tEpoch: 830/1000; 1.36 sec... \tStep: 201690... \tLoss: 0.804... \tVal Loss: 0.665 \tVal Frac: 0.7522\n",
      "\tEpoch: 835/1000; 1.26 sec... \tStep: 202905... \tLoss: 0.904... \tVal Loss: 0.677 \tVal Frac: 0.7599\n",
      "\tEpoch: 840/1000; 1.44 sec... \tStep: 204120... \tLoss: 0.416... \tVal Loss: 0.621 \tVal Frac: 0.7967\n",
      "\tEpoch: 845/1000; 1.43 sec... \tStep: 205335... \tLoss: 0.609... \tVal Loss: 0.674 \tVal Frac: 0.7790\n",
      "\tEpoch: 850/1000; 1.37 sec... \tStep: 206550... \tLoss: 0.996... \tVal Loss: 0.625 \tVal Frac: 0.7952\n",
      "\tEpoch: 855/1000; 1.52 sec... \tStep: 207765... \tLoss: 0.671... \tVal Loss: 0.628 \tVal Frac: 0.7934\n",
      "\tEpoch: 860/1000; 1.32 sec... \tStep: 208980... \tLoss: 1.117... \tVal Loss: 0.644 \tVal Frac: 0.7864\n",
      "\tEpoch: 865/1000; 1.36 sec... \tStep: 210195... \tLoss: 0.651... \tVal Loss: 0.648 \tVal Frac: 0.7809\n",
      "\tEpoch: 870/1000; 1.44 sec... \tStep: 211410... \tLoss: 0.912... \tVal Loss: 0.635 \tVal Frac: 0.7849\n",
      "\tEpoch: 875/1000; 1.37 sec... \tStep: 212625... \tLoss: 0.803... \tVal Loss: 0.672 \tVal Frac: 0.7838\n",
      "\tEpoch: 880/1000; 1.37 sec... \tStep: 213840... \tLoss: 0.904... \tVal Loss: 0.646 \tVal Frac: 0.7945\n",
      "\tEpoch: 885/1000; 1.40 sec... \tStep: 215055... \tLoss: 0.843... \tVal Loss: 0.630 \tVal Frac: 0.7912\n",
      "\tEpoch: 890/1000; 1.37 sec... \tStep: 216270... \tLoss: 0.707... \tVal Loss: 0.616 \tVal Frac: 0.7985\n",
      "\tEpoch: 895/1000; 1.44 sec... \tStep: 217485... \tLoss: 0.920... \tVal Loss: 0.624 \tVal Frac: 0.7739\n",
      "\tEpoch: 900/1000; 1.27 sec... \tStep: 218700... \tLoss: 0.748... \tVal Loss: 0.628 \tVal Frac: 0.7864\n",
      "\tEpoch: 905/1000; 1.29 sec... \tStep: 219915... \tLoss: 1.194... \tVal Loss: 0.635 \tVal Frac: 0.7853\n",
      "\tEpoch: 910/1000; 1.30 sec... \tStep: 221130... \tLoss: 0.388... \tVal Loss: 0.636 \tVal Frac: 0.7831\n",
      "\tEpoch: 915/1000; 1.39 sec... \tStep: 222345... \tLoss: 1.385... \tVal Loss: 0.758 \tVal Frac: 0.7298\n",
      "\tEpoch: 920/1000; 1.43 sec... \tStep: 223560... \tLoss: 0.931... \tVal Loss: 0.666 \tVal Frac: 0.7665\n",
      "\tEpoch: 925/1000; 1.42 sec... \tStep: 224775... \tLoss: 1.081... \tVal Loss: 0.637 \tVal Frac: 0.7724\n",
      "\tEpoch: 930/1000; 1.45 sec... \tStep: 225990... \tLoss: 0.790... \tVal Loss: 0.617 \tVal Frac: 0.7676\n",
      "\tEpoch: 935/1000; 1.27 sec... \tStep: 227205... \tLoss: 0.828... \tVal Loss: 0.620 \tVal Frac: 0.7985\n",
      "\tEpoch: 940/1000; 1.37 sec... \tStep: 228420... \tLoss: 0.822... \tVal Loss: 0.644 \tVal Frac: 0.7805\n",
      "\tEpoch: 945/1000; 1.30 sec... \tStep: 229635... \tLoss: 0.648... \tVal Loss: 0.647 \tVal Frac: 0.7923\n",
      "\tEpoch: 950/1000; 1.28 sec... \tStep: 230850... \tLoss: 0.980... \tVal Loss: 0.687 \tVal Frac: 0.7860\n",
      "\tEpoch: 955/1000; 1.35 sec... \tStep: 232065... \tLoss: 1.051... \tVal Loss: 0.628 \tVal Frac: 0.7937\n",
      "\tEpoch: 960/1000; 1.66 sec... \tStep: 233280... \tLoss: 1.022... \tVal Loss: 0.644 \tVal Frac: 0.8022\n",
      "\tEpoch: 965/1000; 1.56 sec... \tStep: 234495... \tLoss: 0.466... \tVal Loss: 0.622 \tVal Frac: 0.8018\n",
      "\tEpoch: 970/1000; 1.40 sec... \tStep: 235710... \tLoss: 0.880... \tVal Loss: 0.609 \tVal Frac: 0.8004\n",
      "\tValidation loss decreased (0.612 --> 0.609).  Saving model ...\n",
      "\tEpoch: 975/1000; 1.30 sec... \tStep: 236925... \tLoss: 0.886... \tVal Loss: 0.657 \tVal Frac: 0.7930\n",
      "\tEpoch: 980/1000; 1.37 sec... \tStep: 238140... \tLoss: 0.892... \tVal Loss: 0.656 \tVal Frac: 0.7971\n",
      "\tEpoch: 985/1000; 1.39 sec... \tStep: 239355... \tLoss: 0.330... \tVal Loss: 0.636 \tVal Frac: 0.7982\n",
      "\tEpoch: 990/1000; 1.42 sec... \tStep: 240570... \tLoss: 0.984... \tVal Loss: 0.635 \tVal Frac: 0.8011\n",
      "\tEpoch: 995/1000; 1.36 sec... \tStep: 241785... \tLoss: 1.031... \tVal Loss: 0.648 \tVal Frac: 0.7783\n",
      "\tEpoch: 1000/1000; 1.33 sec... \tStep: 243000... \tLoss: 0.966... \tVal Loss: 0.620 \tVal Frac: 0.7886\n",
      "Completed:  hidden_dim :  20 \n",
      "\tloss: 0.609183 \n",
      "\tfrac:0.809926\n",
      "\n",
      "###########\n",
      "Testing with  hidden_dim :  100\n",
      "\tEpoch: 5/1000; 1.52 sec... \tStep: 1215... \tLoss: 0.536... \tVal Loss: 0.668 \tVal Frac: 0.7879\n",
      "\tValidation loss decreased (inf --> 0.668).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.41 sec... \tStep: 2430... \tLoss: 0.935... \tVal Loss: 0.703 \tVal Frac: 0.7754\n",
      "\tEpoch: 15/1000; 1.39 sec... \tStep: 3645... \tLoss: 0.884... \tVal Loss: 0.714 \tVal Frac: 0.7809\n",
      "\tEpoch: 20/1000; 1.36 sec... \tStep: 4860... \tLoss: 0.805... \tVal Loss: 0.671 \tVal Frac: 0.7971\n",
      "\tEpoch: 25/1000; 1.45 sec... \tStep: 6075... \tLoss: 0.631... \tVal Loss: 0.628 \tVal Frac: 0.7908\n",
      "\tValidation loss decreased (0.668 --> 0.628).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 30/1000; 1.65 sec... \tStep: 7290... \tLoss: 0.936... \tVal Loss: 0.648 \tVal Frac: 0.7787\n",
      "\tEpoch: 35/1000; 1.64 sec... \tStep: 8505... \tLoss: 1.146... \tVal Loss: 0.638 \tVal Frac: 0.7926\n",
      "\tEpoch: 40/1000; 1.64 sec... \tStep: 9720... \tLoss: 0.511... \tVal Loss: 0.588 \tVal Frac: 0.8074\n",
      "\tValidation loss decreased (0.628 --> 0.588).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.57 sec... \tStep: 10935... \tLoss: 0.630... \tVal Loss: 0.606 \tVal Frac: 0.8085\n",
      "\tEpoch: 50/1000; 1.91 sec... \tStep: 12150... \tLoss: 1.077... \tVal Loss: 0.656 \tVal Frac: 0.7956\n",
      "\tEpoch: 55/1000; 1.61 sec... \tStep: 13365... \tLoss: 0.895... \tVal Loss: 0.633 \tVal Frac: 0.7849\n",
      "\tEpoch: 60/1000; 1.70 sec... \tStep: 14580... \tLoss: 1.003... \tVal Loss: 0.778 \tVal Frac: 0.7893\n",
      "\tEpoch: 65/1000; 1.67 sec... \tStep: 15795... \tLoss: 1.088... \tVal Loss: 0.635 \tVal Frac: 0.8055\n",
      "\tEpoch: 70/1000; 1.43 sec... \tStep: 17010... \tLoss: 0.848... \tVal Loss: 0.603 \tVal Frac: 0.8040\n",
      "\tEpoch: 75/1000; 1.36 sec... \tStep: 18225... \tLoss: 0.632... \tVal Loss: 0.633 \tVal Frac: 0.7908\n",
      "\tEpoch: 80/1000; 1.40 sec... \tStep: 19440... \tLoss: 0.836... \tVal Loss: 0.608 \tVal Frac: 0.8029\n",
      "\tEpoch: 85/1000; 1.45 sec... \tStep: 20655... \tLoss: 0.814... \tVal Loss: 0.610 \tVal Frac: 0.8044\n",
      "\tEpoch: 90/1000; 1.54 sec... \tStep: 21870... \tLoss: 0.920... \tVal Loss: 0.618 \tVal Frac: 0.8074\n",
      "\tEpoch: 95/1000; 1.42 sec... \tStep: 23085... \tLoss: 1.154... \tVal Loss: 0.630 \tVal Frac: 0.7912\n",
      "\tEpoch: 100/1000; 1.44 sec... \tStep: 24300... \tLoss: 0.381... \tVal Loss: 0.585 \tVal Frac: 0.8121\n",
      "\tValidation loss decreased (0.588 --> 0.585).  Saving model ...\n",
      "\tEpoch: 105/1000; 1.39 sec... \tStep: 25515... \tLoss: 1.030... \tVal Loss: 0.665 \tVal Frac: 0.7919\n",
      "\tEpoch: 110/1000; 1.49 sec... \tStep: 26730... \tLoss: 0.912... \tVal Loss: 0.578 \tVal Frac: 0.8169\n",
      "\tValidation loss decreased (0.585 --> 0.578).  Saving model ...\n",
      "\tEpoch: 115/1000; 1.47 sec... \tStep: 27945... \tLoss: 0.650... \tVal Loss: 0.595 \tVal Frac: 0.8055\n",
      "\tEpoch: 120/1000; 1.37 sec... \tStep: 29160... \tLoss: 0.892... \tVal Loss: 0.586 \tVal Frac: 0.8107\n",
      "\tEpoch: 125/1000; 1.40 sec... \tStep: 30375... \tLoss: 0.848... \tVal Loss: 0.607 \tVal Frac: 0.8125\n",
      "\tEpoch: 130/1000; 1.43 sec... \tStep: 31590... \tLoss: 0.449... \tVal Loss: 0.619 \tVal Frac: 0.7971\n",
      "\tEpoch: 135/1000; 1.47 sec... \tStep: 32805... \tLoss: 0.592... \tVal Loss: 0.627 \tVal Frac: 0.8118\n",
      "\tEpoch: 140/1000; 1.46 sec... \tStep: 34020... \tLoss: 1.132... \tVal Loss: 0.653 \tVal Frac: 0.8040\n",
      "\tEpoch: 145/1000; 1.67 sec... \tStep: 35235... \tLoss: 0.577... \tVal Loss: 0.598 \tVal Frac: 0.8107\n",
      "\tEpoch: 150/1000; 1.40 sec... \tStep: 36450... \tLoss: 0.667... \tVal Loss: 0.596 \tVal Frac: 0.8147\n",
      "\tEpoch: 155/1000; 1.54 sec... \tStep: 37665... \tLoss: 0.866... \tVal Loss: 0.600 \tVal Frac: 0.8092\n",
      "\tEpoch: 160/1000; 1.57 sec... \tStep: 38880... \tLoss: 0.689... \tVal Loss: 0.591 \tVal Frac: 0.8051\n",
      "\tEpoch: 165/1000; 1.47 sec... \tStep: 40095... \tLoss: 0.661... \tVal Loss: 0.595 \tVal Frac: 0.8011\n",
      "\tEpoch: 170/1000; 1.39 sec... \tStep: 41310... \tLoss: 0.305... \tVal Loss: 0.608 \tVal Frac: 0.7993\n",
      "\tEpoch: 175/1000; 1.49 sec... \tStep: 42525... \tLoss: 0.408... \tVal Loss: 0.606 \tVal Frac: 0.8103\n",
      "\tEpoch: 180/1000; 1.55 sec... \tStep: 43740... \tLoss: 0.745... \tVal Loss: 0.579 \tVal Frac: 0.8143\n",
      "\tEpoch: 185/1000; 1.42 sec... \tStep: 44955... \tLoss: 0.711... \tVal Loss: 0.585 \tVal Frac: 0.8114\n",
      "\tEpoch: 190/1000; 1.50 sec... \tStep: 46170... \tLoss: 0.696... \tVal Loss: 0.645 \tVal Frac: 0.8011\n",
      "\tEpoch: 195/1000; 1.42 sec... \tStep: 47385... \tLoss: 0.753... \tVal Loss: 0.590 \tVal Frac: 0.8125\n",
      "\tEpoch: 200/1000; 1.45 sec... \tStep: 48600... \tLoss: 0.839... \tVal Loss: 0.590 \tVal Frac: 0.8048\n",
      "\tEpoch: 205/1000; 1.43 sec... \tStep: 49815... \tLoss: 0.647... \tVal Loss: 0.569 \tVal Frac: 0.8125\n",
      "\tValidation loss decreased (0.578 --> 0.569).  Saving model ...\n",
      "\tEpoch: 210/1000; 1.47 sec... \tStep: 51030... \tLoss: 0.663... \tVal Loss: 0.609 \tVal Frac: 0.8026\n",
      "\tEpoch: 215/1000; 1.40 sec... \tStep: 52245... \tLoss: 0.831... \tVal Loss: 0.588 \tVal Frac: 0.8213\n",
      "\tEpoch: 220/1000; 1.34 sec... \tStep: 53460... \tLoss: 0.886... \tVal Loss: 0.602 \tVal Frac: 0.8081\n",
      "\tEpoch: 225/1000; 1.40 sec... \tStep: 54675... \tLoss: 0.658... \tVal Loss: 0.604 \tVal Frac: 0.8026\n",
      "\tEpoch: 230/1000; 1.56 sec... \tStep: 55890... \tLoss: 0.606... \tVal Loss: 0.614 \tVal Frac: 0.7952\n",
      "\tEpoch: 235/1000; 1.33 sec... \tStep: 57105... \tLoss: 0.352... \tVal Loss: 0.580 \tVal Frac: 0.8074\n",
      "\tEpoch: 240/1000; 1.39 sec... \tStep: 58320... \tLoss: 0.715... \tVal Loss: 0.617 \tVal Frac: 0.8033\n",
      "\tEpoch: 245/1000; 1.33 sec... \tStep: 59535... \tLoss: 0.373... \tVal Loss: 0.593 \tVal Frac: 0.8015\n",
      "\tEpoch: 250/1000; 1.57 sec... \tStep: 60750... \tLoss: 0.604... \tVal Loss: 0.598 \tVal Frac: 0.8085\n",
      "\tEpoch: 255/1000; 1.49 sec... \tStep: 61965... \tLoss: 0.668... \tVal Loss: 0.589 \tVal Frac: 0.8092\n",
      "\tEpoch: 260/1000; 1.43 sec... \tStep: 63180... \tLoss: 0.841... \tVal Loss: 0.578 \tVal Frac: 0.8129\n",
      "\tEpoch: 265/1000; 1.52 sec... \tStep: 64395... \tLoss: 0.957... \tVal Loss: 0.607 \tVal Frac: 0.8018\n",
      "\tEpoch: 270/1000; 1.43 sec... \tStep: 65610... \tLoss: 0.825... \tVal Loss: 0.629 \tVal Frac: 0.7871\n",
      "\tEpoch: 275/1000; 1.52 sec... \tStep: 66825... \tLoss: 0.697... \tVal Loss: 0.648 \tVal Frac: 0.7904\n",
      "\tEpoch: 280/1000; 1.42 sec... \tStep: 68040... \tLoss: 0.655... \tVal Loss: 0.579 \tVal Frac: 0.8136\n",
      "\tEpoch: 285/1000; 1.37 sec... \tStep: 69255... \tLoss: 0.843... \tVal Loss: 0.602 \tVal Frac: 0.7956\n",
      "\tEpoch: 290/1000; 1.37 sec... \tStep: 70470... \tLoss: 0.416... \tVal Loss: 0.588 \tVal Frac: 0.8092\n",
      "\tEpoch: 295/1000; 1.74 sec... \tStep: 71685... \tLoss: 0.962... \tVal Loss: 0.589 \tVal Frac: 0.8011\n",
      "\tEpoch: 300/1000; 1.49 sec... \tStep: 72900... \tLoss: 0.855... \tVal Loss: 0.600 \tVal Frac: 0.8015\n",
      "\tEpoch: 305/1000; 1.33 sec... \tStep: 74115... \tLoss: 0.509... \tVal Loss: 0.620 \tVal Frac: 0.8029\n",
      "\tEpoch: 310/1000; 1.36 sec... \tStep: 75330... \tLoss: 0.797... \tVal Loss: 0.603 \tVal Frac: 0.8011\n",
      "\tEpoch: 315/1000; 1.49 sec... \tStep: 76545... \tLoss: 0.653... \tVal Loss: 0.573 \tVal Frac: 0.8088\n",
      "\tEpoch: 320/1000; 1.49 sec... \tStep: 77760... \tLoss: 0.797... \tVal Loss: 0.614 \tVal Frac: 0.7904\n",
      "\tEpoch: 325/1000; 1.46 sec... \tStep: 78975... \tLoss: 0.635... \tVal Loss: 0.590 \tVal Frac: 0.8011\n",
      "\tEpoch: 330/1000; 1.42 sec... \tStep: 80190... \tLoss: 0.657... \tVal Loss: 0.582 \tVal Frac: 0.7982\n",
      "\tEpoch: 335/1000; 1.59 sec... \tStep: 81405... \tLoss: 0.742... \tVal Loss: 0.579 \tVal Frac: 0.8132\n",
      "\tEpoch: 340/1000; 1.36 sec... \tStep: 82620... \tLoss: 0.909... \tVal Loss: 0.576 \tVal Frac: 0.8099\n",
      "\tEpoch: 345/1000; 1.54 sec... \tStep: 83835... \tLoss: 0.602... \tVal Loss: 0.573 \tVal Frac: 0.8151\n",
      "\tEpoch: 350/1000; 1.40 sec... \tStep: 85050... \tLoss: 0.473... \tVal Loss: 0.569 \tVal Frac: 0.8217\n",
      "\tEpoch: 355/1000; 1.34 sec... \tStep: 86265... \tLoss: 0.843... \tVal Loss: 0.576 \tVal Frac: 0.8162\n",
      "\tEpoch: 360/1000; 1.40 sec... \tStep: 87480... \tLoss: 0.329... \tVal Loss: 0.568 \tVal Frac: 0.8184\n",
      "\tValidation loss decreased (0.569 --> 0.568).  Saving model ...\n",
      "\tEpoch: 365/1000; 1.34 sec... \tStep: 88695... \tLoss: 0.754... \tVal Loss: 0.577 \tVal Frac: 0.8158\n",
      "\tEpoch: 370/1000; 1.74 sec... \tStep: 89910... \tLoss: 0.699... \tVal Loss: 0.575 \tVal Frac: 0.8140\n",
      "\tEpoch: 375/1000; 1.48 sec... \tStep: 91125... \tLoss: 1.216... \tVal Loss: 0.607 \tVal Frac: 0.8022\n",
      "\tEpoch: 380/1000; 1.37 sec... \tStep: 92340... \tLoss: 0.420... \tVal Loss: 0.611 \tVal Frac: 0.8022\n",
      "\tEpoch: 385/1000; 1.50 sec... \tStep: 93555... \tLoss: 0.546... \tVal Loss: 0.577 \tVal Frac: 0.8165\n",
      "\tEpoch: 390/1000; 1.45 sec... \tStep: 94770... \tLoss: 0.804... \tVal Loss: 0.586 \tVal Frac: 0.8063\n",
      "\tEpoch: 395/1000; 1.59 sec... \tStep: 95985... \tLoss: 0.760... \tVal Loss: 0.607 \tVal Frac: 0.7897\n",
      "\tEpoch: 400/1000; 1.47 sec... \tStep: 97200... \tLoss: 0.501... \tVal Loss: 0.597 \tVal Frac: 0.7937\n",
      "\tEpoch: 405/1000; 1.46 sec... \tStep: 98415... \tLoss: 0.317... \tVal Loss: 0.601 \tVal Frac: 0.7989\n",
      "\tEpoch: 410/1000; 1.47 sec... \tStep: 99630... \tLoss: 0.608... \tVal Loss: 0.616 \tVal Frac: 0.7949\n",
      "\tEpoch: 415/1000; 1.40 sec... \tStep: 100845... \tLoss: 1.029... \tVal Loss: 0.564 \tVal Frac: 0.8246\n",
      "\tValidation loss decreased (0.568 --> 0.564).  Saving model ...\n",
      "\tEpoch: 420/1000; 1.39 sec... \tStep: 102060... \tLoss: 0.787... \tVal Loss: 0.613 \tVal Frac: 0.8007\n",
      "\tEpoch: 425/1000; 1.56 sec... \tStep: 103275... \tLoss: 1.039... \tVal Loss: 0.589 \tVal Frac: 0.8026\n",
      "\tEpoch: 430/1000; 1.36 sec... \tStep: 104490... \tLoss: 0.704... \tVal Loss: 0.582 \tVal Frac: 0.8129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 435/1000; 1.40 sec... \tStep: 105705... \tLoss: 0.838... \tVal Loss: 0.577 \tVal Frac: 0.8169\n",
      "\tEpoch: 440/1000; 1.55 sec... \tStep: 106920... \tLoss: 0.601... \tVal Loss: 0.586 \tVal Frac: 0.8015\n",
      "\tEpoch: 445/1000; 1.54 sec... \tStep: 108135... \tLoss: 0.906... \tVal Loss: 0.566 \tVal Frac: 0.8206\n",
      "\tEpoch: 450/1000; 1.43 sec... \tStep: 109350... \tLoss: 0.660... \tVal Loss: 0.582 \tVal Frac: 0.8121\n",
      "\tEpoch: 455/1000; 1.60 sec... \tStep: 110565... \tLoss: 0.767... \tVal Loss: 0.574 \tVal Frac: 0.8210\n",
      "\tEpoch: 460/1000; 1.37 sec... \tStep: 111780... \tLoss: 0.928... \tVal Loss: 0.571 \tVal Frac: 0.8224\n",
      "\tEpoch: 465/1000; 1.49 sec... \tStep: 112995... \tLoss: 0.509... \tVal Loss: 0.584 \tVal Frac: 0.8033\n",
      "\tEpoch: 470/1000; 1.33 sec... \tStep: 114210... \tLoss: 0.547... \tVal Loss: 0.608 \tVal Frac: 0.8070\n",
      "\tEpoch: 475/1000; 1.55 sec... \tStep: 115425... \tLoss: 0.706... \tVal Loss: 0.588 \tVal Frac: 0.8114\n",
      "\tEpoch: 480/1000; 1.50 sec... \tStep: 116640... \tLoss: 0.643... \tVal Loss: 0.570 \tVal Frac: 0.8118\n",
      "\tEpoch: 485/1000; 1.37 sec... \tStep: 117855... \tLoss: 0.548... \tVal Loss: 0.578 \tVal Frac: 0.8154\n",
      "\tEpoch: 490/1000; 1.39 sec... \tStep: 119070... \tLoss: 0.781... \tVal Loss: 0.601 \tVal Frac: 0.8070\n",
      "\tEpoch: 495/1000; 1.32 sec... \tStep: 120285... \tLoss: 0.377... \tVal Loss: 0.572 \tVal Frac: 0.8132\n",
      "\tEpoch: 500/1000; 1.39 sec... \tStep: 121500... \tLoss: 0.704... \tVal Loss: 0.583 \tVal Frac: 0.7996\n",
      "\tEpoch: 505/1000; 1.52 sec... \tStep: 122715... \tLoss: 0.476... \tVal Loss: 0.568 \tVal Frac: 0.8176\n",
      "\tEpoch: 510/1000; 1.67 sec... \tStep: 123930... \tLoss: 0.485... \tVal Loss: 0.560 \tVal Frac: 0.8136\n",
      "\tValidation loss decreased (0.564 --> 0.560).  Saving model ...\n",
      "\tEpoch: 515/1000; 1.57 sec... \tStep: 125145... \tLoss: 0.996... \tVal Loss: 0.580 \tVal Frac: 0.8099\n",
      "\tEpoch: 520/1000; 1.58 sec... \tStep: 126360... \tLoss: 0.847... \tVal Loss: 0.569 \tVal Frac: 0.8143\n",
      "\tEpoch: 525/1000; 1.35 sec... \tStep: 127575... \tLoss: 0.602... \tVal Loss: 0.586 \tVal Frac: 0.8118\n",
      "\tEpoch: 530/1000; 1.42 sec... \tStep: 128790... \tLoss: 0.710... \tVal Loss: 0.594 \tVal Frac: 0.8176\n",
      "\tEpoch: 535/1000; 1.34 sec... \tStep: 130005... \tLoss: 0.749... \tVal Loss: 0.587 \tVal Frac: 0.8180\n",
      "\tEpoch: 540/1000; 1.37 sec... \tStep: 131220... \tLoss: 0.351... \tVal Loss: 0.588 \tVal Frac: 0.8077\n",
      "\tEpoch: 545/1000; 1.40 sec... \tStep: 132435... \tLoss: 0.616... \tVal Loss: 0.573 \tVal Frac: 0.8081\n",
      "\tEpoch: 550/1000; 1.40 sec... \tStep: 133650... \tLoss: 0.510... \tVal Loss: 0.594 \tVal Frac: 0.8176\n",
      "\tEpoch: 555/1000; 1.51 sec... \tStep: 134865... \tLoss: 0.662... \tVal Loss: 0.580 \tVal Frac: 0.8051\n",
      "\tEpoch: 560/1000; 1.40 sec... \tStep: 136080... \tLoss: 0.473... \tVal Loss: 0.550 \tVal Frac: 0.8169\n",
      "\tValidation loss decreased (0.560 --> 0.550).  Saving model ...\n",
      "\tEpoch: 565/1000; 1.31 sec... \tStep: 137295... \tLoss: 0.425... \tVal Loss: 0.551 \tVal Frac: 0.8261\n",
      "\tEpoch: 570/1000; 1.42 sec... \tStep: 138510... \tLoss: 0.674... \tVal Loss: 0.572 \tVal Frac: 0.8184\n",
      "\tEpoch: 575/1000; 1.35 sec... \tStep: 139725... \tLoss: 0.626... \tVal Loss: 0.559 \tVal Frac: 0.8147\n",
      "\tEpoch: 580/1000; 1.38 sec... \tStep: 140940... \tLoss: 0.772... \tVal Loss: 0.565 \tVal Frac: 0.8140\n",
      "\tEpoch: 585/1000; 1.48 sec... \tStep: 142155... \tLoss: 0.633... \tVal Loss: 0.579 \tVal Frac: 0.8114\n",
      "\tEpoch: 590/1000; 1.40 sec... \tStep: 143370... \tLoss: 0.518... \tVal Loss: 0.578 \tVal Frac: 0.8085\n",
      "\tEpoch: 595/1000; 1.46 sec... \tStep: 144585... \tLoss: 0.627... \tVal Loss: 0.559 \tVal Frac: 0.8136\n",
      "\tEpoch: 600/1000; 1.32 sec... \tStep: 145800... \tLoss: 0.471... \tVal Loss: 0.598 \tVal Frac: 0.8018\n",
      "\tEpoch: 605/1000; 1.42 sec... \tStep: 147015... \tLoss: 0.720... \tVal Loss: 0.589 \tVal Frac: 0.8026\n",
      "\tEpoch: 610/1000; 1.60 sec... \tStep: 148230... \tLoss: 0.539... \tVal Loss: 0.573 \tVal Frac: 0.8136\n",
      "\tEpoch: 615/1000; 1.40 sec... \tStep: 149445... \tLoss: 0.719... \tVal Loss: 0.586 \tVal Frac: 0.8162\n",
      "\tEpoch: 620/1000; 1.45 sec... \tStep: 150660... \tLoss: 0.970... \tVal Loss: 0.576 \tVal Frac: 0.8129\n",
      "\tEpoch: 625/1000; 1.39 sec... \tStep: 151875... \tLoss: 0.670... \tVal Loss: 0.592 \tVal Frac: 0.8096\n",
      "\tEpoch: 630/1000; 1.40 sec... \tStep: 153090... \tLoss: 0.544... \tVal Loss: 0.616 \tVal Frac: 0.7879\n",
      "\tEpoch: 635/1000; 1.52 sec... \tStep: 154305... \tLoss: 0.615... \tVal Loss: 0.586 \tVal Frac: 0.8059\n",
      "\tEpoch: 640/1000; 1.31 sec... \tStep: 155520... \tLoss: 0.563... \tVal Loss: 0.584 \tVal Frac: 0.8004\n",
      "\tEpoch: 645/1000; 1.50 sec... \tStep: 156735... \tLoss: 0.734... \tVal Loss: 0.571 \tVal Frac: 0.7971\n",
      "\tEpoch: 650/1000; 1.47 sec... \tStep: 157950... \tLoss: 0.176... \tVal Loss: 0.597 \tVal Frac: 0.7934\n",
      "\tEpoch: 655/1000; 1.29 sec... \tStep: 159165... \tLoss: 0.989... \tVal Loss: 0.566 \tVal Frac: 0.8151\n",
      "\tEpoch: 660/1000; 1.42 sec... \tStep: 160380... \tLoss: 0.728... \tVal Loss: 0.594 \tVal Frac: 0.8092\n",
      "\tEpoch: 665/1000; 1.45 sec... \tStep: 161595... \tLoss: 0.944... \tVal Loss: 0.588 \tVal Frac: 0.8018\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 0.894... \tVal Loss: 0.588 \tVal Frac: 0.8063\n",
      "\tEpoch: 675/1000; 1.30 sec... \tStep: 164025... \tLoss: 0.702... \tVal Loss: 0.605 \tVal Frac: 0.8074\n",
      "\tEpoch: 680/1000; 1.33 sec... \tStep: 165240... \tLoss: 0.454... \tVal Loss: 0.576 \tVal Frac: 0.8096\n",
      "\tEpoch: 685/1000; 1.43 sec... \tStep: 166455... \tLoss: 0.602... \tVal Loss: 0.595 \tVal Frac: 0.8048\n",
      "\tEpoch: 690/1000; 1.40 sec... \tStep: 167670... \tLoss: 0.759... \tVal Loss: 0.583 \tVal Frac: 0.8040\n",
      "\tEpoch: 695/1000; 1.39 sec... \tStep: 168885... \tLoss: 1.006... \tVal Loss: 0.592 \tVal Frac: 0.8004\n",
      "\tEpoch: 700/1000; 1.37 sec... \tStep: 170100... \tLoss: 0.768... \tVal Loss: 0.605 \tVal Frac: 0.7993\n",
      "\tEpoch: 705/1000; 1.37 sec... \tStep: 171315... \tLoss: 0.405... \tVal Loss: 0.578 \tVal Frac: 0.8026\n",
      "\tEpoch: 710/1000; 1.54 sec... \tStep: 172530... \tLoss: 0.920... \tVal Loss: 0.591 \tVal Frac: 0.7937\n",
      "\tEpoch: 715/1000; 1.40 sec... \tStep: 173745... \tLoss: 0.617... \tVal Loss: 0.575 \tVal Frac: 0.8088\n",
      "\tEpoch: 720/1000; 1.29 sec... \tStep: 174960... \tLoss: 0.866... \tVal Loss: 0.571 \tVal Frac: 0.8118\n",
      "\tEpoch: 725/1000; 1.41 sec... \tStep: 176175... \tLoss: 0.543... \tVal Loss: 0.577 \tVal Frac: 0.8059\n",
      "\tEpoch: 730/1000; 1.36 sec... \tStep: 177390... \tLoss: 0.946... \tVal Loss: 0.592 \tVal Frac: 0.8040\n",
      "\tEpoch: 735/1000; 1.34 sec... \tStep: 178605... \tLoss: 0.788... \tVal Loss: 0.577 \tVal Frac: 0.8213\n",
      "\tEpoch: 740/1000; 1.47 sec... \tStep: 179820... \tLoss: 0.587... \tVal Loss: 0.597 \tVal Frac: 0.7926\n",
      "\tEpoch: 745/1000; 1.37 sec... \tStep: 181035... \tLoss: 0.941... \tVal Loss: 0.574 \tVal Frac: 0.8143\n",
      "\tEpoch: 750/1000; 1.38 sec... \tStep: 182250... \tLoss: 0.562... \tVal Loss: 0.562 \tVal Frac: 0.8276\n",
      "\tEpoch: 755/1000; 1.29 sec... \tStep: 183465... \tLoss: 0.688... \tVal Loss: 0.565 \tVal Frac: 0.8125\n",
      "\tEpoch: 760/1000; 1.59 sec... \tStep: 184680... \tLoss: 0.548... \tVal Loss: 0.594 \tVal Frac: 0.8033\n",
      "\tEpoch: 765/1000; 1.28 sec... \tStep: 185895... \tLoss: 0.663... \tVal Loss: 0.569 \tVal Frac: 0.8022\n",
      "\tEpoch: 770/1000; 1.40 sec... \tStep: 187110... \tLoss: 0.635... \tVal Loss: 0.575 \tVal Frac: 0.8092\n",
      "\tEpoch: 775/1000; 1.45 sec... \tStep: 188325... \tLoss: 0.632... \tVal Loss: 0.562 \tVal Frac: 0.8099\n",
      "\tEpoch: 780/1000; 1.47 sec... \tStep: 189540... \tLoss: 0.857... \tVal Loss: 0.584 \tVal Frac: 0.8007\n",
      "\tEpoch: 785/1000; 1.40 sec... \tStep: 190755... \tLoss: 0.981... \tVal Loss: 0.572 \tVal Frac: 0.8136\n",
      "\tEpoch: 790/1000; 1.37 sec... \tStep: 191970... \tLoss: 0.715... \tVal Loss: 0.574 \tVal Frac: 0.8162\n",
      "\tEpoch: 795/1000; 1.42 sec... \tStep: 193185... \tLoss: 0.896... \tVal Loss: 0.569 \tVal Frac: 0.8210\n",
      "\tEpoch: 800/1000; 1.47 sec... \tStep: 194400... \tLoss: 0.364... \tVal Loss: 0.568 \tVal Frac: 0.8132\n",
      "\tEpoch: 805/1000; 1.44 sec... \tStep: 195615... \tLoss: 0.830... \tVal Loss: 0.546 \tVal Frac: 0.8279\n",
      "\tValidation loss decreased (0.550 --> 0.546).  Saving model ...\n",
      "\tEpoch: 810/1000; 1.55 sec... \tStep: 196830... \tLoss: 0.853... \tVal Loss: 0.565 \tVal Frac: 0.8129\n",
      "\tEpoch: 815/1000; 1.35 sec... \tStep: 198045... \tLoss: 0.530... \tVal Loss: 0.587 \tVal Frac: 0.8018\n",
      "\tEpoch: 820/1000; 1.43 sec... \tStep: 199260... \tLoss: 0.855... \tVal Loss: 0.618 \tVal Frac: 0.8151\n",
      "\tEpoch: 825/1000; 1.43 sec... \tStep: 200475... \tLoss: 0.623... \tVal Loss: 0.612 \tVal Frac: 0.8026\n",
      "\tEpoch: 830/1000; 1.43 sec... \tStep: 201690... \tLoss: 0.813... \tVal Loss: 0.574 \tVal Frac: 0.8110\n",
      "\tEpoch: 835/1000; 1.47 sec... \tStep: 202905... \tLoss: 0.550... \tVal Loss: 0.573 \tVal Frac: 0.7934\n",
      "\tEpoch: 840/1000; 1.43 sec... \tStep: 204120... \tLoss: 0.948... \tVal Loss: 0.574 \tVal Frac: 0.7989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 845/1000; 1.47 sec... \tStep: 205335... \tLoss: 0.844... \tVal Loss: 0.598 \tVal Frac: 0.7934\n",
      "\tEpoch: 850/1000; 1.47 sec... \tStep: 206550... \tLoss: 0.705... \tVal Loss: 0.588 \tVal Frac: 0.8004\n",
      "\tEpoch: 855/1000; 1.53 sec... \tStep: 207765... \tLoss: 0.764... \tVal Loss: 0.587 \tVal Frac: 0.7971\n",
      "\tEpoch: 860/1000; 1.33 sec... \tStep: 208980... \tLoss: 0.672... \tVal Loss: 0.587 \tVal Frac: 0.7967\n",
      "\tEpoch: 865/1000; 1.46 sec... \tStep: 210195... \tLoss: 0.388... \tVal Loss: 0.585 \tVal Frac: 0.7978\n",
      "\tEpoch: 870/1000; 1.36 sec... \tStep: 211410... \tLoss: 0.898... \tVal Loss: 0.587 \tVal Frac: 0.7996\n",
      "\tEpoch: 875/1000; 1.50 sec... \tStep: 212625... \tLoss: 0.483... \tVal Loss: 0.588 \tVal Frac: 0.8151\n",
      "\tEpoch: 880/1000; 1.52 sec... \tStep: 213840... \tLoss: 0.655... \tVal Loss: 0.599 \tVal Frac: 0.7945\n",
      "\tEpoch: 885/1000; 1.25 sec... \tStep: 215055... \tLoss: 0.558... \tVal Loss: 0.571 \tVal Frac: 0.8217\n",
      "\tEpoch: 890/1000; 1.50 sec... \tStep: 216270... \tLoss: 0.585... \tVal Loss: 0.567 \tVal Frac: 0.8162\n",
      "\tEpoch: 895/1000; 1.42 sec... \tStep: 217485... \tLoss: 0.796... \tVal Loss: 0.583 \tVal Frac: 0.8048\n",
      "\tEpoch: 900/1000; 1.40 sec... \tStep: 218700... \tLoss: 0.836... \tVal Loss: 0.590 \tVal Frac: 0.8015\n",
      "\tEpoch: 905/1000; 1.49 sec... \tStep: 219915... \tLoss: 0.586... \tVal Loss: 0.558 \tVal Frac: 0.8147\n",
      "\tEpoch: 910/1000; 1.49 sec... \tStep: 221130... \tLoss: 0.547... \tVal Loss: 0.560 \tVal Frac: 0.8272\n",
      "\tEpoch: 915/1000; 1.42 sec... \tStep: 222345... \tLoss: 0.505... \tVal Loss: 0.557 \tVal Frac: 0.8191\n",
      "\tEpoch: 920/1000; 1.29 sec... \tStep: 223560... \tLoss: 0.679... \tVal Loss: 0.565 \tVal Frac: 0.8213\n",
      "\tEpoch: 925/1000; 1.50 sec... \tStep: 224775... \tLoss: 0.561... \tVal Loss: 0.602 \tVal Frac: 0.8026\n",
      "\tEpoch: 930/1000; 1.47 sec... \tStep: 225990... \tLoss: 0.882... \tVal Loss: 0.575 \tVal Frac: 0.7824\n",
      "\tEpoch: 935/1000; 1.44 sec... \tStep: 227205... \tLoss: 0.703... \tVal Loss: 0.593 \tVal Frac: 0.8007\n",
      "\tEpoch: 940/1000; 1.40 sec... \tStep: 228420... \tLoss: 0.465... \tVal Loss: 0.564 \tVal Frac: 0.8154\n",
      "\tEpoch: 945/1000; 1.49 sec... \tStep: 229635... \tLoss: 0.298... \tVal Loss: 0.587 \tVal Frac: 0.8048\n",
      "\tEpoch: 950/1000; 1.49 sec... \tStep: 230850... \tLoss: 0.718... \tVal Loss: 0.570 \tVal Frac: 0.8206\n",
      "\tEpoch: 955/1000; 1.40 sec... \tStep: 232065... \tLoss: 0.455... \tVal Loss: 0.572 \tVal Frac: 0.8213\n",
      "\tEpoch: 960/1000; 1.29 sec... \tStep: 233280... \tLoss: 0.447... \tVal Loss: 0.581 \tVal Frac: 0.8103\n",
      "\tEpoch: 965/1000; 1.45 sec... \tStep: 234495... \tLoss: 0.513... \tVal Loss: 0.553 \tVal Frac: 0.8257\n",
      "\tEpoch: 970/1000; 1.37 sec... \tStep: 235710... \tLoss: 0.338... \tVal Loss: 0.566 \tVal Frac: 0.8243\n",
      "\tEpoch: 975/1000; 1.46 sec... \tStep: 236925... \tLoss: 0.579... \tVal Loss: 0.566 \tVal Frac: 0.8162\n",
      "\tEpoch: 980/1000; 1.49 sec... \tStep: 238140... \tLoss: 0.596... \tVal Loss: 0.563 \tVal Frac: 0.8184\n",
      "\tEpoch: 985/1000; 1.42 sec... \tStep: 239355... \tLoss: 0.443... \tVal Loss: 0.554 \tVal Frac: 0.8184\n",
      "\tEpoch: 990/1000; 1.41 sec... \tStep: 240570... \tLoss: 0.382... \tVal Loss: 0.553 \tVal Frac: 0.8239\n",
      "\tEpoch: 995/1000; 1.47 sec... \tStep: 241785... \tLoss: 0.733... \tVal Loss: 0.547 \tVal Frac: 0.8268\n",
      "\tEpoch: 1000/1000; 1.33 sec... \tStep: 243000... \tLoss: 1.078... \tVal Loss: 0.545 \tVal Frac: 0.8246\n",
      "\tValidation loss decreased (0.546 --> 0.545).  Saving model ...\n",
      "Completed:  hidden_dim :  100 \n",
      "\tloss: 0.545355 \n",
      "\tfrac:0.827941\n",
      "\n",
      "###########\n",
      "Testing with  n_layers_LSTM :  2\n",
      "\tEpoch: 5/1000; 1.93 sec... \tStep: 1215... \tLoss: 0.577... \tVal Loss: 0.522 \tVal Frac: 0.8449\n",
      "\tValidation loss decreased (inf --> 0.522).  Saving model ...\n",
      "\tEpoch: 10/1000; 2.12 sec... \tStep: 2430... \tLoss: 0.700... \tVal Loss: 0.486 \tVal Frac: 0.8544\n",
      "\tValidation loss decreased (0.522 --> 0.486).  Saving model ...\n",
      "\tEpoch: 15/1000; 2.07 sec... \tStep: 3645... \tLoss: 0.638... \tVal Loss: 0.471 \tVal Frac: 0.8596\n",
      "\tValidation loss decreased (0.486 --> 0.471).  Saving model ...\n",
      "\tEpoch: 20/1000; 2.02 sec... \tStep: 4860... \tLoss: 0.530... \tVal Loss: 0.462 \tVal Frac: 0.8596\n",
      "\tValidation loss decreased (0.471 --> 0.462).  Saving model ...\n",
      "\tEpoch: 25/1000; 2.00 sec... \tStep: 6075... \tLoss: 0.480... \tVal Loss: 0.415 \tVal Frac: 0.8702\n",
      "\tValidation loss decreased (0.462 --> 0.415).  Saving model ...\n",
      "\tEpoch: 30/1000; 2.00 sec... \tStep: 7290... \tLoss: 0.296... \tVal Loss: 0.434 \tVal Frac: 0.8710\n",
      "\tEpoch: 35/1000; 2.07 sec... \tStep: 8505... \tLoss: 0.273... \tVal Loss: 0.406 \tVal Frac: 0.8754\n",
      "\tValidation loss decreased (0.415 --> 0.406).  Saving model ...\n",
      "\tEpoch: 40/1000; 2.06 sec... \tStep: 9720... \tLoss: 0.208... \tVal Loss: 0.473 \tVal Frac: 0.8706\n",
      "\tEpoch: 45/1000; 2.04 sec... \tStep: 10935... \tLoss: 0.227... \tVal Loss: 0.439 \tVal Frac: 0.8614\n",
      "\tEpoch: 50/1000; 2.13 sec... \tStep: 12150... \tLoss: 0.255... \tVal Loss: 0.433 \tVal Frac: 0.8621\n",
      "\tEpoch: 55/1000; 2.09 sec... \tStep: 13365... \tLoss: 1.118... \tVal Loss: 0.408 \tVal Frac: 0.8779\n",
      "\tEpoch: 60/1000; 2.00 sec... \tStep: 14580... \tLoss: 0.580... \tVal Loss: 0.426 \tVal Frac: 0.8743\n",
      "\tEpoch: 65/1000; 2.03 sec... \tStep: 15795... \tLoss: 0.336... \tVal Loss: 0.420 \tVal Frac: 0.8864\n",
      "\tEpoch: 70/1000; 2.02 sec... \tStep: 17010... \tLoss: 0.209... \tVal Loss: 0.415 \tVal Frac: 0.8761\n",
      "\tEpoch: 75/1000; 2.11 sec... \tStep: 18225... \tLoss: 0.484... \tVal Loss: 0.400 \tVal Frac: 0.8746\n",
      "\tValidation loss decreased (0.406 --> 0.400).  Saving model ...\n",
      "\tEpoch: 80/1000; 2.14 sec... \tStep: 19440... \tLoss: 0.620... \tVal Loss: 0.440 \tVal Frac: 0.8721\n",
      "\tEpoch: 85/1000; 2.06 sec... \tStep: 20655... \tLoss: 0.220... \tVal Loss: 0.415 \tVal Frac: 0.8783\n",
      "\tEpoch: 90/1000; 2.06 sec... \tStep: 21870... \tLoss: 0.395... \tVal Loss: 0.460 \tVal Frac: 0.8680\n",
      "\tEpoch: 95/1000; 1.96 sec... \tStep: 23085... \tLoss: 0.077... \tVal Loss: 0.463 \tVal Frac: 0.8662\n",
      "\tEpoch: 100/1000; 1.95 sec... \tStep: 24300... \tLoss: 0.421... \tVal Loss: 0.406 \tVal Frac: 0.8809\n",
      "\tEpoch: 105/1000; 1.97 sec... \tStep: 25515... \tLoss: 0.180... \tVal Loss: 0.404 \tVal Frac: 0.8779\n",
      "\tEpoch: 110/1000; 2.08 sec... \tStep: 26730... \tLoss: 0.658... \tVal Loss: 0.456 \tVal Frac: 0.8643\n",
      "\tEpoch: 115/1000; 2.10 sec... \tStep: 27945... \tLoss: 0.392... \tVal Loss: 0.447 \tVal Frac: 0.8614\n",
      "\tEpoch: 120/1000; 2.00 sec... \tStep: 29160... \tLoss: 0.386... \tVal Loss: 0.438 \tVal Frac: 0.8662\n",
      "\tEpoch: 125/1000; 2.00 sec... \tStep: 30375... \tLoss: 0.463... \tVal Loss: 0.536 \tVal Frac: 0.8265\n",
      "\tEpoch: 130/1000; 2.05 sec... \tStep: 31590... \tLoss: 0.157... \tVal Loss: 0.529 \tVal Frac: 0.8515\n",
      "\tEpoch: 135/1000; 1.97 sec... \tStep: 32805... \tLoss: 0.423... \tVal Loss: 0.483 \tVal Frac: 0.8548\n",
      "\tEpoch: 140/1000; 2.07 sec... \tStep: 34020... \tLoss: 0.548... \tVal Loss: 0.439 \tVal Frac: 0.8581\n",
      "\tEpoch: 145/1000; 1.98 sec... \tStep: 35235... \tLoss: 0.656... \tVal Loss: 0.567 \tVal Frac: 0.8246\n",
      "\tEpoch: 150/1000; 2.12 sec... \tStep: 36450... \tLoss: 0.888... \tVal Loss: 0.546 \tVal Frac: 0.8287\n",
      "\tEpoch: 155/1000; 2.11 sec... \tStep: 37665... \tLoss: 0.543... \tVal Loss: 0.548 \tVal Frac: 0.8515\n",
      "\tEpoch: 160/1000; 2.08 sec... \tStep: 38880... \tLoss: 0.634... \tVal Loss: 0.530 \tVal Frac: 0.8430\n",
      "\tEpoch: 165/1000; 2.07 sec... \tStep: 40095... \tLoss: 0.403... \tVal Loss: 0.466 \tVal Frac: 0.8618\n",
      "\tEpoch: 170/1000; 2.00 sec... \tStep: 41310... \tLoss: 0.421... \tVal Loss: 0.448 \tVal Frac: 0.8651\n",
      "\tEpoch: 175/1000; 2.04 sec... \tStep: 42525... \tLoss: 0.500... \tVal Loss: 0.462 \tVal Frac: 0.8551\n",
      "\tEpoch: 180/1000; 2.03 sec... \tStep: 43740... \tLoss: 0.375... \tVal Loss: 0.484 \tVal Frac: 0.8526\n",
      "\tEpoch: 185/1000; 1.97 sec... \tStep: 44955... \tLoss: 0.703... \tVal Loss: 0.444 \tVal Frac: 0.8721\n",
      "\tEpoch: 190/1000; 2.08 sec... \tStep: 46170... \tLoss: 0.235... \tVal Loss: 0.406 \tVal Frac: 0.8761\n",
      "\tEpoch: 195/1000; 1.97 sec... \tStep: 47385... \tLoss: 0.318... \tVal Loss: 0.433 \tVal Frac: 0.8739\n",
      "\tEpoch: 200/1000; 2.06 sec... \tStep: 48600... \tLoss: 0.312... \tVal Loss: 0.441 \tVal Frac: 0.8706\n",
      "\tEpoch: 205/1000; 1.99 sec... \tStep: 49815... \tLoss: 0.524... \tVal Loss: 0.441 \tVal Frac: 0.8746\n",
      "\tEpoch: 210/1000; 1.99 sec... \tStep: 51030... \tLoss: 0.602... \tVal Loss: 0.475 \tVal Frac: 0.8721\n",
      "\tEpoch: 215/1000; 2.07 sec... \tStep: 52245... \tLoss: 0.865... \tVal Loss: 0.500 \tVal Frac: 0.8518\n",
      "\tEpoch: 220/1000; 2.06 sec... \tStep: 53460... \tLoss: 0.421... \tVal Loss: 0.559 \tVal Frac: 0.8426\n",
      "\tEpoch: 225/1000; 2.19 sec... \tStep: 54675... \tLoss: 0.240... \tVal Loss: 0.501 \tVal Frac: 0.8507\n",
      "\tEpoch: 230/1000; 1.93 sec... \tStep: 55890... \tLoss: 0.743... \tVal Loss: 0.469 \tVal Frac: 0.8640\n",
      "\tEpoch: 235/1000; 2.05 sec... \tStep: 57105... \tLoss: 0.676... \tVal Loss: 0.442 \tVal Frac: 0.8688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 240/1000; 2.11 sec... \tStep: 58320... \tLoss: 0.322... \tVal Loss: 0.454 \tVal Frac: 0.8732\n",
      "\tEpoch: 245/1000; 2.00 sec... \tStep: 59535... \tLoss: 0.388... \tVal Loss: 0.463 \tVal Frac: 0.8629\n",
      "\tEpoch: 250/1000; 2.12 sec... \tStep: 60750... \tLoss: 0.749... \tVal Loss: 0.499 \tVal Frac: 0.8574\n",
      "\tEpoch: 255/1000; 2.21 sec... \tStep: 61965... \tLoss: 0.437... \tVal Loss: 0.487 \tVal Frac: 0.8581\n",
      "\tEpoch: 260/1000; 1.94 sec... \tStep: 63180... \tLoss: 0.376... \tVal Loss: 0.452 \tVal Frac: 0.8695\n",
      "\tEpoch: 265/1000; 2.05 sec... \tStep: 64395... \tLoss: 0.403... \tVal Loss: 0.439 \tVal Frac: 0.8621\n",
      "\tEpoch: 270/1000; 2.07 sec... \tStep: 65610... \tLoss: 0.819... \tVal Loss: 0.505 \tVal Frac: 0.8382\n",
      "\tEpoch: 275/1000; 1.98 sec... \tStep: 66825... \tLoss: 0.661... \tVal Loss: 0.492 \tVal Frac: 0.8570\n",
      "\tEpoch: 280/1000; 1.98 sec... \tStep: 68040... \tLoss: 0.474... \tVal Loss: 0.504 \tVal Frac: 0.8397\n",
      "\tEpoch: 285/1000; 1.97 sec... \tStep: 69255... \tLoss: 0.491... \tVal Loss: 0.451 \tVal Frac: 0.8629\n",
      "\tEpoch: 290/1000; 2.00 sec... \tStep: 70470... \tLoss: 0.382... \tVal Loss: 0.441 \tVal Frac: 0.8596\n",
      "\tEpoch: 295/1000; 2.15 sec... \tStep: 71685... \tLoss: 0.465... \tVal Loss: 0.457 \tVal Frac: 0.8596\n",
      "\tEpoch: 300/1000; 2.03 sec... \tStep: 72900... \tLoss: 0.542... \tVal Loss: 0.545 \tVal Frac: 0.8338\n",
      "\tEpoch: 305/1000; 2.09 sec... \tStep: 74115... \tLoss: 0.320... \tVal Loss: 0.494 \tVal Frac: 0.8496\n",
      "\tEpoch: 310/1000; 2.06 sec... \tStep: 75330... \tLoss: 0.468... \tVal Loss: 0.572 \tVal Frac: 0.8210\n",
      "\tEpoch: 315/1000; 2.12 sec... \tStep: 76545... \tLoss: 0.497... \tVal Loss: 0.464 \tVal Frac: 0.8544\n",
      "\tEpoch: 320/1000; 2.07 sec... \tStep: 77760... \tLoss: 0.610... \tVal Loss: 0.468 \tVal Frac: 0.8574\n",
      "\tEpoch: 325/1000; 2.11 sec... \tStep: 78975... \tLoss: 0.711... \tVal Loss: 0.567 \tVal Frac: 0.8063\n",
      "\tEpoch: 330/1000; 2.02 sec... \tStep: 80190... \tLoss: 0.670... \tVal Loss: 0.579 \tVal Frac: 0.8151\n",
      "\tEpoch: 335/1000; 2.03 sec... \tStep: 81405... \tLoss: 0.493... \tVal Loss: 0.566 \tVal Frac: 0.8272\n",
      "\tEpoch: 340/1000; 2.09 sec... \tStep: 82620... \tLoss: 0.536... \tVal Loss: 0.518 \tVal Frac: 0.8426\n",
      "\tEpoch: 345/1000; 2.02 sec... \tStep: 83835... \tLoss: 0.394... \tVal Loss: 0.522 \tVal Frac: 0.8397\n",
      "\tEpoch: 350/1000; 2.17 sec... \tStep: 85050... \tLoss: 0.428... \tVal Loss: 0.529 \tVal Frac: 0.8272\n",
      "\tEpoch: 355/1000; 2.05 sec... \tStep: 86265... \tLoss: 1.098... \tVal Loss: 0.636 \tVal Frac: 0.8129\n",
      "\tEpoch: 360/1000; 2.13 sec... \tStep: 87480... \tLoss: 0.590... \tVal Loss: 0.552 \tVal Frac: 0.8265\n",
      "\tEpoch: 365/1000; 2.05 sec... \tStep: 88695... \tLoss: 0.646... \tVal Loss: 0.586 \tVal Frac: 0.8199\n",
      "\tEpoch: 370/1000; 2.15 sec... \tStep: 89910... \tLoss: 0.581... \tVal Loss: 0.641 \tVal Frac: 0.8033\n",
      "\tEpoch: 375/1000; 2.14 sec... \tStep: 91125... \tLoss: 0.735... \tVal Loss: 0.624 \tVal Frac: 0.8118\n",
      "\tEpoch: 380/1000; 2.11 sec... \tStep: 92340... \tLoss: 1.183... \tVal Loss: 0.661 \tVal Frac: 0.7945\n",
      "\tEpoch: 385/1000; 1.97 sec... \tStep: 93555... \tLoss: 0.624... \tVal Loss: 0.646 \tVal Frac: 0.8096\n",
      "\tEpoch: 390/1000; 2.14 sec... \tStep: 94770... \tLoss: 0.793... \tVal Loss: 0.666 \tVal Frac: 0.7956\n",
      "\tEpoch: 395/1000; 2.00 sec... \tStep: 95985... \tLoss: 0.532... \tVal Loss: 0.638 \tVal Frac: 0.8007\n",
      "\tEpoch: 400/1000; 2.02 sec... \tStep: 97200... \tLoss: 0.882... \tVal Loss: 0.635 \tVal Frac: 0.7989\n",
      "\tEpoch: 405/1000; 2.04 sec... \tStep: 98415... \tLoss: 0.850... \tVal Loss: 0.674 \tVal Frac: 0.7879\n",
      "\tEpoch: 410/1000; 2.07 sec... \tStep: 99630... \tLoss: 0.685... \tVal Loss: 0.690 \tVal Frac: 0.7820\n",
      "\tEpoch: 415/1000; 2.21 sec... \tStep: 100845... \tLoss: 1.144... \tVal Loss: 0.644 \tVal Frac: 0.7824\n",
      "\tEpoch: 420/1000; 2.02 sec... \tStep: 102060... \tLoss: 1.600... \tVal Loss: 0.685 \tVal Frac: 0.7757\n",
      "\tEpoch: 425/1000; 2.27 sec... \tStep: 103275... \tLoss: 0.910... \tVal Loss: 0.667 \tVal Frac: 0.7827\n",
      "\tEpoch: 430/1000; 2.10 sec... \tStep: 104490... \tLoss: 0.568... \tVal Loss: 0.692 \tVal Frac: 0.7846\n",
      "\tEpoch: 435/1000; 2.07 sec... \tStep: 105705... \tLoss: 0.690... \tVal Loss: 0.667 \tVal Frac: 0.7871\n",
      "\tEpoch: 440/1000; 1.99 sec... \tStep: 106920... \tLoss: 0.980... \tVal Loss: 0.666 \tVal Frac: 0.7960\n",
      "\tEpoch: 445/1000; 2.06 sec... \tStep: 108135... \tLoss: 0.807... \tVal Loss: 0.784 \tVal Frac: 0.7533\n",
      "\tEpoch: 450/1000; 1.95 sec... \tStep: 109350... \tLoss: 0.804... \tVal Loss: 0.776 \tVal Frac: 0.7419\n",
      "\tEpoch: 455/1000; 2.11 sec... \tStep: 110565... \tLoss: 0.699... \tVal Loss: 0.827 \tVal Frac: 0.7327\n",
      "\tEpoch: 460/1000; 2.12 sec... \tStep: 111780... \tLoss: 1.136... \tVal Loss: 0.767 \tVal Frac: 0.7482\n",
      "\tEpoch: 465/1000; 2.08 sec... \tStep: 112995... \tLoss: 1.023... \tVal Loss: 0.709 \tVal Frac: 0.7577\n",
      "\tEpoch: 470/1000; 2.04 sec... \tStep: 114210... \tLoss: 0.910... \tVal Loss: 0.689 \tVal Frac: 0.7669\n",
      "\tEpoch: 475/1000; 2.09 sec... \tStep: 115425... \tLoss: 0.770... \tVal Loss: 0.761 \tVal Frac: 0.7603\n",
      "\tEpoch: 480/1000; 2.15 sec... \tStep: 116640... \tLoss: 0.828... \tVal Loss: 0.924 \tVal Frac: 0.7140\n",
      "\tEpoch: 485/1000; 2.04 sec... \tStep: 117855... \tLoss: 1.135... \tVal Loss: 0.821 \tVal Frac: 0.7386\n",
      "\tEpoch: 490/1000; 2.02 sec... \tStep: 119070... \tLoss: 0.770... \tVal Loss: 0.790 \tVal Frac: 0.7574\n",
      "\tEpoch: 495/1000; 2.12 sec... \tStep: 120285... \tLoss: 0.735... \tVal Loss: 0.747 \tVal Frac: 0.7493\n",
      "\tEpoch: 500/1000; 1.95 sec... \tStep: 121500... \tLoss: 1.089... \tVal Loss: 0.995 \tVal Frac: 0.7007\n",
      "\tEpoch: 505/1000; 2.14 sec... \tStep: 122715... \tLoss: 1.286... \tVal Loss: 0.974 \tVal Frac: 0.6879\n",
      "\tEpoch: 510/1000; 2.24 sec... \tStep: 123930... \tLoss: 0.818... \tVal Loss: 0.844 \tVal Frac: 0.7349\n",
      "\tEpoch: 515/1000; 2.21 sec... \tStep: 125145... \tLoss: 0.846... \tVal Loss: 0.806 \tVal Frac: 0.7463\n",
      "\tEpoch: 520/1000; 2.12 sec... \tStep: 126360... \tLoss: 0.913... \tVal Loss: 0.811 \tVal Frac: 0.7301\n",
      "\tEpoch: 525/1000; 2.09 sec... \tStep: 127575... \tLoss: 0.551... \tVal Loss: 0.808 \tVal Frac: 0.7643\n",
      "\tEpoch: 530/1000; 2.14 sec... \tStep: 128790... \tLoss: 0.865... \tVal Loss: 0.780 \tVal Frac: 0.7654\n",
      "\tEpoch: 535/1000; 2.09 sec... \tStep: 130005... \tLoss: 0.588... \tVal Loss: 0.787 \tVal Frac: 0.7397\n",
      "\tEpoch: 540/1000; 2.07 sec... \tStep: 131220... \tLoss: 1.116... \tVal Loss: 0.923 \tVal Frac: 0.7140\n",
      "\tEpoch: 545/1000; 2.00 sec... \tStep: 132435... \tLoss: 0.637... \tVal Loss: 0.998 \tVal Frac: 0.6728\n",
      "\tEpoch: 550/1000; 2.22 sec... \tStep: 133650... \tLoss: 0.977... \tVal Loss: 0.968 \tVal Frac: 0.6982\n",
      "\tEpoch: 555/1000; 2.09 sec... \tStep: 134865... \tLoss: 1.056... \tVal Loss: 0.914 \tVal Frac: 0.7254\n",
      "\tEpoch: 560/1000; 2.07 sec... \tStep: 136080... \tLoss: 0.729... \tVal Loss: 0.894 \tVal Frac: 0.7125\n",
      "\tEpoch: 565/1000; 2.11 sec... \tStep: 137295... \tLoss: 1.281... \tVal Loss: 0.856 \tVal Frac: 0.7191\n",
      "\tEpoch: 570/1000; 2.17 sec... \tStep: 138510... \tLoss: 0.831... \tVal Loss: 0.777 \tVal Frac: 0.7412\n",
      "\tEpoch: 575/1000; 2.09 sec... \tStep: 139725... \tLoss: 1.621... \tVal Loss: 1.078 \tVal Frac: 0.6180\n",
      "\tEpoch: 580/1000; 2.15 sec... \tStep: 140940... \tLoss: 0.772... \tVal Loss: 0.925 \tVal Frac: 0.7037\n",
      "\tEpoch: 585/1000; 2.05 sec... \tStep: 142155... \tLoss: 1.079... \tVal Loss: 0.891 \tVal Frac: 0.7254\n",
      "\tEpoch: 590/1000; 2.12 sec... \tStep: 143370... \tLoss: 0.916... \tVal Loss: 0.864 \tVal Frac: 0.7331\n",
      "\tEpoch: 595/1000; 2.05 sec... \tStep: 144585... \tLoss: 0.667... \tVal Loss: 0.926 \tVal Frac: 0.6695\n",
      "\tEpoch: 600/1000; 2.06 sec... \tStep: 145800... \tLoss: 0.855... \tVal Loss: 0.899 \tVal Frac: 0.7250\n",
      "\tEpoch: 605/1000; 2.16 sec... \tStep: 147015... \tLoss: 1.125... \tVal Loss: 0.848 \tVal Frac: 0.7441\n",
      "\tEpoch: 610/1000; 2.35 sec... \tStep: 148230... \tLoss: 1.229... \tVal Loss: 0.772 \tVal Frac: 0.7522\n",
      "\tEpoch: 615/1000; 2.09 sec... \tStep: 149445... \tLoss: 1.295... \tVal Loss: 0.898 \tVal Frac: 0.7066\n",
      "\tEpoch: 620/1000; 2.06 sec... \tStep: 150660... \tLoss: 1.259... \tVal Loss: 0.807 \tVal Frac: 0.7342\n",
      "\tEpoch: 625/1000; 2.32 sec... \tStep: 151875... \tLoss: 1.114... \tVal Loss: 0.853 \tVal Frac: 0.7312\n",
      "\tEpoch: 630/1000; 2.09 sec... \tStep: 153090... \tLoss: 1.082... \tVal Loss: 0.961 \tVal Frac: 0.7011\n",
      "\tEpoch: 635/1000; 2.14 sec... \tStep: 154305... \tLoss: 1.283... \tVal Loss: 0.905 \tVal Frac: 0.7309\n",
      "\tEpoch: 640/1000; 2.17 sec... \tStep: 155520... \tLoss: 0.984... \tVal Loss: 0.906 \tVal Frac: 0.7375\n",
      "\tEpoch: 645/1000; 2.24 sec... \tStep: 156735... \tLoss: 1.100... \tVal Loss: 1.057 \tVal Frac: 0.6651\n",
      "\tEpoch: 650/1000; 2.04 sec... \tStep: 157950... \tLoss: 1.119... \tVal Loss: 0.854 \tVal Frac: 0.7371\n",
      "\tEpoch: 655/1000; 2.07 sec... \tStep: 159165... \tLoss: 0.707... \tVal Loss: 0.861 \tVal Frac: 0.7316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 660/1000; 2.24 sec... \tStep: 160380... \tLoss: 1.236... \tVal Loss: 0.930 \tVal Frac: 0.7107\n",
      "\tEpoch: 665/1000; 2.11 sec... \tStep: 161595... \tLoss: 0.881... \tVal Loss: 0.957 \tVal Frac: 0.6960\n",
      "\tEpoch: 670/1000; 1.97 sec... \tStep: 162810... \tLoss: 1.266... \tVal Loss: 0.982 \tVal Frac: 0.7202\n",
      "\tEpoch: 675/1000; 2.06 sec... \tStep: 164025... \tLoss: 1.088... \tVal Loss: 0.918 \tVal Frac: 0.7419\n",
      "\tEpoch: 680/1000; 1.99 sec... \tStep: 165240... \tLoss: 0.795... \tVal Loss: 0.955 \tVal Frac: 0.7158\n",
      "\tEpoch: 685/1000; 2.05 sec... \tStep: 166455... \tLoss: 0.686... \tVal Loss: 0.894 \tVal Frac: 0.7257\n",
      "\tEpoch: 690/1000; 2.09 sec... \tStep: 167670... \tLoss: 1.287... \tVal Loss: 1.011 \tVal Frac: 0.7346\n",
      "\tEpoch: 695/1000; 2.03 sec... \tStep: 168885... \tLoss: 1.220... \tVal Loss: 1.025 \tVal Frac: 0.7000\n",
      "\tEpoch: 700/1000; 2.11 sec... \tStep: 170100... \tLoss: 1.061... \tVal Loss: 1.046 \tVal Frac: 0.6360\n",
      "\tEpoch: 705/1000; 2.09 sec... \tStep: 171315... \tLoss: 1.094... \tVal Loss: 0.993 \tVal Frac: 0.6673\n",
      "\tEpoch: 710/1000; 2.42 sec... \tStep: 172530... \tLoss: 0.865... \tVal Loss: 0.898 \tVal Frac: 0.7228\n",
      "\tEpoch: 715/1000; 2.05 sec... \tStep: 173745... \tLoss: 0.902... \tVal Loss: 0.893 \tVal Frac: 0.7309\n",
      "\tEpoch: 720/1000; 2.11 sec... \tStep: 174960... \tLoss: 0.980... \tVal Loss: 0.971 \tVal Frac: 0.7074\n",
      "\tEpoch: 725/1000; 2.07 sec... \tStep: 176175... \tLoss: 0.848... \tVal Loss: 1.021 \tVal Frac: 0.7254\n",
      "\tEpoch: 730/1000; 2.19 sec... \tStep: 177390... \tLoss: 0.766... \tVal Loss: 0.942 \tVal Frac: 0.7375\n",
      "\tEpoch: 735/1000; 2.02 sec... \tStep: 178605... \tLoss: 1.034... \tVal Loss: 0.913 \tVal Frac: 0.7312\n",
      "\tEpoch: 740/1000; 2.05 sec... \tStep: 179820... \tLoss: 0.916... \tVal Loss: 0.980 \tVal Frac: 0.6974\n",
      "\tEpoch: 745/1000; 2.17 sec... \tStep: 181035... \tLoss: 0.784... \tVal Loss: 0.892 \tVal Frac: 0.7430\n",
      "\tEpoch: 750/1000; 1.97 sec... \tStep: 182250... \tLoss: 1.012... \tVal Loss: 0.892 \tVal Frac: 0.7213\n",
      "\tEpoch: 755/1000; 2.11 sec... \tStep: 183465... \tLoss: 0.993... \tVal Loss: 0.876 \tVal Frac: 0.7393\n",
      "\tEpoch: 760/1000; 2.09 sec... \tStep: 184680... \tLoss: 0.707... \tVal Loss: 0.994 \tVal Frac: 0.7114\n",
      "\tEpoch: 765/1000; 2.05 sec... \tStep: 185895... \tLoss: 1.434... \tVal Loss: 0.922 \tVal Frac: 0.7364\n",
      "\tEpoch: 770/1000; 2.15 sec... \tStep: 187110... \tLoss: 0.841... \tVal Loss: 0.859 \tVal Frac: 0.7438\n",
      "\tEpoch: 775/1000; 2.01 sec... \tStep: 188325... \tLoss: 0.750... \tVal Loss: 0.933 \tVal Frac: 0.7268\n",
      "\tEpoch: 780/1000; 2.34 sec... \tStep: 189540... \tLoss: 0.868... \tVal Loss: 0.990 \tVal Frac: 0.7107\n",
      "\tEpoch: 785/1000; 2.16 sec... \tStep: 190755... \tLoss: 1.316... \tVal Loss: 0.848 \tVal Frac: 0.7386\n",
      "\tEpoch: 790/1000; 2.09 sec... \tStep: 191970... \tLoss: 1.242... \tVal Loss: 0.836 \tVal Frac: 0.7434\n",
      "\tEpoch: 795/1000; 1.97 sec... \tStep: 193185... \tLoss: 1.218... \tVal Loss: 0.840 \tVal Frac: 0.7408\n",
      "\tEpoch: 800/1000; 2.01 sec... \tStep: 194400... \tLoss: 1.145... \tVal Loss: 0.895 \tVal Frac: 0.7379\n",
      "\tEpoch: 805/1000; 2.09 sec... \tStep: 195615... \tLoss: 1.175... \tVal Loss: 0.838 \tVal Frac: 0.7335\n",
      "\tEpoch: 810/1000; 2.07 sec... \tStep: 196830... \tLoss: 1.071... \tVal Loss: 0.837 \tVal Frac: 0.7610\n",
      "\tEpoch: 815/1000; 2.06 sec... \tStep: 198045... \tLoss: 1.075... \tVal Loss: 0.881 \tVal Frac: 0.7415\n",
      "\tEpoch: 820/1000; 2.07 sec... \tStep: 199260... \tLoss: 1.089... \tVal Loss: 0.863 \tVal Frac: 0.7482\n",
      "\tEpoch: 825/1000; 1.99 sec... \tStep: 200475... \tLoss: 0.755... \tVal Loss: 0.814 \tVal Frac: 0.7555\n",
      "\tEpoch: 830/1000; 2.55 sec... \tStep: 201690... \tLoss: 0.699... \tVal Loss: 0.842 \tVal Frac: 0.7555\n",
      "\tEpoch: 835/1000; 2.19 sec... \tStep: 202905... \tLoss: 1.135... \tVal Loss: 0.797 \tVal Frac: 0.7482\n",
      "\tEpoch: 840/1000; 2.68 sec... \tStep: 204120... \tLoss: 0.758... \tVal Loss: 0.824 \tVal Frac: 0.7610\n",
      "\tEpoch: 845/1000; 2.04 sec... \tStep: 205335... \tLoss: 0.888... \tVal Loss: 0.883 \tVal Frac: 0.7585\n",
      "\tEpoch: 850/1000; 2.12 sec... \tStep: 206550... \tLoss: 0.765... \tVal Loss: 0.864 \tVal Frac: 0.7471\n",
      "\tEpoch: 855/1000; 2.12 sec... \tStep: 207765... \tLoss: 0.700... \tVal Loss: 0.795 \tVal Frac: 0.7544\n",
      "\tEpoch: 860/1000; 2.07 sec... \tStep: 208980... \tLoss: 1.156... \tVal Loss: 0.849 \tVal Frac: 0.7327\n",
      "\tEpoch: 865/1000; 2.06 sec... \tStep: 210195... \tLoss: 1.058... \tVal Loss: 0.742 \tVal Frac: 0.7607\n",
      "\tEpoch: 870/1000; 2.11 sec... \tStep: 211410... \tLoss: 0.787... \tVal Loss: 0.740 \tVal Frac: 0.7680\n",
      "\tEpoch: 875/1000; 2.02 sec... \tStep: 212625... \tLoss: 1.215... \tVal Loss: 0.773 \tVal Frac: 0.7566\n",
      "\tEpoch: 880/1000; 2.00 sec... \tStep: 213840... \tLoss: 0.741... \tVal Loss: 0.724 \tVal Frac: 0.7695\n",
      "\tEpoch: 885/1000; 2.04 sec... \tStep: 215055... \tLoss: 1.452... \tVal Loss: 0.739 \tVal Frac: 0.7555\n",
      "\tEpoch: 890/1000; 2.14 sec... \tStep: 216270... \tLoss: 0.855... \tVal Loss: 0.743 \tVal Frac: 0.7471\n",
      "\tEpoch: 895/1000; 2.04 sec... \tStep: 217485... \tLoss: 0.773... \tVal Loss: 0.815 \tVal Frac: 0.7676\n",
      "\tEpoch: 900/1000; 2.11 sec... \tStep: 218700... \tLoss: 0.801... \tVal Loss: 0.695 \tVal Frac: 0.7739\n",
      "\tEpoch: 905/1000; 1.97 sec... \tStep: 219915... \tLoss: 0.831... \tVal Loss: 0.695 \tVal Frac: 0.7654\n",
      "\tEpoch: 910/1000; 1.96 sec... \tStep: 221130... \tLoss: 1.351... \tVal Loss: 0.696 \tVal Frac: 0.7574\n",
      "\tEpoch: 915/1000; 2.07 sec... \tStep: 222345... \tLoss: 1.206... \tVal Loss: 0.816 \tVal Frac: 0.7184\n",
      "\tEpoch: 920/1000; 2.00 sec... \tStep: 223560... \tLoss: 1.235... \tVal Loss: 0.788 \tVal Frac: 0.7346\n",
      "\tEpoch: 925/1000; 2.27 sec... \tStep: 224775... \tLoss: 1.304... \tVal Loss: 0.833 \tVal Frac: 0.7169\n",
      "\tEpoch: 930/1000; 2.14 sec... \tStep: 225990... \tLoss: 0.829... \tVal Loss: 0.845 \tVal Frac: 0.7140\n",
      "\tEpoch: 935/1000; 2.67 sec... \tStep: 227205... \tLoss: 0.994... \tVal Loss: 0.796 \tVal Frac: 0.7224\n",
      "\tEpoch: 940/1000; 2.12 sec... \tStep: 228420... \tLoss: 0.884... \tVal Loss: 0.815 \tVal Frac: 0.7088\n",
      "\tEpoch: 945/1000; 2.02 sec... \tStep: 229635... \tLoss: 1.283... \tVal Loss: 0.765 \tVal Frac: 0.7460\n",
      "\tEpoch: 950/1000; 2.11 sec... \tStep: 230850... \tLoss: 0.642... \tVal Loss: 0.783 \tVal Frac: 0.7423\n",
      "\tEpoch: 955/1000; 2.11 sec... \tStep: 232065... \tLoss: 0.883... \tVal Loss: 0.788 \tVal Frac: 0.7382\n",
      "\tEpoch: 960/1000; 2.15 sec... \tStep: 233280... \tLoss: 1.247... \tVal Loss: 0.730 \tVal Frac: 0.7493\n",
      "\tEpoch: 965/1000; 2.11 sec... \tStep: 234495... \tLoss: 1.129... \tVal Loss: 0.735 \tVal Frac: 0.7522\n",
      "\tEpoch: 970/1000; 2.09 sec... \tStep: 235710... \tLoss: 0.707... \tVal Loss: 0.740 \tVal Frac: 0.7364\n",
      "\tEpoch: 975/1000; 2.15 sec... \tStep: 236925... \tLoss: 1.164... \tVal Loss: 0.726 \tVal Frac: 0.7379\n",
      "\tEpoch: 980/1000; 2.03 sec... \tStep: 238140... \tLoss: 0.872... \tVal Loss: 0.740 \tVal Frac: 0.7485\n",
      "\tEpoch: 985/1000; 2.05 sec... \tStep: 239355... \tLoss: 1.016... \tVal Loss: 0.853 \tVal Frac: 0.7191\n",
      "\tEpoch: 990/1000; 2.06 sec... \tStep: 240570... \tLoss: 1.004... \tVal Loss: 0.873 \tVal Frac: 0.7217\n",
      "\tEpoch: 995/1000; 2.09 sec... \tStep: 241785... \tLoss: 0.896... \tVal Loss: 0.832 \tVal Frac: 0.7257\n",
      "\tEpoch: 1000/1000; 2.04 sec... \tStep: 243000... \tLoss: 0.764... \tVal Loss: 0.768 \tVal Frac: 0.7500\n",
      "Completed:  n_layers_LSTM :  2 \n",
      "\tloss: 0.399848 \n",
      "\tfrac:0.886397\n",
      "\n",
      "###########\n",
      "Testing with  n_layers_LSTM :  3\n",
      "\tEpoch: 5/1000; 3.25 sec... \tStep: 1215... \tLoss: 0.513... \tVal Loss: 0.567 \tVal Frac: 0.8309\n",
      "\tValidation loss decreased (inf --> 0.567).  Saving model ...\n",
      "\tEpoch: 10/1000; 3.19 sec... \tStep: 2430... \tLoss: 0.347... \tVal Loss: 0.453 \tVal Frac: 0.8610\n",
      "\tValidation loss decreased (0.567 --> 0.453).  Saving model ...\n",
      "\tEpoch: 15/1000; 3.19 sec... \tStep: 3645... \tLoss: 0.477... \tVal Loss: 0.452 \tVal Frac: 0.8706\n",
      "\tValidation loss decreased (0.453 --> 0.452).  Saving model ...\n",
      "\tEpoch: 20/1000; 3.18 sec... \tStep: 4860... \tLoss: 0.324... \tVal Loss: 0.403 \tVal Frac: 0.8842\n",
      "\tValidation loss decreased (0.452 --> 0.403).  Saving model ...\n",
      "\tEpoch: 25/1000; 3.12 sec... \tStep: 6075... \tLoss: 0.230... \tVal Loss: 0.513 \tVal Frac: 0.8537\n",
      "\tEpoch: 30/1000; 3.19 sec... \tStep: 7290... \tLoss: 0.362... \tVal Loss: 0.450 \tVal Frac: 0.8728\n",
      "\tEpoch: 35/1000; 3.12 sec... \tStep: 8505... \tLoss: 0.363... \tVal Loss: 0.412 \tVal Frac: 0.8765\n",
      "\tEpoch: 40/1000; 3.43 sec... \tStep: 9720... \tLoss: 0.242... \tVal Loss: 0.399 \tVal Frac: 0.8846\n",
      "\tValidation loss decreased (0.403 --> 0.399).  Saving model ...\n",
      "\tEpoch: 45/1000; 3.24 sec... \tStep: 10935... \tLoss: 0.911... \tVal Loss: 0.452 \tVal Frac: 0.8765\n",
      "\tEpoch: 50/1000; 3.22 sec... \tStep: 12150... \tLoss: 0.550... \tVal Loss: 0.464 \tVal Frac: 0.8739\n",
      "\tEpoch: 55/1000; 3.28 sec... \tStep: 13365... \tLoss: 0.310... \tVal Loss: 0.409 \tVal Frac: 0.8879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 60/1000; 3.19 sec... \tStep: 14580... \tLoss: 0.432... \tVal Loss: 0.424 \tVal Frac: 0.8765\n",
      "\tEpoch: 65/1000; 3.16 sec... \tStep: 15795... \tLoss: 0.209... \tVal Loss: 0.394 \tVal Frac: 0.8901\n",
      "\tValidation loss decreased (0.399 --> 0.394).  Saving model ...\n",
      "\tEpoch: 70/1000; 3.11 sec... \tStep: 17010... \tLoss: 0.203... \tVal Loss: 0.401 \tVal Frac: 0.8820\n",
      "\tEpoch: 75/1000; 3.18 sec... \tStep: 18225... \tLoss: 0.601... \tVal Loss: 0.391 \tVal Frac: 0.8919\n",
      "\tValidation loss decreased (0.394 --> 0.391).  Saving model ...\n",
      "\tEpoch: 80/1000; 3.17 sec... \tStep: 19440... \tLoss: 0.687... \tVal Loss: 0.418 \tVal Frac: 0.8875\n",
      "\tEpoch: 85/1000; 3.19 sec... \tStep: 20655... \tLoss: 0.425... \tVal Loss: 0.431 \tVal Frac: 0.8893\n",
      "\tEpoch: 90/1000; 3.27 sec... \tStep: 21870... \tLoss: 0.384... \tVal Loss: 0.439 \tVal Frac: 0.8868\n",
      "\tEpoch: 95/1000; 3.39 sec... \tStep: 23085... \tLoss: 0.322... \tVal Loss: 0.411 \tVal Frac: 0.8860\n",
      "\tEpoch: 100/1000; 3.32 sec... \tStep: 24300... \tLoss: 0.433... \tVal Loss: 0.440 \tVal Frac: 0.8816\n",
      "\tEpoch: 105/1000; 3.18 sec... \tStep: 25515... \tLoss: 0.279... \tVal Loss: 0.450 \tVal Frac: 0.8816\n",
      "\tEpoch: 110/1000; 3.18 sec... \tStep: 26730... \tLoss: 0.416... \tVal Loss: 0.445 \tVal Frac: 0.8812\n",
      "\tEpoch: 115/1000; 3.26 sec... \tStep: 27945... \tLoss: 0.597... \tVal Loss: 0.403 \tVal Frac: 0.8949\n",
      "\tEpoch: 120/1000; 3.23 sec... \tStep: 29160... \tLoss: 0.923... \tVal Loss: 0.405 \tVal Frac: 0.8846\n",
      "\tEpoch: 125/1000; 3.14 sec... \tStep: 30375... \tLoss: 0.360... \tVal Loss: 0.408 \tVal Frac: 0.8904\n",
      "\tEpoch: 130/1000; 3.16 sec... \tStep: 31590... \tLoss: 0.218... \tVal Loss: 0.438 \tVal Frac: 0.8846\n",
      "\tEpoch: 135/1000; 3.21 sec... \tStep: 32805... \tLoss: 0.362... \tVal Loss: 0.418 \tVal Frac: 0.8915\n",
      "\tEpoch: 140/1000; 3.21 sec... \tStep: 34020... \tLoss: 0.301... \tVal Loss: 0.421 \tVal Frac: 0.8912\n",
      "\tEpoch: 145/1000; 3.16 sec... \tStep: 35235... \tLoss: 0.341... \tVal Loss: 0.434 \tVal Frac: 0.8857\n",
      "\tEpoch: 150/1000; 3.58 sec... \tStep: 36450... \tLoss: 0.579... \tVal Loss: 0.453 \tVal Frac: 0.8732\n",
      "\tEpoch: 155/1000; 3.26 sec... \tStep: 37665... \tLoss: 0.236... \tVal Loss: 0.412 \tVal Frac: 0.8938\n",
      "\tEpoch: 160/1000; 3.19 sec... \tStep: 38880... \tLoss: 0.216... \tVal Loss: 0.416 \tVal Frac: 0.8864\n",
      "\tEpoch: 165/1000; 3.22 sec... \tStep: 40095... \tLoss: 0.577... \tVal Loss: 0.449 \tVal Frac: 0.8754\n",
      "\tEpoch: 170/1000; 3.14 sec... \tStep: 41310... \tLoss: 0.339... \tVal Loss: 0.426 \tVal Frac: 0.8868\n",
      "\tEpoch: 175/1000; 3.19 sec... \tStep: 42525... \tLoss: 0.223... \tVal Loss: 0.471 \tVal Frac: 0.8776\n",
      "\tEpoch: 180/1000; 3.24 sec... \tStep: 43740... \tLoss: 0.017... \tVal Loss: 0.464 \tVal Frac: 0.8779\n",
      "\tEpoch: 185/1000; 3.15 sec... \tStep: 44955... \tLoss: 0.643... \tVal Loss: 0.461 \tVal Frac: 0.8739\n",
      "\tEpoch: 190/1000; 3.21 sec... \tStep: 46170... \tLoss: 0.225... \tVal Loss: 0.458 \tVal Frac: 0.8816\n",
      "\tEpoch: 195/1000; 3.15 sec... \tStep: 47385... \tLoss: 0.254... \tVal Loss: 0.471 \tVal Frac: 0.8665\n",
      "\tEpoch: 200/1000; 3.16 sec... \tStep: 48600... \tLoss: 0.345... \tVal Loss: 0.413 \tVal Frac: 0.8893\n",
      "\tEpoch: 205/1000; 3.32 sec... \tStep: 49815... \tLoss: 0.487... \tVal Loss: 0.410 \tVal Frac: 0.8930\n",
      "\tEpoch: 210/1000; 3.34 sec... \tStep: 51030... \tLoss: 0.351... \tVal Loss: 0.417 \tVal Frac: 0.8875\n",
      "\tEpoch: 215/1000; 3.18 sec... \tStep: 52245... \tLoss: 0.471... \tVal Loss: 0.428 \tVal Frac: 0.8860\n",
      "\tEpoch: 220/1000; 3.19 sec... \tStep: 53460... \tLoss: 0.274... \tVal Loss: 0.397 \tVal Frac: 0.8879\n",
      "\tEpoch: 225/1000; 3.15 sec... \tStep: 54675... \tLoss: 0.643... \tVal Loss: 0.425 \tVal Frac: 0.8787\n",
      "\tEpoch: 230/1000; 3.23 sec... \tStep: 55890... \tLoss: 0.485... \tVal Loss: 0.429 \tVal Frac: 0.8853\n",
      "\tEpoch: 235/1000; 3.18 sec... \tStep: 57105... \tLoss: 0.583... \tVal Loss: 0.425 \tVal Frac: 0.8930\n",
      "\tEpoch: 240/1000; 3.17 sec... \tStep: 58320... \tLoss: 0.468... \tVal Loss: 0.412 \tVal Frac: 0.8893\n",
      "\tEpoch: 245/1000; 3.22 sec... \tStep: 59535... \tLoss: 0.307... \tVal Loss: 0.443 \tVal Frac: 0.8849\n",
      "\tEpoch: 250/1000; 3.27 sec... \tStep: 60750... \tLoss: 0.295... \tVal Loss: 0.472 \tVal Frac: 0.8732\n",
      "\tEpoch: 255/1000; 3.18 sec... \tStep: 61965... \tLoss: 0.784... \tVal Loss: 0.464 \tVal Frac: 0.8721\n",
      "\tEpoch: 260/1000; 3.15 sec... \tStep: 63180... \tLoss: 0.713... \tVal Loss: 0.430 \tVal Frac: 0.8706\n",
      "\tEpoch: 265/1000; 3.76 sec... \tStep: 64395... \tLoss: 0.843... \tVal Loss: 0.497 \tVal Frac: 0.8537\n",
      "\tEpoch: 270/1000; 3.21 sec... \tStep: 65610... \tLoss: 0.359... \tVal Loss: 0.525 \tVal Frac: 0.8496\n",
      "\tEpoch: 275/1000; 3.25 sec... \tStep: 66825... \tLoss: 0.611... \tVal Loss: 0.466 \tVal Frac: 0.8640\n",
      "\tEpoch: 280/1000; 3.17 sec... \tStep: 68040... \tLoss: 0.650... \tVal Loss: 0.655 \tVal Frac: 0.8077\n",
      "\tEpoch: 285/1000; 3.24 sec... \tStep: 69255... \tLoss: 0.408... \tVal Loss: 0.728 \tVal Frac: 0.7857\n",
      "\tEpoch: 290/1000; 3.29 sec... \tStep: 70470... \tLoss: 0.941... \tVal Loss: 0.659 \tVal Frac: 0.8184\n",
      "\tEpoch: 295/1000; 3.22 sec... \tStep: 71685... \tLoss: 0.714... \tVal Loss: 0.544 \tVal Frac: 0.8496\n",
      "\tEpoch: 300/1000; 3.18 sec... \tStep: 72900... \tLoss: 0.474... \tVal Loss: 0.537 \tVal Frac: 0.8467\n",
      "\tEpoch: 305/1000; 3.32 sec... \tStep: 74115... \tLoss: 0.876... \tVal Loss: 0.474 \tVal Frac: 0.8596\n",
      "\tEpoch: 310/1000; 3.26 sec... \tStep: 75330... \tLoss: 0.490... \tVal Loss: 0.522 \tVal Frac: 0.8390\n",
      "\tEpoch: 315/1000; 3.36 sec... \tStep: 76545... \tLoss: 0.463... \tVal Loss: 0.542 \tVal Frac: 0.8338\n",
      "\tEpoch: 320/1000; 3.32 sec... \tStep: 77760... \tLoss: 0.763... \tVal Loss: 0.595 \tVal Frac: 0.8107\n",
      "\tEpoch: 325/1000; 3.26 sec... \tStep: 78975... \tLoss: 0.678... \tVal Loss: 0.621 \tVal Frac: 0.8063\n",
      "\tEpoch: 330/1000; 3.34 sec... \tStep: 80190... \tLoss: 0.703... \tVal Loss: 0.627 \tVal Frac: 0.8048\n",
      "\tEpoch: 335/1000; 3.46 sec... \tStep: 81405... \tLoss: 0.499... \tVal Loss: 0.571 \tVal Frac: 0.8246\n",
      "\tEpoch: 340/1000; 3.91 sec... \tStep: 82620... \tLoss: 0.479... \tVal Loss: 0.591 \tVal Frac: 0.8077\n",
      "\tEpoch: 345/1000; 3.25 sec... \tStep: 83835... \tLoss: 0.476... \tVal Loss: 0.575 \tVal Frac: 0.8176\n",
      "\tEpoch: 350/1000; 3.34 sec... \tStep: 85050... \tLoss: 0.708... \tVal Loss: 0.704 \tVal Frac: 0.7849\n",
      "\tEpoch: 355/1000; 3.28 sec... \tStep: 86265... \tLoss: 0.816... \tVal Loss: 0.726 \tVal Frac: 0.7618\n",
      "\tEpoch: 360/1000; 3.27 sec... \tStep: 87480... \tLoss: 0.876... \tVal Loss: 0.802 \tVal Frac: 0.7562\n",
      "\tEpoch: 365/1000; 3.19 sec... \tStep: 88695... \tLoss: 0.724... \tVal Loss: 0.774 \tVal Frac: 0.7739\n",
      "\tEpoch: 370/1000; 3.19 sec... \tStep: 89910... \tLoss: 1.459... \tVal Loss: 0.965 \tVal Frac: 0.7478\n",
      "\tEpoch: 375/1000; 3.18 sec... \tStep: 91125... \tLoss: 0.770... \tVal Loss: 0.677 \tVal Frac: 0.8037\n",
      "\tEpoch: 380/1000; 3.28 sec... \tStep: 92340... \tLoss: 0.499... \tVal Loss: 1.044 \tVal Frac: 0.7125\n",
      "\tEpoch: 385/1000; 3.23 sec... \tStep: 93555... \tLoss: 0.819... \tVal Loss: 0.796 \tVal Frac: 0.7761\n",
      "\tEpoch: 390/1000; 3.25 sec... \tStep: 94770... \tLoss: 1.196... \tVal Loss: 0.849 \tVal Frac: 0.7713\n",
      "\tEpoch: 395/1000; 3.15 sec... \tStep: 95985... \tLoss: 1.001... \tVal Loss: 0.839 \tVal Frac: 0.7471\n",
      "\tEpoch: 400/1000; 3.27 sec... \tStep: 97200... \tLoss: 0.803... \tVal Loss: 0.744 \tVal Frac: 0.7537\n",
      "\tEpoch: 405/1000; 3.25 sec... \tStep: 98415... \tLoss: 0.927... \tVal Loss: 0.865 \tVal Frac: 0.7338\n",
      "\tEpoch: 410/1000; 3.27 sec... \tStep: 99630... \tLoss: 1.249... \tVal Loss: 1.134 \tVal Frac: 0.6456\n",
      "\tEpoch: 415/1000; 3.22 sec... \tStep: 100845... \tLoss: 1.142... \tVal Loss: 0.872 \tVal Frac: 0.7342\n",
      "\tEpoch: 420/1000; 3.24 sec... \tStep: 102060... \tLoss: 1.034... \tVal Loss: 1.053 \tVal Frac: 0.6368\n",
      "\tEpoch: 425/1000; 3.21 sec... \tStep: 103275... \tLoss: 1.073... \tVal Loss: 1.062 \tVal Frac: 0.7151\n",
      "\tEpoch: 430/1000; 3.32 sec... \tStep: 104490... \tLoss: 1.008... \tVal Loss: 0.937 \tVal Frac: 0.6820\n",
      "\tEpoch: 435/1000; 3.31 sec... \tStep: 105705... \tLoss: 0.713... \tVal Loss: 1.154 \tVal Frac: 0.5680\n",
      "\tEpoch: 440/1000; 3.15 sec... \tStep: 106920... \tLoss: 0.778... \tVal Loss: 0.728 \tVal Frac: 0.7213\n",
      "\tEpoch: 445/1000; 3.29 sec... \tStep: 108135... \tLoss: 1.067... \tVal Loss: 0.768 \tVal Frac: 0.7654\n",
      "\tEpoch: 450/1000; 3.26 sec... \tStep: 109350... \tLoss: 0.821... \tVal Loss: 0.799 \tVal Frac: 0.7662\n",
      "\tEpoch: 455/1000; 3.36 sec... \tStep: 110565... \tLoss: 1.300... \tVal Loss: 0.700 \tVal Frac: 0.7897\n",
      "\tEpoch: 460/1000; 3.31 sec... \tStep: 111780... \tLoss: 0.583... \tVal Loss: 0.667 \tVal Frac: 0.8096\n",
      "\tEpoch: 465/1000; 3.38 sec... \tStep: 112995... \tLoss: 0.573... \tVal Loss: 0.662 \tVal Frac: 0.8033\n",
      "\tEpoch: 470/1000; 3.24 sec... \tStep: 114210... \tLoss: 0.570... \tVal Loss: 0.717 \tVal Frac: 0.7809\n",
      "\tEpoch: 475/1000; 3.24 sec... \tStep: 115425... \tLoss: 0.945... \tVal Loss: 0.814 \tVal Frac: 0.7838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 480/1000; 3.41 sec... \tStep: 116640... \tLoss: 0.632... \tVal Loss: 0.645 \tVal Frac: 0.8129\n",
      "\tEpoch: 485/1000; 3.29 sec... \tStep: 117855... \tLoss: 0.398... \tVal Loss: 0.671 \tVal Frac: 0.7996\n",
      "\tEpoch: 490/1000; 3.28 sec... \tStep: 119070... \tLoss: 0.767... \tVal Loss: 0.642 \tVal Frac: 0.8136\n",
      "\tEpoch: 495/1000; 3.27 sec... \tStep: 120285... \tLoss: 0.714... \tVal Loss: 0.823 \tVal Frac: 0.7511\n",
      "\tEpoch: 500/1000; 3.29 sec... \tStep: 121500... \tLoss: 0.855... \tVal Loss: 0.974 \tVal Frac: 0.7228\n",
      "\tEpoch: 505/1000; 3.29 sec... \tStep: 122715... \tLoss: 0.886... \tVal Loss: 0.785 \tVal Frac: 0.7471\n",
      "\tEpoch: 510/1000; 3.34 sec... \tStep: 123930... \tLoss: 0.723... \tVal Loss: 0.781 \tVal Frac: 0.7625\n",
      "\tEpoch: 515/1000; 3.28 sec... \tStep: 125145... \tLoss: 0.858... \tVal Loss: 0.685 \tVal Frac: 0.7846\n",
      "\tEpoch: 520/1000; 3.41 sec... \tStep: 126360... \tLoss: 0.909... \tVal Loss: 0.737 \tVal Frac: 0.7676\n",
      "\tEpoch: 525/1000; 3.19 sec... \tStep: 127575... \tLoss: 0.798... \tVal Loss: 0.697 \tVal Frac: 0.7831\n",
      "\tEpoch: 530/1000; 3.25 sec... \tStep: 128790... \tLoss: 0.523... \tVal Loss: 0.741 \tVal Frac: 0.7559\n",
      "\tEpoch: 535/1000; 3.28 sec... \tStep: 130005... \tLoss: 1.025... \tVal Loss: 0.774 \tVal Frac: 0.7507\n",
      "\tEpoch: 540/1000; 3.26 sec... \tStep: 131220... \tLoss: 0.752... \tVal Loss: 0.677 \tVal Frac: 0.7857\n",
      "\tEpoch: 545/1000; 3.25 sec... \tStep: 132435... \tLoss: 0.965... \tVal Loss: 0.678 \tVal Frac: 0.7805\n",
      "\tEpoch: 550/1000; 3.19 sec... \tStep: 133650... \tLoss: 0.623... \tVal Loss: 0.802 \tVal Frac: 0.7680\n",
      "\tEpoch: 555/1000; 3.21 sec... \tStep: 134865... \tLoss: 0.595... \tVal Loss: 0.626 \tVal Frac: 0.8040\n",
      "\tEpoch: 560/1000; 3.31 sec... \tStep: 136080... \tLoss: 0.959... \tVal Loss: 0.762 \tVal Frac: 0.7831\n",
      "\tEpoch: 565/1000; 3.29 sec... \tStep: 137295... \tLoss: 0.645... \tVal Loss: 0.805 \tVal Frac: 0.7625\n",
      "\tEpoch: 570/1000; 3.18 sec... \tStep: 138510... \tLoss: 0.647... \tVal Loss: 0.698 \tVal Frac: 0.7735\n",
      "\tEpoch: 575/1000; 3.19 sec... \tStep: 139725... \tLoss: 0.840... \tVal Loss: 0.768 \tVal Frac: 0.7717\n",
      "\tEpoch: 580/1000; 3.22 sec... \tStep: 140940... \tLoss: 0.484... \tVal Loss: 0.810 \tVal Frac: 0.7526\n",
      "\tEpoch: 585/1000; 3.28 sec... \tStep: 142155... \tLoss: 0.608... \tVal Loss: 0.791 \tVal Frac: 0.7798\n",
      "\tEpoch: 590/1000; 3.15 sec... \tStep: 143370... \tLoss: 0.639... \tVal Loss: 0.827 \tVal Frac: 0.7452\n",
      "\tEpoch: 595/1000; 3.18 sec... \tStep: 144585... \tLoss: 0.557... \tVal Loss: 0.829 \tVal Frac: 0.7559\n",
      "\tEpoch: 600/1000; 3.21 sec... \tStep: 145800... \tLoss: 1.006... \tVal Loss: 0.722 \tVal Frac: 0.7643\n",
      "\tEpoch: 605/1000; 3.15 sec... \tStep: 147015... \tLoss: 0.700... \tVal Loss: 0.648 \tVal Frac: 0.7923\n",
      "\tEpoch: 610/1000; 3.13 sec... \tStep: 148230... \tLoss: 0.507... \tVal Loss: 0.845 \tVal Frac: 0.7625\n",
      "\tEpoch: 615/1000; 3.14 sec... \tStep: 149445... \tLoss: 0.361... \tVal Loss: 0.747 \tVal Frac: 0.7610\n",
      "\tEpoch: 620/1000; 3.12 sec... \tStep: 150660... \tLoss: 0.426... \tVal Loss: 0.702 \tVal Frac: 0.7879\n",
      "\tEpoch: 625/1000; 3.13 sec... \tStep: 151875... \tLoss: 0.468... \tVal Loss: 0.707 \tVal Frac: 0.7853\n",
      "\tEpoch: 630/1000; 3.66 sec... \tStep: 153090... \tLoss: 0.646... \tVal Loss: 0.752 \tVal Frac: 0.7680\n",
      "\tEpoch: 635/1000; 3.69 sec... \tStep: 154305... \tLoss: 0.538... \tVal Loss: 0.602 \tVal Frac: 0.8044\n",
      "\tEpoch: 640/1000; 3.57 sec... \tStep: 155520... \tLoss: 0.592... \tVal Loss: 0.650 \tVal Frac: 0.8015\n",
      "\tEpoch: 645/1000; 3.50 sec... \tStep: 156735... \tLoss: 0.751... \tVal Loss: 0.660 \tVal Frac: 0.8066\n",
      "\tEpoch: 650/1000; 3.59 sec... \tStep: 157950... \tLoss: 0.933... \tVal Loss: 0.661 \tVal Frac: 0.8055\n",
      "\tEpoch: 655/1000; 3.49 sec... \tStep: 159165... \tLoss: 0.722... \tVal Loss: 0.564 \tVal Frac: 0.8320\n",
      "\tEpoch: 660/1000; 3.47 sec... \tStep: 160380... \tLoss: 0.714... \tVal Loss: 0.562 \tVal Frac: 0.8316\n",
      "\tEpoch: 665/1000; 3.51 sec... \tStep: 161595... \tLoss: 0.590... \tVal Loss: 0.698 \tVal Frac: 0.7960\n",
      "\tEpoch: 670/1000; 3.39 sec... \tStep: 162810... \tLoss: 0.484... \tVal Loss: 0.608 \tVal Frac: 0.8110\n",
      "\tEpoch: 675/1000; 3.61 sec... \tStep: 164025... \tLoss: 0.621... \tVal Loss: 0.617 \tVal Frac: 0.8026\n",
      "\tEpoch: 680/1000; 3.53 sec... \tStep: 165240... \tLoss: 0.696... \tVal Loss: 0.553 \tVal Frac: 0.8246\n",
      "\tEpoch: 685/1000; 3.46 sec... \tStep: 166455... \tLoss: 0.418... \tVal Loss: 0.588 \tVal Frac: 0.8088\n",
      "\tEpoch: 690/1000; 3.70 sec... \tStep: 167670... \tLoss: 0.344... \tVal Loss: 0.603 \tVal Frac: 0.8044\n",
      "\tEpoch: 695/1000; 3.66 sec... \tStep: 168885... \tLoss: 0.621... \tVal Loss: 0.663 \tVal Frac: 0.7978\n",
      "\tEpoch: 700/1000; 3.56 sec... \tStep: 170100... \tLoss: 0.535... \tVal Loss: 0.702 \tVal Frac: 0.7540\n",
      "\tEpoch: 705/1000; 3.53 sec... \tStep: 171315... \tLoss: 0.767... \tVal Loss: 0.646 \tVal Frac: 0.7735\n",
      "\tEpoch: 710/1000; 3.49 sec... \tStep: 172530... \tLoss: 0.726... \tVal Loss: 0.601 \tVal Frac: 0.7967\n",
      "\tEpoch: 715/1000; 3.33 sec... \tStep: 173745... \tLoss: 0.460... \tVal Loss: 0.596 \tVal Frac: 0.8187\n",
      "\tEpoch: 720/1000; 3.53 sec... \tStep: 174960... \tLoss: 0.279... \tVal Loss: 0.661 \tVal Frac: 0.7901\n",
      "\tEpoch: 725/1000; 3.56 sec... \tStep: 176175... \tLoss: 0.929... \tVal Loss: 0.627 \tVal Frac: 0.8048\n",
      "\tEpoch: 730/1000; 3.71 sec... \tStep: 177390... \tLoss: 0.985... \tVal Loss: 0.718 \tVal Frac: 0.7809\n",
      "\tEpoch: 735/1000; 3.51 sec... \tStep: 178605... \tLoss: 0.445... \tVal Loss: 0.744 \tVal Frac: 0.7765\n",
      "\tEpoch: 740/1000; 3.43 sec... \tStep: 179820... \tLoss: 0.647... \tVal Loss: 0.675 \tVal Frac: 0.8040\n",
      "\tEpoch: 745/1000; 3.56 sec... \tStep: 181035... \tLoss: 0.836... \tVal Loss: 0.663 \tVal Frac: 0.8004\n",
      "\tEpoch: 750/1000; 3.63 sec... \tStep: 182250... \tLoss: 0.915... \tVal Loss: 0.677 \tVal Frac: 0.7805\n",
      "\tEpoch: 755/1000; 3.53 sec... \tStep: 183465... \tLoss: 1.038... \tVal Loss: 0.751 \tVal Frac: 0.8040\n",
      "\tEpoch: 760/1000; 3.66 sec... \tStep: 184680... \tLoss: 0.322... \tVal Loss: 0.695 \tVal Frac: 0.7886\n",
      "\tEpoch: 765/1000; 3.58 sec... \tStep: 185895... \tLoss: 0.625... \tVal Loss: 0.844 \tVal Frac: 0.7305\n",
      "\tEpoch: 770/1000; 3.53 sec... \tStep: 187110... \tLoss: 0.384... \tVal Loss: 0.979 \tVal Frac: 0.7195\n",
      "\tEpoch: 775/1000; 3.61 sec... \tStep: 188325... \tLoss: 0.343... \tVal Loss: 0.701 \tVal Frac: 0.7952\n",
      "\tEpoch: 780/1000; 3.59 sec... \tStep: 189540... \tLoss: 0.745... \tVal Loss: 0.622 \tVal Frac: 0.8099\n",
      "\tEpoch: 785/1000; 3.54 sec... \tStep: 190755... \tLoss: 0.559... \tVal Loss: 0.794 \tVal Frac: 0.7787\n",
      "\tEpoch: 790/1000; 3.60 sec... \tStep: 191970... \tLoss: 0.573... \tVal Loss: 0.673 \tVal Frac: 0.7949\n",
      "\tEpoch: 795/1000; 3.67 sec... \tStep: 193185... \tLoss: 0.519... \tVal Loss: 0.761 \tVal Frac: 0.7585\n",
      "\tEpoch: 800/1000; 3.58 sec... \tStep: 194400... \tLoss: 0.462... \tVal Loss: 0.867 \tVal Frac: 0.7158\n",
      "\tEpoch: 805/1000; 3.63 sec... \tStep: 195615... \tLoss: 0.500... \tVal Loss: 0.800 \tVal Frac: 0.7210\n",
      "\tEpoch: 810/1000; 3.83 sec... \tStep: 196830... \tLoss: 1.401... \tVal Loss: 0.884 \tVal Frac: 0.7184\n",
      "\tEpoch: 815/1000; 3.78 sec... \tStep: 198045... \tLoss: 0.662... \tVal Loss: 0.871 \tVal Frac: 0.7188\n",
      "\tEpoch: 820/1000; 3.51 sec... \tStep: 199260... \tLoss: 0.589... \tVal Loss: 0.727 \tVal Frac: 0.7629\n",
      "\tEpoch: 825/1000; 3.72 sec... \tStep: 200475... \tLoss: 0.690... \tVal Loss: 0.772 \tVal Frac: 0.7588\n",
      "\tEpoch: 830/1000; 3.24 sec... \tStep: 201690... \tLoss: 1.239... \tVal Loss: 0.689 \tVal Frac: 0.7643\n",
      "\tEpoch: 835/1000; 3.29 sec... \tStep: 202905... \tLoss: 1.209... \tVal Loss: 0.770 \tVal Frac: 0.7482\n",
      "\tEpoch: 840/1000; 3.19 sec... \tStep: 204120... \tLoss: 0.745... \tVal Loss: 0.766 \tVal Frac: 0.7658\n",
      "\tEpoch: 845/1000; 3.59 sec... \tStep: 205335... \tLoss: 1.161... \tVal Loss: 0.811 \tVal Frac: 0.7423\n",
      "\tEpoch: 850/1000; 3.23 sec... \tStep: 206550... \tLoss: 0.805... \tVal Loss: 0.690 \tVal Frac: 0.8011\n",
      "\tEpoch: 855/1000; 3.21 sec... \tStep: 207765... \tLoss: 0.920... \tVal Loss: 0.897 \tVal Frac: 0.7268\n",
      "\tEpoch: 860/1000; 3.18 sec... \tStep: 208980... \tLoss: 0.860... \tVal Loss: 0.944 \tVal Frac: 0.7228\n",
      "\tEpoch: 865/1000; 3.17 sec... \tStep: 210195... \tLoss: 0.847... \tVal Loss: 0.859 \tVal Frac: 0.7438\n",
      "\tEpoch: 870/1000; 3.13 sec... \tStep: 211410... \tLoss: 0.687... \tVal Loss: 0.885 \tVal Frac: 0.7070\n",
      "\tEpoch: 875/1000; 3.25 sec... \tStep: 212625... \tLoss: 0.481... \tVal Loss: 0.848 \tVal Frac: 0.7015\n",
      "\tEpoch: 880/1000; 3.26 sec... \tStep: 213840... \tLoss: 0.689... \tVal Loss: 0.939 \tVal Frac: 0.7110\n",
      "\tEpoch: 885/1000; 3.13 sec... \tStep: 215055... \tLoss: 1.121... \tVal Loss: 0.913 \tVal Frac: 0.7283\n",
      "\tEpoch: 890/1000; 3.13 sec... \tStep: 216270... \tLoss: 0.876... \tVal Loss: 0.879 \tVal Frac: 0.7276\n",
      "\tEpoch: 895/1000; 3.17 sec... \tStep: 217485... \tLoss: 0.567... \tVal Loss: 0.871 \tVal Frac: 0.7371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 900/1000; 3.23 sec... \tStep: 218700... \tLoss: 0.980... \tVal Loss: 0.838 \tVal Frac: 0.7566\n",
      "\tEpoch: 905/1000; 3.27 sec... \tStep: 219915... \tLoss: 1.038... \tVal Loss: 0.927 \tVal Frac: 0.7110\n",
      "\tEpoch: 910/1000; 3.28 sec... \tStep: 221130... \tLoss: 1.080... \tVal Loss: 1.501 \tVal Frac: 0.4511\n",
      "\tEpoch: 915/1000; 3.23 sec... \tStep: 222345... \tLoss: 1.139... \tVal Loss: 1.071 \tVal Frac: 0.6504\n",
      "\tEpoch: 920/1000; 3.14 sec... \tStep: 223560... \tLoss: 1.282... \tVal Loss: 1.059 \tVal Frac: 0.6621\n",
      "\tEpoch: 925/1000; 3.19 sec... \tStep: 224775... \tLoss: 0.900... \tVal Loss: 1.000 \tVal Frac: 0.6871\n",
      "\tEpoch: 930/1000; 3.30 sec... \tStep: 225990... \tLoss: 0.961... \tVal Loss: 1.296 \tVal Frac: 0.6235\n",
      "\tEpoch: 935/1000; 3.16 sec... \tStep: 227205... \tLoss: 0.934... \tVal Loss: 1.048 \tVal Frac: 0.6974\n",
      "\tEpoch: 940/1000; 3.21 sec... \tStep: 228420... \tLoss: 1.305... \tVal Loss: 1.203 \tVal Frac: 0.6912\n",
      "\tEpoch: 945/1000; 3.29 sec... \tStep: 229635... \tLoss: 1.083... \tVal Loss: 0.883 \tVal Frac: 0.7441\n",
      "\tEpoch: 950/1000; 3.21 sec... \tStep: 230850... \tLoss: 0.880... \tVal Loss: 0.777 \tVal Frac: 0.7776\n",
      "\tEpoch: 955/1000; 3.28 sec... \tStep: 232065... \tLoss: 0.769... \tVal Loss: 1.051 \tVal Frac: 0.7129\n",
      "\tEpoch: 960/1000; 3.26 sec... \tStep: 233280... \tLoss: 0.540... \tVal Loss: 0.767 \tVal Frac: 0.7801\n",
      "\tEpoch: 965/1000; 3.19 sec... \tStep: 234495... \tLoss: 1.231... \tVal Loss: 0.939 \tVal Frac: 0.7026\n",
      "\tEpoch: 970/1000; 3.19 sec... \tStep: 235710... \tLoss: 1.216... \tVal Loss: 0.995 \tVal Frac: 0.6996\n",
      "\tEpoch: 975/1000; 3.31 sec... \tStep: 236925... \tLoss: 0.919... \tVal Loss: 1.025 \tVal Frac: 0.6809\n",
      "\tEpoch: 980/1000; 3.26 sec... \tStep: 238140... \tLoss: 1.386... \tVal Loss: 1.222 \tVal Frac: 0.6018\n",
      "\tEpoch: 985/1000; 3.21 sec... \tStep: 239355... \tLoss: 1.023... \tVal Loss: 0.897 \tVal Frac: 0.7228\n",
      "\tEpoch: 990/1000; 3.17 sec... \tStep: 240570... \tLoss: 0.832... \tVal Loss: 0.960 \tVal Frac: 0.7301\n",
      "\tEpoch: 995/1000; 3.23 sec... \tStep: 241785... \tLoss: 1.429... \tVal Loss: 0.953 \tVal Frac: 0.7213\n",
      "\tEpoch: 1000/1000; 3.33 sec... \tStep: 243000... \tLoss: 1.327... \tVal Loss: 0.992 \tVal Frac: 0.6827\n",
      "Completed:  n_layers_LSTM :  3 \n",
      "\tloss: 0.391459 \n",
      "\tfrac:0.894853\n",
      "\n",
      "###########\n",
      "Testing with  n_layers_LSTM :  5\n",
      "\tEpoch: 5/1000; 5.75 sec... \tStep: 1215... \tLoss: 0.924... \tVal Loss: 0.614 \tVal Frac: 0.8276\n",
      "\tValidation loss decreased (inf --> 0.614).  Saving model ...\n",
      "\tEpoch: 10/1000; 5.75 sec... \tStep: 2430... \tLoss: 0.398... \tVal Loss: 0.489 \tVal Frac: 0.8430\n",
      "\tValidation loss decreased (0.614 --> 0.489).  Saving model ...\n",
      "\tEpoch: 15/1000; 5.73 sec... \tStep: 3645... \tLoss: 0.470... \tVal Loss: 0.467 \tVal Frac: 0.8632\n",
      "\tValidation loss decreased (0.489 --> 0.467).  Saving model ...\n",
      "\tEpoch: 20/1000; 5.71 sec... \tStep: 4860... \tLoss: 0.839... \tVal Loss: 0.440 \tVal Frac: 0.8676\n",
      "\tValidation loss decreased (0.467 --> 0.440).  Saving model ...\n",
      "\tEpoch: 25/1000; 5.70 sec... \tStep: 6075... \tLoss: 0.678... \tVal Loss: 0.415 \tVal Frac: 0.8717\n",
      "\tValidation loss decreased (0.440 --> 0.415).  Saving model ...\n",
      "\tEpoch: 30/1000; 5.66 sec... \tStep: 7290... \tLoss: 0.325... \tVal Loss: 0.430 \tVal Frac: 0.8761\n",
      "\tEpoch: 35/1000; 5.71 sec... \tStep: 8505... \tLoss: 0.687... \tVal Loss: 0.397 \tVal Frac: 0.8787\n",
      "\tValidation loss decreased (0.415 --> 0.397).  Saving model ...\n",
      "\tEpoch: 40/1000; 5.73 sec... \tStep: 9720... \tLoss: 0.708... \tVal Loss: 0.415 \tVal Frac: 0.8820\n",
      "\tEpoch: 45/1000; 5.70 sec... \tStep: 10935... \tLoss: 0.334... \tVal Loss: 0.396 \tVal Frac: 0.8798\n",
      "\tValidation loss decreased (0.397 --> 0.396).  Saving model ...\n",
      "\tEpoch: 50/1000; 5.80 sec... \tStep: 12150... \tLoss: 0.347... \tVal Loss: 0.398 \tVal Frac: 0.8842\n",
      "\tEpoch: 55/1000; 5.67 sec... \tStep: 13365... \tLoss: 0.264... \tVal Loss: 0.449 \tVal Frac: 0.8765\n",
      "\tEpoch: 60/1000; 5.78 sec... \tStep: 14580... \tLoss: 0.562... \tVal Loss: 0.401 \tVal Frac: 0.8827\n",
      "\tEpoch: 65/1000; 5.76 sec... \tStep: 15795... \tLoss: 0.467... \tVal Loss: 0.429 \tVal Frac: 0.8805\n",
      "\tEpoch: 70/1000; 5.78 sec... \tStep: 17010... \tLoss: 0.348... \tVal Loss: 0.405 \tVal Frac: 0.8835\n",
      "\tEpoch: 75/1000; 5.71 sec... \tStep: 18225... \tLoss: 0.489... \tVal Loss: 0.446 \tVal Frac: 0.8761\n",
      "\tEpoch: 80/1000; 5.70 sec... \tStep: 19440... \tLoss: 0.451... \tVal Loss: 0.467 \tVal Frac: 0.8820\n",
      "\tEpoch: 85/1000; 5.76 sec... \tStep: 20655... \tLoss: 0.204... \tVal Loss: 0.446 \tVal Frac: 0.8831\n",
      "\tEpoch: 90/1000; 5.71 sec... \tStep: 21870... \tLoss: 0.471... \tVal Loss: 0.422 \tVal Frac: 0.8772\n",
      "\tEpoch: 95/1000; 5.78 sec... \tStep: 23085... \tLoss: 0.545... \tVal Loss: 0.432 \tVal Frac: 0.8886\n",
      "\tEpoch: 100/1000; 5.63 sec... \tStep: 24300... \tLoss: 0.702... \tVal Loss: 0.410 \tVal Frac: 0.8846\n",
      "\tEpoch: 105/1000; 5.71 sec... \tStep: 25515... \tLoss: 0.474... \tVal Loss: 0.423 \tVal Frac: 0.8750\n",
      "\tEpoch: 110/1000; 5.71 sec... \tStep: 26730... \tLoss: 0.078... \tVal Loss: 0.433 \tVal Frac: 0.8743\n",
      "\tEpoch: 115/1000; 5.71 sec... \tStep: 27945... \tLoss: 0.321... \tVal Loss: 0.396 \tVal Frac: 0.8835\n",
      "\tValidation loss decreased (0.396 --> 0.396).  Saving model ...\n",
      "\tEpoch: 120/1000; 5.65 sec... \tStep: 29160... \tLoss: 0.291... \tVal Loss: 0.403 \tVal Frac: 0.8849\n",
      "\tEpoch: 125/1000; 5.71 sec... \tStep: 30375... \tLoss: 0.378... \tVal Loss: 0.457 \tVal Frac: 0.8765\n",
      "\tEpoch: 130/1000; 5.71 sec... \tStep: 31590... \tLoss: 0.303... \tVal Loss: 0.419 \tVal Frac: 0.8860\n",
      "\tEpoch: 135/1000; 5.67 sec... \tStep: 32805... \tLoss: 0.748... \tVal Loss: 0.430 \tVal Frac: 0.8926\n",
      "\tEpoch: 140/1000; 5.75 sec... \tStep: 34020... \tLoss: 0.340... \tVal Loss: 0.435 \tVal Frac: 0.8820\n",
      "\tEpoch: 145/1000; 5.71 sec... \tStep: 35235... \tLoss: 0.179... \tVal Loss: 0.378 \tVal Frac: 0.8934\n",
      "\tValidation loss decreased (0.396 --> 0.378).  Saving model ...\n",
      "\tEpoch: 150/1000; 5.80 sec... \tStep: 36450... \tLoss: 0.449... \tVal Loss: 0.443 \tVal Frac: 0.8860\n",
      "\tEpoch: 155/1000; 5.66 sec... \tStep: 37665... \tLoss: 0.376... \tVal Loss: 0.397 \tVal Frac: 0.8879\n",
      "\tEpoch: 160/1000; 5.77 sec... \tStep: 38880... \tLoss: 0.360... \tVal Loss: 0.399 \tVal Frac: 0.8908\n",
      "\tEpoch: 165/1000; 5.74 sec... \tStep: 40095... \tLoss: 0.301... \tVal Loss: 0.489 \tVal Frac: 0.8688\n",
      "\tEpoch: 170/1000; 5.95 sec... \tStep: 41310... \tLoss: 0.332... \tVal Loss: 0.428 \tVal Frac: 0.8871\n",
      "\tEpoch: 175/1000; 5.73 sec... \tStep: 42525... \tLoss: 0.383... \tVal Loss: 0.445 \tVal Frac: 0.8875\n",
      "\tEpoch: 180/1000; 5.76 sec... \tStep: 43740... \tLoss: 0.462... \tVal Loss: 0.418 \tVal Frac: 0.8842\n",
      "\tEpoch: 185/1000; 5.71 sec... \tStep: 44955... \tLoss: 0.395... \tVal Loss: 0.389 \tVal Frac: 0.8908\n",
      "\tEpoch: 190/1000; 5.75 sec... \tStep: 46170... \tLoss: 0.728... \tVal Loss: 0.471 \tVal Frac: 0.8713\n",
      "\tEpoch: 195/1000; 5.85 sec... \tStep: 47385... \tLoss: 0.317... \tVal Loss: 0.408 \tVal Frac: 0.8934\n",
      "\tEpoch: 200/1000; 5.71 sec... \tStep: 48600... \tLoss: 0.342... \tVal Loss: 0.395 \tVal Frac: 0.8890\n",
      "\tEpoch: 205/1000; 5.76 sec... \tStep: 49815... \tLoss: 0.465... \tVal Loss: 0.431 \tVal Frac: 0.8827\n",
      "\tEpoch: 210/1000; 5.74 sec... \tStep: 51030... \tLoss: 0.292... \tVal Loss: 0.431 \tVal Frac: 0.8838\n",
      "\tEpoch: 215/1000; 5.64 sec... \tStep: 52245... \tLoss: 0.395... \tVal Loss: 0.423 \tVal Frac: 0.8864\n",
      "\tEpoch: 220/1000; 5.70 sec... \tStep: 53460... \tLoss: 0.386... \tVal Loss: 0.417 \tVal Frac: 0.8820\n",
      "\tEpoch: 225/1000; 5.87 sec... \tStep: 54675... \tLoss: 0.229... \tVal Loss: 0.400 \tVal Frac: 0.8886\n",
      "\tEpoch: 230/1000; 5.71 sec... \tStep: 55890... \tLoss: 0.414... \tVal Loss: 0.399 \tVal Frac: 0.8882\n",
      "\tEpoch: 235/1000; 5.68 sec... \tStep: 57105... \tLoss: 0.341... \tVal Loss: 0.464 \tVal Frac: 0.8757\n",
      "\tEpoch: 240/1000; 5.70 sec... \tStep: 58320... \tLoss: 0.425... \tVal Loss: 0.440 \tVal Frac: 0.8794\n",
      "\tEpoch: 245/1000; 5.68 sec... \tStep: 59535... \tLoss: 0.306... \tVal Loss: 0.422 \tVal Frac: 0.8860\n",
      "\tEpoch: 250/1000; 5.80 sec... \tStep: 60750... \tLoss: 0.361... \tVal Loss: 0.394 \tVal Frac: 0.8886\n",
      "\tEpoch: 255/1000; 5.65 sec... \tStep: 61965... \tLoss: 0.377... \tVal Loss: 0.415 \tVal Frac: 0.8908\n",
      "\tEpoch: 260/1000; 5.68 sec... \tStep: 63180... \tLoss: 0.165... \tVal Loss: 0.413 \tVal Frac: 0.8871\n",
      "\tEpoch: 265/1000; 5.66 sec... \tStep: 64395... \tLoss: 0.242... \tVal Loss: 0.411 \tVal Frac: 0.8893\n",
      "\tEpoch: 270/1000; 5.63 sec... \tStep: 65610... \tLoss: 0.413... \tVal Loss: 0.424 \tVal Frac: 0.8838\n",
      "\tEpoch: 275/1000; 5.69 sec... \tStep: 66825... \tLoss: 0.260... \tVal Loss: 0.397 \tVal Frac: 0.8886\n",
      "\tEpoch: 280/1000; 5.75 sec... \tStep: 68040... \tLoss: 0.382... \tVal Loss: 0.405 \tVal Frac: 0.8846\n",
      "\tEpoch: 285/1000; 5.78 sec... \tStep: 69255... \tLoss: 0.235... \tVal Loss: 0.392 \tVal Frac: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 290/1000; 5.71 sec... \tStep: 70470... \tLoss: 0.190... \tVal Loss: 0.401 \tVal Frac: 0.8897\n",
      "\tEpoch: 295/1000; 5.73 sec... \tStep: 71685... \tLoss: 0.052... \tVal Loss: 0.416 \tVal Frac: 0.8904\n",
      "\tEpoch: 300/1000; 5.77 sec... \tStep: 72900... \tLoss: 0.369... \tVal Loss: 0.463 \tVal Frac: 0.8860\n",
      "\tEpoch: 305/1000; 5.85 sec... \tStep: 74115... \tLoss: 0.393... \tVal Loss: 0.422 \tVal Frac: 0.8897\n",
      "\tEpoch: 310/1000; 5.80 sec... \tStep: 75330... \tLoss: 0.486... \tVal Loss: 0.431 \tVal Frac: 0.8864\n",
      "\tEpoch: 315/1000; 5.70 sec... \tStep: 76545... \tLoss: 0.234... \tVal Loss: 0.390 \tVal Frac: 0.8919\n",
      "\tEpoch: 320/1000; 5.65 sec... \tStep: 77760... \tLoss: 0.141... \tVal Loss: 0.443 \tVal Frac: 0.8879\n",
      "\tEpoch: 325/1000; 5.75 sec... \tStep: 78975... \tLoss: 0.367... \tVal Loss: 0.461 \tVal Frac: 0.8824\n",
      "\tEpoch: 330/1000; 5.73 sec... \tStep: 80190... \tLoss: 0.287... \tVal Loss: 0.438 \tVal Frac: 0.8879\n",
      "\tEpoch: 335/1000; 5.85 sec... \tStep: 81405... \tLoss: 0.479... \tVal Loss: 0.473 \tVal Frac: 0.8857\n",
      "\tEpoch: 340/1000; 5.72 sec... \tStep: 82620... \tLoss: 0.307... \tVal Loss: 0.439 \tVal Frac: 0.8860\n",
      "\tEpoch: 345/1000; 5.75 sec... \tStep: 83835... \tLoss: 0.424... \tVal Loss: 0.407 \tVal Frac: 0.8897\n",
      "\tEpoch: 350/1000; 5.78 sec... \tStep: 85050... \tLoss: 0.470... \tVal Loss: 0.419 \tVal Frac: 0.8853\n",
      "\tEpoch: 355/1000; 5.70 sec... \tStep: 86265... \tLoss: 0.327... \tVal Loss: 0.444 \tVal Frac: 0.8908\n",
      "\tEpoch: 360/1000; 5.92 sec... \tStep: 87480... \tLoss: 0.437... \tVal Loss: 0.407 \tVal Frac: 0.8893\n",
      "\tEpoch: 365/1000; 5.96 sec... \tStep: 88695... \tLoss: 0.442... \tVal Loss: 0.424 \tVal Frac: 0.8915\n",
      "\tEpoch: 370/1000; 5.87 sec... \tStep: 89910... \tLoss: 0.367... \tVal Loss: 0.418 \tVal Frac: 0.8915\n",
      "\tEpoch: 375/1000; 5.99 sec... \tStep: 91125... \tLoss: 0.325... \tVal Loss: 0.424 \tVal Frac: 0.8960\n",
      "\tEpoch: 380/1000; 5.78 sec... \tStep: 92340... \tLoss: 0.150... \tVal Loss: 0.414 \tVal Frac: 0.8952\n",
      "\tEpoch: 385/1000; 5.88 sec... \tStep: 93555... \tLoss: 0.240... \tVal Loss: 0.425 \tVal Frac: 0.8904\n",
      "\tEpoch: 390/1000; 5.78 sec... \tStep: 94770... \tLoss: 0.152... \tVal Loss: 0.456 \tVal Frac: 0.8794\n",
      "\tEpoch: 395/1000; 5.71 sec... \tStep: 95985... \tLoss: 0.540... \tVal Loss: 0.418 \tVal Frac: 0.8901\n",
      "\tEpoch: 400/1000; 5.75 sec... \tStep: 97200... \tLoss: 0.142... \tVal Loss: 0.413 \tVal Frac: 0.8938\n",
      "\tEpoch: 405/1000; 5.71 sec... \tStep: 98415... \tLoss: 0.502... \tVal Loss: 0.420 \tVal Frac: 0.8926\n",
      "\tEpoch: 410/1000; 5.77 sec... \tStep: 99630... \tLoss: 0.139... \tVal Loss: 0.449 \tVal Frac: 0.8779\n",
      "\tEpoch: 415/1000; 5.70 sec... \tStep: 100845... \tLoss: 0.529... \tVal Loss: 0.426 \tVal Frac: 0.8908\n",
      "\tEpoch: 420/1000; 5.70 sec... \tStep: 102060... \tLoss: 0.177... \tVal Loss: 0.402 \tVal Frac: 0.8879\n",
      "\tEpoch: 425/1000; 5.75 sec... \tStep: 103275... \tLoss: 0.208... \tVal Loss: 0.405 \tVal Frac: 0.8875\n",
      "\tEpoch: 430/1000; 5.68 sec... \tStep: 104490... \tLoss: 0.598... \tVal Loss: 0.404 \tVal Frac: 0.8886\n",
      "\tEpoch: 435/1000; 5.61 sec... \tStep: 105705... \tLoss: 0.368... \tVal Loss: 0.425 \tVal Frac: 0.8820\n",
      "\tEpoch: 440/1000; 5.70 sec... \tStep: 106920... \tLoss: 0.238... \tVal Loss: 0.398 \tVal Frac: 0.8849\n",
      "\tEpoch: 445/1000; 5.67 sec... \tStep: 108135... \tLoss: 0.491... \tVal Loss: 0.407 \tVal Frac: 0.8835\n",
      "\tEpoch: 450/1000; 5.74 sec... \tStep: 109350... \tLoss: 0.381... \tVal Loss: 0.416 \tVal Frac: 0.8849\n",
      "\tEpoch: 455/1000; 5.83 sec... \tStep: 110565... \tLoss: 0.169... \tVal Loss: 0.397 \tVal Frac: 0.8868\n",
      "\tEpoch: 460/1000; 5.70 sec... \tStep: 111780... \tLoss: 0.303... \tVal Loss: 0.469 \tVal Frac: 0.8809\n",
      "\tEpoch: 465/1000; 5.75 sec... \tStep: 112995... \tLoss: 0.601... \tVal Loss: 0.444 \tVal Frac: 0.8846\n",
      "\tEpoch: 470/1000; 5.75 sec... \tStep: 114210... \tLoss: 0.374... \tVal Loss: 0.428 \tVal Frac: 0.8835\n",
      "\tEpoch: 475/1000; 5.70 sec... \tStep: 115425... \tLoss: 0.541... \tVal Loss: 0.433 \tVal Frac: 0.8853\n",
      "\tEpoch: 480/1000; 5.75 sec... \tStep: 116640... \tLoss: 0.466... \tVal Loss: 0.460 \tVal Frac: 0.8860\n",
      "\tEpoch: 485/1000; 5.75 sec... \tStep: 117855... \tLoss: 0.156... \tVal Loss: 0.433 \tVal Frac: 0.8842\n",
      "\tEpoch: 490/1000; 5.75 sec... \tStep: 119070... \tLoss: 0.361... \tVal Loss: 0.434 \tVal Frac: 0.8901\n",
      "\tEpoch: 495/1000; 5.75 sec... \tStep: 120285... \tLoss: 0.156... \tVal Loss: 0.414 \tVal Frac: 0.8812\n",
      "\tEpoch: 500/1000; 5.75 sec... \tStep: 121500... \tLoss: 0.229... \tVal Loss: 0.425 \tVal Frac: 0.8835\n",
      "\tEpoch: 505/1000; 5.78 sec... \tStep: 122715... \tLoss: 0.295... \tVal Loss: 0.441 \tVal Frac: 0.8805\n",
      "\tEpoch: 510/1000; 5.68 sec... \tStep: 123930... \tLoss: 0.288... \tVal Loss: 0.432 \tVal Frac: 0.8857\n",
      "\tEpoch: 515/1000; 5.65 sec... \tStep: 125145... \tLoss: 0.368... \tVal Loss: 0.429 \tVal Frac: 0.8853\n",
      "\tEpoch: 520/1000; 5.73 sec... \tStep: 126360... \tLoss: 0.719... \tVal Loss: 0.422 \tVal Frac: 0.8912\n",
      "\tEpoch: 525/1000; 5.70 sec... \tStep: 127575... \tLoss: 0.727... \tVal Loss: 0.419 \tVal Frac: 0.8827\n",
      "\tEpoch: 530/1000; 5.77 sec... \tStep: 128790... \tLoss: 0.136... \tVal Loss: 0.479 \tVal Frac: 0.8746\n",
      "\tEpoch: 535/1000; 5.70 sec... \tStep: 130005... \tLoss: 0.284... \tVal Loss: 0.435 \tVal Frac: 0.8864\n",
      "\tEpoch: 540/1000; 5.65 sec... \tStep: 131220... \tLoss: 0.553... \tVal Loss: 0.443 \tVal Frac: 0.8849\n",
      "\tEpoch: 545/1000; 5.78 sec... \tStep: 132435... \tLoss: 0.403... \tVal Loss: 0.421 \tVal Frac: 0.8864\n",
      "\tEpoch: 550/1000; 5.76 sec... \tStep: 133650... \tLoss: 0.424... \tVal Loss: 0.411 \tVal Frac: 0.8890\n",
      "\tEpoch: 555/1000; 5.71 sec... \tStep: 134865... \tLoss: 0.460... \tVal Loss: 0.454 \tVal Frac: 0.8768\n",
      "\tEpoch: 560/1000; 5.91 sec... \tStep: 136080... \tLoss: 0.181... \tVal Loss: 0.414 \tVal Frac: 0.8827\n",
      "\tEpoch: 565/1000; 5.86 sec... \tStep: 137295... \tLoss: 0.219... \tVal Loss: 0.397 \tVal Frac: 0.8838\n",
      "\tEpoch: 570/1000; 5.73 sec... \tStep: 138510... \tLoss: 0.266... \tVal Loss: 0.425 \tVal Frac: 0.8849\n",
      "\tEpoch: 575/1000; 5.70 sec... \tStep: 139725... \tLoss: 0.269... \tVal Loss: 0.407 \tVal Frac: 0.8827\n",
      "\tEpoch: 580/1000; 5.71 sec... \tStep: 140940... \tLoss: 0.651... \tVal Loss: 0.415 \tVal Frac: 0.8820\n",
      "\tEpoch: 585/1000; 5.70 sec... \tStep: 142155... \tLoss: 0.205... \tVal Loss: 0.396 \tVal Frac: 0.8897\n",
      "\tEpoch: 590/1000; 5.78 sec... \tStep: 143370... \tLoss: 0.218... \tVal Loss: 0.452 \tVal Frac: 0.8787\n",
      "\tEpoch: 595/1000; 5.73 sec... \tStep: 144585... \tLoss: 0.670... \tVal Loss: 0.450 \tVal Frac: 0.8871\n",
      "\tEpoch: 600/1000; 5.68 sec... \tStep: 145800... \tLoss: 0.247... \tVal Loss: 0.398 \tVal Frac: 0.8882\n",
      "\tEpoch: 605/1000; 5.77 sec... \tStep: 147015... \tLoss: 0.319... \tVal Loss: 0.407 \tVal Frac: 0.8904\n",
      "\tEpoch: 610/1000; 5.78 sec... \tStep: 148230... \tLoss: 0.102... \tVal Loss: 0.402 \tVal Frac: 0.8824\n",
      "\tEpoch: 615/1000; 5.78 sec... \tStep: 149445... \tLoss: 0.322... \tVal Loss: 0.399 \tVal Frac: 0.8857\n",
      "\tEpoch: 620/1000; 5.77 sec... \tStep: 150660... \tLoss: 0.303... \tVal Loss: 0.426 \tVal Frac: 0.8897\n",
      "\tEpoch: 625/1000; 5.74 sec... \tStep: 151875... \tLoss: 0.435... \tVal Loss: 0.394 \tVal Frac: 0.8849\n",
      "\tEpoch: 630/1000; 5.71 sec... \tStep: 153090... \tLoss: 0.336... \tVal Loss: 0.434 \tVal Frac: 0.8765\n",
      "\tEpoch: 635/1000; 5.71 sec... \tStep: 154305... \tLoss: 0.330... \tVal Loss: 0.434 \tVal Frac: 0.8801\n",
      "\tEpoch: 640/1000; 5.70 sec... \tStep: 155520... \tLoss: 0.291... \tVal Loss: 0.413 \tVal Frac: 0.8820\n",
      "\tEpoch: 645/1000; 5.77 sec... \tStep: 156735... \tLoss: 0.813... \tVal Loss: 0.447 \tVal Frac: 0.8801\n",
      "\tEpoch: 650/1000; 5.65 sec... \tStep: 157950... \tLoss: 0.441... \tVal Loss: 0.459 \tVal Frac: 0.8824\n",
      "\tEpoch: 655/1000; 5.71 sec... \tStep: 159165... \tLoss: 0.528... \tVal Loss: 0.459 \tVal Frac: 0.8790\n",
      "\tEpoch: 660/1000; 5.70 sec... \tStep: 160380... \tLoss: 0.393... \tVal Loss: 0.456 \tVal Frac: 0.8798\n",
      "\tEpoch: 665/1000; 5.68 sec... \tStep: 161595... \tLoss: 0.507... \tVal Loss: 0.458 \tVal Frac: 0.8787\n",
      "\tEpoch: 670/1000; 5.77 sec... \tStep: 162810... \tLoss: 0.363... \tVal Loss: 0.451 \tVal Frac: 0.8790\n",
      "\tEpoch: 675/1000; 5.77 sec... \tStep: 164025... \tLoss: 0.045... \tVal Loss: 0.451 \tVal Frac: 0.8831\n",
      "\tEpoch: 680/1000; 5.77 sec... \tStep: 165240... \tLoss: 0.350... \tVal Loss: 0.425 \tVal Frac: 0.8835\n",
      "\tEpoch: 685/1000; 5.79 sec... \tStep: 166455... \tLoss: 0.276... \tVal Loss: 0.426 \tVal Frac: 0.8842\n",
      "\tEpoch: 690/1000; 5.65 sec... \tStep: 167670... \tLoss: 0.535... \tVal Loss: 0.455 \tVal Frac: 0.8831\n",
      "\tEpoch: 695/1000; 5.68 sec... \tStep: 168885... \tLoss: 0.519... \tVal Loss: 0.428 \tVal Frac: 0.8846\n",
      "\tEpoch: 700/1000; 5.77 sec... \tStep: 170100... \tLoss: 0.775... \tVal Loss: 0.421 \tVal Frac: 0.8868\n",
      "\tEpoch: 705/1000; 5.71 sec... \tStep: 171315... \tLoss: 0.451... \tVal Loss: 0.412 \tVal Frac: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 710/1000; 5.83 sec... \tStep: 172530... \tLoss: 0.157... \tVal Loss: 0.443 \tVal Frac: 0.8787\n",
      "\tEpoch: 715/1000; 5.68 sec... \tStep: 173745... \tLoss: 0.203... \tVal Loss: 0.467 \tVal Frac: 0.8743\n",
      "\tEpoch: 720/1000; 5.71 sec... \tStep: 174960... \tLoss: 0.218... \tVal Loss: 0.472 \tVal Frac: 0.8735\n",
      "\tEpoch: 725/1000; 5.81 sec... \tStep: 176175... \tLoss: 0.295... \tVal Loss: 0.439 \tVal Frac: 0.8779\n",
      "\tEpoch: 730/1000; 5.90 sec... \tStep: 177390... \tLoss: 0.520... \tVal Loss: 0.438 \tVal Frac: 0.8735\n",
      "\tEpoch: 735/1000; 5.71 sec... \tStep: 178605... \tLoss: 0.419... \tVal Loss: 0.489 \tVal Frac: 0.8702\n",
      "\tEpoch: 740/1000; 5.75 sec... \tStep: 179820... \tLoss: 0.747... \tVal Loss: 0.452 \tVal Frac: 0.8787\n",
      "\tEpoch: 745/1000; 5.73 sec... \tStep: 181035... \tLoss: 0.354... \tVal Loss: 0.454 \tVal Frac: 0.8746\n",
      "\tEpoch: 750/1000; 6.53 sec... \tStep: 182250... \tLoss: 0.117... \tVal Loss: 0.445 \tVal Frac: 0.8765\n",
      "\tEpoch: 755/1000; 5.70 sec... \tStep: 183465... \tLoss: 0.607... \tVal Loss: 0.431 \tVal Frac: 0.8812\n",
      "\tEpoch: 760/1000; 5.79 sec... \tStep: 184680... \tLoss: 0.480... \tVal Loss: 0.467 \tVal Frac: 0.8772\n",
      "\tEpoch: 765/1000; 5.83 sec... \tStep: 185895... \tLoss: 0.629... \tVal Loss: 0.454 \tVal Frac: 0.8809\n",
      "\tEpoch: 770/1000; 5.79 sec... \tStep: 187110... \tLoss: 0.387... \tVal Loss: 0.500 \tVal Frac: 0.8713\n",
      "\tEpoch: 775/1000; 5.83 sec... \tStep: 188325... \tLoss: 0.587... \tVal Loss: 0.493 \tVal Frac: 0.8713\n",
      "\tEpoch: 780/1000; 5.78 sec... \tStep: 189540... \tLoss: 0.453... \tVal Loss: 0.491 \tVal Frac: 0.8621\n",
      "\tEpoch: 785/1000; 5.86 sec... \tStep: 190755... \tLoss: 0.554... \tVal Loss: 0.479 \tVal Frac: 0.8643\n",
      "\tEpoch: 790/1000; 5.83 sec... \tStep: 191970... \tLoss: 0.507... \tVal Loss: 0.560 \tVal Frac: 0.8397\n",
      "\tEpoch: 795/1000; 5.86 sec... \tStep: 193185... \tLoss: 0.409... \tVal Loss: 0.565 \tVal Frac: 0.8187\n",
      "\tEpoch: 800/1000; 5.82 sec... \tStep: 194400... \tLoss: 0.556... \tVal Loss: 0.539 \tVal Frac: 0.8382\n",
      "\tEpoch: 805/1000; 5.84 sec... \tStep: 195615... \tLoss: 0.510... \tVal Loss: 0.523 \tVal Frac: 0.8500\n",
      "\tEpoch: 810/1000; 5.85 sec... \tStep: 196830... \tLoss: 0.629... \tVal Loss: 0.518 \tVal Frac: 0.8430\n",
      "\tEpoch: 815/1000; 5.87 sec... \tStep: 198045... \tLoss: 0.657... \tVal Loss: 0.560 \tVal Frac: 0.8386\n",
      "\tEpoch: 820/1000; 5.75 sec... \tStep: 199260... \tLoss: 0.521... \tVal Loss: 0.632 \tVal Frac: 0.8015\n",
      "\tEpoch: 825/1000; 5.81 sec... \tStep: 200475... \tLoss: 0.823... \tVal Loss: 0.636 \tVal Frac: 0.8125\n",
      "\tEpoch: 830/1000; 5.88 sec... \tStep: 201690... \tLoss: 0.500... \tVal Loss: 0.646 \tVal Frac: 0.7985\n",
      "\tEpoch: 835/1000; 5.88 sec... \tStep: 202905... \tLoss: 0.643... \tVal Loss: 0.765 \tVal Frac: 0.7471\n",
      "\tEpoch: 840/1000; 5.79 sec... \tStep: 204120... \tLoss: 0.749... \tVal Loss: 0.692 \tVal Frac: 0.7941\n",
      "\tEpoch: 845/1000; 5.81 sec... \tStep: 205335... \tLoss: 0.623... \tVal Loss: 0.680 \tVal Frac: 0.7765\n",
      "\tEpoch: 850/1000; 5.70 sec... \tStep: 206550... \tLoss: 0.788... \tVal Loss: 1.174 \tVal Frac: 0.5735\n",
      "\tEpoch: 855/1000; 5.70 sec... \tStep: 207765... \tLoss: 0.907... \tVal Loss: 0.691 \tVal Frac: 0.7710\n",
      "\tEpoch: 860/1000; 5.85 sec... \tStep: 208980... \tLoss: 0.896... \tVal Loss: 0.690 \tVal Frac: 0.7599\n",
      "\tEpoch: 865/1000; 5.83 sec... \tStep: 210195... \tLoss: 0.658... \tVal Loss: 0.716 \tVal Frac: 0.7460\n",
      "\tEpoch: 870/1000; 5.81 sec... \tStep: 211410... \tLoss: 0.808... \tVal Loss: 0.712 \tVal Frac: 0.7651\n",
      "\tEpoch: 875/1000; 6.01 sec... \tStep: 212625... \tLoss: 0.781... \tVal Loss: 0.711 \tVal Frac: 0.7790\n",
      "\tEpoch: 880/1000; 5.85 sec... \tStep: 213840... \tLoss: 1.047... \tVal Loss: 0.702 \tVal Frac: 0.7790\n",
      "\tEpoch: 885/1000; 5.85 sec... \tStep: 215055... \tLoss: 0.709... \tVal Loss: 0.855 \tVal Frac: 0.7246\n",
      "\tEpoch: 890/1000; 5.82 sec... \tStep: 216270... \tLoss: 0.891... \tVal Loss: 0.767 \tVal Frac: 0.7504\n",
      "\tEpoch: 895/1000; 5.80 sec... \tStep: 217485... \tLoss: 1.355... \tVal Loss: 1.458 \tVal Frac: 0.3206\n",
      "\tEpoch: 900/1000; 5.80 sec... \tStep: 218700... \tLoss: 1.622... \tVal Loss: 1.295 \tVal Frac: 0.5801\n",
      "\tEpoch: 905/1000; 5.77 sec... \tStep: 219915... \tLoss: 1.287... \tVal Loss: 1.219 \tVal Frac: 0.6496\n",
      "\tEpoch: 910/1000; 5.83 sec... \tStep: 221130... \tLoss: 1.352... \tVal Loss: 0.954 \tVal Frac: 0.7022\n",
      "\tEpoch: 915/1000; 5.85 sec... \tStep: 222345... \tLoss: 0.788... \tVal Loss: 1.117 \tVal Frac: 0.6349\n",
      "\tEpoch: 920/1000; 5.92 sec... \tStep: 223560... \tLoss: 1.102... \tVal Loss: 0.823 \tVal Frac: 0.7298\n",
      "\tEpoch: 925/1000; 5.83 sec... \tStep: 224775... \tLoss: 1.533... \tVal Loss: 0.785 \tVal Frac: 0.7316\n",
      "\tEpoch: 930/1000; 5.90 sec... \tStep: 225990... \tLoss: 1.621... \tVal Loss: 0.870 \tVal Frac: 0.7324\n",
      "\tEpoch: 935/1000; 5.92 sec... \tStep: 227205... \tLoss: 0.729... \tVal Loss: 1.166 \tVal Frac: 0.6415\n",
      "\tEpoch: 940/1000; 5.83 sec... \tStep: 228420... \tLoss: 0.731... \tVal Loss: 1.090 \tVal Frac: 0.6607\n",
      "\tEpoch: 945/1000; 5.85 sec... \tStep: 229635... \tLoss: 1.098... \tVal Loss: 1.319 \tVal Frac: 0.5515\n",
      "\tEpoch: 950/1000; 5.87 sec... \tStep: 230850... \tLoss: 1.285... \tVal Loss: 1.387 \tVal Frac: 0.4974\n",
      "\tEpoch: 955/1000; 5.85 sec... \tStep: 232065... \tLoss: 1.279... \tVal Loss: 1.436 \tVal Frac: 0.4691\n",
      "\tEpoch: 960/1000; 5.77 sec... \tStep: 233280... \tLoss: 1.248... \tVal Loss: 1.506 \tVal Frac: 0.3941\n",
      "\tEpoch: 965/1000; 5.79 sec... \tStep: 234495... \tLoss: 1.023... \tVal Loss: 1.478 \tVal Frac: 0.4287\n",
      "\tEpoch: 970/1000; 5.95 sec... \tStep: 235710... \tLoss: 1.014... \tVal Loss: 1.449 \tVal Frac: 0.4724\n",
      "\tEpoch: 975/1000; 5.79 sec... \tStep: 236925... \tLoss: 1.299... \tVal Loss: 1.492 \tVal Frac: 0.4114\n",
      "\tEpoch: 980/1000; 5.83 sec... \tStep: 238140... \tLoss: 1.303... \tVal Loss: 1.478 \tVal Frac: 0.4515\n",
      "\tEpoch: 985/1000; 5.88 sec... \tStep: 239355... \tLoss: 1.261... \tVal Loss: 1.421 \tVal Frac: 0.4673\n",
      "\tEpoch: 990/1000; 5.87 sec... \tStep: 240570... \tLoss: 1.518... \tVal Loss: 1.392 \tVal Frac: 0.5140\n",
      "\tEpoch: 995/1000; 5.93 sec... \tStep: 241785... \tLoss: 1.238... \tVal Loss: 1.340 \tVal Frac: 0.5379\n",
      "\tEpoch: 1000/1000; 5.80 sec... \tStep: 243000... \tLoss: 1.158... \tVal Loss: 1.421 \tVal Frac: 0.4893\n",
      "Completed:  n_layers_LSTM :  5 \n",
      "\tloss: 0.378481 \n",
      "\tfrac:0.895956\n",
      "\n",
      "###########\n",
      "Testing with  lr :  0.001\n",
      "\tEpoch: 5/1000; 1.40 sec... \tStep: 1215... \tLoss: 0.854... \tVal Loss: 0.783 \tVal Frac: 0.7511\n",
      "\tValidation loss decreased (inf --> 0.783).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.40 sec... \tStep: 2430... \tLoss: 0.985... \tVal Loss: 0.804 \tVal Frac: 0.7846\n",
      "\tEpoch: 15/1000; 1.39 sec... \tStep: 3645... \tLoss: 1.013... \tVal Loss: 0.771 \tVal Frac: 0.7596\n",
      "\tValidation loss decreased (0.783 --> 0.771).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.33 sec... \tStep: 4860... \tLoss: 1.376... \tVal Loss: 1.028 \tVal Frac: 0.7562\n",
      "\tEpoch: 25/1000; 1.50 sec... \tStep: 6075... \tLoss: 0.699... \tVal Loss: 0.686 \tVal Frac: 0.7728\n",
      "\tValidation loss decreased (0.771 --> 0.686).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.37 sec... \tStep: 7290... \tLoss: 0.895... \tVal Loss: 0.719 \tVal Frac: 0.7629\n",
      "\tEpoch: 35/1000; 1.36 sec... \tStep: 8505... \tLoss: 0.725... \tVal Loss: 0.664 \tVal Frac: 0.7695\n",
      "\tValidation loss decreased (0.686 --> 0.664).  Saving model ...\n",
      "\tEpoch: 40/1000; 1.41 sec... \tStep: 9720... \tLoss: 0.855... \tVal Loss: 0.722 \tVal Frac: 0.7533\n",
      "\tEpoch: 45/1000; 1.27 sec... \tStep: 10935... \tLoss: 1.009... \tVal Loss: 0.673 \tVal Frac: 0.7746\n",
      "\tEpoch: 50/1000; 1.48 sec... \tStep: 12150... \tLoss: 0.939... \tVal Loss: 0.709 \tVal Frac: 0.7375\n",
      "\tEpoch: 55/1000; 1.54 sec... \tStep: 13365... \tLoss: 0.962... \tVal Loss: 0.691 \tVal Frac: 0.7526\n",
      "\tEpoch: 60/1000; 1.40 sec... \tStep: 14580... \tLoss: 0.655... \tVal Loss: 0.670 \tVal Frac: 0.7794\n",
      "\tEpoch: 65/1000; 1.48 sec... \tStep: 15795... \tLoss: 0.927... \tVal Loss: 0.635 \tVal Frac: 0.7893\n",
      "\tValidation loss decreased (0.664 --> 0.635).  Saving model ...\n",
      "\tEpoch: 70/1000; 1.51 sec... \tStep: 17010... \tLoss: 0.845... \tVal Loss: 0.657 \tVal Frac: 0.7743\n",
      "\tEpoch: 75/1000; 1.42 sec... \tStep: 18225... \tLoss: 1.129... \tVal Loss: 0.643 \tVal Frac: 0.7801\n",
      "\tEpoch: 80/1000; 1.46 sec... \tStep: 19440... \tLoss: 0.541... \tVal Loss: 0.643 \tVal Frac: 0.7963\n",
      "\tEpoch: 85/1000; 1.38 sec... \tStep: 20655... \tLoss: 1.019... \tVal Loss: 0.649 \tVal Frac: 0.7915\n",
      "\tEpoch: 90/1000; 1.49 sec... \tStep: 21870... \tLoss: 0.755... \tVal Loss: 0.613 \tVal Frac: 0.7996\n",
      "\tValidation loss decreased (0.635 --> 0.613).  Saving model ...\n",
      "\tEpoch: 95/1000; 1.44 sec... \tStep: 23085... \tLoss: 0.806... \tVal Loss: 0.633 \tVal Frac: 0.8037\n",
      "\tEpoch: 100/1000; 1.43 sec... \tStep: 24300... \tLoss: 0.790... \tVal Loss: 0.627 \tVal Frac: 0.8070\n",
      "\tEpoch: 105/1000; 1.39 sec... \tStep: 25515... \tLoss: 0.603... \tVal Loss: 0.639 \tVal Frac: 0.7963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 110/1000; 1.55 sec... \tStep: 26730... \tLoss: 1.476... \tVal Loss: 0.820 \tVal Frac: 0.7640\n",
      "\tEpoch: 115/1000; 1.30 sec... \tStep: 27945... \tLoss: 0.913... \tVal Loss: 0.682 \tVal Frac: 0.8004\n",
      "\tEpoch: 120/1000; 1.35 sec... \tStep: 29160... \tLoss: 0.934... \tVal Loss: 0.642 \tVal Frac: 0.8044\n",
      "\tEpoch: 125/1000; 1.44 sec... \tStep: 30375... \tLoss: 1.265... \tVal Loss: 0.683 \tVal Frac: 0.7978\n",
      "\tEpoch: 130/1000; 1.33 sec... \tStep: 31590... \tLoss: 0.758... \tVal Loss: 0.666 \tVal Frac: 0.7890\n",
      "\tEpoch: 135/1000; 1.44 sec... \tStep: 32805... \tLoss: 0.733... \tVal Loss: 0.633 \tVal Frac: 0.7952\n",
      "\tEpoch: 140/1000; 1.49 sec... \tStep: 34020... \tLoss: 0.883... \tVal Loss: 0.655 \tVal Frac: 0.7790\n",
      "\tEpoch: 145/1000; 1.43 sec... \tStep: 35235... \tLoss: 0.549... \tVal Loss: 0.618 \tVal Frac: 0.7952\n",
      "\tEpoch: 150/1000; 1.37 sec... \tStep: 36450... \tLoss: 0.860... \tVal Loss: 0.694 \tVal Frac: 0.7750\n",
      "\tEpoch: 155/1000; 1.50 sec... \tStep: 37665... \tLoss: 0.782... \tVal Loss: 0.662 \tVal Frac: 0.7820\n",
      "\tEpoch: 160/1000; 1.40 sec... \tStep: 38880... \tLoss: 0.859... \tVal Loss: 0.701 \tVal Frac: 0.7875\n",
      "\tEpoch: 165/1000; 1.45 sec... \tStep: 40095... \tLoss: 0.880... \tVal Loss: 0.642 \tVal Frac: 0.7607\n",
      "\tEpoch: 170/1000; 1.49 sec... \tStep: 41310... \tLoss: 1.035... \tVal Loss: 0.653 \tVal Frac: 0.7765\n",
      "\tEpoch: 175/1000; 1.50 sec... \tStep: 42525... \tLoss: 0.792... \tVal Loss: 0.620 \tVal Frac: 0.7812\n",
      "\tEpoch: 180/1000; 1.39 sec... \tStep: 43740... \tLoss: 0.488... \tVal Loss: 0.635 \tVal Frac: 0.7923\n",
      "\tEpoch: 185/1000; 1.39 sec... \tStep: 44955... \tLoss: 0.890... \tVal Loss: 0.698 \tVal Frac: 0.7827\n",
      "\tEpoch: 190/1000; 1.37 sec... \tStep: 46170... \tLoss: 0.639... \tVal Loss: 0.639 \tVal Frac: 0.7732\n",
      "\tEpoch: 195/1000; 1.39 sec... \tStep: 47385... \tLoss: 0.635... \tVal Loss: 0.641 \tVal Frac: 0.7691\n",
      "\tEpoch: 200/1000; 1.39 sec... \tStep: 48600... \tLoss: 0.854... \tVal Loss: 0.643 \tVal Frac: 0.7688\n",
      "\tEpoch: 205/1000; 1.46 sec... \tStep: 49815... \tLoss: 0.872... \tVal Loss: 0.610 \tVal Frac: 0.7857\n",
      "\tValidation loss decreased (0.613 --> 0.610).  Saving model ...\n",
      "\tEpoch: 210/1000; 1.37 sec... \tStep: 51030... \tLoss: 0.698... \tVal Loss: 0.640 \tVal Frac: 0.7886\n",
      "\tEpoch: 215/1000; 1.45 sec... \tStep: 52245... \tLoss: 1.353... \tVal Loss: 0.664 \tVal Frac: 0.7691\n",
      "\tEpoch: 220/1000; 1.56 sec... \tStep: 53460... \tLoss: 0.836... \tVal Loss: 0.704 \tVal Frac: 0.7732\n",
      "\tEpoch: 225/1000; 1.47 sec... \tStep: 54675... \tLoss: 0.907... \tVal Loss: 0.648 \tVal Frac: 0.7739\n",
      "\tEpoch: 230/1000; 1.35 sec... \tStep: 55890... \tLoss: 0.942... \tVal Loss: 0.623 \tVal Frac: 0.7805\n",
      "\tEpoch: 235/1000; 1.39 sec... \tStep: 57105... \tLoss: 0.911... \tVal Loss: 0.623 \tVal Frac: 0.7945\n",
      "\tEpoch: 240/1000; 1.29 sec... \tStep: 58320... \tLoss: 1.033... \tVal Loss: 0.618 \tVal Frac: 0.7963\n",
      "\tEpoch: 245/1000; 1.37 sec... \tStep: 59535... \tLoss: 1.011... \tVal Loss: 0.640 \tVal Frac: 0.7728\n",
      "\tEpoch: 250/1000; 1.46 sec... \tStep: 60750... \tLoss: 0.927... \tVal Loss: 0.664 \tVal Frac: 0.7629\n",
      "\tEpoch: 255/1000; 1.33 sec... \tStep: 61965... \tLoss: 0.910... \tVal Loss: 0.624 \tVal Frac: 0.7779\n",
      "\tEpoch: 260/1000; 1.45 sec... \tStep: 63180... \tLoss: 0.856... \tVal Loss: 0.672 \tVal Frac: 0.7544\n",
      "\tEpoch: 265/1000; 1.37 sec... \tStep: 64395... \tLoss: 0.857... \tVal Loss: 0.657 \tVal Frac: 0.7684\n",
      "\tEpoch: 270/1000; 1.30 sec... \tStep: 65610... \tLoss: 0.770... \tVal Loss: 0.619 \tVal Frac: 0.7794\n",
      "\tEpoch: 275/1000; 1.35 sec... \tStep: 66825... \tLoss: 0.893... \tVal Loss: 0.623 \tVal Frac: 0.7857\n",
      "\tEpoch: 280/1000; 1.41 sec... \tStep: 68040... \tLoss: 1.063... \tVal Loss: 0.710 \tVal Frac: 0.7743\n",
      "\tEpoch: 285/1000; 1.46 sec... \tStep: 69255... \tLoss: 0.710... \tVal Loss: 0.711 \tVal Frac: 0.7537\n",
      "\tEpoch: 290/1000; 1.41 sec... \tStep: 70470... \tLoss: 0.852... \tVal Loss: 0.659 \tVal Frac: 0.7783\n",
      "\tEpoch: 295/1000; 1.45 sec... \tStep: 71685... \tLoss: 0.643... \tVal Loss: 0.632 \tVal Frac: 0.7798\n",
      "\tEpoch: 300/1000; 1.42 sec... \tStep: 72900... \tLoss: 0.825... \tVal Loss: 0.691 \tVal Frac: 0.7607\n",
      "\tEpoch: 305/1000; 1.36 sec... \tStep: 74115... \tLoss: 1.096... \tVal Loss: 0.655 \tVal Frac: 0.7827\n",
      "\tEpoch: 310/1000; 1.43 sec... \tStep: 75330... \tLoss: 0.621... \tVal Loss: 0.646 \tVal Frac: 0.7827\n",
      "\tEpoch: 315/1000; 1.52 sec... \tStep: 76545... \tLoss: 1.097... \tVal Loss: 0.823 \tVal Frac: 0.7287\n",
      "\tEpoch: 320/1000; 1.46 sec... \tStep: 77760... \tLoss: 0.545... \tVal Loss: 0.701 \tVal Frac: 0.7555\n",
      "\tEpoch: 325/1000; 1.40 sec... \tStep: 78975... \tLoss: 1.265... \tVal Loss: 0.652 \tVal Frac: 0.7805\n",
      "\tEpoch: 330/1000; 1.37 sec... \tStep: 80190... \tLoss: 0.770... \tVal Loss: 0.733 \tVal Frac: 0.7610\n",
      "\tEpoch: 335/1000; 1.43 sec... \tStep: 81405... \tLoss: 0.684... \tVal Loss: 0.641 \tVal Frac: 0.7790\n",
      "\tEpoch: 340/1000; 1.43 sec... \tStep: 82620... \tLoss: 0.691... \tVal Loss: 0.665 \tVal Frac: 0.7684\n",
      "\tEpoch: 345/1000; 1.39 sec... \tStep: 83835... \tLoss: 0.988... \tVal Loss: 0.614 \tVal Frac: 0.7835\n",
      "\tEpoch: 350/1000; 1.35 sec... \tStep: 85050... \tLoss: 0.399... \tVal Loss: 0.653 \tVal Frac: 0.7728\n",
      "\tEpoch: 355/1000; 1.33 sec... \tStep: 86265... \tLoss: 1.043... \tVal Loss: 0.664 \tVal Frac: 0.7739\n",
      "\tEpoch: 360/1000; 1.47 sec... \tStep: 87480... \tLoss: 1.024... \tVal Loss: 0.661 \tVal Frac: 0.7875\n",
      "\tEpoch: 365/1000; 1.43 sec... \tStep: 88695... \tLoss: 0.821... \tVal Loss: 0.620 \tVal Frac: 0.8000\n",
      "\tEpoch: 370/1000; 1.37 sec... \tStep: 89910... \tLoss: 0.754... \tVal Loss: 0.621 \tVal Frac: 0.8110\n",
      "\tEpoch: 375/1000; 1.29 sec... \tStep: 91125... \tLoss: 1.168... \tVal Loss: 0.702 \tVal Frac: 0.7882\n",
      "\tEpoch: 380/1000; 1.43 sec... \tStep: 92340... \tLoss: 1.172... \tVal Loss: 0.724 \tVal Frac: 0.7614\n",
      "\tEpoch: 385/1000; 1.49 sec... \tStep: 93555... \tLoss: 0.835... \tVal Loss: 0.626 \tVal Frac: 0.7993\n",
      "\tEpoch: 390/1000; 1.39 sec... \tStep: 94770... \tLoss: 0.844... \tVal Loss: 0.699 \tVal Frac: 0.7904\n",
      "\tEpoch: 395/1000; 1.36 sec... \tStep: 95985... \tLoss: 0.621... \tVal Loss: 0.616 \tVal Frac: 0.8118\n",
      "\tEpoch: 400/1000; 1.33 sec... \tStep: 97200... \tLoss: 0.741... \tVal Loss: 0.608 \tVal Frac: 0.8048\n",
      "\tValidation loss decreased (0.610 --> 0.608).  Saving model ...\n",
      "\tEpoch: 405/1000; 1.47 sec... \tStep: 98415... \tLoss: 0.560... \tVal Loss: 0.610 \tVal Frac: 0.7993\n",
      "\tEpoch: 410/1000; 1.43 sec... \tStep: 99630... \tLoss: 0.731... \tVal Loss: 0.614 \tVal Frac: 0.8066\n",
      "\tEpoch: 415/1000; 1.48 sec... \tStep: 100845... \tLoss: 0.610... \tVal Loss: 0.594 \tVal Frac: 0.8037\n",
      "\tValidation loss decreased (0.608 --> 0.594).  Saving model ...\n",
      "\tEpoch: 420/1000; 1.35 sec... \tStep: 102060... \tLoss: 1.164... \tVal Loss: 0.589 \tVal Frac: 0.8169\n",
      "\tValidation loss decreased (0.594 --> 0.589).  Saving model ...\n",
      "\tEpoch: 425/1000; 1.40 sec... \tStep: 103275... \tLoss: 0.635... \tVal Loss: 0.585 \tVal Frac: 0.8143\n",
      "\tValidation loss decreased (0.589 --> 0.585).  Saving model ...\n",
      "\tEpoch: 430/1000; 1.30 sec... \tStep: 104490... \tLoss: 1.066... \tVal Loss: 0.600 \tVal Frac: 0.7993\n",
      "\tEpoch: 435/1000; 1.33 sec... \tStep: 105705... \tLoss: 0.845... \tVal Loss: 0.613 \tVal Frac: 0.7882\n",
      "\tEpoch: 440/1000; 1.39 sec... \tStep: 106920... \tLoss: 0.732... \tVal Loss: 0.586 \tVal Frac: 0.8066\n",
      "\tEpoch: 445/1000; 1.33 sec... \tStep: 108135... \tLoss: 0.723... \tVal Loss: 0.570 \tVal Frac: 0.8180\n",
      "\tValidation loss decreased (0.585 --> 0.570).  Saving model ...\n",
      "\tEpoch: 450/1000; 1.43 sec... \tStep: 109350... \tLoss: 0.575... \tVal Loss: 0.595 \tVal Frac: 0.8018\n",
      "\tEpoch: 455/1000; 1.39 sec... \tStep: 110565... \tLoss: 0.908... \tVal Loss: 0.580 \tVal Frac: 0.8085\n",
      "\tEpoch: 460/1000; 1.40 sec... \tStep: 111780... \tLoss: 1.023... \tVal Loss: 0.613 \tVal Frac: 0.8040\n",
      "\tEpoch: 465/1000; 1.39 sec... \tStep: 112995... \tLoss: 0.827... \tVal Loss: 0.592 \tVal Frac: 0.8070\n",
      "\tEpoch: 470/1000; 1.49 sec... \tStep: 114210... \tLoss: 0.475... \tVal Loss: 0.623 \tVal Frac: 0.7989\n",
      "\tEpoch: 475/1000; 1.47 sec... \tStep: 115425... \tLoss: 0.586... \tVal Loss: 0.606 \tVal Frac: 0.8055\n",
      "\tEpoch: 480/1000; 1.43 sec... \tStep: 116640... \tLoss: 0.654... \tVal Loss: 0.709 \tVal Frac: 0.7809\n",
      "\tEpoch: 485/1000; 1.52 sec... \tStep: 117855... \tLoss: 0.517... \tVal Loss: 0.619 \tVal Frac: 0.8015\n",
      "\tEpoch: 490/1000; 1.56 sec... \tStep: 119070... \tLoss: 0.532... \tVal Loss: 0.618 \tVal Frac: 0.7937\n",
      "\tEpoch: 495/1000; 1.50 sec... \tStep: 120285... \tLoss: 1.030... \tVal Loss: 0.647 \tVal Frac: 0.7842\n",
      "\tEpoch: 500/1000; 1.35 sec... \tStep: 121500... \tLoss: 0.674... \tVal Loss: 0.617 \tVal Frac: 0.8040\n",
      "\tEpoch: 505/1000; 1.43 sec... \tStep: 122715... \tLoss: 0.751... \tVal Loss: 0.598 \tVal Frac: 0.8044\n",
      "\tEpoch: 510/1000; 1.45 sec... \tStep: 123930... \tLoss: 1.053... \tVal Loss: 0.595 \tVal Frac: 0.8051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 515/1000; 1.46 sec... \tStep: 125145... \tLoss: 0.408... \tVal Loss: 0.597 \tVal Frac: 0.8092\n",
      "\tEpoch: 520/1000; 1.36 sec... \tStep: 126360... \tLoss: 0.942... \tVal Loss: 0.585 \tVal Frac: 0.8143\n",
      "\tEpoch: 525/1000; 1.37 sec... \tStep: 127575... \tLoss: 0.406... \tVal Loss: 0.564 \tVal Frac: 0.8162\n",
      "\tValidation loss decreased (0.570 --> 0.564).  Saving model ...\n",
      "\tEpoch: 530/1000; 1.40 sec... \tStep: 128790... \tLoss: 0.711... \tVal Loss: 0.551 \tVal Frac: 0.8224\n",
      "\tValidation loss decreased (0.564 --> 0.551).  Saving model ...\n",
      "\tEpoch: 535/1000; 1.54 sec... \tStep: 130005... \tLoss: 0.910... \tVal Loss: 0.568 \tVal Frac: 0.8184\n",
      "\tEpoch: 540/1000; 1.30 sec... \tStep: 131220... \tLoss: 0.681... \tVal Loss: 0.612 \tVal Frac: 0.8191\n",
      "\tEpoch: 545/1000; 1.50 sec... \tStep: 132435... \tLoss: 0.941... \tVal Loss: 0.631 \tVal Frac: 0.7993\n",
      "\tEpoch: 550/1000; 1.38 sec... \tStep: 133650... \tLoss: 0.906... \tVal Loss: 0.566 \tVal Frac: 0.8140\n",
      "\tEpoch: 555/1000; 1.46 sec... \tStep: 134865... \tLoss: 0.899... \tVal Loss: 0.576 \tVal Frac: 0.8184\n",
      "\tEpoch: 560/1000; 1.42 sec... \tStep: 136080... \tLoss: 0.860... \tVal Loss: 0.599 \tVal Frac: 0.8125\n",
      "\tEpoch: 565/1000; 1.40 sec... \tStep: 137295... \tLoss: 0.787... \tVal Loss: 0.563 \tVal Frac: 0.8243\n",
      "\tEpoch: 570/1000; 1.38 sec... \tStep: 138510... \tLoss: 1.059... \tVal Loss: 0.573 \tVal Frac: 0.8191\n",
      "\tEpoch: 575/1000; 1.35 sec... \tStep: 139725... \tLoss: 0.649... \tVal Loss: 0.615 \tVal Frac: 0.7952\n",
      "\tEpoch: 580/1000; 1.49 sec... \tStep: 140940... \tLoss: 0.569... \tVal Loss: 0.614 \tVal Frac: 0.8118\n",
      "\tEpoch: 585/1000; 1.29 sec... \tStep: 142155... \tLoss: 0.709... \tVal Loss: 0.580 \tVal Frac: 0.8121\n",
      "\tEpoch: 590/1000; 1.47 sec... \tStep: 143370... \tLoss: 0.909... \tVal Loss: 0.568 \tVal Frac: 0.8169\n",
      "\tEpoch: 595/1000; 1.39 sec... \tStep: 144585... \tLoss: 0.814... \tVal Loss: 0.599 \tVal Frac: 0.8092\n",
      "\tEpoch: 600/1000; 1.32 sec... \tStep: 145800... \tLoss: 1.525... \tVal Loss: 0.555 \tVal Frac: 0.8301\n",
      "\tEpoch: 605/1000; 1.37 sec... \tStep: 147015... \tLoss: 0.851... \tVal Loss: 0.541 \tVal Frac: 0.8254\n",
      "\tValidation loss decreased (0.551 --> 0.541).  Saving model ...\n",
      "\tEpoch: 610/1000; 1.36 sec... \tStep: 148230... \tLoss: 1.166... \tVal Loss: 0.548 \tVal Frac: 0.8298\n",
      "\tEpoch: 615/1000; 1.39 sec... \tStep: 149445... \tLoss: 0.741... \tVal Loss: 0.549 \tVal Frac: 0.8301\n",
      "\tEpoch: 620/1000; 1.43 sec... \tStep: 150660... \tLoss: 0.580... \tVal Loss: 0.511 \tVal Frac: 0.8360\n",
      "\tValidation loss decreased (0.541 --> 0.511).  Saving model ...\n",
      "\tEpoch: 625/1000; 1.44 sec... \tStep: 151875... \tLoss: 0.761... \tVal Loss: 0.625 \tVal Frac: 0.8074\n",
      "\tEpoch: 630/1000; 1.40 sec... \tStep: 153090... \tLoss: 1.112... \tVal Loss: 0.579 \tVal Frac: 0.8232\n",
      "\tEpoch: 635/1000; 1.25 sec... \tStep: 154305... \tLoss: 0.711... \tVal Loss: 0.562 \tVal Frac: 0.8187\n",
      "\tEpoch: 640/1000; 1.32 sec... \tStep: 155520... \tLoss: 0.530... \tVal Loss: 0.518 \tVal Frac: 0.8353\n",
      "\tEpoch: 645/1000; 1.29 sec... \tStep: 156735... \tLoss: 0.651... \tVal Loss: 0.514 \tVal Frac: 0.8331\n",
      "\tEpoch: 650/1000; 1.35 sec... \tStep: 157950... \tLoss: 0.347... \tVal Loss: 0.518 \tVal Frac: 0.8320\n",
      "\tEpoch: 655/1000; 1.49 sec... \tStep: 159165... \tLoss: 0.610... \tVal Loss: 0.497 \tVal Frac: 0.8371\n",
      "\tValidation loss decreased (0.511 --> 0.497).  Saving model ...\n",
      "\tEpoch: 660/1000; 1.46 sec... \tStep: 160380... \tLoss: 0.620... \tVal Loss: 0.540 \tVal Frac: 0.8176\n",
      "\tEpoch: 665/1000; 1.47 sec... \tStep: 161595... \tLoss: 0.704... \tVal Loss: 0.557 \tVal Frac: 0.8364\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 0.645... \tVal Loss: 0.533 \tVal Frac: 0.8279\n",
      "\tEpoch: 675/1000; 1.47 sec... \tStep: 164025... \tLoss: 0.596... \tVal Loss: 0.573 \tVal Frac: 0.8169\n",
      "\tEpoch: 680/1000; 1.35 sec... \tStep: 165240... \tLoss: 0.715... \tVal Loss: 0.520 \tVal Frac: 0.8309\n",
      "\tEpoch: 685/1000; 1.38 sec... \tStep: 166455... \tLoss: 0.378... \tVal Loss: 0.530 \tVal Frac: 0.8327\n",
      "\tEpoch: 690/1000; 1.48 sec... \tStep: 167670... \tLoss: 0.691... \tVal Loss: 0.523 \tVal Frac: 0.8327\n",
      "\tEpoch: 695/1000; 1.40 sec... \tStep: 168885... \tLoss: 0.584... \tVal Loss: 0.529 \tVal Frac: 0.8224\n",
      "\tEpoch: 700/1000; 1.49 sec... \tStep: 170100... \tLoss: 0.855... \tVal Loss: 0.547 \tVal Frac: 0.8272\n",
      "\tEpoch: 705/1000; 1.45 sec... \tStep: 171315... \tLoss: 0.692... \tVal Loss: 0.524 \tVal Frac: 0.8313\n",
      "\tEpoch: 710/1000; 1.44 sec... \tStep: 172530... \tLoss: 0.830... \tVal Loss: 0.521 \tVal Frac: 0.8298\n",
      "\tEpoch: 715/1000; 1.47 sec... \tStep: 173745... \tLoss: 0.587... \tVal Loss: 0.519 \tVal Frac: 0.8301\n",
      "\tEpoch: 720/1000; 1.40 sec... \tStep: 174960... \tLoss: 0.604... \tVal Loss: 0.510 \tVal Frac: 0.8301\n",
      "\tEpoch: 725/1000; 1.56 sec... \tStep: 176175... \tLoss: 0.557... \tVal Loss: 0.599 \tVal Frac: 0.8158\n",
      "\tEpoch: 730/1000; 1.44 sec... \tStep: 177390... \tLoss: 0.980... \tVal Loss: 0.517 \tVal Frac: 0.8301\n",
      "\tEpoch: 735/1000; 1.37 sec... \tStep: 178605... \tLoss: 0.566... \tVal Loss: 0.545 \tVal Frac: 0.8254\n",
      "\tEpoch: 740/1000; 1.43 sec... \tStep: 179820... \tLoss: 0.819... \tVal Loss: 0.506 \tVal Frac: 0.8349\n",
      "\tEpoch: 745/1000; 1.37 sec... \tStep: 181035... \tLoss: 0.791... \tVal Loss: 0.498 \tVal Frac: 0.8415\n",
      "\tEpoch: 750/1000; 1.40 sec... \tStep: 182250... \tLoss: 0.675... \tVal Loss: 0.515 \tVal Frac: 0.8357\n",
      "\tEpoch: 755/1000; 1.39 sec... \tStep: 183465... \tLoss: 0.730... \tVal Loss: 0.517 \tVal Frac: 0.8371\n",
      "\tEpoch: 760/1000; 1.42 sec... \tStep: 184680... \tLoss: 0.715... \tVal Loss: 0.534 \tVal Frac: 0.8239\n",
      "\tEpoch: 765/1000; 1.38 sec... \tStep: 185895... \tLoss: 0.373... \tVal Loss: 0.529 \tVal Frac: 0.8268\n",
      "\tEpoch: 770/1000; 1.32 sec... \tStep: 187110... \tLoss: 0.821... \tVal Loss: 0.534 \tVal Frac: 0.8287\n",
      "\tEpoch: 775/1000; 1.42 sec... \tStep: 188325... \tLoss: 0.568... \tVal Loss: 0.514 \tVal Frac: 0.8320\n",
      "\tEpoch: 780/1000; 1.37 sec... \tStep: 189540... \tLoss: 0.582... \tVal Loss: 0.525 \tVal Frac: 0.8309\n",
      "\tEpoch: 785/1000; 1.44 sec... \tStep: 190755... \tLoss: 0.568... \tVal Loss: 0.546 \tVal Frac: 0.8279\n",
      "\tEpoch: 790/1000; 1.38 sec... \tStep: 191970... \tLoss: 0.660... \tVal Loss: 0.519 \tVal Frac: 0.8279\n",
      "\tEpoch: 795/1000; 1.56 sec... \tStep: 193185... \tLoss: 0.788... \tVal Loss: 0.566 \tVal Frac: 0.8265\n",
      "\tEpoch: 800/1000; 1.50 sec... \tStep: 194400... \tLoss: 0.558... \tVal Loss: 0.550 \tVal Frac: 0.8305\n",
      "\tEpoch: 805/1000; 1.46 sec... \tStep: 195615... \tLoss: 0.410... \tVal Loss: 0.512 \tVal Frac: 0.8379\n",
      "\tEpoch: 810/1000; 1.35 sec... \tStep: 196830... \tLoss: 0.353... \tVal Loss: 0.497 \tVal Frac: 0.8471\n",
      "\tValidation loss decreased (0.497 --> 0.497).  Saving model ...\n",
      "\tEpoch: 815/1000; 1.35 sec... \tStep: 198045... \tLoss: 0.432... \tVal Loss: 0.528 \tVal Frac: 0.8199\n",
      "\tEpoch: 820/1000; 1.32 sec... \tStep: 199260... \tLoss: 0.708... \tVal Loss: 0.565 \tVal Frac: 0.8158\n",
      "\tEpoch: 825/1000; 1.47 sec... \tStep: 200475... \tLoss: 1.027... \tVal Loss: 0.548 \tVal Frac: 0.8187\n",
      "\tEpoch: 830/1000; 1.44 sec... \tStep: 201690... \tLoss: 0.829... \tVal Loss: 0.519 \tVal Frac: 0.8283\n",
      "\tEpoch: 835/1000; 1.35 sec... \tStep: 202905... \tLoss: 0.414... \tVal Loss: 0.533 \tVal Frac: 0.8375\n",
      "\tEpoch: 840/1000; 1.50 sec... \tStep: 204120... \tLoss: 0.582... \tVal Loss: 0.532 \tVal Frac: 0.8357\n",
      "\tEpoch: 845/1000; 1.36 sec... \tStep: 205335... \tLoss: 0.506... \tVal Loss: 0.527 \tVal Frac: 0.8382\n",
      "\tEpoch: 850/1000; 1.37 sec... \tStep: 206550... \tLoss: 0.494... \tVal Loss: 0.572 \tVal Frac: 0.8191\n",
      "\tEpoch: 855/1000; 1.50 sec... \tStep: 207765... \tLoss: 0.621... \tVal Loss: 0.543 \tVal Frac: 0.8232\n",
      "\tEpoch: 860/1000; 1.42 sec... \tStep: 208980... \tLoss: 1.064... \tVal Loss: 0.540 \tVal Frac: 0.8265\n",
      "\tEpoch: 865/1000; 1.32 sec... \tStep: 210195... \tLoss: 0.448... \tVal Loss: 0.554 \tVal Frac: 0.8250\n",
      "\tEpoch: 870/1000; 1.36 sec... \tStep: 211410... \tLoss: 0.530... \tVal Loss: 0.540 \tVal Frac: 0.8228\n",
      "\tEpoch: 875/1000; 1.48 sec... \tStep: 212625... \tLoss: 0.589... \tVal Loss: 0.543 \tVal Frac: 0.8224\n",
      "\tEpoch: 880/1000; 1.38 sec... \tStep: 213840... \tLoss: 0.691... \tVal Loss: 0.526 \tVal Frac: 0.8265\n",
      "\tEpoch: 885/1000; 1.46 sec... \tStep: 215055... \tLoss: 0.714... \tVal Loss: 0.569 \tVal Frac: 0.8173\n",
      "\tEpoch: 890/1000; 1.57 sec... \tStep: 216270... \tLoss: 0.792... \tVal Loss: 0.568 \tVal Frac: 0.8283\n",
      "\tEpoch: 895/1000; 1.42 sec... \tStep: 217485... \tLoss: 0.402... \tVal Loss: 0.588 \tVal Frac: 0.8221\n",
      "\tEpoch: 900/1000; 1.47 sec... \tStep: 218700... \tLoss: 0.943... \tVal Loss: 0.583 \tVal Frac: 0.8110\n",
      "\tEpoch: 905/1000; 1.40 sec... \tStep: 219915... \tLoss: 0.460... \tVal Loss: 0.537 \tVal Frac: 0.8232\n",
      "\tEpoch: 910/1000; 1.32 sec... \tStep: 221130... \tLoss: 0.487... \tVal Loss: 0.529 \tVal Frac: 0.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 915/1000; 1.29 sec... \tStep: 222345... \tLoss: 0.450... \tVal Loss: 0.519 \tVal Frac: 0.8327\n",
      "\tEpoch: 920/1000; 1.34 sec... \tStep: 223560... \tLoss: 0.504... \tVal Loss: 0.547 \tVal Frac: 0.8298\n",
      "\tEpoch: 925/1000; 1.44 sec... \tStep: 224775... \tLoss: 0.976... \tVal Loss: 0.538 \tVal Frac: 0.8342\n",
      "\tEpoch: 930/1000; 1.38 sec... \tStep: 225990... \tLoss: 0.283... \tVal Loss: 0.511 \tVal Frac: 0.8287\n",
      "\tEpoch: 935/1000; 1.45 sec... \tStep: 227205... \tLoss: 0.538... \tVal Loss: 0.562 \tVal Frac: 0.8195\n",
      "\tEpoch: 940/1000; 1.44 sec... \tStep: 228420... \tLoss: 0.508... \tVal Loss: 0.547 \tVal Frac: 0.8232\n",
      "\tEpoch: 945/1000; 1.35 sec... \tStep: 229635... \tLoss: 0.361... \tVal Loss: 0.543 \tVal Frac: 0.8287\n",
      "\tEpoch: 950/1000; 1.34 sec... \tStep: 230850... \tLoss: 0.821... \tVal Loss: 0.512 \tVal Frac: 0.8298\n",
      "\tEpoch: 955/1000; 1.34 sec... \tStep: 232065... \tLoss: 0.500... \tVal Loss: 0.533 \tVal Frac: 0.8243\n",
      "\tEpoch: 960/1000; 1.50 sec... \tStep: 233280... \tLoss: 0.741... \tVal Loss: 0.523 \tVal Frac: 0.8276\n",
      "\tEpoch: 965/1000; 1.27 sec... \tStep: 234495... \tLoss: 0.660... \tVal Loss: 0.512 \tVal Frac: 0.8224\n",
      "\tEpoch: 970/1000; 1.40 sec... \tStep: 235710... \tLoss: 0.638... \tVal Loss: 0.521 \tVal Frac: 0.8294\n",
      "\tEpoch: 975/1000; 1.46 sec... \tStep: 236925... \tLoss: 0.523... \tVal Loss: 0.527 \tVal Frac: 0.8287\n",
      "\tEpoch: 980/1000; 1.40 sec... \tStep: 238140... \tLoss: 0.900... \tVal Loss: 0.528 \tVal Frac: 0.8265\n",
      "\tEpoch: 985/1000; 1.39 sec... \tStep: 239355... \tLoss: 0.864... \tVal Loss: 0.539 \tVal Frac: 0.8165\n",
      "\tEpoch: 990/1000; 1.42 sec... \tStep: 240570... \tLoss: 0.592... \tVal Loss: 0.494 \tVal Frac: 0.8357\n",
      "\tValidation loss decreased (0.497 --> 0.494).  Saving model ...\n",
      "\tEpoch: 995/1000; 1.39 sec... \tStep: 241785... \tLoss: 0.485... \tVal Loss: 0.495 \tVal Frac: 0.8331\n",
      "\tEpoch: 1000/1000; 1.47 sec... \tStep: 243000... \tLoss: 0.702... \tVal Loss: 0.518 \tVal Frac: 0.8290\n",
      "Completed:  lr :  0.001 \n",
      "\tloss: 0.493958 \n",
      "\tfrac:0.847059\n",
      "\n",
      "###########\n",
      "Testing with  lr :  0.01\n",
      "\tEpoch: 5/1000; 1.32 sec... \tStep: 1215... \tLoss: 1.069... \tVal Loss: 0.818 \tVal Frac: 0.7629\n",
      "\tValidation loss decreased (inf --> 0.818).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.45 sec... \tStep: 2430... \tLoss: 0.943... \tVal Loss: 0.765 \tVal Frac: 0.7353\n",
      "\tValidation loss decreased (0.818 --> 0.765).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.42 sec... \tStep: 3645... \tLoss: 0.909... \tVal Loss: 0.724 \tVal Frac: 0.7607\n",
      "\tValidation loss decreased (0.765 --> 0.724).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.39 sec... \tStep: 4860... \tLoss: 1.015... \tVal Loss: 0.764 \tVal Frac: 0.7710\n",
      "\tEpoch: 25/1000; 1.40 sec... \tStep: 6075... \tLoss: 0.530... \tVal Loss: 0.644 \tVal Frac: 0.7886\n",
      "\tValidation loss decreased (0.724 --> 0.644).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.44 sec... \tStep: 7290... \tLoss: 0.740... \tVal Loss: 0.658 \tVal Frac: 0.7996\n",
      "\tEpoch: 35/1000; 1.75 sec... \tStep: 8505... \tLoss: 0.470... \tVal Loss: 0.667 \tVal Frac: 0.7985\n",
      "\tEpoch: 40/1000; 1.42 sec... \tStep: 9720... \tLoss: 0.808... \tVal Loss: 0.736 \tVal Frac: 0.7533\n",
      "\tEpoch: 45/1000; 1.40 sec... \tStep: 10935... \tLoss: 1.045... \tVal Loss: 0.749 \tVal Frac: 0.7555\n",
      "\tEpoch: 50/1000; 1.37 sec... \tStep: 12150... \tLoss: 1.020... \tVal Loss: 0.694 \tVal Frac: 0.7643\n",
      "\tEpoch: 55/1000; 1.44 sec... \tStep: 13365... \tLoss: 0.893... \tVal Loss: 0.633 \tVal Frac: 0.7798\n",
      "\tValidation loss decreased (0.644 --> 0.633).  Saving model ...\n",
      "\tEpoch: 60/1000; 1.37 sec... \tStep: 14580... \tLoss: 0.735... \tVal Loss: 0.662 \tVal Frac: 0.7783\n",
      "\tEpoch: 65/1000; 1.39 sec... \tStep: 15795... \tLoss: 0.819... \tVal Loss: 0.667 \tVal Frac: 0.7673\n",
      "\tEpoch: 70/1000; 1.42 sec... \tStep: 17010... \tLoss: 0.720... \tVal Loss: 0.639 \tVal Frac: 0.7993\n",
      "\tEpoch: 75/1000; 1.40 sec... \tStep: 18225... \tLoss: 1.163... \tVal Loss: 0.641 \tVal Frac: 0.7967\n",
      "\tEpoch: 80/1000; 1.35 sec... \tStep: 19440... \tLoss: 0.735... \tVal Loss: 0.659 \tVal Frac: 0.7706\n",
      "\tEpoch: 85/1000; 1.40 sec... \tStep: 20655... \tLoss: 0.924... \tVal Loss: 0.683 \tVal Frac: 0.7658\n",
      "\tEpoch: 90/1000; 1.35 sec... \tStep: 21870... \tLoss: 0.855... \tVal Loss: 0.659 \tVal Frac: 0.7908\n",
      "\tEpoch: 95/1000; 1.44 sec... \tStep: 23085... \tLoss: 0.822... \tVal Loss: 0.608 \tVal Frac: 0.8044\n",
      "\tValidation loss decreased (0.633 --> 0.608).  Saving model ...\n",
      "\tEpoch: 100/1000; 1.30 sec... \tStep: 24300... \tLoss: 0.680... \tVal Loss: 0.628 \tVal Frac: 0.7710\n",
      "\tEpoch: 105/1000; 1.26 sec... \tStep: 25515... \tLoss: 0.641... \tVal Loss: 0.635 \tVal Frac: 0.7746\n",
      "\tEpoch: 110/1000; 1.40 sec... \tStep: 26730... \tLoss: 0.919... \tVal Loss: 0.589 \tVal Frac: 0.7960\n",
      "\tValidation loss decreased (0.608 --> 0.589).  Saving model ...\n",
      "\tEpoch: 115/1000; 1.40 sec... \tStep: 27945... \tLoss: 0.727... \tVal Loss: 0.603 \tVal Frac: 0.8055\n",
      "\tEpoch: 120/1000; 1.37 sec... \tStep: 29160... \tLoss: 1.003... \tVal Loss: 0.635 \tVal Frac: 0.7974\n",
      "\tEpoch: 125/1000; 1.34 sec... \tStep: 30375... \tLoss: 1.296... \tVal Loss: 0.618 \tVal Frac: 0.8000\n",
      "\tEpoch: 130/1000; 1.47 sec... \tStep: 31590... \tLoss: 0.751... \tVal Loss: 0.635 \tVal Frac: 0.8048\n",
      "\tEpoch: 135/1000; 1.39 sec... \tStep: 32805... \tLoss: 0.747... \tVal Loss: 0.603 \tVal Frac: 0.8125\n",
      "\tEpoch: 140/1000; 1.47 sec... \tStep: 34020... \tLoss: 1.145... \tVal Loss: 0.619 \tVal Frac: 0.7974\n",
      "\tEpoch: 145/1000; 1.42 sec... \tStep: 35235... \tLoss: 0.457... \tVal Loss: 0.625 \tVal Frac: 0.7842\n",
      "\tEpoch: 150/1000; 1.34 sec... \tStep: 36450... \tLoss: 1.201... \tVal Loss: 0.664 \tVal Frac: 0.7798\n",
      "\tEpoch: 155/1000; 1.40 sec... \tStep: 37665... \tLoss: 0.948... \tVal Loss: 0.591 \tVal Frac: 0.8077\n",
      "\tEpoch: 160/1000; 1.41 sec... \tStep: 38880... \tLoss: 0.545... \tVal Loss: 0.619 \tVal Frac: 0.7952\n",
      "\tEpoch: 165/1000; 1.37 sec... \tStep: 40095... \tLoss: 0.878... \tVal Loss: 0.616 \tVal Frac: 0.7945\n",
      "\tEpoch: 170/1000; 1.46 sec... \tStep: 41310... \tLoss: 0.944... \tVal Loss: 0.602 \tVal Frac: 0.8000\n",
      "\tEpoch: 175/1000; 1.37 sec... \tStep: 42525... \tLoss: 0.927... \tVal Loss: 0.633 \tVal Frac: 0.7919\n",
      "\tEpoch: 180/1000; 1.42 sec... \tStep: 43740... \tLoss: 0.553... \tVal Loss: 0.619 \tVal Frac: 0.7934\n",
      "\tEpoch: 185/1000; 1.40 sec... \tStep: 44955... \tLoss: 0.725... \tVal Loss: 0.615 \tVal Frac: 0.7919\n",
      "\tEpoch: 190/1000; 1.37 sec... \tStep: 46170... \tLoss: 0.457... \tVal Loss: 0.603 \tVal Frac: 0.7996\n",
      "\tEpoch: 195/1000; 1.44 sec... \tStep: 47385... \tLoss: 0.669... \tVal Loss: 0.608 \tVal Frac: 0.7882\n",
      "\tEpoch: 200/1000; 1.30 sec... \tStep: 48600... \tLoss: 0.750... \tVal Loss: 0.609 \tVal Frac: 0.7971\n",
      "\tEpoch: 205/1000; 1.53 sec... \tStep: 49815... \tLoss: 0.716... \tVal Loss: 0.629 \tVal Frac: 0.7754\n",
      "\tEpoch: 210/1000; 1.40 sec... \tStep: 51030... \tLoss: 0.754... \tVal Loss: 0.623 \tVal Frac: 0.7846\n",
      "\tEpoch: 215/1000; 1.36 sec... \tStep: 52245... \tLoss: 1.170... \tVal Loss: 0.635 \tVal Frac: 0.7886\n",
      "\tEpoch: 220/1000; 1.54 sec... \tStep: 53460... \tLoss: 0.572... \tVal Loss: 0.609 \tVal Frac: 0.7816\n",
      "\tEpoch: 225/1000; 1.39 sec... \tStep: 54675... \tLoss: 0.877... \tVal Loss: 0.618 \tVal Frac: 0.8004\n",
      "\tEpoch: 230/1000; 1.40 sec... \tStep: 55890... \tLoss: 1.319... \tVal Loss: 0.610 \tVal Frac: 0.8187\n",
      "\tEpoch: 235/1000; 1.38 sec... \tStep: 57105... \tLoss: 1.090... \tVal Loss: 0.602 \tVal Frac: 0.7989\n",
      "\tEpoch: 240/1000; 1.42 sec... \tStep: 58320... \tLoss: 0.661... \tVal Loss: 0.620 \tVal Frac: 0.7993\n",
      "\tEpoch: 245/1000; 1.27 sec... \tStep: 59535... \tLoss: 0.843... \tVal Loss: 0.623 \tVal Frac: 0.7993\n",
      "\tEpoch: 250/1000; 1.44 sec... \tStep: 60750... \tLoss: 0.666... \tVal Loss: 0.602 \tVal Frac: 0.8184\n",
      "\tEpoch: 255/1000; 1.44 sec... \tStep: 61965... \tLoss: 0.753... \tVal Loss: 0.575 \tVal Frac: 0.8232\n",
      "\tValidation loss decreased (0.589 --> 0.575).  Saving model ...\n",
      "\tEpoch: 260/1000; 1.52 sec... \tStep: 63180... \tLoss: 0.713... \tVal Loss: 0.596 \tVal Frac: 0.7985\n",
      "\tEpoch: 265/1000; 1.35 sec... \tStep: 64395... \tLoss: 0.735... \tVal Loss: 0.602 \tVal Frac: 0.7952\n",
      "\tEpoch: 270/1000; 1.42 sec... \tStep: 65610... \tLoss: 1.013... \tVal Loss: 0.594 \tVal Frac: 0.8063\n",
      "\tEpoch: 275/1000; 1.27 sec... \tStep: 66825... \tLoss: 0.930... \tVal Loss: 0.606 \tVal Frac: 0.8051\n",
      "\tEpoch: 280/1000; 1.34 sec... \tStep: 68040... \tLoss: 0.972... \tVal Loss: 0.612 \tVal Frac: 0.7949\n",
      "\tEpoch: 285/1000; 1.42 sec... \tStep: 69255... \tLoss: 0.815... \tVal Loss: 0.666 \tVal Frac: 0.7574\n",
      "\tEpoch: 290/1000; 1.40 sec... \tStep: 70470... \tLoss: 0.655... \tVal Loss: 0.603 \tVal Frac: 0.8004\n",
      "\tEpoch: 295/1000; 1.27 sec... \tStep: 71685... \tLoss: 0.660... \tVal Loss: 0.596 \tVal Frac: 0.8000\n",
      "\tEpoch: 300/1000; 1.47 sec... \tStep: 72900... \tLoss: 0.577... \tVal Loss: 0.596 \tVal Frac: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 305/1000; 1.32 sec... \tStep: 74115... \tLoss: 0.837... \tVal Loss: 0.595 \tVal Frac: 0.7982\n",
      "\tEpoch: 310/1000; 1.37 sec... \tStep: 75330... \tLoss: 0.587... \tVal Loss: 0.603 \tVal Frac: 0.8000\n",
      "\tEpoch: 315/1000; 1.49 sec... \tStep: 76545... \tLoss: 0.890... \tVal Loss: 0.613 \tVal Frac: 0.7761\n",
      "\tEpoch: 320/1000; 1.42 sec... \tStep: 77760... \tLoss: 0.484... \tVal Loss: 0.592 \tVal Frac: 0.8107\n",
      "\tEpoch: 325/1000; 1.32 sec... \tStep: 78975... \tLoss: 1.005... \tVal Loss: 0.601 \tVal Frac: 0.8004\n",
      "\tEpoch: 330/1000; 1.37 sec... \tStep: 80190... \tLoss: 0.627... \tVal Loss: 0.611 \tVal Frac: 0.7952\n",
      "\tEpoch: 335/1000; 1.50 sec... \tStep: 81405... \tLoss: 0.516... \tVal Loss: 0.621 \tVal Frac: 0.7945\n",
      "\tEpoch: 340/1000; 1.42 sec... \tStep: 82620... \tLoss: 0.708... \tVal Loss: 0.600 \tVal Frac: 0.7897\n",
      "\tEpoch: 345/1000; 1.47 sec... \tStep: 83835... \tLoss: 0.837... \tVal Loss: 0.612 \tVal Frac: 0.8026\n",
      "\tEpoch: 350/1000; 1.26 sec... \tStep: 85050... \tLoss: 0.389... \tVal Loss: 0.612 \tVal Frac: 0.7783\n",
      "\tEpoch: 355/1000; 1.35 sec... \tStep: 86265... \tLoss: 0.565... \tVal Loss: 0.621 \tVal Frac: 0.7890\n",
      "\tEpoch: 360/1000; 1.39 sec... \tStep: 87480... \tLoss: 1.148... \tVal Loss: 0.611 \tVal Frac: 0.7971\n",
      "\tEpoch: 365/1000; 1.26 sec... \tStep: 88695... \tLoss: 0.818... \tVal Loss: 0.596 \tVal Frac: 0.7978\n",
      "\tEpoch: 370/1000; 1.54 sec... \tStep: 89910... \tLoss: 0.484... \tVal Loss: 0.589 \tVal Frac: 0.7960\n",
      "\tEpoch: 375/1000; 1.39 sec... \tStep: 91125... \tLoss: 1.088... \tVal Loss: 0.571 \tVal Frac: 0.8118\n",
      "\tValidation loss decreased (0.575 --> 0.571).  Saving model ...\n",
      "\tEpoch: 380/1000; 1.35 sec... \tStep: 92340... \tLoss: 0.613... \tVal Loss: 0.582 \tVal Frac: 0.7949\n",
      "\tEpoch: 385/1000; 1.45 sec... \tStep: 93555... \tLoss: 0.695... \tVal Loss: 0.578 \tVal Frac: 0.8151\n",
      "\tEpoch: 390/1000; 1.30 sec... \tStep: 94770... \tLoss: 0.657... \tVal Loss: 0.579 \tVal Frac: 0.7919\n",
      "\tEpoch: 395/1000; 1.30 sec... \tStep: 95985... \tLoss: 0.535... \tVal Loss: 0.613 \tVal Frac: 0.7967\n",
      "\tEpoch: 400/1000; 1.30 sec... \tStep: 97200... \tLoss: 0.858... \tVal Loss: 0.606 \tVal Frac: 0.7956\n",
      "\tEpoch: 405/1000; 1.32 sec... \tStep: 98415... \tLoss: 0.676... \tVal Loss: 0.587 \tVal Frac: 0.8044\n",
      "\tEpoch: 410/1000; 1.34 sec... \tStep: 99630... \tLoss: 0.573... \tVal Loss: 0.604 \tVal Frac: 0.7875\n",
      "\tEpoch: 415/1000; 1.37 sec... \tStep: 100845... \tLoss: 0.476... \tVal Loss: 0.598 \tVal Frac: 0.7952\n",
      "\tEpoch: 420/1000; 1.27 sec... \tStep: 102060... \tLoss: 0.931... \tVal Loss: 0.592 \tVal Frac: 0.7996\n",
      "\tEpoch: 425/1000; 1.49 sec... \tStep: 103275... \tLoss: 0.434... \tVal Loss: 0.589 \tVal Frac: 0.8099\n",
      "\tEpoch: 430/1000; 1.40 sec... \tStep: 104490... \tLoss: 0.935... \tVal Loss: 0.601 \tVal Frac: 0.7967\n",
      "\tEpoch: 435/1000; 1.38 sec... \tStep: 105705... \tLoss: 0.910... \tVal Loss: 0.606 \tVal Frac: 0.7993\n",
      "\tEpoch: 440/1000; 1.44 sec... \tStep: 106920... \tLoss: 1.074... \tVal Loss: 0.602 \tVal Frac: 0.7974\n",
      "\tEpoch: 445/1000; 1.44 sec... \tStep: 108135... \tLoss: 0.615... \tVal Loss: 0.596 \tVal Frac: 0.8187\n",
      "\tEpoch: 450/1000; 1.39 sec... \tStep: 109350... \tLoss: 0.715... \tVal Loss: 0.601 \tVal Frac: 0.8173\n",
      "\tEpoch: 455/1000; 1.42 sec... \tStep: 110565... \tLoss: 0.826... \tVal Loss: 0.603 \tVal Frac: 0.8114\n",
      "\tEpoch: 460/1000; 1.45 sec... \tStep: 111780... \tLoss: 0.898... \tVal Loss: 0.596 \tVal Frac: 0.8118\n",
      "\tEpoch: 465/1000; 1.30 sec... \tStep: 112995... \tLoss: 0.827... \tVal Loss: 0.603 \tVal Frac: 0.8037\n",
      "\tEpoch: 470/1000; 1.44 sec... \tStep: 114210... \tLoss: 0.409... \tVal Loss: 0.578 \tVal Frac: 0.8081\n",
      "\tEpoch: 475/1000; 1.39 sec... \tStep: 115425... \tLoss: 0.451... \tVal Loss: 0.588 \tVal Frac: 0.8165\n",
      "\tEpoch: 480/1000; 1.39 sec... \tStep: 116640... \tLoss: 0.543... \tVal Loss: 0.641 \tVal Frac: 0.7812\n",
      "\tEpoch: 485/1000; 1.44 sec... \tStep: 117855... \tLoss: 0.566... \tVal Loss: 0.620 \tVal Frac: 0.7978\n",
      "\tEpoch: 490/1000; 1.37 sec... \tStep: 119070... \tLoss: 0.565... \tVal Loss: 0.592 \tVal Frac: 0.8040\n",
      "\tEpoch: 495/1000; 1.34 sec... \tStep: 120285... \tLoss: 0.854... \tVal Loss: 0.616 \tVal Frac: 0.8103\n",
      "\tEpoch: 500/1000; 1.47 sec... \tStep: 121500... \tLoss: 0.629... \tVal Loss: 0.597 \tVal Frac: 0.8154\n",
      "\tEpoch: 505/1000; 1.34 sec... \tStep: 122715... \tLoss: 0.908... \tVal Loss: 0.607 \tVal Frac: 0.8040\n",
      "\tEpoch: 510/1000; 1.42 sec... \tStep: 123930... \tLoss: 0.763... \tVal Loss: 0.622 \tVal Frac: 0.7886\n",
      "\tEpoch: 515/1000; 1.30 sec... \tStep: 125145... \tLoss: 0.612... \tVal Loss: 0.608 \tVal Frac: 0.8110\n",
      "\tEpoch: 520/1000; 1.35 sec... \tStep: 126360... \tLoss: 0.920... \tVal Loss: 0.614 \tVal Frac: 0.7971\n",
      "\tEpoch: 525/1000; 1.39 sec... \tStep: 127575... \tLoss: 0.616... \tVal Loss: 0.607 \tVal Frac: 0.8077\n",
      "\tEpoch: 530/1000; 1.44 sec... \tStep: 128790... \tLoss: 1.039... \tVal Loss: 0.600 \tVal Frac: 0.8044\n",
      "\tEpoch: 535/1000; 1.47 sec... \tStep: 130005... \tLoss: 0.714... \tVal Loss: 0.606 \tVal Frac: 0.7985\n",
      "\tEpoch: 540/1000; 1.50 sec... \tStep: 131220... \tLoss: 0.583... \tVal Loss: 0.605 \tVal Frac: 0.8088\n",
      "\tEpoch: 545/1000; 1.39 sec... \tStep: 132435... \tLoss: 0.789... \tVal Loss: 0.607 \tVal Frac: 0.7886\n",
      "\tEpoch: 550/1000; 1.40 sec... \tStep: 133650... \tLoss: 0.890... \tVal Loss: 0.604 \tVal Frac: 0.8077\n",
      "\tEpoch: 555/1000; 1.37 sec... \tStep: 134865... \tLoss: 0.923... \tVal Loss: 0.606 \tVal Frac: 0.7857\n",
      "\tEpoch: 560/1000; 1.42 sec... \tStep: 136080... \tLoss: 0.583... \tVal Loss: 0.586 \tVal Frac: 0.8081\n",
      "\tEpoch: 565/1000; 1.39 sec... \tStep: 137295... \tLoss: 0.761... \tVal Loss: 0.600 \tVal Frac: 0.8125\n",
      "\tEpoch: 570/1000; 1.39 sec... \tStep: 138510... \tLoss: 0.924... \tVal Loss: 0.576 \tVal Frac: 0.8224\n",
      "\tEpoch: 575/1000; 1.40 sec... \tStep: 139725... \tLoss: 0.691... \tVal Loss: 0.634 \tVal Frac: 0.7754\n",
      "\tEpoch: 580/1000; 1.45 sec... \tStep: 140940... \tLoss: 0.479... \tVal Loss: 0.624 \tVal Frac: 0.8007\n",
      "\tEpoch: 585/1000; 1.44 sec... \tStep: 142155... \tLoss: 0.697... \tVal Loss: 0.582 \tVal Frac: 0.7982\n",
      "\tEpoch: 590/1000; 1.38 sec... \tStep: 143370... \tLoss: 0.707... \tVal Loss: 0.641 \tVal Frac: 0.7809\n",
      "\tEpoch: 595/1000; 1.47 sec... \tStep: 144585... \tLoss: 0.542... \tVal Loss: 0.595 \tVal Frac: 0.7912\n",
      "\tEpoch: 600/1000; 1.40 sec... \tStep: 145800... \tLoss: 1.082... \tVal Loss: 0.602 \tVal Frac: 0.7934\n",
      "\tEpoch: 605/1000; 1.34 sec... \tStep: 147015... \tLoss: 0.846... \tVal Loss: 0.605 \tVal Frac: 0.7904\n",
      "\tEpoch: 610/1000; 1.37 sec... \tStep: 148230... \tLoss: 1.110... \tVal Loss: 0.650 \tVal Frac: 0.7768\n",
      "\tEpoch: 615/1000; 1.32 sec... \tStep: 149445... \tLoss: 0.671... \tVal Loss: 0.611 \tVal Frac: 0.7860\n",
      "\tEpoch: 620/1000; 1.52 sec... \tStep: 150660... \tLoss: 0.504... \tVal Loss: 0.591 \tVal Frac: 0.8070\n",
      "\tEpoch: 625/1000; 1.35 sec... \tStep: 151875... \tLoss: 0.653... \tVal Loss: 0.587 \tVal Frac: 0.8147\n",
      "\tEpoch: 630/1000; 1.24 sec... \tStep: 153090... \tLoss: 1.094... \tVal Loss: 0.624 \tVal Frac: 0.7963\n",
      "\tEpoch: 635/1000; 1.42 sec... \tStep: 154305... \tLoss: 0.729... \tVal Loss: 0.596 \tVal Frac: 0.7934\n",
      "\tEpoch: 640/1000; 1.40 sec... \tStep: 155520... \tLoss: 0.448... \tVal Loss: 0.589 \tVal Frac: 0.8011\n",
      "\tEpoch: 645/1000; 1.47 sec... \tStep: 156735... \tLoss: 0.948... \tVal Loss: 0.629 \tVal Frac: 0.7746\n",
      "\tEpoch: 650/1000; 1.48 sec... \tStep: 157950... \tLoss: 0.355... \tVal Loss: 0.628 \tVal Frac: 0.7735\n",
      "\tEpoch: 655/1000; 1.39 sec... \tStep: 159165... \tLoss: 0.655... \tVal Loss: 0.604 \tVal Frac: 0.7956\n",
      "\tEpoch: 660/1000; 1.46 sec... \tStep: 160380... \tLoss: 0.912... \tVal Loss: 0.612 \tVal Frac: 0.8059\n",
      "\tEpoch: 665/1000; 1.47 sec... \tStep: 161595... \tLoss: 0.616... \tVal Loss: 0.600 \tVal Frac: 0.8000\n",
      "\tEpoch: 670/1000; 1.46 sec... \tStep: 162810... \tLoss: 0.647... \tVal Loss: 0.623 \tVal Frac: 0.8051\n",
      "\tEpoch: 675/1000; 1.44 sec... \tStep: 164025... \tLoss: 0.423... \tVal Loss: 0.639 \tVal Frac: 0.7743\n",
      "\tEpoch: 680/1000; 1.44 sec... \tStep: 165240... \tLoss: 0.698... \tVal Loss: 0.591 \tVal Frac: 0.8059\n",
      "\tEpoch: 685/1000; 1.37 sec... \tStep: 166455... \tLoss: 0.434... \tVal Loss: 0.609 \tVal Frac: 0.8044\n",
      "\tEpoch: 690/1000; 1.38 sec... \tStep: 167670... \tLoss: 0.586... \tVal Loss: 0.604 \tVal Frac: 0.8066\n",
      "\tEpoch: 695/1000; 1.44 sec... \tStep: 168885... \tLoss: 0.738... \tVal Loss: 0.624 \tVal Frac: 0.7982\n",
      "\tEpoch: 700/1000; 1.40 sec... \tStep: 170100... \tLoss: 0.888... \tVal Loss: 0.599 \tVal Frac: 0.8051\n",
      "\tEpoch: 705/1000; 1.37 sec... \tStep: 171315... \tLoss: 0.984... \tVal Loss: 0.595 \tVal Frac: 0.8022\n",
      "\tEpoch: 710/1000; 1.32 sec... \tStep: 172530... \tLoss: 1.008... \tVal Loss: 0.622 \tVal Frac: 0.7812\n",
      "\tEpoch: 715/1000; 1.38 sec... \tStep: 173745... \tLoss: 0.625... \tVal Loss: 0.600 \tVal Frac: 0.7886\n",
      "\tEpoch: 720/1000; 1.47 sec... \tStep: 174960... \tLoss: 0.814... \tVal Loss: 0.601 \tVal Frac: 0.7846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 725/1000; 1.47 sec... \tStep: 176175... \tLoss: 0.675... \tVal Loss: 0.583 \tVal Frac: 0.8110\n",
      "\tEpoch: 730/1000; 1.42 sec... \tStep: 177390... \tLoss: 1.114... \tVal Loss: 0.608 \tVal Frac: 0.7835\n",
      "\tEpoch: 735/1000; 1.42 sec... \tStep: 178605... \tLoss: 0.600... \tVal Loss: 0.591 \tVal Frac: 0.7985\n",
      "\tEpoch: 740/1000; 1.43 sec... \tStep: 179820... \tLoss: 0.823... \tVal Loss: 0.597 \tVal Frac: 0.8044\n",
      "\tEpoch: 745/1000; 1.37 sec... \tStep: 181035... \tLoss: 0.815... \tVal Loss: 0.600 \tVal Frac: 0.8007\n",
      "\tEpoch: 750/1000; 1.52 sec... \tStep: 182250... \tLoss: 0.991... \tVal Loss: 0.599 \tVal Frac: 0.8055\n",
      "\tEpoch: 755/1000; 1.36 sec... \tStep: 183465... \tLoss: 1.003... \tVal Loss: 0.614 \tVal Frac: 0.8011\n",
      "\tEpoch: 760/1000; 1.49 sec... \tStep: 184680... \tLoss: 0.624... \tVal Loss: 0.597 \tVal Frac: 0.7989\n",
      "\tEpoch: 765/1000; 1.40 sec... \tStep: 185895... \tLoss: 0.517... \tVal Loss: 0.610 \tVal Frac: 0.7941\n",
      "\tEpoch: 770/1000; 1.34 sec... \tStep: 187110... \tLoss: 0.582... \tVal Loss: 0.597 \tVal Frac: 0.8077\n",
      "\tEpoch: 775/1000; 1.39 sec... \tStep: 188325... \tLoss: 0.878... \tVal Loss: 0.609 \tVal Frac: 0.8007\n",
      "\tEpoch: 780/1000; 1.36 sec... \tStep: 189540... \tLoss: 0.735... \tVal Loss: 0.590 \tVal Frac: 0.7937\n",
      "\tEpoch: 785/1000; 1.39 sec... \tStep: 190755... \tLoss: 0.763... \tVal Loss: 0.571 \tVal Frac: 0.8136\n",
      "\tValidation loss decreased (0.571 --> 0.571).  Saving model ...\n",
      "\tEpoch: 790/1000; 1.35 sec... \tStep: 191970... \tLoss: 0.741... \tVal Loss: 0.600 \tVal Frac: 0.8085\n",
      "\tEpoch: 795/1000; 1.40 sec... \tStep: 193185... \tLoss: 0.733... \tVal Loss: 0.571 \tVal Frac: 0.8162\n",
      "\tEpoch: 800/1000; 1.32 sec... \tStep: 194400... \tLoss: 0.807... \tVal Loss: 0.615 \tVal Frac: 0.8026\n",
      "\tEpoch: 805/1000; 1.35 sec... \tStep: 195615... \tLoss: 0.530... \tVal Loss: 0.628 \tVal Frac: 0.8000\n",
      "\tEpoch: 810/1000; 1.33 sec... \tStep: 196830... \tLoss: 0.670... \tVal Loss: 0.607 \tVal Frac: 0.7926\n",
      "\tEpoch: 815/1000; 1.38 sec... \tStep: 198045... \tLoss: 0.566... \tVal Loss: 0.620 \tVal Frac: 0.7956\n",
      "\tEpoch: 820/1000; 1.36 sec... \tStep: 199260... \tLoss: 0.598... \tVal Loss: 0.589 \tVal Frac: 0.8051\n",
      "\tEpoch: 825/1000; 1.47 sec... \tStep: 200475... \tLoss: 0.945... \tVal Loss: 0.570 \tVal Frac: 0.8257\n",
      "\tValidation loss decreased (0.571 --> 0.570).  Saving model ...\n",
      "\tEpoch: 830/1000; 1.52 sec... \tStep: 201690... \tLoss: 0.955... \tVal Loss: 0.609 \tVal Frac: 0.7993\n",
      "\tEpoch: 835/1000; 1.44 sec... \tStep: 202905... \tLoss: 0.465... \tVal Loss: 0.588 \tVal Frac: 0.8129\n",
      "\tEpoch: 840/1000; 1.38 sec... \tStep: 204120... \tLoss: 0.704... \tVal Loss: 0.608 \tVal Frac: 0.8074\n",
      "\tEpoch: 845/1000; 1.43 sec... \tStep: 205335... \tLoss: 0.709... \tVal Loss: 0.620 \tVal Frac: 0.8037\n",
      "\tEpoch: 850/1000; 1.43 sec... \tStep: 206550... \tLoss: 0.668... \tVal Loss: 0.627 \tVal Frac: 0.7923\n",
      "\tEpoch: 855/1000; 1.38 sec... \tStep: 207765... \tLoss: 0.587... \tVal Loss: 0.587 \tVal Frac: 0.7941\n",
      "\tEpoch: 860/1000; 1.30 sec... \tStep: 208980... \tLoss: 1.215... \tVal Loss: 0.594 \tVal Frac: 0.8085\n",
      "\tEpoch: 865/1000; 1.42 sec... \tStep: 210195... \tLoss: 0.688... \tVal Loss: 0.607 \tVal Frac: 0.8136\n",
      "\tEpoch: 870/1000; 1.54 sec... \tStep: 211410... \tLoss: 0.553... \tVal Loss: 0.600 \tVal Frac: 0.7945\n",
      "\tEpoch: 875/1000; 1.44 sec... \tStep: 212625... \tLoss: 0.645... \tVal Loss: 0.597 \tVal Frac: 0.8063\n",
      "\tEpoch: 880/1000; 1.40 sec... \tStep: 213840... \tLoss: 0.598... \tVal Loss: 0.587 \tVal Frac: 0.8022\n",
      "\tEpoch: 885/1000; 1.45 sec... \tStep: 215055... \tLoss: 0.576... \tVal Loss: 0.610 \tVal Frac: 0.7945\n",
      "\tEpoch: 890/1000; 1.44 sec... \tStep: 216270... \tLoss: 0.681... \tVal Loss: 0.574 \tVal Frac: 0.8187\n",
      "\tEpoch: 895/1000; 1.44 sec... \tStep: 217485... \tLoss: 0.425... \tVal Loss: 0.598 \tVal Frac: 0.7982\n",
      "\tEpoch: 900/1000; 1.40 sec... \tStep: 218700... \tLoss: 0.821... \tVal Loss: 0.592 \tVal Frac: 0.7993\n",
      "\tEpoch: 905/1000; 1.40 sec... \tStep: 219915... \tLoss: 0.436... \tVal Loss: 0.580 \tVal Frac: 0.8077\n",
      "\tEpoch: 910/1000; 1.37 sec... \tStep: 221130... \tLoss: 0.699... \tVal Loss: 0.591 \tVal Frac: 0.8110\n",
      "\tEpoch: 915/1000; 1.44 sec... \tStep: 222345... \tLoss: 0.520... \tVal Loss: 0.586 \tVal Frac: 0.8070\n",
      "\tEpoch: 920/1000; 1.34 sec... \tStep: 223560... \tLoss: 0.551... \tVal Loss: 0.602 \tVal Frac: 0.8059\n",
      "\tEpoch: 925/1000; 1.40 sec... \tStep: 224775... \tLoss: 0.975... \tVal Loss: 0.593 \tVal Frac: 0.8029\n",
      "\tEpoch: 930/1000; 1.29 sec... \tStep: 225990... \tLoss: 0.454... \tVal Loss: 0.586 \tVal Frac: 0.8162\n",
      "\tEpoch: 935/1000; 1.37 sec... \tStep: 227205... \tLoss: 0.657... \tVal Loss: 0.605 \tVal Frac: 0.7919\n",
      "\tEpoch: 940/1000; 1.34 sec... \tStep: 228420... \tLoss: 0.642... \tVal Loss: 0.588 \tVal Frac: 0.8018\n",
      "\tEpoch: 945/1000; 1.29 sec... \tStep: 229635... \tLoss: 0.565... \tVal Loss: 0.590 \tVal Frac: 0.7989\n",
      "\tEpoch: 950/1000; 1.50 sec... \tStep: 230850... \tLoss: 0.864... \tVal Loss: 0.601 \tVal Frac: 0.7886\n",
      "\tEpoch: 955/1000; 1.47 sec... \tStep: 232065... \tLoss: 0.794... \tVal Loss: 0.618 \tVal Frac: 0.7801\n",
      "\tEpoch: 960/1000; 1.44 sec... \tStep: 233280... \tLoss: 0.891... \tVal Loss: 0.607 \tVal Frac: 0.7824\n",
      "\tEpoch: 965/1000; 1.30 sec... \tStep: 234495... \tLoss: 0.835... \tVal Loss: 0.596 \tVal Frac: 0.7937\n",
      "\tEpoch: 970/1000; 1.43 sec... \tStep: 235710... \tLoss: 0.645... \tVal Loss: 0.601 \tVal Frac: 0.8051\n",
      "\tEpoch: 975/1000; 1.40 sec... \tStep: 236925... \tLoss: 0.563... \tVal Loss: 0.592 \tVal Frac: 0.8029\n",
      "\tEpoch: 980/1000; 1.50 sec... \tStep: 238140... \tLoss: 1.034... \tVal Loss: 0.602 \tVal Frac: 0.7993\n",
      "\tEpoch: 985/1000; 1.40 sec... \tStep: 239355... \tLoss: 0.993... \tVal Loss: 0.607 \tVal Frac: 0.7949\n",
      "\tEpoch: 990/1000; 1.39 sec... \tStep: 240570... \tLoss: 0.792... \tVal Loss: 0.596 \tVal Frac: 0.7967\n",
      "\tEpoch: 995/1000; 1.44 sec... \tStep: 241785... \tLoss: 0.623... \tVal Loss: 0.597 \tVal Frac: 0.7835\n",
      "\tEpoch: 1000/1000; 1.47 sec... \tStep: 243000... \tLoss: 0.605... \tVal Loss: 0.621 \tVal Frac: 0.7691\n",
      "Completed:  lr :  0.01 \n",
      "\tloss: 0.569573 \n",
      "\tfrac:0.825735\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  1\n",
      "\tEpoch: 5/1000; 0.97 sec... \tStep: 1215... \tLoss: 0.775... \tVal Loss: 0.442 \tVal Frac: 0.8798\n",
      "\tValidation loss decreased (inf --> 0.442).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.03 sec... \tStep: 2430... \tLoss: 0.475... \tVal Loss: 0.404 \tVal Frac: 0.8809\n",
      "\tValidation loss decreased (0.442 --> 0.404).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.13 sec... \tStep: 3645... \tLoss: 0.322... \tVal Loss: 0.338 \tVal Frac: 0.8926\n",
      "\tValidation loss decreased (0.404 --> 0.338).  Saving model ...\n",
      "\tEpoch: 20/1000; 0.97 sec... \tStep: 4860... \tLoss: 0.251... \tVal Loss: 0.391 \tVal Frac: 0.8871\n",
      "\tEpoch: 25/1000; 0.98 sec... \tStep: 6075... \tLoss: 0.250... \tVal Loss: 0.335 \tVal Frac: 0.8949\n",
      "\tValidation loss decreased (0.338 --> 0.335).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.07 sec... \tStep: 7290... \tLoss: 0.267... \tVal Loss: 0.330 \tVal Frac: 0.9074\n",
      "\tValidation loss decreased (0.335 --> 0.330).  Saving model ...\n",
      "\tEpoch: 35/1000; 1.06 sec... \tStep: 8505... \tLoss: 0.497... \tVal Loss: 0.344 \tVal Frac: 0.8978\n",
      "\tEpoch: 40/1000; 1.02 sec... \tStep: 9720... \tLoss: 0.609... \tVal Loss: 0.325 \tVal Frac: 0.9000\n",
      "\tValidation loss decreased (0.330 --> 0.325).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.10 sec... \tStep: 10935... \tLoss: 0.447... \tVal Loss: 0.333 \tVal Frac: 0.9007\n",
      "\tEpoch: 50/1000; 1.10 sec... \tStep: 12150... \tLoss: 0.922... \tVal Loss: 0.336 \tVal Frac: 0.8974\n",
      "\tEpoch: 55/1000; 1.06 sec... \tStep: 13365... \tLoss: 0.452... \tVal Loss: 0.334 \tVal Frac: 0.8930\n",
      "\tEpoch: 60/1000; 1.00 sec... \tStep: 14580... \tLoss: 0.505... \tVal Loss: 0.295 \tVal Frac: 0.9051\n",
      "\tValidation loss decreased (0.325 --> 0.295).  Saving model ...\n",
      "\tEpoch: 65/1000; 1.00 sec... \tStep: 15795... \tLoss: 0.308... \tVal Loss: 0.313 \tVal Frac: 0.9048\n",
      "\tEpoch: 70/1000; 1.05 sec... \tStep: 17010... \tLoss: 0.214... \tVal Loss: 0.314 \tVal Frac: 0.9044\n",
      "\tEpoch: 75/1000; 0.99 sec... \tStep: 18225... \tLoss: 0.470... \tVal Loss: 0.302 \tVal Frac: 0.9118\n",
      "\tEpoch: 80/1000; 1.05 sec... \tStep: 19440... \tLoss: 0.502... \tVal Loss: 0.301 \tVal Frac: 0.9096\n",
      "\tEpoch: 85/1000; 1.04 sec... \tStep: 20655... \tLoss: 0.499... \tVal Loss: 0.309 \tVal Frac: 0.9059\n",
      "\tEpoch: 90/1000; 1.15 sec... \tStep: 21870... \tLoss: 0.359... \tVal Loss: 0.291 \tVal Frac: 0.9077\n",
      "\tValidation loss decreased (0.295 --> 0.291).  Saving model ...\n",
      "\tEpoch: 95/1000; 1.07 sec... \tStep: 23085... \tLoss: 0.392... \tVal Loss: 0.310 \tVal Frac: 0.9062\n",
      "\tEpoch: 100/1000; 1.00 sec... \tStep: 24300... \tLoss: 0.215... \tVal Loss: 0.303 \tVal Frac: 0.9099\n",
      "\tEpoch: 105/1000; 1.13 sec... \tStep: 25515... \tLoss: 0.189... \tVal Loss: 0.302 \tVal Frac: 0.9099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 110/1000; 1.04 sec... \tStep: 26730... \tLoss: 0.382... \tVal Loss: 0.290 \tVal Frac: 0.9066\n",
      "\tValidation loss decreased (0.291 --> 0.290).  Saving model ...\n",
      "\tEpoch: 115/1000; 0.93 sec... \tStep: 27945... \tLoss: 0.397... \tVal Loss: 0.295 \tVal Frac: 0.9033\n",
      "\tEpoch: 120/1000; 1.07 sec... \tStep: 29160... \tLoss: 0.195... \tVal Loss: 0.298 \tVal Frac: 0.9081\n",
      "\tEpoch: 125/1000; 1.05 sec... \tStep: 30375... \tLoss: 0.503... \tVal Loss: 0.304 \tVal Frac: 0.9018\n",
      "\tEpoch: 130/1000; 0.99 sec... \tStep: 31590... \tLoss: 0.111... \tVal Loss: 0.302 \tVal Frac: 0.9040\n",
      "\tEpoch: 135/1000; 1.02 sec... \tStep: 32805... \tLoss: 0.413... \tVal Loss: 0.293 \tVal Frac: 0.9048\n",
      "\tEpoch: 140/1000; 0.98 sec... \tStep: 34020... \tLoss: 0.633... \tVal Loss: 0.307 \tVal Frac: 0.9004\n",
      "\tEpoch: 145/1000; 0.97 sec... \tStep: 35235... \tLoss: 0.139... \tVal Loss: 0.303 \tVal Frac: 0.9096\n",
      "\tEpoch: 150/1000; 1.20 sec... \tStep: 36450... \tLoss: 0.148... \tVal Loss: 0.301 \tVal Frac: 0.9077\n",
      "\tEpoch: 155/1000; 1.00 sec... \tStep: 37665... \tLoss: 0.309... \tVal Loss: 0.302 \tVal Frac: 0.9022\n",
      "\tEpoch: 160/1000; 0.99 sec... \tStep: 38880... \tLoss: 0.325... \tVal Loss: 0.306 \tVal Frac: 0.9048\n",
      "\tEpoch: 165/1000; 1.00 sec... \tStep: 40095... \tLoss: 0.625... \tVal Loss: 0.336 \tVal Frac: 0.8938\n",
      "\tEpoch: 170/1000; 0.99 sec... \tStep: 41310... \tLoss: 0.419... \tVal Loss: 0.313 \tVal Frac: 0.8996\n",
      "\tEpoch: 175/1000; 1.12 sec... \tStep: 42525... \tLoss: 0.444... \tVal Loss: 0.313 \tVal Frac: 0.9048\n",
      "\tEpoch: 180/1000; 0.98 sec... \tStep: 43740... \tLoss: 0.316... \tVal Loss: 0.308 \tVal Frac: 0.9022\n",
      "\tEpoch: 185/1000; 1.01 sec... \tStep: 44955... \tLoss: 0.254... \tVal Loss: 0.307 \tVal Frac: 0.9004\n",
      "\tEpoch: 190/1000; 1.10 sec... \tStep: 46170... \tLoss: 0.558... \tVal Loss: 0.306 \tVal Frac: 0.9011\n",
      "\tEpoch: 195/1000; 0.99 sec... \tStep: 47385... \tLoss: 0.355... \tVal Loss: 0.317 \tVal Frac: 0.9018\n",
      "\tEpoch: 200/1000; 0.99 sec... \tStep: 48600... \tLoss: 0.393... \tVal Loss: 0.316 \tVal Frac: 0.9029\n",
      "\tEpoch: 205/1000; 1.06 sec... \tStep: 49815... \tLoss: 0.359... \tVal Loss: 0.313 \tVal Frac: 0.9066\n",
      "\tEpoch: 210/1000; 1.02 sec... \tStep: 51030... \tLoss: 0.581... \tVal Loss: 0.334 \tVal Frac: 0.8945\n",
      "\tEpoch: 215/1000; 1.10 sec... \tStep: 52245... \tLoss: 0.656... \tVal Loss: 0.312 \tVal Frac: 0.9066\n",
      "\tEpoch: 220/1000; 1.09 sec... \tStep: 53460... \tLoss: 0.527... \tVal Loss: 0.362 \tVal Frac: 0.8919\n",
      "\tEpoch: 225/1000; 1.06 sec... \tStep: 54675... \tLoss: 0.174... \tVal Loss: 0.308 \tVal Frac: 0.9040\n",
      "\tEpoch: 230/1000; 1.09 sec... \tStep: 55890... \tLoss: 0.501... \tVal Loss: 0.329 \tVal Frac: 0.8904\n",
      "\tEpoch: 235/1000; 0.99 sec... \tStep: 57105... \tLoss: 0.344... \tVal Loss: 0.318 \tVal Frac: 0.8930\n",
      "\tEpoch: 240/1000; 1.13 sec... \tStep: 58320... \tLoss: 0.476... \tVal Loss: 0.310 \tVal Frac: 0.8904\n",
      "\tEpoch: 245/1000; 0.97 sec... \tStep: 59535... \tLoss: 0.109... \tVal Loss: 0.295 \tVal Frac: 0.9066\n",
      "\tEpoch: 250/1000; 0.99 sec... \tStep: 60750... \tLoss: 0.161... \tVal Loss: 0.300 \tVal Frac: 0.9015\n",
      "\tEpoch: 255/1000; 1.06 sec... \tStep: 61965... \tLoss: 0.255... \tVal Loss: 0.314 \tVal Frac: 0.8985\n",
      "\tEpoch: 260/1000; 1.10 sec... \tStep: 63180... \tLoss: 0.229... \tVal Loss: 0.293 \tVal Frac: 0.9051\n",
      "\tEpoch: 265/1000; 0.94 sec... \tStep: 64395... \tLoss: 0.454... \tVal Loss: 0.296 \tVal Frac: 0.9103\n",
      "\tEpoch: 270/1000; 1.03 sec... \tStep: 65610... \tLoss: 0.301... \tVal Loss: 0.278 \tVal Frac: 0.9151\n",
      "\tValidation loss decreased (0.290 --> 0.278).  Saving model ...\n",
      "\tEpoch: 275/1000; 1.09 sec... \tStep: 66825... \tLoss: 0.386... \tVal Loss: 0.286 \tVal Frac: 0.9129\n",
      "\tEpoch: 280/1000; 1.00 sec... \tStep: 68040... \tLoss: 0.177... \tVal Loss: 0.279 \tVal Frac: 0.9118\n",
      "\tEpoch: 285/1000; 0.92 sec... \tStep: 69255... \tLoss: 0.164... \tVal Loss: 0.297 \tVal Frac: 0.9096\n",
      "\tEpoch: 290/1000; 1.04 sec... \tStep: 70470... \tLoss: 0.598... \tVal Loss: 0.276 \tVal Frac: 0.9121\n",
      "\tValidation loss decreased (0.278 --> 0.276).  Saving model ...\n",
      "\tEpoch: 295/1000; 0.99 sec... \tStep: 71685... \tLoss: 0.237... \tVal Loss: 0.288 \tVal Frac: 0.9132\n",
      "\tEpoch: 300/1000; 1.10 sec... \tStep: 72900... \tLoss: 0.549... \tVal Loss: 0.298 \tVal Frac: 0.9062\n",
      "\tEpoch: 305/1000; 0.99 sec... \tStep: 74115... \tLoss: 0.427... \tVal Loss: 0.283 \tVal Frac: 0.9092\n",
      "\tEpoch: 310/1000; 1.15 sec... \tStep: 75330... \tLoss: 0.405... \tVal Loss: 0.284 \tVal Frac: 0.9099\n",
      "\tEpoch: 315/1000; 1.03 sec... \tStep: 76545... \tLoss: 0.178... \tVal Loss: 0.291 \tVal Frac: 0.9070\n",
      "\tEpoch: 320/1000; 1.09 sec... \tStep: 77760... \tLoss: 0.358... \tVal Loss: 0.288 \tVal Frac: 0.9033\n",
      "\tEpoch: 325/1000; 0.99 sec... \tStep: 78975... \tLoss: 0.597... \tVal Loss: 0.297 \tVal Frac: 0.9099\n",
      "\tEpoch: 330/1000; 0.97 sec... \tStep: 80190... \tLoss: 0.642... \tVal Loss: 0.298 \tVal Frac: 0.9066\n",
      "\tEpoch: 335/1000; 1.12 sec... \tStep: 81405... \tLoss: 0.272... \tVal Loss: 0.292 \tVal Frac: 0.9110\n",
      "\tEpoch: 340/1000; 1.03 sec... \tStep: 82620... \tLoss: 0.392... \tVal Loss: 0.302 \tVal Frac: 0.9018\n",
      "\tEpoch: 345/1000; 1.00 sec... \tStep: 83835... \tLoss: 0.255... \tVal Loss: 0.292 \tVal Frac: 0.9040\n",
      "\tEpoch: 350/1000; 1.03 sec... \tStep: 85050... \tLoss: 0.187... \tVal Loss: 0.291 \tVal Frac: 0.9077\n",
      "\tEpoch: 355/1000; 0.97 sec... \tStep: 86265... \tLoss: 0.221... \tVal Loss: 0.286 \tVal Frac: 0.9110\n",
      "\tEpoch: 360/1000; 1.07 sec... \tStep: 87480... \tLoss: 0.271... \tVal Loss: 0.297 \tVal Frac: 0.9103\n",
      "\tEpoch: 365/1000; 1.07 sec... \tStep: 88695... \tLoss: 0.369... \tVal Loss: 0.290 \tVal Frac: 0.9143\n",
      "\tEpoch: 370/1000; 1.13 sec... \tStep: 89910... \tLoss: 0.137... \tVal Loss: 0.284 \tVal Frac: 0.9088\n",
      "\tEpoch: 375/1000; 1.02 sec... \tStep: 91125... \tLoss: 0.550... \tVal Loss: 0.274 \tVal Frac: 0.9114\n",
      "\tValidation loss decreased (0.276 --> 0.274).  Saving model ...\n",
      "\tEpoch: 380/1000; 1.09 sec... \tStep: 92340... \tLoss: 0.490... \tVal Loss: 0.276 \tVal Frac: 0.9114\n",
      "\tEpoch: 385/1000; 1.09 sec... \tStep: 93555... \tLoss: 0.308... \tVal Loss: 0.284 \tVal Frac: 0.9136\n",
      "\tEpoch: 390/1000; 1.02 sec... \tStep: 94770... \tLoss: 0.228... \tVal Loss: 0.283 \tVal Frac: 0.9099\n",
      "\tEpoch: 395/1000; 1.06 sec... \tStep: 95985... \tLoss: 0.455... \tVal Loss: 0.289 \tVal Frac: 0.9107\n",
      "\tEpoch: 400/1000; 1.13 sec... \tStep: 97200... \tLoss: 0.649... \tVal Loss: 0.291 \tVal Frac: 0.9096\n",
      "\tEpoch: 405/1000; 0.90 sec... \tStep: 98415... \tLoss: 0.244... \tVal Loss: 0.295 \tVal Frac: 0.9092\n",
      "\tEpoch: 410/1000; 1.09 sec... \tStep: 99630... \tLoss: 0.569... \tVal Loss: 0.296 \tVal Frac: 0.9140\n",
      "\tEpoch: 415/1000; 1.07 sec... \tStep: 100845... \tLoss: 0.140... \tVal Loss: 0.280 \tVal Frac: 0.9132\n",
      "\tEpoch: 420/1000; 0.97 sec... \tStep: 102060... \tLoss: 0.572... \tVal Loss: 0.283 \tVal Frac: 0.9154\n",
      "\tEpoch: 425/1000; 0.99 sec... \tStep: 103275... \tLoss: 0.297... \tVal Loss: 0.294 \tVal Frac: 0.9103\n",
      "\tEpoch: 430/1000; 1.02 sec... \tStep: 104490... \tLoss: 0.384... \tVal Loss: 0.289 \tVal Frac: 0.9085\n",
      "\tEpoch: 435/1000; 1.09 sec... \tStep: 105705... \tLoss: 0.439... \tVal Loss: 0.280 \tVal Frac: 0.9110\n",
      "\tEpoch: 440/1000; 1.05 sec... \tStep: 106920... \tLoss: 0.522... \tVal Loss: 0.299 \tVal Frac: 0.9074\n",
      "\tEpoch: 445/1000; 1.02 sec... \tStep: 108135... \tLoss: 0.335... \tVal Loss: 0.294 \tVal Frac: 0.9033\n",
      "\tEpoch: 450/1000; 1.00 sec... \tStep: 109350... \tLoss: 0.311... \tVal Loss: 0.307 \tVal Frac: 0.9029\n",
      "\tEpoch: 455/1000; 0.92 sec... \tStep: 110565... \tLoss: 0.654... \tVal Loss: 0.301 \tVal Frac: 0.9007\n",
      "\tEpoch: 460/1000; 1.06 sec... \tStep: 111780... \tLoss: 0.728... \tVal Loss: 0.295 \tVal Frac: 0.9037\n",
      "\tEpoch: 465/1000; 0.94 sec... \tStep: 112995... \tLoss: 0.349... \tVal Loss: 0.288 \tVal Frac: 0.9077\n",
      "\tEpoch: 470/1000; 1.03 sec... \tStep: 114210... \tLoss: 0.234... \tVal Loss: 0.291 \tVal Frac: 0.9055\n",
      "\tEpoch: 475/1000; 0.97 sec... \tStep: 115425... \tLoss: 0.134... \tVal Loss: 0.288 \tVal Frac: 0.9085\n",
      "\tEpoch: 480/1000; 1.03 sec... \tStep: 116640... \tLoss: 0.487... \tVal Loss: 0.280 \tVal Frac: 0.9051\n",
      "\tEpoch: 485/1000; 1.07 sec... \tStep: 117855... \tLoss: 0.472... \tVal Loss: 0.294 \tVal Frac: 0.9011\n",
      "\tEpoch: 490/1000; 1.06 sec... \tStep: 119070... \tLoss: 0.572... \tVal Loss: 0.297 \tVal Frac: 0.9037\n",
      "\tEpoch: 495/1000; 1.10 sec... \tStep: 120285... \tLoss: 0.398... \tVal Loss: 0.300 \tVal Frac: 0.9048\n",
      "\tEpoch: 500/1000; 1.02 sec... \tStep: 121500... \tLoss: 0.474... \tVal Loss: 0.303 \tVal Frac: 0.8989\n",
      "\tEpoch: 505/1000; 1.06 sec... \tStep: 122715... \tLoss: 0.394... \tVal Loss: 0.298 \tVal Frac: 0.9018\n",
      "\tEpoch: 510/1000; 0.98 sec... \tStep: 123930... \tLoss: 0.290... \tVal Loss: 0.279 \tVal Frac: 0.9051\n",
      "\tEpoch: 515/1000; 0.94 sec... \tStep: 125145... \tLoss: 0.441... \tVal Loss: 0.291 \tVal Frac: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 520/1000; 1.03 sec... \tStep: 126360... \tLoss: 0.420... \tVal Loss: 0.312 \tVal Frac: 0.9007\n",
      "\tEpoch: 525/1000; 1.05 sec... \tStep: 127575... \tLoss: 0.345... \tVal Loss: 0.301 \tVal Frac: 0.9055\n",
      "\tEpoch: 530/1000; 0.97 sec... \tStep: 128790... \tLoss: 0.258... \tVal Loss: 0.299 \tVal Frac: 0.9026\n",
      "\tEpoch: 535/1000; 0.99 sec... \tStep: 130005... \tLoss: 0.131... \tVal Loss: 0.282 \tVal Frac: 0.9048\n",
      "\tEpoch: 540/1000; 1.02 sec... \tStep: 131220... \tLoss: 0.273... \tVal Loss: 0.266 \tVal Frac: 0.9110\n",
      "\tValidation loss decreased (0.274 --> 0.266).  Saving model ...\n",
      "\tEpoch: 545/1000; 1.07 sec... \tStep: 132435... \tLoss: 0.585... \tVal Loss: 0.272 \tVal Frac: 0.9040\n",
      "\tEpoch: 550/1000; 0.94 sec... \tStep: 133650... \tLoss: 0.119... \tVal Loss: 0.272 \tVal Frac: 0.9066\n",
      "\tEpoch: 555/1000; 1.07 sec... \tStep: 134865... \tLoss: 0.261... \tVal Loss: 0.267 \tVal Frac: 0.9118\n",
      "\tEpoch: 560/1000; 0.99 sec... \tStep: 136080... \tLoss: 0.286... \tVal Loss: 0.253 \tVal Frac: 0.9169\n",
      "\tValidation loss decreased (0.266 --> 0.253).  Saving model ...\n",
      "\tEpoch: 565/1000; 0.96 sec... \tStep: 137295... \tLoss: 0.249... \tVal Loss: 0.260 \tVal Frac: 0.9176\n",
      "\tEpoch: 570/1000; 1.09 sec... \tStep: 138510... \tLoss: 0.196... \tVal Loss: 0.257 \tVal Frac: 0.9121\n",
      "\tEpoch: 575/1000; 0.95 sec... \tStep: 139725... \tLoss: 0.431... \tVal Loss: 0.245 \tVal Frac: 0.9180\n",
      "\tValidation loss decreased (0.253 --> 0.245).  Saving model ...\n",
      "\tEpoch: 580/1000; 1.10 sec... \tStep: 140940... \tLoss: 0.258... \tVal Loss: 0.243 \tVal Frac: 0.9232\n",
      "\tValidation loss decreased (0.245 --> 0.243).  Saving model ...\n",
      "\tEpoch: 585/1000; 1.04 sec... \tStep: 142155... \tLoss: 0.228... \tVal Loss: 0.251 \tVal Frac: 0.9158\n",
      "\tEpoch: 590/1000; 0.96 sec... \tStep: 143370... \tLoss: 0.461... \tVal Loss: 0.233 \tVal Frac: 0.9235\n",
      "\tValidation loss decreased (0.243 --> 0.233).  Saving model ...\n",
      "\tEpoch: 595/1000; 1.07 sec... \tStep: 144585... \tLoss: 0.401... \tVal Loss: 0.250 \tVal Frac: 0.9147\n",
      "\tEpoch: 600/1000; 0.92 sec... \tStep: 145800... \tLoss: 0.221... \tVal Loss: 0.273 \tVal Frac: 0.9074\n",
      "\tEpoch: 605/1000; 1.06 sec... \tStep: 147015... \tLoss: 0.096... \tVal Loss: 0.261 \tVal Frac: 0.9114\n",
      "\tEpoch: 610/1000; 1.04 sec... \tStep: 148230... \tLoss: 0.161... \tVal Loss: 0.261 \tVal Frac: 0.9191\n",
      "\tEpoch: 615/1000; 1.02 sec... \tStep: 149445... \tLoss: 0.407... \tVal Loss: 0.268 \tVal Frac: 0.9129\n",
      "\tEpoch: 620/1000; 1.00 sec... \tStep: 150660... \tLoss: 0.368... \tVal Loss: 0.272 \tVal Frac: 0.9118\n",
      "\tEpoch: 625/1000; 1.03 sec... \tStep: 151875... \tLoss: 0.469... \tVal Loss: 0.283 \tVal Frac: 0.9129\n",
      "\tEpoch: 630/1000; 1.00 sec... \tStep: 153090... \tLoss: 0.473... \tVal Loss: 0.279 \tVal Frac: 0.9162\n",
      "\tEpoch: 635/1000; 1.02 sec... \tStep: 154305... \tLoss: 0.325... \tVal Loss: 0.292 \tVal Frac: 0.9085\n",
      "\tEpoch: 640/1000; 1.03 sec... \tStep: 155520... \tLoss: 0.142... \tVal Loss: 0.286 \tVal Frac: 0.9066\n",
      "\tEpoch: 645/1000; 1.00 sec... \tStep: 156735... \tLoss: 0.408... \tVal Loss: 0.280 \tVal Frac: 0.9085\n",
      "\tEpoch: 650/1000; 1.01 sec... \tStep: 157950... \tLoss: 0.486... \tVal Loss: 0.272 \tVal Frac: 0.9136\n",
      "\tEpoch: 655/1000; 1.07 sec... \tStep: 159165... \tLoss: 0.227... \tVal Loss: 0.273 \tVal Frac: 0.9118\n",
      "\tEpoch: 660/1000; 0.99 sec... \tStep: 160380... \tLoss: 0.189... \tVal Loss: 0.286 \tVal Frac: 0.9118\n",
      "\tEpoch: 665/1000; 1.06 sec... \tStep: 161595... \tLoss: 0.311... \tVal Loss: 0.292 \tVal Frac: 0.9099\n",
      "\tEpoch: 670/1000; 0.99 sec... \tStep: 162810... \tLoss: 0.253... \tVal Loss: 0.297 \tVal Frac: 0.9114\n",
      "\tEpoch: 675/1000; 1.03 sec... \tStep: 164025... \tLoss: 0.468... \tVal Loss: 0.284 \tVal Frac: 0.9136\n",
      "\tEpoch: 680/1000; 1.03 sec... \tStep: 165240... \tLoss: 0.264... \tVal Loss: 0.290 \tVal Frac: 0.9099\n",
      "\tEpoch: 685/1000; 1.00 sec... \tStep: 166455... \tLoss: 0.206... \tVal Loss: 0.291 \tVal Frac: 0.9107\n",
      "\tEpoch: 690/1000; 0.95 sec... \tStep: 167670... \tLoss: 0.348... \tVal Loss: 0.297 \tVal Frac: 0.9081\n",
      "\tEpoch: 695/1000; 1.02 sec... \tStep: 168885... \tLoss: 0.209... \tVal Loss: 0.285 \tVal Frac: 0.9077\n",
      "\tEpoch: 700/1000; 1.10 sec... \tStep: 170100... \tLoss: 0.451... \tVal Loss: 0.274 \tVal Frac: 0.9121\n",
      "\tEpoch: 705/1000; 1.02 sec... \tStep: 171315... \tLoss: 0.463... \tVal Loss: 0.276 \tVal Frac: 0.9158\n",
      "\tEpoch: 710/1000; 1.01 sec... \tStep: 172530... \tLoss: 0.129... \tVal Loss: 0.274 \tVal Frac: 0.9103\n",
      "\tEpoch: 715/1000; 1.09 sec... \tStep: 173745... \tLoss: 0.163... \tVal Loss: 0.282 \tVal Frac: 0.9103\n",
      "\tEpoch: 720/1000; 1.03 sec... \tStep: 174960... \tLoss: 0.093... \tVal Loss: 0.285 \tVal Frac: 0.9118\n",
      "\tEpoch: 725/1000; 1.03 sec... \tStep: 176175... \tLoss: 0.278... \tVal Loss: 0.288 \tVal Frac: 0.9066\n",
      "\tEpoch: 730/1000; 1.02 sec... \tStep: 177390... \tLoss: 0.148... \tVal Loss: 0.285 \tVal Frac: 0.9074\n",
      "\tEpoch: 735/1000; 0.98 sec... \tStep: 178605... \tLoss: 0.329... \tVal Loss: 0.288 \tVal Frac: 0.9077\n",
      "\tEpoch: 740/1000; 1.17 sec... \tStep: 179820... \tLoss: 0.187... \tVal Loss: 0.276 \tVal Frac: 0.9118\n",
      "\tEpoch: 745/1000; 0.97 sec... \tStep: 181035... \tLoss: 0.393... \tVal Loss: 0.282 \tVal Frac: 0.9158\n",
      "\tEpoch: 750/1000; 0.99 sec... \tStep: 182250... \tLoss: 0.449... \tVal Loss: 0.296 \tVal Frac: 0.9085\n",
      "\tEpoch: 755/1000; 1.02 sec... \tStep: 183465... \tLoss: 0.450... \tVal Loss: 0.284 \tVal Frac: 0.9077\n",
      "\tEpoch: 760/1000; 0.98 sec... \tStep: 184680... \tLoss: 0.301... \tVal Loss: 0.288 \tVal Frac: 0.9107\n",
      "\tEpoch: 765/1000; 1.06 sec... \tStep: 185895... \tLoss: 0.242... \tVal Loss: 0.289 \tVal Frac: 0.9114\n",
      "\tEpoch: 770/1000; 1.02 sec... \tStep: 187110... \tLoss: 0.260... \tVal Loss: 0.294 \tVal Frac: 0.9059\n",
      "\tEpoch: 775/1000; 1.07 sec... \tStep: 188325... \tLoss: 0.082... \tVal Loss: 0.295 \tVal Frac: 0.9066\n",
      "\tEpoch: 780/1000; 1.06 sec... \tStep: 189540... \tLoss: 0.230... \tVal Loss: 0.297 \tVal Frac: 0.9077\n",
      "\tEpoch: 785/1000; 1.02 sec... \tStep: 190755... \tLoss: 0.564... \tVal Loss: 0.293 \tVal Frac: 0.9077\n",
      "\tEpoch: 790/1000; 0.97 sec... \tStep: 191970... \tLoss: 0.164... \tVal Loss: 0.292 \tVal Frac: 0.9110\n",
      "\tEpoch: 795/1000; 1.06 sec... \tStep: 193185... \tLoss: 0.073... \tVal Loss: 0.295 \tVal Frac: 0.9092\n",
      "\tEpoch: 800/1000; 1.07 sec... \tStep: 194400... \tLoss: 0.104... \tVal Loss: 0.281 \tVal Frac: 0.9099\n",
      "\tEpoch: 805/1000; 0.90 sec... \tStep: 195615... \tLoss: 0.289... \tVal Loss: 0.282 \tVal Frac: 0.9114\n",
      "\tEpoch: 810/1000; 0.97 sec... \tStep: 196830... \tLoss: 0.152... \tVal Loss: 0.285 \tVal Frac: 0.9121\n",
      "\tEpoch: 815/1000; 0.94 sec... \tStep: 198045... \tLoss: 0.379... \tVal Loss: 0.288 \tVal Frac: 0.9107\n",
      "\tEpoch: 820/1000; 1.12 sec... \tStep: 199260... \tLoss: 0.161... \tVal Loss: 0.288 \tVal Frac: 0.9074\n",
      "\tEpoch: 825/1000; 1.09 sec... \tStep: 200475... \tLoss: 0.307... \tVal Loss: 0.284 \tVal Frac: 0.9092\n",
      "\tEpoch: 830/1000; 1.00 sec... \tStep: 201690... \tLoss: 0.285... \tVal Loss: 0.281 \tVal Frac: 0.9088\n",
      "\tEpoch: 835/1000; 1.07 sec... \tStep: 202905... \tLoss: 0.069... \tVal Loss: 0.284 \tVal Frac: 0.9121\n",
      "\tEpoch: 840/1000; 1.10 sec... \tStep: 204120... \tLoss: 0.189... \tVal Loss: 0.275 \tVal Frac: 0.9151\n",
      "\tEpoch: 845/1000; 1.00 sec... \tStep: 205335... \tLoss: 0.448... \tVal Loss: 0.276 \tVal Frac: 0.9136\n",
      "\tEpoch: 850/1000; 1.06 sec... \tStep: 206550... \tLoss: 0.173... \tVal Loss: 0.265 \tVal Frac: 0.9136\n",
      "\tEpoch: 855/1000; 1.08 sec... \tStep: 207765... \tLoss: 0.326... \tVal Loss: 0.263 \tVal Frac: 0.9165\n",
      "\tEpoch: 860/1000; 1.03 sec... \tStep: 208980... \tLoss: 0.451... \tVal Loss: 0.254 \tVal Frac: 0.9180\n",
      "\tEpoch: 865/1000; 1.05 sec... \tStep: 210195... \tLoss: 0.237... \tVal Loss: 0.260 \tVal Frac: 0.9206\n",
      "\tEpoch: 870/1000; 1.05 sec... \tStep: 211410... \tLoss: 0.119... \tVal Loss: 0.264 \tVal Frac: 0.9180\n",
      "\tEpoch: 875/1000; 1.15 sec... \tStep: 212625... \tLoss: 0.228... \tVal Loss: 0.279 \tVal Frac: 0.9165\n",
      "\tEpoch: 880/1000; 1.02 sec... \tStep: 213840... \tLoss: 0.320... \tVal Loss: 0.271 \tVal Frac: 0.9254\n",
      "\tEpoch: 885/1000; 0.96 sec... \tStep: 215055... \tLoss: 0.266... \tVal Loss: 0.288 \tVal Frac: 0.9165\n",
      "\tEpoch: 890/1000; 1.07 sec... \tStep: 216270... \tLoss: 0.318... \tVal Loss: 0.292 \tVal Frac: 0.9162\n",
      "\tEpoch: 895/1000; 1.04 sec... \tStep: 217485... \tLoss: 0.424... \tVal Loss: 0.279 \tVal Frac: 0.9147\n",
      "\tEpoch: 900/1000; 0.99 sec... \tStep: 218700... \tLoss: 0.163... \tVal Loss: 0.278 \tVal Frac: 0.9129\n",
      "\tEpoch: 905/1000; 0.97 sec... \tStep: 219915... \tLoss: 0.475... \tVal Loss: 0.288 \tVal Frac: 0.9143\n",
      "\tEpoch: 910/1000; 1.00 sec... \tStep: 221130... \tLoss: 0.426... \tVal Loss: 0.271 \tVal Frac: 0.9180\n",
      "\tEpoch: 915/1000; 1.07 sec... \tStep: 222345... \tLoss: 0.294... \tVal Loss: 0.270 \tVal Frac: 0.9147\n",
      "\tEpoch: 920/1000; 0.99 sec... \tStep: 223560... \tLoss: 0.209... \tVal Loss: 0.265 \tVal Frac: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 925/1000; 1.02 sec... \tStep: 224775... \tLoss: 0.325... \tVal Loss: 0.269 \tVal Frac: 0.9191\n",
      "\tEpoch: 930/1000; 1.03 sec... \tStep: 225990... \tLoss: 0.389... \tVal Loss: 0.281 \tVal Frac: 0.9147\n",
      "\tEpoch: 935/1000; 1.07 sec... \tStep: 227205... \tLoss: 0.281... \tVal Loss: 0.268 \tVal Frac: 0.9143\n",
      "\tEpoch: 940/1000; 1.10 sec... \tStep: 228420... \tLoss: 0.572... \tVal Loss: 0.290 \tVal Frac: 0.9059\n",
      "\tEpoch: 945/1000; 1.13 sec... \tStep: 229635... \tLoss: 0.230... \tVal Loss: 0.284 \tVal Frac: 0.9081\n",
      "\tEpoch: 950/1000; 1.07 sec... \tStep: 230850... \tLoss: 0.175... \tVal Loss: 0.273 \tVal Frac: 0.9096\n",
      "\tEpoch: 955/1000; 0.97 sec... \tStep: 232065... \tLoss: 0.326... \tVal Loss: 0.274 \tVal Frac: 0.9077\n",
      "\tEpoch: 960/1000; 1.00 sec... \tStep: 233280... \tLoss: 0.117... \tVal Loss: 0.279 \tVal Frac: 0.9121\n",
      "\tEpoch: 965/1000; 0.96 sec... \tStep: 234495... \tLoss: 0.136... \tVal Loss: 0.268 \tVal Frac: 0.9154\n",
      "\tEpoch: 970/1000; 1.02 sec... \tStep: 235710... \tLoss: 0.259... \tVal Loss: 0.269 \tVal Frac: 0.9132\n",
      "\tEpoch: 975/1000; 1.13 sec... \tStep: 236925... \tLoss: 0.459... \tVal Loss: 0.274 \tVal Frac: 0.9132\n",
      "\tEpoch: 980/1000; 1.00 sec... \tStep: 238140... \tLoss: 0.560... \tVal Loss: 0.265 \tVal Frac: 0.9191\n",
      "\tEpoch: 985/1000; 1.07 sec... \tStep: 239355... \tLoss: 0.322... \tVal Loss: 0.271 \tVal Frac: 0.9118\n",
      "\tEpoch: 990/1000; 0.99 sec... \tStep: 240570... \tLoss: 0.153... \tVal Loss: 0.256 \tVal Frac: 0.9213\n",
      "\tEpoch: 995/1000; 1.07 sec... \tStep: 241785... \tLoss: 0.245... \tVal Loss: 0.272 \tVal Frac: 0.9151\n",
      "\tEpoch: 1000/1000; 1.10 sec... \tStep: 243000... \tLoss: 0.154... \tVal Loss: 0.268 \tVal Frac: 0.9158\n",
      "Completed:  n_windows :  1 \n",
      "\tloss: 0.232972 \n",
      "\tfrac:0.925368\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  20\n",
      "\tEpoch: 5/1000; 1.13 sec... \tStep: 1215... \tLoss: 1.227... \tVal Loss: 1.298 \tVal Frac: 0.6610\n",
      "\tValidation loss decreased (inf --> 1.298).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.19 sec... \tStep: 2430... \tLoss: 1.618... \tVal Loss: 1.344 \tVal Frac: 0.7335\n",
      "\tEpoch: 15/1000; 1.23 sec... \tStep: 3645... \tLoss: 1.357... \tVal Loss: 1.149 \tVal Frac: 0.7404\n",
      "\tValidation loss decreased (1.298 --> 1.149).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.17 sec... \tStep: 4860... \tLoss: 1.139... \tVal Loss: 1.205 \tVal Frac: 0.7390\n",
      "\tEpoch: 25/1000; 1.12 sec... \tStep: 6075... \tLoss: 1.404... \tVal Loss: 1.231 \tVal Frac: 0.7423\n",
      "\tEpoch: 30/1000; 1.16 sec... \tStep: 7290... \tLoss: 1.185... \tVal Loss: 1.221 \tVal Frac: 0.7379\n",
      "\tEpoch: 35/1000; 1.10 sec... \tStep: 8505... \tLoss: 1.385... \tVal Loss: 1.208 \tVal Frac: 0.7140\n",
      "\tEpoch: 40/1000; 1.27 sec... \tStep: 9720... \tLoss: 1.063... \tVal Loss: 0.991 \tVal Frac: 0.7574\n",
      "\tValidation loss decreased (1.149 --> 0.991).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.07 sec... \tStep: 10935... \tLoss: 1.240... \tVal Loss: 1.277 \tVal Frac: 0.7232\n",
      "\tEpoch: 50/1000; 1.23 sec... \tStep: 12150... \tLoss: 1.772... \tVal Loss: 1.339 \tVal Frac: 0.7026\n",
      "\tEpoch: 55/1000; 1.26 sec... \tStep: 13365... \tLoss: 1.379... \tVal Loss: 1.151 \tVal Frac: 0.7136\n",
      "\tEpoch: 60/1000; 1.14 sec... \tStep: 14580... \tLoss: 1.607... \tVal Loss: 1.185 \tVal Frac: 0.7298\n",
      "\tEpoch: 65/1000; 1.20 sec... \tStep: 15795... \tLoss: 1.340... \tVal Loss: 1.231 \tVal Frac: 0.7250\n",
      "\tEpoch: 70/1000; 1.21 sec... \tStep: 17010... \tLoss: 1.065... \tVal Loss: 1.188 \tVal Frac: 0.4507\n",
      "\tEpoch: 75/1000; 1.20 sec... \tStep: 18225... \tLoss: 1.560... \tVal Loss: 1.186 \tVal Frac: 0.7474\n",
      "\tEpoch: 80/1000; 1.10 sec... \tStep: 19440... \tLoss: 1.116... \tVal Loss: 1.223 \tVal Frac: 0.7088\n",
      "\tEpoch: 85/1000; 1.19 sec... \tStep: 20655... \tLoss: 1.390... \tVal Loss: 1.182 \tVal Frac: 0.7371\n",
      "\tEpoch: 90/1000; 1.05 sec... \tStep: 21870... \tLoss: 0.972... \tVal Loss: 1.071 \tVal Frac: 0.7482\n",
      "\tEpoch: 95/1000; 1.19 sec... \tStep: 23085... \tLoss: 1.413... \tVal Loss: 1.272 \tVal Frac: 0.4658\n",
      "\tEpoch: 100/1000; 1.19 sec... \tStep: 24300... \tLoss: 1.227... \tVal Loss: 1.263 \tVal Frac: 0.4004\n",
      "\tEpoch: 105/1000; 1.19 sec... \tStep: 25515... \tLoss: 1.181... \tVal Loss: 1.257 \tVal Frac: 0.7011\n",
      "\tEpoch: 110/1000; 1.17 sec... \tStep: 26730... \tLoss: 1.454... \tVal Loss: 1.335 \tVal Frac: 0.0673\n",
      "\tEpoch: 115/1000; 1.17 sec... \tStep: 27945... \tLoss: 1.309... \tVal Loss: 1.230 \tVal Frac: 0.0555\n",
      "\tEpoch: 120/1000; 1.10 sec... \tStep: 29160... \tLoss: 1.255... \tVal Loss: 1.069 \tVal Frac: 0.7360\n",
      "\tEpoch: 125/1000; 1.03 sec... \tStep: 30375... \tLoss: 1.469... \tVal Loss: 1.209 \tVal Frac: 0.0651\n",
      "\tEpoch: 130/1000; 1.27 sec... \tStep: 31590... \tLoss: 0.986... \tVal Loss: 1.227 \tVal Frac: 0.4265\n",
      "\tEpoch: 135/1000; 1.17 sec... \tStep: 32805... \tLoss: 1.184... \tVal Loss: 1.196 \tVal Frac: 0.2768\n",
      "\tEpoch: 140/1000; 1.10 sec... \tStep: 34020... \tLoss: 1.446... \tVal Loss: 1.180 \tVal Frac: 0.7195\n",
      "\tEpoch: 145/1000; 1.19 sec... \tStep: 35235... \tLoss: 1.193... \tVal Loss: 1.221 \tVal Frac: 0.7338\n",
      "\tEpoch: 150/1000; 1.32 sec... \tStep: 36450... \tLoss: 1.321... \tVal Loss: 1.218 \tVal Frac: 0.7500\n",
      "\tEpoch: 155/1000; 1.20 sec... \tStep: 37665... \tLoss: 1.151... \tVal Loss: 1.246 \tVal Frac: 0.0739\n",
      "\tEpoch: 160/1000; 1.20 sec... \tStep: 38880... \tLoss: 1.371... \tVal Loss: 1.239 \tVal Frac: 0.4489\n",
      "\tEpoch: 165/1000; 1.19 sec... \tStep: 40095... \tLoss: 1.346... \tVal Loss: 1.221 \tVal Frac: 0.7375\n",
      "\tEpoch: 170/1000; 1.25 sec... \tStep: 41310... \tLoss: 1.135... \tVal Loss: 1.202 \tVal Frac: 0.6860\n",
      "\tEpoch: 175/1000; 1.28 sec... \tStep: 42525... \tLoss: 1.265... \tVal Loss: 1.109 \tVal Frac: 0.7445\n",
      "\tEpoch: 180/1000; 1.16 sec... \tStep: 43740... \tLoss: 1.139... \tVal Loss: 1.159 \tVal Frac: 0.0985\n",
      "\tEpoch: 185/1000; 1.20 sec... \tStep: 44955... \tLoss: 1.381... \tVal Loss: 1.191 \tVal Frac: 0.7301\n",
      "\tEpoch: 190/1000; 1.24 sec... \tStep: 46170... \tLoss: 1.256... \tVal Loss: 1.204 \tVal Frac: 0.7272\n",
      "\tEpoch: 195/1000; 1.20 sec... \tStep: 47385... \tLoss: 1.456... \tVal Loss: 1.241 \tVal Frac: 0.2423\n",
      "\tEpoch: 200/1000; 1.23 sec... \tStep: 48600... \tLoss: 1.205... \tVal Loss: 1.097 \tVal Frac: 0.7232\n",
      "\tEpoch: 205/1000; 1.09 sec... \tStep: 49815... \tLoss: 1.236... \tVal Loss: 1.041 \tVal Frac: 0.7261\n",
      "\tEpoch: 210/1000; 1.19 sec... \tStep: 51030... \tLoss: 1.230... \tVal Loss: 0.954 \tVal Frac: 0.7243\n",
      "\tValidation loss decreased (0.991 --> 0.954).  Saving model ...\n",
      "\tEpoch: 215/1000; 1.30 sec... \tStep: 52245... \tLoss: 1.358... \tVal Loss: 1.182 \tVal Frac: 0.7051\n",
      "\tEpoch: 220/1000; 1.19 sec... \tStep: 53460... \tLoss: 0.879... \tVal Loss: 0.957 \tVal Frac: 0.7246\n",
      "\tEpoch: 225/1000; 1.09 sec... \tStep: 54675... \tLoss: 1.275... \tVal Loss: 1.034 \tVal Frac: 0.7353\n",
      "\tEpoch: 230/1000; 1.20 sec... \tStep: 55890... \tLoss: 1.158... \tVal Loss: 1.197 \tVal Frac: 0.7301\n",
      "\tEpoch: 235/1000; 1.26 sec... \tStep: 57105... \tLoss: 1.025... \tVal Loss: 1.185 \tVal Frac: 0.7298\n",
      "\tEpoch: 240/1000; 1.31 sec... \tStep: 58320... \tLoss: 1.306... \tVal Loss: 1.204 \tVal Frac: 0.7217\n",
      "\tEpoch: 245/1000; 1.22 sec... \tStep: 59535... \tLoss: 1.084... \tVal Loss: 1.167 \tVal Frac: 0.7342\n",
      "\tEpoch: 250/1000; 1.35 sec... \tStep: 60750... \tLoss: 1.322... \tVal Loss: 1.159 \tVal Frac: 0.7276\n",
      "\tEpoch: 255/1000; 1.13 sec... \tStep: 61965... \tLoss: 1.227... \tVal Loss: 1.068 \tVal Frac: 0.7555\n",
      "\tEpoch: 260/1000; 1.17 sec... \tStep: 63180... \tLoss: 1.361... \tVal Loss: 1.161 \tVal Frac: 0.7312\n",
      "\tEpoch: 265/1000; 1.17 sec... \tStep: 64395... \tLoss: 1.092... \tVal Loss: 1.138 \tVal Frac: 0.7401\n",
      "\tEpoch: 270/1000; 1.27 sec... \tStep: 65610... \tLoss: 1.312... \tVal Loss: 1.166 \tVal Frac: 0.7732\n",
      "\tEpoch: 275/1000; 1.18 sec... \tStep: 66825... \tLoss: 0.962... \tVal Loss: 1.047 \tVal Frac: 0.7415\n",
      "\tEpoch: 280/1000; 1.22 sec... \tStep: 68040... \tLoss: 1.133... \tVal Loss: 1.094 \tVal Frac: 0.7199\n",
      "\tEpoch: 285/1000; 1.25 sec... \tStep: 69255... \tLoss: 0.969... \tVal Loss: 1.146 \tVal Frac: 0.7430\n",
      "\tEpoch: 290/1000; 1.29 sec... \tStep: 70470... \tLoss: 1.277... \tVal Loss: 1.037 \tVal Frac: 0.7324\n",
      "\tEpoch: 295/1000; 1.17 sec... \tStep: 71685... \tLoss: 0.958... \tVal Loss: 1.115 \tVal Frac: 0.7449\n",
      "\tEpoch: 300/1000; 1.17 sec... \tStep: 72900... \tLoss: 1.305... \tVal Loss: 1.179 \tVal Frac: 0.0746\n",
      "\tEpoch: 305/1000; 1.20 sec... \tStep: 74115... \tLoss: 1.120... \tVal Loss: 1.205 \tVal Frac: 0.0831\n",
      "\tEpoch: 310/1000; 1.13 sec... \tStep: 75330... \tLoss: 1.033... \tVal Loss: 1.159 \tVal Frac: 0.7202\n",
      "\tEpoch: 315/1000; 1.11 sec... \tStep: 76545... \tLoss: 1.139... \tVal Loss: 1.184 \tVal Frac: 0.0684\n",
      "\tEpoch: 320/1000; 1.25 sec... \tStep: 77760... \tLoss: 1.281... \tVal Loss: 1.193 \tVal Frac: 0.0728\n",
      "\tEpoch: 325/1000; 1.22 sec... \tStep: 78975... \tLoss: 1.107... \tVal Loss: 1.187 \tVal Frac: 0.7290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 330/1000; 1.20 sec... \tStep: 80190... \tLoss: 1.191... \tVal Loss: 1.167 \tVal Frac: 0.7390\n",
      "\tEpoch: 335/1000; 1.13 sec... \tStep: 81405... \tLoss: 1.027... \tVal Loss: 1.185 \tVal Frac: 0.7305\n",
      "\tEpoch: 340/1000; 1.19 sec... \tStep: 82620... \tLoss: 1.409... \tVal Loss: 1.172 \tVal Frac: 0.6331\n",
      "\tEpoch: 345/1000; 1.20 sec... \tStep: 83835... \tLoss: 1.463... \tVal Loss: 1.185 \tVal Frac: 0.7265\n",
      "\tEpoch: 350/1000; 1.08 sec... \tStep: 85050... \tLoss: 1.179... \tVal Loss: 1.159 \tVal Frac: 0.7460\n",
      "\tEpoch: 355/1000; 1.17 sec... \tStep: 86265... \tLoss: 0.961... \tVal Loss: 1.016 \tVal Frac: 0.7390\n",
      "\tEpoch: 360/1000; 1.19 sec... \tStep: 87480... \tLoss: 1.375... \tVal Loss: 1.133 \tVal Frac: 0.7268\n",
      "\tEpoch: 365/1000; 1.08 sec... \tStep: 88695... \tLoss: 1.132... \tVal Loss: 1.177 \tVal Frac: 0.7364\n",
      "\tEpoch: 370/1000; 1.09 sec... \tStep: 89910... \tLoss: 1.644... \tVal Loss: 1.176 \tVal Frac: 0.4132\n",
      "\tEpoch: 375/1000; 1.19 sec... \tStep: 91125... \tLoss: 1.309... \tVal Loss: 1.172 \tVal Frac: 0.7316\n",
      "\tEpoch: 380/1000; 1.20 sec... \tStep: 92340... \tLoss: 1.477... \tVal Loss: 1.178 \tVal Frac: 0.6210\n",
      "\tEpoch: 385/1000; 1.18 sec... \tStep: 93555... \tLoss: 1.570... \tVal Loss: 1.169 \tVal Frac: 0.6228\n",
      "\tEpoch: 390/1000; 1.17 sec... \tStep: 94770... \tLoss: 1.098... \tVal Loss: 1.191 \tVal Frac: 0.0669\n",
      "\tEpoch: 395/1000; 1.17 sec... \tStep: 95985... \tLoss: 1.290... \tVal Loss: 1.179 \tVal Frac: 0.6294\n",
      "\tEpoch: 400/1000; 1.17 sec... \tStep: 97200... \tLoss: 1.354... \tVal Loss: 1.229 \tVal Frac: 0.5963\n",
      "\tEpoch: 405/1000; 1.18 sec... \tStep: 98415... \tLoss: 1.191... \tVal Loss: 1.163 \tVal Frac: 0.4801\n",
      "\tEpoch: 410/1000; 1.20 sec... \tStep: 99630... \tLoss: 1.381... \tVal Loss: 1.159 \tVal Frac: 0.7471\n",
      "\tEpoch: 415/1000; 1.18 sec... \tStep: 100845... \tLoss: 1.303... \tVal Loss: 1.188 \tVal Frac: 0.0754\n",
      "\tEpoch: 420/1000; 1.23 sec... \tStep: 102060... \tLoss: 1.381... \tVal Loss: 1.215 \tVal Frac: 0.6026\n",
      "\tEpoch: 425/1000; 1.23 sec... \tStep: 103275... \tLoss: 1.216... \tVal Loss: 1.216 \tVal Frac: 0.5761\n",
      "\tEpoch: 430/1000; 1.29 sec... \tStep: 104490... \tLoss: 1.255... \tVal Loss: 1.214 \tVal Frac: 0.6574\n",
      "\tEpoch: 435/1000; 1.16 sec... \tStep: 105705... \tLoss: 1.332... \tVal Loss: 1.220 \tVal Frac: 0.6562\n",
      "\tEpoch: 440/1000; 1.32 sec... \tStep: 106920... \tLoss: 1.398... \tVal Loss: 1.270 \tVal Frac: 0.1574\n",
      "\tEpoch: 445/1000; 1.12 sec... \tStep: 108135... \tLoss: 1.261... \tVal Loss: 1.172 \tVal Frac: 0.6765\n",
      "\tEpoch: 450/1000; 1.27 sec... \tStep: 109350... \tLoss: 1.510... \tVal Loss: 1.404 \tVal Frac: 0.1882\n",
      "\tEpoch: 455/1000; 1.13 sec... \tStep: 110565... \tLoss: 1.311... \tVal Loss: 1.182 \tVal Frac: 0.7033\n",
      "\tEpoch: 460/1000; 1.13 sec... \tStep: 111780... \tLoss: 1.100... \tVal Loss: 1.073 \tVal Frac: 0.7276\n",
      "\tEpoch: 465/1000; 1.19 sec... \tStep: 112995... \tLoss: 1.342... \tVal Loss: 1.205 \tVal Frac: 0.6838\n",
      "\tEpoch: 470/1000; 1.20 sec... \tStep: 114210... \tLoss: 1.282... \tVal Loss: 1.215 \tVal Frac: 0.7287\n",
      "\tEpoch: 475/1000; 1.23 sec... \tStep: 115425... \tLoss: 1.196... \tVal Loss: 1.184 \tVal Frac: 0.7143\n",
      "\tEpoch: 480/1000; 1.28 sec... \tStep: 116640... \tLoss: 1.625... \tVal Loss: 1.208 \tVal Frac: 0.7169\n",
      "\tEpoch: 485/1000; 1.12 sec... \tStep: 117855... \tLoss: 1.248... \tVal Loss: 1.176 \tVal Frac: 0.0864\n",
      "\tEpoch: 490/1000; 1.16 sec... \tStep: 119070... \tLoss: 1.372... \tVal Loss: 1.212 \tVal Frac: 0.0743\n",
      "\tEpoch: 495/1000; 1.13 sec... \tStep: 120285... \tLoss: 1.149... \tVal Loss: 1.185 \tVal Frac: 0.7434\n",
      "\tEpoch: 500/1000; 1.14 sec... \tStep: 121500... \tLoss: 1.352... \tVal Loss: 1.192 \tVal Frac: 0.7346\n",
      "\tEpoch: 505/1000; 1.09 sec... \tStep: 122715... \tLoss: 1.403... \tVal Loss: 1.172 \tVal Frac: 0.7393\n",
      "\tEpoch: 510/1000; 1.17 sec... \tStep: 123930... \tLoss: 1.123... \tVal Loss: 1.150 \tVal Frac: 0.6985\n",
      "\tEpoch: 515/1000; 1.04 sec... \tStep: 125145... \tLoss: 1.382... \tVal Loss: 1.154 \tVal Frac: 0.7316\n",
      "\tEpoch: 520/1000; 1.19 sec... \tStep: 126360... \tLoss: 1.331... \tVal Loss: 1.229 \tVal Frac: 0.6548\n",
      "\tEpoch: 525/1000; 1.13 sec... \tStep: 127575... \tLoss: 1.552... \tVal Loss: 1.218 \tVal Frac: 0.7257\n",
      "\tEpoch: 530/1000; 1.05 sec... \tStep: 128790... \tLoss: 0.990... \tVal Loss: 1.183 \tVal Frac: 0.4346\n",
      "\tEpoch: 535/1000; 1.23 sec... \tStep: 130005... \tLoss: 1.148... \tVal Loss: 1.186 \tVal Frac: 0.7162\n",
      "\tEpoch: 540/1000; 1.22 sec... \tStep: 131220... \tLoss: 1.026... \tVal Loss: 1.154 \tVal Frac: 0.6989\n",
      "\tEpoch: 545/1000; 1.20 sec... \tStep: 132435... \tLoss: 1.391... \tVal Loss: 1.172 \tVal Frac: 0.7055\n",
      "\tEpoch: 550/1000; 1.13 sec... \tStep: 133650... \tLoss: 1.223... \tVal Loss: 1.182 \tVal Frac: 0.6919\n",
      "\tEpoch: 555/1000; 1.23 sec... \tStep: 134865... \tLoss: 1.312... \tVal Loss: 1.189 \tVal Frac: 0.0640\n",
      "\tEpoch: 560/1000; 1.16 sec... \tStep: 136080... \tLoss: 1.267... \tVal Loss: 1.174 \tVal Frac: 0.6463\n",
      "\tEpoch: 565/1000; 1.20 sec... \tStep: 137295... \tLoss: 1.294... \tVal Loss: 1.134 \tVal Frac: 0.4588\n",
      "\tEpoch: 570/1000; 1.22 sec... \tStep: 138510... \tLoss: 1.398... \tVal Loss: 1.187 \tVal Frac: 0.7195\n",
      "\tEpoch: 575/1000; 1.17 sec... \tStep: 139725... \tLoss: 1.240... \tVal Loss: 1.162 \tVal Frac: 0.7213\n",
      "\tEpoch: 580/1000; 1.15 sec... \tStep: 140940... \tLoss: 1.571... \tVal Loss: 1.181 \tVal Frac: 0.6162\n",
      "\tEpoch: 585/1000; 1.13 sec... \tStep: 142155... \tLoss: 1.217... \tVal Loss: 1.147 \tVal Frac: 0.7059\n",
      "\tEpoch: 590/1000; 1.19 sec... \tStep: 143370... \tLoss: 1.325... \tVal Loss: 1.162 \tVal Frac: 0.7349\n",
      "\tEpoch: 595/1000; 1.19 sec... \tStep: 144585... \tLoss: 1.249... \tVal Loss: 1.171 \tVal Frac: 0.7485\n",
      "\tEpoch: 600/1000; 1.19 sec... \tStep: 145800... \tLoss: 1.116... \tVal Loss: 1.156 \tVal Frac: 0.0919\n",
      "\tEpoch: 605/1000; 1.09 sec... \tStep: 147015... \tLoss: 1.115... \tVal Loss: 1.157 \tVal Frac: 0.7342\n",
      "\tEpoch: 610/1000; 1.17 sec... \tStep: 148230... \tLoss: 1.270... \tVal Loss: 1.155 \tVal Frac: 0.7438\n",
      "\tEpoch: 615/1000; 1.17 sec... \tStep: 149445... \tLoss: 1.257... \tVal Loss: 1.194 \tVal Frac: 0.6324\n",
      "\tEpoch: 620/1000; 1.12 sec... \tStep: 150660... \tLoss: 1.142... \tVal Loss: 1.117 \tVal Frac: 0.7478\n",
      "\tEpoch: 625/1000; 1.22 sec... \tStep: 151875... \tLoss: 1.309... \tVal Loss: 1.121 \tVal Frac: 0.7540\n",
      "\tEpoch: 630/1000; 1.18 sec... \tStep: 153090... \tLoss: 1.384... \tVal Loss: 1.076 \tVal Frac: 0.7618\n",
      "\tEpoch: 635/1000; 1.11 sec... \tStep: 154305... \tLoss: 1.281... \tVal Loss: 1.184 \tVal Frac: 0.7559\n",
      "\tEpoch: 640/1000; 1.22 sec... \tStep: 155520... \tLoss: 1.041... \tVal Loss: 1.116 \tVal Frac: 0.7643\n",
      "\tEpoch: 645/1000; 1.20 sec... \tStep: 156735... \tLoss: 1.174... \tVal Loss: 1.121 \tVal Frac: 0.7640\n",
      "\tEpoch: 650/1000; 1.19 sec... \tStep: 157950... \tLoss: 1.420... \tVal Loss: 1.126 \tVal Frac: 0.6386\n",
      "\tEpoch: 655/1000; 1.26 sec... \tStep: 159165... \tLoss: 1.020... \tVal Loss: 1.144 \tVal Frac: 0.2371\n",
      "\tEpoch: 660/1000; 1.27 sec... \tStep: 160380... \tLoss: 1.326... \tVal Loss: 1.043 \tVal Frac: 0.7544\n",
      "\tEpoch: 665/1000; 1.25 sec... \tStep: 161595... \tLoss: 1.257... \tVal Loss: 1.238 \tVal Frac: 0.0989\n",
      "\tEpoch: 670/1000; 1.17 sec... \tStep: 162810... \tLoss: 1.048... \tVal Loss: 1.120 \tVal Frac: 0.7371\n",
      "\tEpoch: 675/1000; 1.10 sec... \tStep: 164025... \tLoss: 1.406... \tVal Loss: 1.044 \tVal Frac: 0.7540\n",
      "\tEpoch: 680/1000; 1.17 sec... \tStep: 165240... \tLoss: 1.473... \tVal Loss: 1.040 \tVal Frac: 0.7305\n",
      "\tEpoch: 685/1000; 1.18 sec... \tStep: 166455... \tLoss: 1.127... \tVal Loss: 1.058 \tVal Frac: 0.7478\n",
      "\tEpoch: 690/1000; 1.10 sec... \tStep: 167670... \tLoss: 1.421... \tVal Loss: 1.166 \tVal Frac: 0.3066\n",
      "\tEpoch: 695/1000; 1.23 sec... \tStep: 168885... \tLoss: 1.107... \tVal Loss: 0.993 \tVal Frac: 0.7643\n",
      "\tEpoch: 700/1000; 1.13 sec... \tStep: 170100... \tLoss: 1.121... \tVal Loss: 1.032 \tVal Frac: 0.7732\n",
      "\tEpoch: 705/1000; 1.24 sec... \tStep: 171315... \tLoss: 1.255... \tVal Loss: 1.081 \tVal Frac: 0.7559\n",
      "\tEpoch: 710/1000; 1.15 sec... \tStep: 172530... \tLoss: 1.194... \tVal Loss: 1.038 \tVal Frac: 0.7790\n",
      "\tEpoch: 715/1000; 1.16 sec... \tStep: 173745... \tLoss: 1.154... \tVal Loss: 1.029 \tVal Frac: 0.7544\n",
      "\tEpoch: 720/1000; 1.23 sec... \tStep: 174960... \tLoss: 1.107... \tVal Loss: 1.061 \tVal Frac: 0.7658\n",
      "\tEpoch: 725/1000; 1.24 sec... \tStep: 176175... \tLoss: 1.163... \tVal Loss: 1.082 \tVal Frac: 0.7496\n",
      "\tEpoch: 730/1000; 1.22 sec... \tStep: 177390... \tLoss: 1.199... \tVal Loss: 1.034 \tVal Frac: 0.7423\n",
      "\tEpoch: 735/1000; 1.19 sec... \tStep: 178605... \tLoss: 1.504... \tVal Loss: 1.128 \tVal Frac: 0.5224\n",
      "\tEpoch: 740/1000; 1.23 sec... \tStep: 179820... \tLoss: 1.280... \tVal Loss: 1.051 \tVal Frac: 0.7559\n",
      "\tEpoch: 745/1000; 1.18 sec... \tStep: 181035... \tLoss: 1.635... \tVal Loss: 1.118 \tVal Frac: 0.5224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 750/1000; 1.20 sec... \tStep: 182250... \tLoss: 1.007... \tVal Loss: 1.120 \tVal Frac: 0.5456\n",
      "\tEpoch: 755/1000; 1.16 sec... \tStep: 183465... \tLoss: 1.190... \tVal Loss: 1.116 \tVal Frac: 0.6077\n",
      "\tEpoch: 760/1000; 1.17 sec... \tStep: 184680... \tLoss: 1.110... \tVal Loss: 1.015 \tVal Frac: 0.7279\n",
      "\tEpoch: 765/1000; 1.23 sec... \tStep: 185895... \tLoss: 1.284... \tVal Loss: 1.162 \tVal Frac: 0.1276\n",
      "\tEpoch: 770/1000; 1.09 sec... \tStep: 187110... \tLoss: 1.236... \tVal Loss: 1.140 \tVal Frac: 0.4923\n",
      "\tEpoch: 775/1000; 1.30 sec... \tStep: 188325... \tLoss: 1.067... \tVal Loss: 1.015 \tVal Frac: 0.7493\n",
      "\tEpoch: 780/1000; 1.16 sec... \tStep: 189540... \tLoss: 1.194... \tVal Loss: 1.143 \tVal Frac: 0.5092\n",
      "\tEpoch: 785/1000; 1.27 sec... \tStep: 190755... \tLoss: 1.050... \tVal Loss: 1.062 \tVal Frac: 0.7588\n",
      "\tEpoch: 790/1000; 1.19 sec... \tStep: 191970... \tLoss: 1.077... \tVal Loss: 1.095 \tVal Frac: 0.7779\n",
      "\tEpoch: 795/1000; 1.20 sec... \tStep: 193185... \tLoss: 1.076... \tVal Loss: 1.033 \tVal Frac: 0.7438\n",
      "\tEpoch: 800/1000; 1.09 sec... \tStep: 194400... \tLoss: 0.907... \tVal Loss: 0.976 \tVal Frac: 0.7493\n",
      "\tEpoch: 805/1000; 1.23 sec... \tStep: 195615... \tLoss: 1.366... \tVal Loss: 1.196 \tVal Frac: 0.5978\n",
      "\tEpoch: 810/1000; 1.22 sec... \tStep: 196830... \tLoss: 1.079... \tVal Loss: 1.152 \tVal Frac: 0.7489\n",
      "\tEpoch: 815/1000; 1.22 sec... \tStep: 198045... \tLoss: 1.197... \tVal Loss: 1.156 \tVal Frac: 0.4846\n",
      "\tEpoch: 820/1000; 1.22 sec... \tStep: 199260... \tLoss: 1.159... \tVal Loss: 0.982 \tVal Frac: 0.7713\n",
      "\tEpoch: 825/1000; 1.17 sec... \tStep: 200475... \tLoss: 1.256... \tVal Loss: 1.146 \tVal Frac: 0.7680\n",
      "\tEpoch: 830/1000; 1.09 sec... \tStep: 201690... \tLoss: 1.230... \tVal Loss: 1.147 \tVal Frac: 0.7430\n",
      "\tEpoch: 835/1000; 1.27 sec... \tStep: 202905... \tLoss: 0.974... \tVal Loss: 1.026 \tVal Frac: 0.7415\n",
      "\tEpoch: 840/1000; 1.22 sec... \tStep: 204120... \tLoss: 1.206... \tVal Loss: 1.011 \tVal Frac: 0.7717\n",
      "\tEpoch: 845/1000; 1.16 sec... \tStep: 205335... \tLoss: 1.410... \tVal Loss: 1.128 \tVal Frac: 0.7710\n",
      "\tEpoch: 850/1000; 1.18 sec... \tStep: 206550... \tLoss: 1.197... \tVal Loss: 1.129 \tVal Frac: 0.1173\n",
      "\tEpoch: 855/1000; 1.15 sec... \tStep: 207765... \tLoss: 1.337... \tVal Loss: 1.124 \tVal Frac: 0.0974\n",
      "\tEpoch: 860/1000; 1.16 sec... \tStep: 208980... \tLoss: 1.065... \tVal Loss: 1.104 \tVal Frac: 0.7783\n",
      "\tEpoch: 865/1000; 1.20 sec... \tStep: 210195... \tLoss: 0.985... \tVal Loss: 1.141 \tVal Frac: 0.7625\n",
      "\tEpoch: 870/1000; 1.17 sec... \tStep: 211410... \tLoss: 1.101... \tVal Loss: 1.127 \tVal Frac: 0.7732\n",
      "\tEpoch: 875/1000; 1.20 sec... \tStep: 212625... \tLoss: 1.236... \tVal Loss: 1.154 \tVal Frac: 0.7739\n",
      "\tEpoch: 880/1000; 1.10 sec... \tStep: 213840... \tLoss: 1.084... \tVal Loss: 1.137 \tVal Frac: 0.3077\n",
      "\tEpoch: 885/1000; 1.18 sec... \tStep: 215055... \tLoss: 1.222... \tVal Loss: 1.150 \tVal Frac: 0.6088\n",
      "\tEpoch: 890/1000; 1.22 sec... \tStep: 216270... \tLoss: 1.225... \tVal Loss: 1.133 \tVal Frac: 0.7460\n",
      "\tEpoch: 895/1000; 1.14 sec... \tStep: 217485... \tLoss: 1.296... \tVal Loss: 1.125 \tVal Frac: 0.7485\n",
      "\tEpoch: 900/1000; 1.12 sec... \tStep: 218700... \tLoss: 1.193... \tVal Loss: 1.155 \tVal Frac: 0.5276\n",
      "\tEpoch: 905/1000; 1.10 sec... \tStep: 219915... \tLoss: 1.475... \tVal Loss: 1.119 \tVal Frac: 0.7121\n",
      "\tEpoch: 910/1000; 1.07 sec... \tStep: 221130... \tLoss: 1.096... \tVal Loss: 1.107 \tVal Frac: 0.7548\n",
      "\tEpoch: 915/1000; 1.17 sec... \tStep: 222345... \tLoss: 1.221... \tVal Loss: 1.110 \tVal Frac: 0.6982\n",
      "\tEpoch: 920/1000; 1.15 sec... \tStep: 223560... \tLoss: 1.021... \tVal Loss: 1.122 \tVal Frac: 0.7732\n",
      "\tEpoch: 925/1000; 1.16 sec... \tStep: 224775... \tLoss: 1.170... \tVal Loss: 1.110 \tVal Frac: 0.7669\n",
      "\tEpoch: 930/1000; 1.24 sec... \tStep: 225990... \tLoss: 1.148... \tVal Loss: 1.100 \tVal Frac: 0.7518\n",
      "\tEpoch: 935/1000; 1.19 sec... \tStep: 227205... \tLoss: 1.059... \tVal Loss: 1.096 \tVal Frac: 0.1261\n",
      "\tEpoch: 940/1000; 1.19 sec... \tStep: 228420... \tLoss: 1.414... \tVal Loss: 1.111 \tVal Frac: 0.4059\n",
      "\tEpoch: 945/1000; 1.20 sec... \tStep: 229635... \tLoss: 1.128... \tVal Loss: 1.130 \tVal Frac: 0.1033\n",
      "\tEpoch: 950/1000; 1.22 sec... \tStep: 230850... \tLoss: 1.216... \tVal Loss: 1.114 \tVal Frac: 0.1114\n",
      "\tEpoch: 955/1000; 1.15 sec... \tStep: 232065... \tLoss: 1.196... \tVal Loss: 1.139 \tVal Frac: 0.7728\n",
      "\tEpoch: 960/1000; 1.15 sec... \tStep: 233280... \tLoss: 1.171... \tVal Loss: 1.120 \tVal Frac: 0.7393\n",
      "\tEpoch: 965/1000; 1.20 sec... \tStep: 234495... \tLoss: 0.930... \tVal Loss: 1.159 \tVal Frac: 0.4342\n",
      "\tEpoch: 970/1000; 1.27 sec... \tStep: 235710... \tLoss: 1.073... \tVal Loss: 1.167 \tVal Frac: 0.0989\n",
      "\tEpoch: 975/1000; 1.15 sec... \tStep: 236925... \tLoss: 1.094... \tVal Loss: 1.085 \tVal Frac: 0.7632\n",
      "\tEpoch: 980/1000; 1.17 sec... \tStep: 238140... \tLoss: 1.252... \tVal Loss: 1.118 \tVal Frac: 0.7438\n",
      "\tEpoch: 985/1000; 1.15 sec... \tStep: 239355... \tLoss: 1.223... \tVal Loss: 1.101 \tVal Frac: 0.7651\n",
      "\tEpoch: 990/1000; 1.23 sec... \tStep: 240570... \tLoss: 0.992... \tVal Loss: 1.095 \tVal Frac: 0.7724\n",
      "\tEpoch: 995/1000; 1.17 sec... \tStep: 241785... \tLoss: 1.132... \tVal Loss: 1.118 \tVal Frac: 0.1210\n",
      "\tEpoch: 1000/1000; 1.26 sec... \tStep: 243000... \tLoss: 0.927... \tVal Loss: 1.103 \tVal Frac: 0.6930\n",
      "Completed:  n_windows :  20 \n",
      "\tloss: 0.954318 \n",
      "\tfrac:0.779044\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  50\n",
      "\tEpoch: 5/1000; 1.37 sec... \tStep: 1215... \tLoss: 0.931... \tVal Loss: 0.797 \tVal Frac: 0.7235\n",
      "\tValidation loss decreased (inf --> 0.797).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.37 sec... \tStep: 2430... \tLoss: 0.929... \tVal Loss: 0.705 \tVal Frac: 0.7790\n",
      "\tValidation loss decreased (0.797 --> 0.705).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.27 sec... \tStep: 3645... \tLoss: 0.865... \tVal Loss: 0.762 \tVal Frac: 0.7621\n",
      "\tEpoch: 20/1000; 1.40 sec... \tStep: 4860... \tLoss: 0.880... \tVal Loss: 0.788 \tVal Frac: 0.7798\n",
      "\tEpoch: 25/1000; 1.38 sec... \tStep: 6075... \tLoss: 0.775... \tVal Loss: 0.750 \tVal Frac: 0.7772\n",
      "\tEpoch: 30/1000; 1.44 sec... \tStep: 7290... \tLoss: 1.196... \tVal Loss: 0.746 \tVal Frac: 0.7724\n",
      "\tEpoch: 35/1000; 1.49 sec... \tStep: 8505... \tLoss: 0.673... \tVal Loss: 0.652 \tVal Frac: 0.7651\n",
      "\tValidation loss decreased (0.705 --> 0.652).  Saving model ...\n",
      "\tEpoch: 40/1000; 1.40 sec... \tStep: 9720... \tLoss: 0.685... \tVal Loss: 0.643 \tVal Frac: 0.7945\n",
      "\tValidation loss decreased (0.652 --> 0.643).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.47 sec... \tStep: 10935... \tLoss: 0.784... \tVal Loss: 0.617 \tVal Frac: 0.8000\n",
      "\tValidation loss decreased (0.643 --> 0.617).  Saving model ...\n",
      "\tEpoch: 50/1000; 1.34 sec... \tStep: 12150... \tLoss: 1.282... \tVal Loss: 0.666 \tVal Frac: 0.7824\n",
      "\tEpoch: 55/1000; 1.32 sec... \tStep: 13365... \tLoss: 0.928... \tVal Loss: 0.690 \tVal Frac: 0.7713\n",
      "\tEpoch: 60/1000; 1.36 sec... \tStep: 14580... \tLoss: 0.634... \tVal Loss: 0.657 \tVal Frac: 0.7809\n",
      "\tEpoch: 65/1000; 1.45 sec... \tStep: 15795... \tLoss: 0.939... \tVal Loss: 0.798 \tVal Frac: 0.7555\n",
      "\tEpoch: 70/1000; 1.46 sec... \tStep: 17010... \tLoss: 0.890... \tVal Loss: 0.750 \tVal Frac: 0.7551\n",
      "\tEpoch: 75/1000; 1.42 sec... \tStep: 18225... \tLoss: 1.017... \tVal Loss: 0.631 \tVal Frac: 0.8055\n",
      "\tEpoch: 80/1000; 1.39 sec... \tStep: 19440... \tLoss: 0.635... \tVal Loss: 0.703 \tVal Frac: 0.7864\n",
      "\tEpoch: 85/1000; 1.40 sec... \tStep: 20655... \tLoss: 1.003... \tVal Loss: 0.629 \tVal Frac: 0.7816\n",
      "\tEpoch: 90/1000; 1.37 sec... \tStep: 21870... \tLoss: 0.651... \tVal Loss: 0.619 \tVal Frac: 0.7739\n",
      "\tEpoch: 95/1000; 1.45 sec... \tStep: 23085... \tLoss: 0.769... \tVal Loss: 0.622 \tVal Frac: 0.8055\n",
      "\tEpoch: 100/1000; 1.32 sec... \tStep: 24300... \tLoss: 0.837... \tVal Loss: 0.666 \tVal Frac: 0.7919\n",
      "\tEpoch: 105/1000; 1.44 sec... \tStep: 25515... \tLoss: 0.551... \tVal Loss: 0.624 \tVal Frac: 0.7989\n",
      "\tEpoch: 110/1000; 1.47 sec... \tStep: 26730... \tLoss: 1.140... \tVal Loss: 0.678 \tVal Frac: 0.7897\n",
      "\tEpoch: 115/1000; 1.33 sec... \tStep: 27945... \tLoss: 0.811... \tVal Loss: 0.628 \tVal Frac: 0.8011\n",
      "\tEpoch: 120/1000; 1.35 sec... \tStep: 29160... \tLoss: 0.916... \tVal Loss: 0.659 \tVal Frac: 0.7776\n",
      "\tEpoch: 125/1000; 1.34 sec... \tStep: 30375... \tLoss: 1.253... \tVal Loss: 0.632 \tVal Frac: 0.7996\n",
      "\tEpoch: 130/1000; 1.35 sec... \tStep: 31590... \tLoss: 0.626... \tVal Loss: 0.621 \tVal Frac: 0.7985\n",
      "\tEpoch: 135/1000; 1.50 sec... \tStep: 32805... \tLoss: 0.691... \tVal Loss: 0.629 \tVal Frac: 0.7952\n",
      "\tEpoch: 140/1000; 1.44 sec... \tStep: 34020... \tLoss: 1.030... \tVal Loss: 0.633 \tVal Frac: 0.7960\n",
      "\tEpoch: 145/1000; 1.49 sec... \tStep: 35235... \tLoss: 0.492... \tVal Loss: 0.632 \tVal Frac: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 150/1000; 1.47 sec... \tStep: 36450... \tLoss: 0.975... \tVal Loss: 0.634 \tVal Frac: 0.8059\n",
      "\tEpoch: 155/1000; 1.36 sec... \tStep: 37665... \tLoss: 1.044... \tVal Loss: 0.600 \tVal Frac: 0.8004\n",
      "\tValidation loss decreased (0.617 --> 0.600).  Saving model ...\n",
      "\tEpoch: 160/1000; 1.34 sec... \tStep: 38880... \tLoss: 0.487... \tVal Loss: 0.612 \tVal Frac: 0.8051\n",
      "\tEpoch: 165/1000; 1.42 sec... \tStep: 40095... \tLoss: 0.880... \tVal Loss: 0.630 \tVal Frac: 0.8000\n",
      "\tEpoch: 170/1000; 1.47 sec... \tStep: 41310... \tLoss: 0.912... \tVal Loss: 0.598 \tVal Frac: 0.7864\n",
      "\tValidation loss decreased (0.600 --> 0.598).  Saving model ...\n",
      "\tEpoch: 175/1000; 1.42 sec... \tStep: 42525... \tLoss: 0.869... \tVal Loss: 0.629 \tVal Frac: 0.7901\n",
      "\tEpoch: 180/1000; 1.36 sec... \tStep: 43740... \tLoss: 0.668... \tVal Loss: 0.631 \tVal Frac: 0.7978\n",
      "\tEpoch: 185/1000; 1.36 sec... \tStep: 44955... \tLoss: 0.612... \tVal Loss: 0.630 \tVal Frac: 0.7937\n",
      "\tEpoch: 190/1000; 1.40 sec... \tStep: 46170... \tLoss: 0.541... \tVal Loss: 0.587 \tVal Frac: 0.8162\n",
      "\tValidation loss decreased (0.598 --> 0.587).  Saving model ...\n",
      "\tEpoch: 195/1000; 1.37 sec... \tStep: 47385... \tLoss: 0.785... \tVal Loss: 0.636 \tVal Frac: 0.8029\n",
      "\tEpoch: 200/1000; 1.39 sec... \tStep: 48600... \tLoss: 0.841... \tVal Loss: 0.659 \tVal Frac: 0.7886\n",
      "\tEpoch: 205/1000; 1.37 sec... \tStep: 49815... \tLoss: 0.681... \tVal Loss: 0.605 \tVal Frac: 0.8033\n",
      "\tEpoch: 210/1000; 1.36 sec... \tStep: 51030... \tLoss: 0.724... \tVal Loss: 0.663 \tVal Frac: 0.7831\n",
      "\tEpoch: 215/1000; 1.32 sec... \tStep: 52245... \tLoss: 1.183... \tVal Loss: 0.631 \tVal Frac: 0.7835\n",
      "\tEpoch: 220/1000; 1.60 sec... \tStep: 53460... \tLoss: 0.458... \tVal Loss: 0.633 \tVal Frac: 0.7908\n",
      "\tEpoch: 225/1000; 1.37 sec... \tStep: 54675... \tLoss: 0.962... \tVal Loss: 0.604 \tVal Frac: 0.8055\n",
      "\tEpoch: 230/1000; 1.36 sec... \tStep: 55890... \tLoss: 1.321... \tVal Loss: 0.677 \tVal Frac: 0.7890\n",
      "\tEpoch: 235/1000; 1.45 sec... \tStep: 57105... \tLoss: 0.897... \tVal Loss: 0.613 \tVal Frac: 0.7967\n",
      "\tEpoch: 240/1000; 1.40 sec... \tStep: 58320... \tLoss: 0.817... \tVal Loss: 0.621 \tVal Frac: 0.8022\n",
      "\tEpoch: 245/1000; 1.34 sec... \tStep: 59535... \tLoss: 0.761... \tVal Loss: 0.614 \tVal Frac: 0.8051\n",
      "\tEpoch: 250/1000; 1.32 sec... \tStep: 60750... \tLoss: 0.628... \tVal Loss: 0.598 \tVal Frac: 0.7956\n",
      "\tEpoch: 255/1000; 1.27 sec... \tStep: 61965... \tLoss: 0.807... \tVal Loss: 0.589 \tVal Frac: 0.8055\n",
      "\tEpoch: 260/1000; 1.42 sec... \tStep: 63180... \tLoss: 0.718... \tVal Loss: 0.618 \tVal Frac: 0.7882\n",
      "\tEpoch: 265/1000; 1.30 sec... \tStep: 64395... \tLoss: 0.562... \tVal Loss: 0.610 \tVal Frac: 0.7779\n",
      "\tEpoch: 270/1000; 1.42 sec... \tStep: 65610... \tLoss: 1.130... \tVal Loss: 0.606 \tVal Frac: 0.7982\n",
      "\tEpoch: 275/1000; 1.50 sec... \tStep: 66825... \tLoss: 0.947... \tVal Loss: 0.619 \tVal Frac: 0.7945\n",
      "\tEpoch: 280/1000; 1.44 sec... \tStep: 68040... \tLoss: 1.101... \tVal Loss: 0.619 \tVal Frac: 0.7967\n",
      "\tEpoch: 285/1000; 1.32 sec... \tStep: 69255... \tLoss: 0.634... \tVal Loss: 0.614 \tVal Frac: 0.7960\n",
      "\tEpoch: 290/1000; 1.34 sec... \tStep: 70470... \tLoss: 0.926... \tVal Loss: 0.656 \tVal Frac: 0.7949\n",
      "\tEpoch: 295/1000; 1.40 sec... \tStep: 71685... \tLoss: 0.511... \tVal Loss: 0.621 \tVal Frac: 0.7893\n",
      "\tEpoch: 300/1000; 1.50 sec... \tStep: 72900... \tLoss: 0.676... \tVal Loss: 0.645 \tVal Frac: 0.7776\n",
      "\tEpoch: 305/1000; 1.32 sec... \tStep: 74115... \tLoss: 0.942... \tVal Loss: 0.647 \tVal Frac: 0.7952\n",
      "\tEpoch: 310/1000; 1.49 sec... \tStep: 75330... \tLoss: 0.698... \tVal Loss: 0.655 \tVal Frac: 0.7728\n",
      "\tEpoch: 315/1000; 1.45 sec... \tStep: 76545... \tLoss: 0.918... \tVal Loss: 0.632 \tVal Frac: 0.7860\n",
      "\tEpoch: 320/1000; 1.42 sec... \tStep: 77760... \tLoss: 0.534... \tVal Loss: 0.578 \tVal Frac: 0.8132\n",
      "\tValidation loss decreased (0.587 --> 0.578).  Saving model ...\n",
      "\tEpoch: 325/1000; 1.32 sec... \tStep: 78975... \tLoss: 1.068... \tVal Loss: 0.600 \tVal Frac: 0.8099\n",
      "\tEpoch: 330/1000; 1.47 sec... \tStep: 80190... \tLoss: 0.585... \tVal Loss: 0.621 \tVal Frac: 0.7915\n",
      "\tEpoch: 335/1000; 1.42 sec... \tStep: 81405... \tLoss: 0.558... \tVal Loss: 0.606 \tVal Frac: 0.8044\n",
      "\tEpoch: 340/1000; 1.50 sec... \tStep: 82620... \tLoss: 0.769... \tVal Loss: 0.628 \tVal Frac: 0.7897\n",
      "\tEpoch: 345/1000; 1.45 sec... \tStep: 83835... \tLoss: 0.910... \tVal Loss: 0.603 \tVal Frac: 0.8055\n",
      "\tEpoch: 350/1000; 1.37 sec... \tStep: 85050... \tLoss: 0.253... \tVal Loss: 0.588 \tVal Frac: 0.7919\n",
      "\tEpoch: 355/1000; 1.27 sec... \tStep: 86265... \tLoss: 0.683... \tVal Loss: 0.591 \tVal Frac: 0.8015\n",
      "\tEpoch: 360/1000; 1.46 sec... \tStep: 87480... \tLoss: 1.112... \tVal Loss: 0.581 \tVal Frac: 0.8118\n",
      "\tEpoch: 365/1000; 1.27 sec... \tStep: 88695... \tLoss: 0.865... \tVal Loss: 0.594 \tVal Frac: 0.7963\n",
      "\tEpoch: 370/1000; 1.40 sec... \tStep: 89910... \tLoss: 0.638... \tVal Loss: 0.648 \tVal Frac: 0.7941\n",
      "\tEpoch: 375/1000; 1.39 sec... \tStep: 91125... \tLoss: 1.115... \tVal Loss: 0.591 \tVal Frac: 0.8125\n",
      "\tEpoch: 380/1000; 1.31 sec... \tStep: 92340... \tLoss: 0.891... \tVal Loss: 0.626 \tVal Frac: 0.7871\n",
      "\tEpoch: 385/1000; 1.45 sec... \tStep: 93555... \tLoss: 0.842... \tVal Loss: 0.630 \tVal Frac: 0.8044\n",
      "\tEpoch: 390/1000; 1.37 sec... \tStep: 94770... \tLoss: 0.773... \tVal Loss: 0.615 \tVal Frac: 0.8048\n",
      "\tEpoch: 395/1000; 1.36 sec... \tStep: 95985... \tLoss: 0.494... \tVal Loss: 0.601 \tVal Frac: 0.8092\n",
      "\tEpoch: 400/1000; 1.31 sec... \tStep: 97200... \tLoss: 0.691... \tVal Loss: 0.593 \tVal Frac: 0.8081\n",
      "\tEpoch: 405/1000; 1.46 sec... \tStep: 98415... \tLoss: 0.560... \tVal Loss: 0.579 \tVal Frac: 0.8085\n",
      "\tEpoch: 410/1000; 1.35 sec... \tStep: 99630... \tLoss: 0.567... \tVal Loss: 0.613 \tVal Frac: 0.7835\n",
      "\tEpoch: 415/1000; 1.30 sec... \tStep: 100845... \tLoss: 0.648... \tVal Loss: 0.631 \tVal Frac: 0.7849\n",
      "\tEpoch: 420/1000; 1.49 sec... \tStep: 102060... \tLoss: 0.938... \tVal Loss: 0.603 \tVal Frac: 0.7963\n",
      "\tEpoch: 425/1000; 1.44 sec... \tStep: 103275... \tLoss: 0.665... \tVal Loss: 0.610 \tVal Frac: 0.8011\n",
      "\tEpoch: 430/1000; 1.43 sec... \tStep: 104490... \tLoss: 1.090... \tVal Loss: 0.615 \tVal Frac: 0.8000\n",
      "\tEpoch: 435/1000; 1.36 sec... \tStep: 105705... \tLoss: 0.979... \tVal Loss: 0.617 \tVal Frac: 0.7989\n",
      "\tEpoch: 440/1000; 1.36 sec... \tStep: 106920... \tLoss: 0.896... \tVal Loss: 0.605 \tVal Frac: 0.7930\n",
      "\tEpoch: 445/1000; 1.42 sec... \tStep: 108135... \tLoss: 0.644... \tVal Loss: 0.586 \tVal Frac: 0.7937\n",
      "\tEpoch: 450/1000; 1.47 sec... \tStep: 109350... \tLoss: 0.813... \tVal Loss: 0.625 \tVal Frac: 0.7816\n",
      "\tEpoch: 455/1000; 1.47 sec... \tStep: 110565... \tLoss: 0.765... \tVal Loss: 0.610 \tVal Frac: 0.7956\n",
      "\tEpoch: 460/1000; 1.32 sec... \tStep: 111780... \tLoss: 0.937... \tVal Loss: 0.609 \tVal Frac: 0.7989\n",
      "\tEpoch: 465/1000; 1.44 sec... \tStep: 112995... \tLoss: 0.906... \tVal Loss: 0.607 \tVal Frac: 0.8070\n",
      "\tEpoch: 470/1000; 1.47 sec... \tStep: 114210... \tLoss: 0.370... \tVal Loss: 0.622 \tVal Frac: 0.7934\n",
      "\tEpoch: 475/1000; 1.44 sec... \tStep: 115425... \tLoss: 0.455... \tVal Loss: 0.596 \tVal Frac: 0.8154\n",
      "\tEpoch: 480/1000; 1.41 sec... \tStep: 116640... \tLoss: 0.494... \tVal Loss: 0.598 \tVal Frac: 0.8110\n",
      "\tEpoch: 485/1000; 1.53 sec... \tStep: 117855... \tLoss: 0.678... \tVal Loss: 0.615 \tVal Frac: 0.7963\n",
      "\tEpoch: 490/1000; 1.38 sec... \tStep: 119070... \tLoss: 0.440... \tVal Loss: 0.593 \tVal Frac: 0.7934\n",
      "\tEpoch: 495/1000; 1.30 sec... \tStep: 120285... \tLoss: 1.017... \tVal Loss: 0.617 \tVal Frac: 0.7949\n",
      "\tEpoch: 500/1000; 1.46 sec... \tStep: 121500... \tLoss: 0.649... \tVal Loss: 0.602 \tVal Frac: 0.7827\n",
      "\tEpoch: 505/1000; 1.34 sec... \tStep: 122715... \tLoss: 0.843... \tVal Loss: 0.642 \tVal Frac: 0.7794\n",
      "\tEpoch: 510/1000; 1.45 sec... \tStep: 123930... \tLoss: 0.828... \tVal Loss: 0.622 \tVal Frac: 0.7827\n",
      "\tEpoch: 515/1000; 1.35 sec... \tStep: 125145... \tLoss: 0.607... \tVal Loss: 0.646 \tVal Frac: 0.7746\n",
      "\tEpoch: 520/1000; 1.46 sec... \tStep: 126360... \tLoss: 0.813... \tVal Loss: 0.610 \tVal Frac: 0.7846\n",
      "\tEpoch: 525/1000; 1.27 sec... \tStep: 127575... \tLoss: 0.518... \tVal Loss: 0.586 \tVal Frac: 0.8026\n",
      "\tEpoch: 530/1000; 1.39 sec... \tStep: 128790... \tLoss: 0.850... \tVal Loss: 0.612 \tVal Frac: 0.7934\n",
      "\tEpoch: 535/1000; 1.45 sec... \tStep: 130005... \tLoss: 0.902... \tVal Loss: 0.642 \tVal Frac: 0.7952\n",
      "\tEpoch: 540/1000; 1.34 sec... \tStep: 131220... \tLoss: 0.678... \tVal Loss: 0.607 \tVal Frac: 0.7816\n",
      "\tEpoch: 545/1000; 1.34 sec... \tStep: 132435... \tLoss: 0.875... \tVal Loss: 0.588 \tVal Frac: 0.7993\n",
      "\tEpoch: 550/1000; 1.39 sec... \tStep: 133650... \tLoss: 0.748... \tVal Loss: 0.573 \tVal Frac: 0.8099\n",
      "\tValidation loss decreased (0.578 --> 0.573).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 555/1000; 1.35 sec... \tStep: 134865... \tLoss: 0.948... \tVal Loss: 0.585 \tVal Frac: 0.8114\n",
      "\tEpoch: 560/1000; 1.42 sec... \tStep: 136080... \tLoss: 0.691... \tVal Loss: 0.589 \tVal Frac: 0.7978\n",
      "\tEpoch: 565/1000; 1.32 sec... \tStep: 137295... \tLoss: 0.593... \tVal Loss: 0.584 \tVal Frac: 0.8096\n",
      "\tEpoch: 570/1000; 1.54 sec... \tStep: 138510... \tLoss: 1.006... \tVal Loss: 0.617 \tVal Frac: 0.7926\n",
      "\tEpoch: 575/1000; 1.40 sec... \tStep: 139725... \tLoss: 0.812... \tVal Loss: 0.616 \tVal Frac: 0.7941\n",
      "\tEpoch: 580/1000; 1.44 sec... \tStep: 140940... \tLoss: 0.541... \tVal Loss: 0.624 \tVal Frac: 0.8029\n",
      "\tEpoch: 585/1000; 1.39 sec... \tStep: 142155... \tLoss: 0.642... \tVal Loss: 0.586 \tVal Frac: 0.7949\n",
      "\tEpoch: 590/1000; 1.47 sec... \tStep: 143370... \tLoss: 0.896... \tVal Loss: 0.612 \tVal Frac: 0.7912\n",
      "\tEpoch: 595/1000; 1.36 sec... \tStep: 144585... \tLoss: 0.583... \tVal Loss: 0.586 \tVal Frac: 0.8081\n",
      "\tEpoch: 600/1000; 1.37 sec... \tStep: 145800... \tLoss: 1.224... \tVal Loss: 0.620 \tVal Frac: 0.7993\n",
      "\tEpoch: 605/1000; 1.29 sec... \tStep: 147015... \tLoss: 0.770... \tVal Loss: 0.608 \tVal Frac: 0.7908\n",
      "\tEpoch: 610/1000; 1.37 sec... \tStep: 148230... \tLoss: 0.973... \tVal Loss: 0.611 \tVal Frac: 0.7904\n",
      "\tEpoch: 615/1000; 1.39 sec... \tStep: 149445... \tLoss: 0.737... \tVal Loss: 0.616 \tVal Frac: 0.7904\n",
      "\tEpoch: 620/1000; 1.42 sec... \tStep: 150660... \tLoss: 0.557... \tVal Loss: 0.572 \tVal Frac: 0.7996\n",
      "\tValidation loss decreased (0.573 --> 0.572).  Saving model ...\n",
      "\tEpoch: 625/1000; 1.47 sec... \tStep: 151875... \tLoss: 0.372... \tVal Loss: 0.594 \tVal Frac: 0.8033\n",
      "\tEpoch: 630/1000; 1.35 sec... \tStep: 153090... \tLoss: 0.809... \tVal Loss: 0.608 \tVal Frac: 0.8004\n",
      "\tEpoch: 635/1000; 1.35 sec... \tStep: 154305... \tLoss: 0.664... \tVal Loss: 0.608 \tVal Frac: 0.7732\n",
      "\tEpoch: 640/1000; 1.42 sec... \tStep: 155520... \tLoss: 0.667... \tVal Loss: 0.615 \tVal Frac: 0.7761\n",
      "\tEpoch: 645/1000; 1.42 sec... \tStep: 156735... \tLoss: 0.853... \tVal Loss: 0.625 \tVal Frac: 0.7798\n",
      "\tEpoch: 650/1000; 1.47 sec... \tStep: 157950... \tLoss: 0.334... \tVal Loss: 0.606 \tVal Frac: 0.7930\n",
      "\tEpoch: 655/1000; 1.46 sec... \tStep: 159165... \tLoss: 0.725... \tVal Loss: 0.578 \tVal Frac: 0.8140\n",
      "\tEpoch: 660/1000; 1.46 sec... \tStep: 160380... \tLoss: 0.655... \tVal Loss: 0.590 \tVal Frac: 0.8037\n",
      "\tEpoch: 665/1000; 1.37 sec... \tStep: 161595... \tLoss: 0.779... \tVal Loss: 0.606 \tVal Frac: 0.7930\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 0.637... \tVal Loss: 0.619 \tVal Frac: 0.7908\n",
      "\tEpoch: 675/1000; 1.44 sec... \tStep: 164025... \tLoss: 0.547... \tVal Loss: 0.597 \tVal Frac: 0.8007\n",
      "\tEpoch: 680/1000; 1.38 sec... \tStep: 165240... \tLoss: 0.765... \tVal Loss: 0.582 \tVal Frac: 0.7989\n",
      "\tEpoch: 685/1000; 1.46 sec... \tStep: 166455... \tLoss: 0.415... \tVal Loss: 0.610 \tVal Frac: 0.7919\n",
      "\tEpoch: 690/1000; 1.46 sec... \tStep: 167670... \tLoss: 0.639... \tVal Loss: 0.611 \tVal Frac: 0.7919\n",
      "\tEpoch: 695/1000; 1.32 sec... \tStep: 168885... \tLoss: 0.760... \tVal Loss: 0.603 \tVal Frac: 0.7963\n",
      "\tEpoch: 700/1000; 1.37 sec... \tStep: 170100... \tLoss: 0.829... \tVal Loss: 0.609 \tVal Frac: 0.7879\n",
      "\tEpoch: 705/1000; 1.34 sec... \tStep: 171315... \tLoss: 0.737... \tVal Loss: 0.592 \tVal Frac: 0.7937\n",
      "\tEpoch: 710/1000; 1.40 sec... \tStep: 172530... \tLoss: 0.841... \tVal Loss: 0.594 \tVal Frac: 0.7794\n",
      "\tEpoch: 715/1000; 1.52 sec... \tStep: 173745... \tLoss: 0.680... \tVal Loss: 0.604 \tVal Frac: 0.7835\n",
      "\tEpoch: 720/1000; 1.40 sec... \tStep: 174960... \tLoss: 0.721... \tVal Loss: 0.589 \tVal Frac: 0.7996\n",
      "\tEpoch: 725/1000; 1.42 sec... \tStep: 176175... \tLoss: 0.719... \tVal Loss: 0.586 \tVal Frac: 0.7945\n",
      "\tEpoch: 730/1000; 1.36 sec... \tStep: 177390... \tLoss: 0.825... \tVal Loss: 0.608 \tVal Frac: 0.7820\n",
      "\tEpoch: 735/1000; 1.54 sec... \tStep: 178605... \tLoss: 0.603... \tVal Loss: 0.609 \tVal Frac: 0.7912\n",
      "\tEpoch: 740/1000; 1.49 sec... \tStep: 179820... \tLoss: 0.776... \tVal Loss: 0.604 \tVal Frac: 0.7831\n",
      "\tEpoch: 745/1000; 1.40 sec... \tStep: 181035... \tLoss: 0.866... \tVal Loss: 0.607 \tVal Frac: 0.7890\n",
      "\tEpoch: 750/1000; 1.47 sec... \tStep: 182250... \tLoss: 0.718... \tVal Loss: 0.581 \tVal Frac: 0.8107\n",
      "\tEpoch: 755/1000; 1.42 sec... \tStep: 183465... \tLoss: 0.859... \tVal Loss: 0.599 \tVal Frac: 0.7926\n",
      "\tEpoch: 760/1000; 1.52 sec... \tStep: 184680... \tLoss: 0.767... \tVal Loss: 0.603 \tVal Frac: 0.7827\n",
      "\tEpoch: 765/1000; 1.37 sec... \tStep: 185895... \tLoss: 0.459... \tVal Loss: 0.611 \tVal Frac: 0.7864\n",
      "\tEpoch: 770/1000; 1.44 sec... \tStep: 187110... \tLoss: 0.576... \tVal Loss: 0.604 \tVal Frac: 0.8015\n",
      "\tEpoch: 775/1000; 1.48 sec... \tStep: 188325... \tLoss: 0.762... \tVal Loss: 0.609 \tVal Frac: 0.7912\n",
      "\tEpoch: 780/1000; 1.40 sec... \tStep: 189540... \tLoss: 0.669... \tVal Loss: 0.580 \tVal Frac: 0.8099\n",
      "\tEpoch: 785/1000; 1.40 sec... \tStep: 190755... \tLoss: 0.742... \tVal Loss: 0.593 \tVal Frac: 0.8070\n",
      "\tEpoch: 790/1000; 1.47 sec... \tStep: 191970... \tLoss: 0.914... \tVal Loss: 0.612 \tVal Frac: 0.7937\n",
      "\tEpoch: 795/1000; 1.49 sec... \tStep: 193185... \tLoss: 0.641... \tVal Loss: 0.586 \tVal Frac: 0.8121\n",
      "\tEpoch: 800/1000; 1.44 sec... \tStep: 194400... \tLoss: 0.715... \tVal Loss: 0.593 \tVal Frac: 0.7985\n",
      "\tEpoch: 805/1000; 1.44 sec... \tStep: 195615... \tLoss: 0.400... \tVal Loss: 0.583 \tVal Frac: 0.8074\n",
      "\tEpoch: 810/1000; 1.45 sec... \tStep: 196830... \tLoss: 0.678... \tVal Loss: 0.597 \tVal Frac: 0.8103\n",
      "\tEpoch: 815/1000; 1.30 sec... \tStep: 198045... \tLoss: 0.524... \tVal Loss: 0.627 \tVal Frac: 0.7875\n",
      "\tEpoch: 820/1000; 1.40 sec... \tStep: 199260... \tLoss: 0.803... \tVal Loss: 0.571 \tVal Frac: 0.8110\n",
      "\tValidation loss decreased (0.572 --> 0.571).  Saving model ...\n",
      "\tEpoch: 825/1000; 1.36 sec... \tStep: 200475... \tLoss: 1.021... \tVal Loss: 0.581 \tVal Frac: 0.7989\n",
      "\tEpoch: 830/1000; 1.48 sec... \tStep: 201690... \tLoss: 0.807... \tVal Loss: 0.583 \tVal Frac: 0.7993\n",
      "\tEpoch: 835/1000; 1.37 sec... \tStep: 202905... \tLoss: 0.545... \tVal Loss: 0.608 \tVal Frac: 0.8007\n",
      "\tEpoch: 840/1000; 1.42 sec... \tStep: 204120... \tLoss: 0.535... \tVal Loss: 0.582 \tVal Frac: 0.8048\n",
      "\tEpoch: 845/1000; 1.44 sec... \tStep: 205335... \tLoss: 0.916... \tVal Loss: 0.584 \tVal Frac: 0.8114\n",
      "\tEpoch: 850/1000; 1.35 sec... \tStep: 206550... \tLoss: 0.651... \tVal Loss: 0.640 \tVal Frac: 0.7971\n",
      "\tEpoch: 855/1000; 1.47 sec... \tStep: 207765... \tLoss: 0.686... \tVal Loss: 0.587 \tVal Frac: 0.8132\n",
      "\tEpoch: 860/1000; 1.52 sec... \tStep: 208980... \tLoss: 1.198... \tVal Loss: 0.600 \tVal Frac: 0.8044\n",
      "\tEpoch: 865/1000; 1.50 sec... \tStep: 210195... \tLoss: 0.440... \tVal Loss: 0.600 \tVal Frac: 0.7982\n",
      "\tEpoch: 870/1000; 1.44 sec... \tStep: 211410... \tLoss: 0.579... \tVal Loss: 0.607 \tVal Frac: 0.7926\n",
      "\tEpoch: 875/1000; 1.36 sec... \tStep: 212625... \tLoss: 0.796... \tVal Loss: 0.614 \tVal Frac: 0.7963\n",
      "\tEpoch: 880/1000; 1.38 sec... \tStep: 213840... \tLoss: 0.538... \tVal Loss: 0.590 \tVal Frac: 0.8026\n",
      "\tEpoch: 885/1000; 1.39 sec... \tStep: 215055... \tLoss: 0.582... \tVal Loss: 0.602 \tVal Frac: 0.7956\n",
      "\tEpoch: 890/1000; 1.42 sec... \tStep: 216270... \tLoss: 0.668... \tVal Loss: 0.591 \tVal Frac: 0.8004\n",
      "\tEpoch: 895/1000; 1.47 sec... \tStep: 217485... \tLoss: 0.499... \tVal Loss: 0.607 \tVal Frac: 0.7812\n",
      "\tEpoch: 900/1000; 1.29 sec... \tStep: 218700... \tLoss: 0.729... \tVal Loss: 0.613 \tVal Frac: 0.7860\n",
      "\tEpoch: 905/1000; 1.35 sec... \tStep: 219915... \tLoss: 0.392... \tVal Loss: 0.577 \tVal Frac: 0.7996\n",
      "\tEpoch: 910/1000; 1.39 sec... \tStep: 221130... \tLoss: 0.640... \tVal Loss: 0.596 \tVal Frac: 0.8132\n",
      "\tEpoch: 915/1000; 1.34 sec... \tStep: 222345... \tLoss: 0.621... \tVal Loss: 0.604 \tVal Frac: 0.7982\n",
      "\tEpoch: 920/1000; 1.27 sec... \tStep: 223560... \tLoss: 0.417... \tVal Loss: 0.577 \tVal Frac: 0.7989\n",
      "\tEpoch: 925/1000; 1.35 sec... \tStep: 224775... \tLoss: 0.991... \tVal Loss: 0.587 \tVal Frac: 0.8029\n",
      "\tEpoch: 930/1000; 1.45 sec... \tStep: 225990... \tLoss: 0.459... \tVal Loss: 0.601 \tVal Frac: 0.7919\n",
      "\tEpoch: 935/1000; 1.32 sec... \tStep: 227205... \tLoss: 0.560... \tVal Loss: 0.596 \tVal Frac: 0.8033\n",
      "\tEpoch: 940/1000; 1.37 sec... \tStep: 228420... \tLoss: 0.651... \tVal Loss: 0.581 \tVal Frac: 0.8048\n",
      "\tEpoch: 945/1000; 1.34 sec... \tStep: 229635... \tLoss: 0.479... \tVal Loss: 0.575 \tVal Frac: 0.8147\n",
      "\tEpoch: 950/1000; 1.34 sec... \tStep: 230850... \tLoss: 0.811... \tVal Loss: 0.578 \tVal Frac: 0.8000\n",
      "\tEpoch: 955/1000; 1.42 sec... \tStep: 232065... \tLoss: 0.640... \tVal Loss: 0.594 \tVal Frac: 0.8048\n",
      "\tEpoch: 960/1000; 1.36 sec... \tStep: 233280... \tLoss: 0.887... \tVal Loss: 0.576 \tVal Frac: 0.8077\n",
      "\tEpoch: 965/1000; 1.37 sec... \tStep: 234495... \tLoss: 0.760... \tVal Loss: 0.573 \tVal Frac: 0.8011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 970/1000; 1.38 sec... \tStep: 235710... \tLoss: 0.672... \tVal Loss: 0.581 \tVal Frac: 0.8114\n",
      "\tEpoch: 975/1000; 1.36 sec... \tStep: 236925... \tLoss: 0.512... \tVal Loss: 0.602 \tVal Frac: 0.8026\n",
      "\tEpoch: 980/1000; 1.34 sec... \tStep: 238140... \tLoss: 0.921... \tVal Loss: 0.582 \tVal Frac: 0.8096\n",
      "\tEpoch: 985/1000; 1.32 sec... \tStep: 239355... \tLoss: 0.928... \tVal Loss: 0.596 \tVal Frac: 0.8096\n",
      "\tEpoch: 990/1000; 1.40 sec... \tStep: 240570... \tLoss: 0.737... \tVal Loss: 0.596 \tVal Frac: 0.8029\n",
      "\tEpoch: 995/1000; 1.39 sec... \tStep: 241785... \tLoss: 0.601... \tVal Loss: 0.585 \tVal Frac: 0.8022\n",
      "\tEpoch: 1000/1000; 1.30 sec... \tStep: 243000... \tLoss: 0.599... \tVal Loss: 0.577 \tVal Frac: 0.8110\n",
      "Completed:  n_windows :  50 \n",
      "\tloss: 0.571452 \n",
      "\tfrac:0.816176\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  100\n",
      "\tEpoch: 5/1000; 1.57 sec... \tStep: 1210... \tLoss: 1.522... \tVal Loss: 1.323 \tVal Frac: 0.0763\n",
      "\tValidation loss decreased (inf --> 1.323).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.78 sec... \tStep: 2420... \tLoss: 1.389... \tVal Loss: 1.289 \tVal Frac: 0.0510\n",
      "\tValidation loss decreased (1.323 --> 1.289).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.67 sec... \tStep: 3630... \tLoss: 1.501... \tVal Loss: 1.280 \tVal Frac: 0.0536\n",
      "\tValidation loss decreased (1.289 --> 1.280).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.70 sec... \tStep: 4840... \tLoss: 1.646... \tVal Loss: 1.316 \tVal Frac: 0.0432\n",
      "\tEpoch: 25/1000; 1.65 sec... \tStep: 6050... \tLoss: 1.312... \tVal Loss: 1.256 \tVal Frac: 0.0625\n",
      "\tValidation loss decreased (1.280 --> 1.256).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.71 sec... \tStep: 7260... \tLoss: 1.445... \tVal Loss: 1.285 \tVal Frac: 0.0711\n",
      "\tEpoch: 35/1000; 1.64 sec... \tStep: 8470... \tLoss: 1.299... \tVal Loss: 1.277 \tVal Frac: 0.0778\n",
      "\tEpoch: 40/1000; 1.60 sec... \tStep: 9680... \tLoss: 1.352... \tVal Loss: 1.196 \tVal Frac: 0.0565\n",
      "\tValidation loss decreased (1.256 --> 1.196).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.64 sec... \tStep: 10890... \tLoss: 1.206... \tVal Loss: 1.197 \tVal Frac: 0.0580\n",
      "\tEpoch: 50/1000; 1.66 sec... \tStep: 12100... \tLoss: 1.304... \tVal Loss: 1.217 \tVal Frac: 0.0837\n",
      "\tEpoch: 55/1000; 1.72 sec... \tStep: 13310... \tLoss: 1.379... \tVal Loss: 1.216 \tVal Frac: 0.0711\n",
      "\tEpoch: 60/1000; 1.76 sec... \tStep: 14520... \tLoss: 1.319... \tVal Loss: 1.256 \tVal Frac: 0.0528\n",
      "\tEpoch: 65/1000; 1.71 sec... \tStep: 15730... \tLoss: 1.431... \tVal Loss: 1.226 \tVal Frac: 0.0547\n",
      "\tEpoch: 70/1000; 1.71 sec... \tStep: 16940... \tLoss: 1.585... \tVal Loss: 1.281 \tVal Frac: 0.0818\n",
      "\tEpoch: 75/1000; 1.74 sec... \tStep: 18150... \tLoss: 1.665... \tVal Loss: 1.224 \tVal Frac: 0.1019\n",
      "\tEpoch: 80/1000; 1.74 sec... \tStep: 19360... \tLoss: 1.519... \tVal Loss: 1.216 \tVal Frac: 0.0673\n",
      "\tEpoch: 85/1000; 1.87 sec... \tStep: 20570... \tLoss: 1.558... \tVal Loss: 1.221 \tVal Frac: 0.0480\n",
      "\tEpoch: 90/1000; 1.83 sec... \tStep: 21780... \tLoss: 1.344... \tVal Loss: 1.241 \tVal Frac: 0.0461\n",
      "\tEpoch: 95/1000; 1.57 sec... \tStep: 22990... \tLoss: 1.514... \tVal Loss: 1.227 \tVal Frac: 0.0528\n",
      "\tEpoch: 100/1000; 1.75 sec... \tStep: 24200... \tLoss: 1.270... \tVal Loss: 1.239 \tVal Frac: 0.0551\n",
      "\tEpoch: 105/1000; 1.67 sec... \tStep: 25410... \tLoss: 1.674... \tVal Loss: 1.201 \tVal Frac: 0.0554\n",
      "\tEpoch: 110/1000; 1.64 sec... \tStep: 26620... \tLoss: 1.526... \tVal Loss: 1.246 \tVal Frac: 0.0435\n",
      "\tEpoch: 115/1000; 1.72 sec... \tStep: 27830... \tLoss: 1.710... \tVal Loss: 1.237 \tVal Frac: 0.0774\n",
      "\tEpoch: 120/1000; 1.74 sec... \tStep: 29040... \tLoss: 1.256... \tVal Loss: 1.197 \tVal Frac: 0.0495\n",
      "\tEpoch: 125/1000; 1.79 sec... \tStep: 30250... \tLoss: 1.240... \tVal Loss: 1.202 \tVal Frac: 0.0528\n",
      "\tEpoch: 130/1000; 1.68 sec... \tStep: 31460... \tLoss: 1.597... \tVal Loss: 1.197 \tVal Frac: 0.0495\n",
      "\tEpoch: 135/1000; 1.79 sec... \tStep: 32670... \tLoss: 1.172... \tVal Loss: 1.213 \tVal Frac: 0.0532\n",
      "\tEpoch: 140/1000; 1.69 sec... \tStep: 33880... \tLoss: 1.147... \tVal Loss: 1.211 \tVal Frac: 0.0461\n",
      "\tEpoch: 145/1000; 1.70 sec... \tStep: 35090... \tLoss: 1.289... \tVal Loss: 1.192 \tVal Frac: 0.0472\n",
      "\tValidation loss decreased (1.196 --> 1.192).  Saving model ...\n",
      "\tEpoch: 150/1000; 1.64 sec... \tStep: 36300... \tLoss: 1.378... \tVal Loss: 1.199 \tVal Frac: 0.0804\n",
      "\tEpoch: 155/1000; 1.81 sec... \tStep: 37510... \tLoss: 1.198... \tVal Loss: 1.197 \tVal Frac: 0.0923\n",
      "\tEpoch: 160/1000; 1.60 sec... \tStep: 38720... \tLoss: 1.506... \tVal Loss: 1.229 \tVal Frac: 0.0900\n",
      "\tEpoch: 165/1000; 1.75 sec... \tStep: 39930... \tLoss: 1.148... \tVal Loss: 1.206 \tVal Frac: 0.0867\n",
      "\tEpoch: 170/1000; 1.82 sec... \tStep: 41140... \tLoss: 1.366... \tVal Loss: 1.213 \tVal Frac: 0.0830\n",
      "\tEpoch: 175/1000; 1.71 sec... \tStep: 42350... \tLoss: 1.294... \tVal Loss: 1.241 \tVal Frac: 0.0621\n",
      "\tEpoch: 180/1000; 1.64 sec... \tStep: 43560... \tLoss: 1.316... \tVal Loss: 1.187 \tVal Frac: 0.0789\n",
      "\tValidation loss decreased (1.192 --> 1.187).  Saving model ...\n",
      "\tEpoch: 185/1000; 1.65 sec... \tStep: 44770... \tLoss: 1.384... \tVal Loss: 1.204 \tVal Frac: 0.0778\n",
      "\tEpoch: 190/1000; 1.77 sec... \tStep: 45980... \tLoss: 1.249... \tVal Loss: 1.173 \tVal Frac: 0.0800\n",
      "\tValidation loss decreased (1.187 --> 1.173).  Saving model ...\n",
      "\tEpoch: 195/1000; 1.59 sec... \tStep: 47190... \tLoss: 1.393... \tVal Loss: 1.199 \tVal Frac: 0.0577\n",
      "\tEpoch: 200/1000; 1.82 sec... \tStep: 48400... \tLoss: 1.445... \tVal Loss: 1.182 \tVal Frac: 0.0990\n",
      "\tEpoch: 205/1000; 1.74 sec... \tStep: 49610... \tLoss: 1.329... \tVal Loss: 1.199 \tVal Frac: 0.0930\n",
      "\tEpoch: 210/1000; 1.74 sec... \tStep: 50820... \tLoss: 1.059... \tVal Loss: 1.211 \tVal Frac: 0.0837\n",
      "\tEpoch: 215/1000; 1.72 sec... \tStep: 52030... \tLoss: 1.335... \tVal Loss: 1.203 \tVal Frac: 0.0569\n",
      "\tEpoch: 220/1000; 1.78 sec... \tStep: 53240... \tLoss: 1.545... \tVal Loss: 1.200 \tVal Frac: 0.0770\n",
      "\tEpoch: 225/1000; 1.89 sec... \tStep: 54450... \tLoss: 1.423... \tVal Loss: 1.196 \tVal Frac: 0.0755\n",
      "\tEpoch: 230/1000; 1.76 sec... \tStep: 55660... \tLoss: 1.519... \tVal Loss: 1.178 \tVal Frac: 0.0554\n",
      "\tEpoch: 235/1000; 1.76 sec... \tStep: 56870... \tLoss: 1.208... \tVal Loss: 1.181 \tVal Frac: 0.0755\n",
      "\tEpoch: 240/1000; 1.62 sec... \tStep: 58080... \tLoss: 1.049... \tVal Loss: 1.159 \tVal Frac: 0.0833\n",
      "\tValidation loss decreased (1.173 --> 1.159).  Saving model ...\n",
      "\tEpoch: 245/1000; 1.59 sec... \tStep: 59290... \tLoss: 1.608... \tVal Loss: 1.210 \tVal Frac: 0.0543\n",
      "\tEpoch: 250/1000; 1.74 sec... \tStep: 60500... \tLoss: 1.426... \tVal Loss: 1.162 \tVal Frac: 0.0874\n",
      "\tEpoch: 255/1000; 1.69 sec... \tStep: 61710... \tLoss: 1.207... \tVal Loss: 1.158 \tVal Frac: 0.0938\n",
      "\tValidation loss decreased (1.159 --> 1.158).  Saving model ...\n",
      "\tEpoch: 260/1000; 1.72 sec... \tStep: 62920... \tLoss: 1.505... \tVal Loss: 1.195 \tVal Frac: 0.0506\n",
      "\tEpoch: 265/1000; 1.72 sec... \tStep: 64130... \tLoss: 1.359... \tVal Loss: 1.184 \tVal Frac: 0.0625\n",
      "\tEpoch: 270/1000; 1.81 sec... \tStep: 65340... \tLoss: 1.029... \tVal Loss: 1.148 \tVal Frac: 0.0688\n",
      "\tValidation loss decreased (1.158 --> 1.148).  Saving model ...\n",
      "\tEpoch: 275/1000; 1.71 sec... \tStep: 66550... \tLoss: 1.183... \tVal Loss: 1.178 \tVal Frac: 0.0618\n",
      "\tEpoch: 280/1000; 1.81 sec... \tStep: 67760... \tLoss: 1.045... \tVal Loss: 1.159 \tVal Frac: 0.0863\n",
      "\tEpoch: 285/1000; 1.64 sec... \tStep: 68970... \tLoss: 1.136... \tVal Loss: 1.196 \tVal Frac: 0.0662\n",
      "\tEpoch: 290/1000; 1.71 sec... \tStep: 70180... \tLoss: 0.995... \tVal Loss: 1.167 \tVal Frac: 0.0543\n",
      "\tEpoch: 295/1000; 1.64 sec... \tStep: 71390... \tLoss: 1.166... \tVal Loss: 1.156 \tVal Frac: 0.0949\n",
      "\tEpoch: 300/1000; 1.82 sec... \tStep: 72600... \tLoss: 1.363... \tVal Loss: 1.152 \tVal Frac: 0.0573\n",
      "\tEpoch: 305/1000; 1.64 sec... \tStep: 73810... \tLoss: 1.173... \tVal Loss: 1.163 \tVal Frac: 0.0878\n",
      "\tEpoch: 310/1000; 1.69 sec... \tStep: 75020... \tLoss: 1.216... \tVal Loss: 1.148 \tVal Frac: 0.0670\n",
      "\tValidation loss decreased (1.148 --> 1.148).  Saving model ...\n",
      "\tEpoch: 315/1000; 1.86 sec... \tStep: 76230... \tLoss: 1.183... \tVal Loss: 1.137 \tVal Frac: 0.0685\n",
      "\tValidation loss decreased (1.148 --> 1.137).  Saving model ...\n",
      "\tEpoch: 320/1000; 1.76 sec... \tStep: 77440... \tLoss: 1.513... \tVal Loss: 1.144 \tVal Frac: 0.0707\n",
      "\tEpoch: 325/1000; 1.74 sec... \tStep: 78650... \tLoss: 1.236... \tVal Loss: 1.165 \tVal Frac: 0.0837\n",
      "\tEpoch: 330/1000; 1.86 sec... \tStep: 79860... \tLoss: 1.439... \tVal Loss: 1.172 \tVal Frac: 0.0707\n",
      "\tEpoch: 335/1000; 1.69 sec... \tStep: 81070... \tLoss: 1.382... \tVal Loss: 1.146 \tVal Frac: 0.0975\n",
      "\tEpoch: 340/1000; 1.71 sec... \tStep: 82280... \tLoss: 1.154... \tVal Loss: 1.177 \tVal Frac: 0.0755\n",
      "\tEpoch: 345/1000; 1.67 sec... \tStep: 83490... \tLoss: 1.282... \tVal Loss: 1.159 \tVal Frac: 0.0822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 350/1000; 1.64 sec... \tStep: 84700... \tLoss: 1.467... \tVal Loss: 1.156 \tVal Frac: 0.0729\n",
      "\tEpoch: 355/1000; 1.71 sec... \tStep: 85910... \tLoss: 1.278... \tVal Loss: 1.146 \tVal Frac: 0.0986\n",
      "\tEpoch: 360/1000; 1.62 sec... \tStep: 87120... \tLoss: 1.255... \tVal Loss: 1.148 \tVal Frac: 0.1097\n",
      "\tEpoch: 365/1000; 1.72 sec... \tStep: 88330... \tLoss: 1.116... \tVal Loss: 1.146 \tVal Frac: 0.1109\n",
      "\tEpoch: 370/1000; 1.58 sec... \tStep: 89540... \tLoss: 1.190... \tVal Loss: 1.142 \tVal Frac: 0.1112\n",
      "\tEpoch: 375/1000; 1.87 sec... \tStep: 90750... \tLoss: 1.292... \tVal Loss: 1.139 \tVal Frac: 0.1008\n",
      "\tEpoch: 380/1000; 1.65 sec... \tStep: 91960... \tLoss: 1.125... \tVal Loss: 1.153 \tVal Frac: 0.1008\n",
      "\tEpoch: 385/1000; 1.59 sec... \tStep: 93170... \tLoss: 1.430... \tVal Loss: 1.149 \tVal Frac: 0.0975\n",
      "\tEpoch: 390/1000; 1.56 sec... \tStep: 94380... \tLoss: 1.444... \tVal Loss: 1.189 \tVal Frac: 0.0696\n",
      "\tEpoch: 395/1000; 1.67 sec... \tStep: 95590... \tLoss: 1.333... \tVal Loss: 1.157 \tVal Frac: 0.0547\n",
      "\tEpoch: 400/1000; 1.76 sec... \tStep: 96800... \tLoss: 1.012... \tVal Loss: 1.159 \tVal Frac: 0.0666\n",
      "\tEpoch: 405/1000; 1.61 sec... \tStep: 98010... \tLoss: 1.548... \tVal Loss: 1.134 \tVal Frac: 0.0766\n",
      "\tValidation loss decreased (1.137 --> 1.134).  Saving model ...\n",
      "\tEpoch: 410/1000; 1.74 sec... \tStep: 99220... \tLoss: 1.207... \tVal Loss: 1.156 \tVal Frac: 0.0666\n",
      "\tEpoch: 415/1000; 1.66 sec... \tStep: 100430... \tLoss: 1.309... \tVal Loss: 1.148 \tVal Frac: 0.0848\n",
      "\tEpoch: 420/1000; 1.74 sec... \tStep: 101640... \tLoss: 1.091... \tVal Loss: 1.154 \tVal Frac: 0.0625\n",
      "\tEpoch: 425/1000; 1.74 sec... \tStep: 102850... \tLoss: 1.189... \tVal Loss: 1.166 \tVal Frac: 0.0725\n",
      "\tEpoch: 430/1000; 1.75 sec... \tStep: 104060... \tLoss: 1.092... \tVal Loss: 1.158 \tVal Frac: 0.0629\n",
      "\tEpoch: 435/1000; 1.74 sec... \tStep: 105270... \tLoss: 1.353... \tVal Loss: 1.135 \tVal Frac: 0.0655\n",
      "\tEpoch: 440/1000; 1.62 sec... \tStep: 106480... \tLoss: 1.306... \tVal Loss: 1.141 \tVal Frac: 0.0588\n",
      "\tEpoch: 445/1000; 1.76 sec... \tStep: 107690... \tLoss: 1.242... \tVal Loss: 1.178 \tVal Frac: 0.0658\n",
      "\tEpoch: 450/1000; 1.79 sec... \tStep: 108900... \tLoss: 1.321... \tVal Loss: 1.124 \tVal Frac: 0.0711\n",
      "\tValidation loss decreased (1.134 --> 1.124).  Saving model ...\n",
      "\tEpoch: 455/1000; 1.65 sec... \tStep: 110110... \tLoss: 1.098... \tVal Loss: 1.125 \tVal Frac: 0.0982\n",
      "\tEpoch: 460/1000; 1.86 sec... \tStep: 111320... \tLoss: 1.329... \tVal Loss: 1.138 \tVal Frac: 0.0759\n",
      "\tEpoch: 465/1000; 1.68 sec... \tStep: 112530... \tLoss: 1.212... \tVal Loss: 1.138 \tVal Frac: 0.0651\n",
      "\tEpoch: 470/1000; 1.75 sec... \tStep: 113740... \tLoss: 1.302... \tVal Loss: 1.147 \tVal Frac: 0.0960\n",
      "\tEpoch: 475/1000; 1.77 sec... \tStep: 114950... \tLoss: 1.495... \tVal Loss: 1.132 \tVal Frac: 0.0815\n",
      "\tEpoch: 480/1000; 1.72 sec... \tStep: 116160... \tLoss: 1.437... \tVal Loss: 1.150 \tVal Frac: 0.0763\n",
      "\tEpoch: 485/1000; 1.77 sec... \tStep: 117370... \tLoss: 1.214... \tVal Loss: 1.149 \tVal Frac: 0.0673\n",
      "\tEpoch: 490/1000; 1.82 sec... \tStep: 118580... \tLoss: 1.263... \tVal Loss: 1.152 \tVal Frac: 0.0733\n",
      "\tEpoch: 495/1000; 1.68 sec... \tStep: 119790... \tLoss: 1.133... \tVal Loss: 1.146 \tVal Frac: 0.0748\n",
      "\tEpoch: 500/1000; 1.72 sec... \tStep: 121000... \tLoss: 1.282... \tVal Loss: 1.160 \tVal Frac: 0.0647\n",
      "\tEpoch: 505/1000; 1.77 sec... \tStep: 122210... \tLoss: 1.177... \tVal Loss: 1.147 \tVal Frac: 0.0577\n",
      "\tEpoch: 510/1000; 1.71 sec... \tStep: 123420... \tLoss: 1.100... \tVal Loss: 1.143 \tVal Frac: 0.0759\n",
      "\tEpoch: 515/1000; 1.77 sec... \tStep: 124630... \tLoss: 1.568... \tVal Loss: 1.145 \tVal Frac: 0.0647\n",
      "\tEpoch: 520/1000; 1.72 sec... \tStep: 125840... \tLoss: 1.024... \tVal Loss: 1.142 \tVal Frac: 0.0737\n",
      "\tEpoch: 525/1000; 1.74 sec... \tStep: 127050... \tLoss: 1.308... \tVal Loss: 1.175 \tVal Frac: 0.0573\n",
      "\tEpoch: 530/1000; 1.67 sec... \tStep: 128260... \tLoss: 1.051... \tVal Loss: 1.125 \tVal Frac: 0.1083\n",
      "\tEpoch: 535/1000; 1.67 sec... \tStep: 129470... \tLoss: 1.272... \tVal Loss: 1.147 \tVal Frac: 0.0744\n",
      "\tEpoch: 540/1000; 1.74 sec... \tStep: 130680... \tLoss: 1.412... \tVal Loss: 1.123 \tVal Frac: 0.0792\n",
      "\tValidation loss decreased (1.124 --> 1.123).  Saving model ...\n",
      "\tEpoch: 545/1000; 1.75 sec... \tStep: 131890... \tLoss: 1.392... \tVal Loss: 1.135 \tVal Frac: 0.0595\n",
      "\tEpoch: 550/1000; 1.72 sec... \tStep: 133100... \tLoss: 1.315... \tVal Loss: 1.132 \tVal Frac: 0.0800\n",
      "\tEpoch: 555/1000; 1.75 sec... \tStep: 134310... \tLoss: 1.319... \tVal Loss: 1.125 \tVal Frac: 0.0696\n",
      "\tEpoch: 560/1000; 1.64 sec... \tStep: 135520... \tLoss: 1.088... \tVal Loss: 1.136 \tVal Frac: 0.0703\n",
      "\tEpoch: 565/1000; 1.64 sec... \tStep: 136730... \tLoss: 1.106... \tVal Loss: 1.137 \tVal Frac: 0.0640\n",
      "\tEpoch: 570/1000; 1.82 sec... \tStep: 137940... \tLoss: 1.353... \tVal Loss: 1.123 \tVal Frac: 0.0751\n",
      "\tEpoch: 575/1000; 1.60 sec... \tStep: 139150... \tLoss: 1.093... \tVal Loss: 1.126 \tVal Frac: 0.0610\n",
      "\tEpoch: 580/1000; 1.77 sec... \tStep: 140360... \tLoss: 1.163... \tVal Loss: 1.148 \tVal Frac: 0.0573\n",
      "\tEpoch: 585/1000; 1.69 sec... \tStep: 141570... \tLoss: 1.255... \tVal Loss: 1.137 \tVal Frac: 0.0800\n",
      "\tEpoch: 590/1000; 1.81 sec... \tStep: 142780... \tLoss: 1.035... \tVal Loss: 1.123 \tVal Frac: 0.0766\n",
      "\tEpoch: 595/1000; 1.87 sec... \tStep: 143990... \tLoss: 1.087... \tVal Loss: 1.130 \tVal Frac: 0.0822\n",
      "\tEpoch: 600/1000; 1.65 sec... \tStep: 145200... \tLoss: 1.034... \tVal Loss: 1.115 \tVal Frac: 0.0774\n",
      "\tValidation loss decreased (1.123 --> 1.115).  Saving model ...\n",
      "\tEpoch: 605/1000; 1.66 sec... \tStep: 146410... \tLoss: 1.143... \tVal Loss: 1.112 \tVal Frac: 0.0956\n",
      "\tValidation loss decreased (1.115 --> 1.112).  Saving model ...\n",
      "\tEpoch: 610/1000; 1.65 sec... \tStep: 147620... \tLoss: 1.319... \tVal Loss: 1.147 \tVal Frac: 0.0759\n",
      "\tEpoch: 615/1000; 1.82 sec... \tStep: 148830... \tLoss: 1.210... \tVal Loss: 1.125 \tVal Frac: 0.0606\n",
      "\tEpoch: 620/1000; 1.74 sec... \tStep: 150040... \tLoss: 1.106... \tVal Loss: 1.128 \tVal Frac: 0.0614\n",
      "\tEpoch: 625/1000; 1.83 sec... \tStep: 151250... \tLoss: 1.403... \tVal Loss: 1.136 \tVal Frac: 0.0930\n",
      "\tEpoch: 630/1000; 1.79 sec... \tStep: 152460... \tLoss: 1.062... \tVal Loss: 1.116 \tVal Frac: 0.1034\n",
      "\tEpoch: 635/1000; 1.74 sec... \tStep: 153670... \tLoss: 1.188... \tVal Loss: 1.119 \tVal Frac: 0.0677\n",
      "\tEpoch: 640/1000; 1.61 sec... \tStep: 154880... \tLoss: 0.995... \tVal Loss: 1.147 \tVal Frac: 0.0655\n",
      "\tEpoch: 645/1000; 1.60 sec... \tStep: 156090... \tLoss: 1.281... \tVal Loss: 1.115 \tVal Frac: 0.0926\n",
      "\tEpoch: 650/1000; 1.54 sec... \tStep: 157300... \tLoss: 1.321... \tVal Loss: 1.135 \tVal Frac: 0.0699\n",
      "\tEpoch: 655/1000; 1.64 sec... \tStep: 158510... \tLoss: 1.262... \tVal Loss: 1.121 \tVal Frac: 0.0692\n",
      "\tEpoch: 660/1000; 1.68 sec... \tStep: 159720... \tLoss: 1.249... \tVal Loss: 1.141 \tVal Frac: 0.0785\n",
      "\tEpoch: 665/1000; 1.77 sec... \tStep: 160930... \tLoss: 1.348... \tVal Loss: 1.170 \tVal Frac: 0.0930\n",
      "\tEpoch: 670/1000; 1.71 sec... \tStep: 162140... \tLoss: 1.351... \tVal Loss: 1.103 \tVal Frac: 0.0990\n",
      "\tValidation loss decreased (1.112 --> 1.103).  Saving model ...\n",
      "\tEpoch: 675/1000; 1.74 sec... \tStep: 163350... \tLoss: 1.121... \tVal Loss: 1.107 \tVal Frac: 0.0952\n",
      "\tEpoch: 680/1000; 1.66 sec... \tStep: 164560... \tLoss: 1.292... \tVal Loss: 1.115 \tVal Frac: 0.1142\n",
      "\tEpoch: 685/1000; 1.69 sec... \tStep: 165770... \tLoss: 1.213... \tVal Loss: 1.126 \tVal Frac: 0.0911\n",
      "\tEpoch: 690/1000; 1.65 sec... \tStep: 166980... \tLoss: 1.177... \tVal Loss: 1.113 \tVal Frac: 0.0759\n",
      "\tEpoch: 695/1000; 1.84 sec... \tStep: 168190... \tLoss: 1.313... \tVal Loss: 1.101 \tVal Frac: 0.0729\n",
      "\tValidation loss decreased (1.103 --> 1.101).  Saving model ...\n",
      "\tEpoch: 700/1000; 1.71 sec... \tStep: 169400... \tLoss: 1.130... \tVal Loss: 1.104 \tVal Frac: 0.0770\n",
      "\tEpoch: 705/1000; 1.67 sec... \tStep: 170610... \tLoss: 1.066... \tVal Loss: 1.114 \tVal Frac: 0.1064\n",
      "\tEpoch: 710/1000; 1.74 sec... \tStep: 171820... \tLoss: 1.054... \tVal Loss: 1.144 \tVal Frac: 0.0889\n",
      "\tEpoch: 715/1000; 1.74 sec... \tStep: 173030... \tLoss: 1.196... \tVal Loss: 1.116 \tVal Frac: 0.0822\n",
      "\tEpoch: 720/1000; 1.79 sec... \tStep: 174240... \tLoss: 1.120... \tVal Loss: 1.120 \tVal Frac: 0.1034\n",
      "\tEpoch: 725/1000; 1.74 sec... \tStep: 175450... \tLoss: 1.129... \tVal Loss: 1.139 \tVal Frac: 0.0800\n",
      "\tEpoch: 730/1000; 1.61 sec... \tStep: 176660... \tLoss: 1.475... \tVal Loss: 1.136 \tVal Frac: 0.0774\n",
      "\tEpoch: 735/1000; 1.71 sec... \tStep: 177870... \tLoss: 1.372... \tVal Loss: 1.142 \tVal Frac: 0.0644\n",
      "\tEpoch: 740/1000; 1.65 sec... \tStep: 179080... \tLoss: 1.042... \tVal Loss: 1.141 \tVal Frac: 0.0670\n",
      "\tEpoch: 745/1000; 1.83 sec... \tStep: 180290... \tLoss: 1.128... \tVal Loss: 1.120 \tVal Frac: 0.0592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 750/1000; 1.67 sec... \tStep: 181500... \tLoss: 1.533... \tVal Loss: 1.120 \tVal Frac: 0.0934\n",
      "\tEpoch: 755/1000; 1.82 sec... \tStep: 182710... \tLoss: 1.132... \tVal Loss: 1.119 \tVal Frac: 0.0856\n",
      "\tEpoch: 760/1000; 1.66 sec... \tStep: 183920... \tLoss: 1.239... \tVal Loss: 1.128 \tVal Frac: 0.0923\n",
      "\tEpoch: 765/1000; 1.63 sec... \tStep: 185130... \tLoss: 1.326... \tVal Loss: 1.129 \tVal Frac: 0.0822\n",
      "\tEpoch: 770/1000; 1.75 sec... \tStep: 186340... \tLoss: 1.149... \tVal Loss: 1.134 \tVal Frac: 0.0725\n",
      "\tEpoch: 775/1000; 1.82 sec... \tStep: 187550... \tLoss: 1.223... \tVal Loss: 1.125 \tVal Frac: 0.0804\n",
      "\tEpoch: 780/1000; 1.64 sec... \tStep: 188760... \tLoss: 0.911... \tVal Loss: 1.123 \tVal Frac: 0.0666\n",
      "\tEpoch: 785/1000; 1.84 sec... \tStep: 189970... \tLoss: 1.231... \tVal Loss: 1.108 \tVal Frac: 0.0722\n",
      "\tEpoch: 790/1000; 1.67 sec... \tStep: 191180... \tLoss: 1.170... \tVal Loss: 1.105 \tVal Frac: 0.0744\n",
      "\tEpoch: 795/1000; 1.69 sec... \tStep: 192390... \tLoss: 1.193... \tVal Loss: 1.148 \tVal Frac: 0.0744\n",
      "\tEpoch: 800/1000; 1.77 sec... \tStep: 193600... \tLoss: 1.092... \tVal Loss: 1.100 \tVal Frac: 0.0900\n",
      "\tValidation loss decreased (1.101 --> 1.100).  Saving model ...\n",
      "\tEpoch: 805/1000; 1.64 sec... \tStep: 194810... \tLoss: 1.275... \tVal Loss: 1.119 \tVal Frac: 0.0673\n",
      "\tEpoch: 810/1000; 1.76 sec... \tStep: 196020... \tLoss: 1.134... \tVal Loss: 1.120 \tVal Frac: 0.0941\n",
      "\tEpoch: 815/1000; 1.72 sec... \tStep: 197230... \tLoss: 1.180... \tVal Loss: 1.116 \tVal Frac: 0.1138\n",
      "\tEpoch: 820/1000; 1.74 sec... \tStep: 198440... \tLoss: 0.925... \tVal Loss: 1.140 \tVal Frac: 0.1190\n",
      "\tEpoch: 825/1000; 1.77 sec... \tStep: 199650... \tLoss: 1.180... \tVal Loss: 1.122 \tVal Frac: 0.0811\n",
      "\tEpoch: 830/1000; 1.64 sec... \tStep: 200860... \tLoss: 1.273... \tVal Loss: 1.107 \tVal Frac: 0.1071\n",
      "\tEpoch: 835/1000; 1.74 sec... \tStep: 202070... \tLoss: 1.173... \tVal Loss: 1.107 \tVal Frac: 0.0964\n",
      "\tEpoch: 840/1000; 1.64 sec... \tStep: 203280... \tLoss: 1.246... \tVal Loss: 1.116 \tVal Frac: 0.0990\n",
      "\tEpoch: 845/1000; 1.66 sec... \tStep: 204490... \tLoss: 1.381... \tVal Loss: 1.096 \tVal Frac: 0.0748\n",
      "\tValidation loss decreased (1.100 --> 1.096).  Saving model ...\n",
      "\tEpoch: 850/1000; 1.81 sec... \tStep: 205700... \tLoss: 1.080... \tVal Loss: 1.152 \tVal Frac: 0.0655\n",
      "\tEpoch: 855/1000; 1.77 sec... \tStep: 206910... \tLoss: 1.341... \tVal Loss: 1.125 \tVal Frac: 0.0770\n",
      "\tEpoch: 860/1000; 1.69 sec... \tStep: 208120... \tLoss: 1.154... \tVal Loss: 1.099 \tVal Frac: 0.0740\n",
      "\tEpoch: 865/1000; 1.86 sec... \tStep: 209330... \tLoss: 1.022... \tVal Loss: 1.116 \tVal Frac: 0.0997\n",
      "\tEpoch: 870/1000; 1.62 sec... \tStep: 210540... \tLoss: 1.043... \tVal Loss: 1.104 \tVal Frac: 0.0844\n",
      "\tEpoch: 875/1000; 1.77 sec... \tStep: 211750... \tLoss: 1.197... \tVal Loss: 1.116 \tVal Frac: 0.0792\n",
      "\tEpoch: 880/1000; 1.79 sec... \tStep: 212960... \tLoss: 1.326... \tVal Loss: 1.136 \tVal Frac: 0.0711\n",
      "\tEpoch: 885/1000; 1.80 sec... \tStep: 214170... \tLoss: 1.411... \tVal Loss: 1.145 \tVal Frac: 0.0763\n",
      "\tEpoch: 890/1000; 1.74 sec... \tStep: 215380... \tLoss: 1.520... \tVal Loss: 1.118 \tVal Frac: 0.0792\n",
      "\tEpoch: 895/1000; 1.77 sec... \tStep: 216590... \tLoss: 1.376... \tVal Loss: 1.095 \tVal Frac: 0.0696\n",
      "\tValidation loss decreased (1.096 --> 1.095).  Saving model ...\n",
      "\tEpoch: 900/1000; 1.78 sec... \tStep: 217800... \tLoss: 1.430... \tVal Loss: 1.117 \tVal Frac: 0.0975\n",
      "\tEpoch: 905/1000; 1.89 sec... \tStep: 219010... \tLoss: 0.999... \tVal Loss: 1.119 \tVal Frac: 0.0833\n",
      "\tEpoch: 910/1000; 1.71 sec... \tStep: 220220... \tLoss: 1.222... \tVal Loss: 1.108 \tVal Frac: 0.1064\n",
      "\tEpoch: 915/1000; 1.60 sec... \tStep: 221430... \tLoss: 0.908... \tVal Loss: 1.109 \tVal Frac: 0.0871\n",
      "\tEpoch: 920/1000; 1.70 sec... \tStep: 222640... \tLoss: 1.580... \tVal Loss: 1.100 \tVal Frac: 0.0826\n",
      "\tEpoch: 925/1000; 1.74 sec... \tStep: 223850... \tLoss: 1.099... \tVal Loss: 1.120 \tVal Frac: 0.0893\n",
      "\tEpoch: 930/1000; 1.61 sec... \tStep: 225060... \tLoss: 1.315... \tVal Loss: 1.108 \tVal Frac: 0.0867\n",
      "\tEpoch: 935/1000; 1.65 sec... \tStep: 226270... \tLoss: 1.212... \tVal Loss: 1.119 \tVal Frac: 0.0785\n",
      "\tEpoch: 940/1000; 1.69 sec... \tStep: 227480... \tLoss: 1.338... \tVal Loss: 1.140 \tVal Frac: 0.1057\n",
      "\tEpoch: 945/1000; 1.71 sec... \tStep: 228690... \tLoss: 1.053... \tVal Loss: 1.121 \tVal Frac: 0.1150\n",
      "\tEpoch: 950/1000; 1.71 sec... \tStep: 229900... \tLoss: 1.241... \tVal Loss: 1.106 \tVal Frac: 0.0744\n",
      "\tEpoch: 955/1000; 1.69 sec... \tStep: 231110... \tLoss: 1.220... \tVal Loss: 1.117 \tVal Frac: 0.0737\n",
      "\tEpoch: 960/1000; 1.66 sec... \tStep: 232320... \tLoss: 1.276... \tVal Loss: 1.100 \tVal Frac: 0.1116\n",
      "\tEpoch: 965/1000; 1.73 sec... \tStep: 233530... \tLoss: 1.265... \tVal Loss: 1.110 \tVal Frac: 0.1068\n",
      "\tEpoch: 970/1000; 1.66 sec... \tStep: 234740... \tLoss: 1.401... \tVal Loss: 1.109 \tVal Frac: 0.0956\n",
      "\tEpoch: 975/1000; 1.54 sec... \tStep: 235950... \tLoss: 1.152... \tVal Loss: 1.108 \tVal Frac: 0.0681\n",
      "\tEpoch: 980/1000; 1.72 sec... \tStep: 237160... \tLoss: 1.114... \tVal Loss: 1.109 \tVal Frac: 0.0688\n",
      "\tEpoch: 985/1000; 1.75 sec... \tStep: 238370... \tLoss: 0.954... \tVal Loss: 1.119 \tVal Frac: 0.0506\n",
      "\tEpoch: 990/1000; 1.63 sec... \tStep: 239580... \tLoss: 1.371... \tVal Loss: 1.115 \tVal Frac: 0.1097\n",
      "\tEpoch: 995/1000; 1.61 sec... \tStep: 240790... \tLoss: 1.342... \tVal Loss: 1.105 \tVal Frac: 0.1157\n",
      "\tEpoch: 1000/1000; 1.67 sec... \tStep: 242000... \tLoss: 1.483... \tVal Loss: 1.136 \tVal Frac: 0.0878\n",
      "Completed:  n_windows :  100 \n",
      "\tloss: 1.094808 \n",
      "\tfrac:0.119048\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  250\n",
      "\tEpoch: 5/1000; 3.53 sec... \tStep: 1205... \tLoss: 1.237... \tVal Loss: 1.008 \tVal Frac: 0.7433\n",
      "\tValidation loss decreased (inf --> 1.008).  Saving model ...\n",
      "\tEpoch: 10/1000; 3.44 sec... \tStep: 2410... \tLoss: 0.951... \tVal Loss: 0.855 \tVal Frac: 0.7556\n",
      "\tValidation loss decreased (1.008 --> 0.855).  Saving model ...\n",
      "\tEpoch: 15/1000; 3.60 sec... \tStep: 3615... \tLoss: 1.124... \tVal Loss: 0.758 \tVal Frac: 0.7634\n",
      "\tValidation loss decreased (0.855 --> 0.758).  Saving model ...\n",
      "\tEpoch: 20/1000; 3.43 sec... \tStep: 4820... \tLoss: 1.794... \tVal Loss: 0.850 \tVal Frac: 0.7240\n",
      "\tEpoch: 25/1000; 3.47 sec... \tStep: 6025... \tLoss: 1.273... \tVal Loss: 0.733 \tVal Frac: 0.7593\n",
      "\tValidation loss decreased (0.758 --> 0.733).  Saving model ...\n",
      "\tEpoch: 30/1000; 3.41 sec... \tStep: 7230... \tLoss: 0.961... \tVal Loss: 0.728 \tVal Frac: 0.7630\n",
      "\tValidation loss decreased (0.733 --> 0.728).  Saving model ...\n",
      "\tEpoch: 35/1000; 3.40 sec... \tStep: 8435... \tLoss: 0.511... \tVal Loss: 0.746 \tVal Frac: 0.7191\n",
      "\tEpoch: 40/1000; 3.50 sec... \tStep: 9640... \tLoss: 1.034... \tVal Loss: 0.674 \tVal Frac: 0.7641\n",
      "\tValidation loss decreased (0.728 --> 0.674).  Saving model ...\n",
      "\tEpoch: 45/1000; 3.37 sec... \tStep: 10845... \tLoss: 0.928... \tVal Loss: 0.713 \tVal Frac: 0.7690\n",
      "\tEpoch: 50/1000; 3.49 sec... \tStep: 12050... \tLoss: 0.788... \tVal Loss: 0.681 \tVal Frac: 0.7690\n",
      "\tEpoch: 55/1000; 3.43 sec... \tStep: 13255... \tLoss: 0.791... \tVal Loss: 0.709 \tVal Frac: 0.7626\n",
      "\tEpoch: 60/1000; 3.59 sec... \tStep: 14460... \tLoss: 0.938... \tVal Loss: 0.714 \tVal Frac: 0.7656\n",
      "\tEpoch: 65/1000; 3.36 sec... \tStep: 15665... \tLoss: 0.707... \tVal Loss: 0.689 \tVal Frac: 0.7675\n",
      "\tEpoch: 70/1000; 3.43 sec... \tStep: 16870... \tLoss: 0.754... \tVal Loss: 0.695 \tVal Frac: 0.7619\n",
      "\tEpoch: 75/1000; 3.50 sec... \tStep: 18075... \tLoss: 0.731... \tVal Loss: 0.668 \tVal Frac: 0.7742\n",
      "\tValidation loss decreased (0.674 --> 0.668).  Saving model ...\n",
      "\tEpoch: 80/1000; 3.50 sec... \tStep: 19280... \tLoss: 0.884... \tVal Loss: 0.694 \tVal Frac: 0.7872\n",
      "\tEpoch: 85/1000; 3.43 sec... \tStep: 20485... \tLoss: 1.019... \tVal Loss: 0.680 \tVal Frac: 0.7667\n",
      "\tEpoch: 90/1000; 3.41 sec... \tStep: 21690... \tLoss: 0.920... \tVal Loss: 0.696 \tVal Frac: 0.7768\n",
      "\tEpoch: 95/1000; 3.41 sec... \tStep: 22895... \tLoss: 1.015... \tVal Loss: 0.701 \tVal Frac: 0.7556\n",
      "\tEpoch: 100/1000; 3.54 sec... \tStep: 24100... \tLoss: 0.368... \tVal Loss: 0.707 \tVal Frac: 0.7764\n",
      "\tEpoch: 105/1000; 3.41 sec... \tStep: 25305... \tLoss: 0.936... \tVal Loss: 0.723 \tVal Frac: 0.7533\n",
      "\tEpoch: 110/1000; 3.41 sec... \tStep: 26510... \tLoss: 0.566... \tVal Loss: 0.674 \tVal Frac: 0.7824\n",
      "\tEpoch: 115/1000; 3.53 sec... \tStep: 27715... \tLoss: 1.016... \tVal Loss: 0.697 \tVal Frac: 0.7690\n",
      "\tEpoch: 120/1000; 3.35 sec... \tStep: 28920... \tLoss: 0.991... \tVal Loss: 0.694 \tVal Frac: 0.7708\n",
      "\tEpoch: 125/1000; 3.47 sec... \tStep: 30125... \tLoss: 0.945... \tVal Loss: 0.660 \tVal Frac: 0.7768\n",
      "\tValidation loss decreased (0.668 --> 0.660).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 130/1000; 3.44 sec... \tStep: 31330... \tLoss: 0.751... \tVal Loss: 0.715 \tVal Frac: 0.7630\n",
      "\tEpoch: 135/1000; 3.33 sec... \tStep: 32535... \tLoss: 0.852... \tVal Loss: 0.658 \tVal Frac: 0.7909\n",
      "\tValidation loss decreased (0.660 --> 0.658).  Saving model ...\n",
      "\tEpoch: 140/1000; 3.54 sec... \tStep: 33740... \tLoss: 0.952... \tVal Loss: 0.742 \tVal Frac: 0.7597\n",
      "\tEpoch: 145/1000; 3.41 sec... \tStep: 34945... \tLoss: 1.024... \tVal Loss: 0.677 \tVal Frac: 0.7667\n",
      "\tEpoch: 150/1000; 3.39 sec... \tStep: 36150... \tLoss: 0.600... \tVal Loss: 0.750 \tVal Frac: 0.7556\n",
      "\tEpoch: 155/1000; 3.39 sec... \tStep: 37355... \tLoss: 0.658... \tVal Loss: 0.675 \tVal Frac: 0.7775\n",
      "\tEpoch: 160/1000; 3.58 sec... \tStep: 38560... \tLoss: 0.543... \tVal Loss: 0.644 \tVal Frac: 0.7842\n",
      "\tValidation loss decreased (0.658 --> 0.644).  Saving model ...\n",
      "\tEpoch: 165/1000; 3.40 sec... \tStep: 39765... \tLoss: 1.101... \tVal Loss: 0.667 \tVal Frac: 0.7805\n",
      "\tEpoch: 170/1000; 3.41 sec... \tStep: 40970... \tLoss: 0.832... \tVal Loss: 0.669 \tVal Frac: 0.7760\n",
      "\tEpoch: 175/1000; 3.44 sec... \tStep: 42175... \tLoss: 0.989... \tVal Loss: 0.661 \tVal Frac: 0.7920\n",
      "\tEpoch: 180/1000; 3.56 sec... \tStep: 43380... \tLoss: 0.667... \tVal Loss: 0.672 \tVal Frac: 0.7861\n",
      "\tEpoch: 185/1000; 3.48 sec... \tStep: 44585... \tLoss: 0.418... \tVal Loss: 0.664 \tVal Frac: 0.7846\n",
      "\tEpoch: 190/1000; 3.31 sec... \tStep: 45790... \tLoss: 0.952... \tVal Loss: 0.679 \tVal Frac: 0.7671\n",
      "\tEpoch: 195/1000; 3.41 sec... \tStep: 46995... \tLoss: 0.719... \tVal Loss: 0.677 \tVal Frac: 0.7742\n",
      "\tEpoch: 200/1000; 3.42 sec... \tStep: 48200... \tLoss: 0.794... \tVal Loss: 0.663 \tVal Frac: 0.7861\n",
      "\tEpoch: 205/1000; 3.52 sec... \tStep: 49405... \tLoss: 0.777... \tVal Loss: 0.677 \tVal Frac: 0.7753\n",
      "\tEpoch: 210/1000; 3.51 sec... \tStep: 50610... \tLoss: 0.780... \tVal Loss: 0.668 \tVal Frac: 0.7760\n",
      "\tEpoch: 215/1000; 3.68 sec... \tStep: 51815... \tLoss: 0.436... \tVal Loss: 0.672 \tVal Frac: 0.7760\n",
      "\tEpoch: 220/1000; 3.49 sec... \tStep: 53020... \tLoss: 0.786... \tVal Loss: 0.671 \tVal Frac: 0.7816\n",
      "\tEpoch: 225/1000; 3.41 sec... \tStep: 54225... \tLoss: 1.292... \tVal Loss: 0.653 \tVal Frac: 0.7693\n",
      "\tEpoch: 230/1000; 3.47 sec... \tStep: 55430... \tLoss: 0.749... \tVal Loss: 0.678 \tVal Frac: 0.7634\n",
      "\tEpoch: 235/1000; 3.43 sec... \tStep: 56635... \tLoss: 0.435... \tVal Loss: 0.639 \tVal Frac: 0.7887\n",
      "\tValidation loss decreased (0.644 --> 0.639).  Saving model ...\n",
      "\tEpoch: 240/1000; 3.45 sec... \tStep: 57840... \tLoss: 0.952... \tVal Loss: 0.638 \tVal Frac: 0.7954\n",
      "\tValidation loss decreased (0.639 --> 0.638).  Saving model ...\n",
      "\tEpoch: 245/1000; 3.41 sec... \tStep: 59045... \tLoss: 0.694... \tVal Loss: 0.659 \tVal Frac: 0.7879\n",
      "\tEpoch: 250/1000; 3.39 sec... \tStep: 60250... \tLoss: 0.572... \tVal Loss: 0.686 \tVal Frac: 0.7746\n",
      "\tEpoch: 255/1000; 3.39 sec... \tStep: 61455... \tLoss: 0.872... \tVal Loss: 0.661 \tVal Frac: 0.7749\n",
      "\tEpoch: 260/1000; 3.47 sec... \tStep: 62660... \tLoss: 0.810... \tVal Loss: 0.632 \tVal Frac: 0.7879\n",
      "\tValidation loss decreased (0.638 --> 0.632).  Saving model ...\n",
      "\tEpoch: 265/1000; 3.53 sec... \tStep: 63865... \tLoss: 0.561... \tVal Loss: 0.678 \tVal Frac: 0.7656\n",
      "\tEpoch: 270/1000; 3.42 sec... \tStep: 65070... \tLoss: 0.701... \tVal Loss: 0.678 \tVal Frac: 0.7716\n",
      "\tEpoch: 275/1000; 3.41 sec... \tStep: 66275... \tLoss: 0.920... \tVal Loss: 0.700 \tVal Frac: 0.7734\n",
      "\tEpoch: 280/1000; 3.48 sec... \tStep: 67480... \tLoss: 0.703... \tVal Loss: 0.655 \tVal Frac: 0.7887\n",
      "\tEpoch: 285/1000; 3.36 sec... \tStep: 68685... \tLoss: 1.043... \tVal Loss: 0.649 \tVal Frac: 0.7972\n",
      "\tEpoch: 290/1000; 3.42 sec... \tStep: 69890... \tLoss: 0.669... \tVal Loss: 0.652 \tVal Frac: 0.7891\n",
      "\tEpoch: 295/1000; 3.47 sec... \tStep: 71095... \tLoss: 0.566... \tVal Loss: 0.638 \tVal Frac: 0.7984\n",
      "\tEpoch: 300/1000; 3.41 sec... \tStep: 72300... \tLoss: 0.668... \tVal Loss: 0.675 \tVal Frac: 0.7842\n",
      "\tEpoch: 305/1000; 3.51 sec... \tStep: 73505... \tLoss: 0.563... \tVal Loss: 0.672 \tVal Frac: 0.7805\n",
      "\tEpoch: 310/1000; 3.43 sec... \tStep: 74710... \tLoss: 1.102... \tVal Loss: 0.652 \tVal Frac: 0.7861\n",
      "\tEpoch: 315/1000; 3.52 sec... \tStep: 75915... \tLoss: 0.633... \tVal Loss: 0.634 \tVal Frac: 0.7876\n",
      "\tEpoch: 320/1000; 3.47 sec... \tStep: 77120... \tLoss: 0.343... \tVal Loss: 0.635 \tVal Frac: 0.7883\n",
      "\tEpoch: 325/1000; 3.42 sec... \tStep: 78325... \tLoss: 1.163... \tVal Loss: 0.663 \tVal Frac: 0.7861\n",
      "\tEpoch: 330/1000; 3.44 sec... \tStep: 79530... \tLoss: 1.053... \tVal Loss: 0.666 \tVal Frac: 0.7816\n",
      "\tEpoch: 335/1000; 3.59 sec... \tStep: 80735... \tLoss: 1.000... \tVal Loss: 0.662 \tVal Frac: 0.7757\n",
      "\tEpoch: 340/1000; 3.44 sec... \tStep: 81940... \tLoss: 0.656... \tVal Loss: 0.651 \tVal Frac: 0.7809\n",
      "\tEpoch: 345/1000; 3.43 sec... \tStep: 83145... \tLoss: 0.910... \tVal Loss: 0.650 \tVal Frac: 0.7872\n",
      "\tEpoch: 350/1000; 3.42 sec... \tStep: 84350... \tLoss: 0.717... \tVal Loss: 0.654 \tVal Frac: 0.7812\n",
      "\tEpoch: 355/1000; 3.46 sec... \tStep: 85555... \tLoss: 0.830... \tVal Loss: 0.628 \tVal Frac: 0.7946\n",
      "\tValidation loss decreased (0.632 --> 0.628).  Saving model ...\n",
      "\tEpoch: 360/1000; 3.65 sec... \tStep: 86760... \tLoss: 0.998... \tVal Loss: 0.644 \tVal Frac: 0.7820\n",
      "\tEpoch: 365/1000; 3.48 sec... \tStep: 87965... \tLoss: 0.582... \tVal Loss: 0.661 \tVal Frac: 0.7682\n",
      "\tEpoch: 370/1000; 3.36 sec... \tStep: 89170... \tLoss: 1.077... \tVal Loss: 0.655 \tVal Frac: 0.7768\n",
      "\tEpoch: 375/1000; 3.42 sec... \tStep: 90375... \tLoss: 0.860... \tVal Loss: 0.634 \tVal Frac: 0.7801\n",
      "\tEpoch: 380/1000; 3.54 sec... \tStep: 91580... \tLoss: 1.099... \tVal Loss: 0.660 \tVal Frac: 0.7827\n",
      "\tEpoch: 385/1000; 3.41 sec... \tStep: 92785... \tLoss: 0.843... \tVal Loss: 0.653 \tVal Frac: 0.7831\n",
      "\tEpoch: 390/1000; 3.48 sec... \tStep: 93990... \tLoss: 0.356... \tVal Loss: 0.672 \tVal Frac: 0.7772\n",
      "\tEpoch: 395/1000; 3.49 sec... \tStep: 95195... \tLoss: 0.926... \tVal Loss: 0.631 \tVal Frac: 0.7865\n",
      "\tEpoch: 400/1000; 3.51 sec... \tStep: 96400... \tLoss: 0.818... \tVal Loss: 0.662 \tVal Frac: 0.7853\n",
      "\tEpoch: 405/1000; 3.46 sec... \tStep: 97605... \tLoss: 0.679... \tVal Loss: 0.682 \tVal Frac: 0.7738\n",
      "\tEpoch: 410/1000; 3.52 sec... \tStep: 98810... \tLoss: 0.819... \tVal Loss: 0.689 \tVal Frac: 0.7697\n",
      "\tEpoch: 415/1000; 3.64 sec... \tStep: 100015... \tLoss: 0.678... \tVal Loss: 0.655 \tVal Frac: 0.7879\n",
      "\tEpoch: 420/1000; 3.39 sec... \tStep: 101220... \tLoss: 0.694... \tVal Loss: 0.656 \tVal Frac: 0.7835\n",
      "\tEpoch: 425/1000; 3.48 sec... \tStep: 102425... \tLoss: 0.792... \tVal Loss: 0.659 \tVal Frac: 0.7898\n",
      "\tEpoch: 430/1000; 3.46 sec... \tStep: 103630... \tLoss: 0.697... \tVal Loss: 0.688 \tVal Frac: 0.7716\n",
      "\tEpoch: 435/1000; 3.32 sec... \tStep: 104835... \tLoss: 0.721... \tVal Loss: 0.667 \tVal Frac: 0.7805\n",
      "\tEpoch: 440/1000; 3.41 sec... \tStep: 106040... \tLoss: 0.689... \tVal Loss: 0.667 \tVal Frac: 0.7809\n",
      "\tEpoch: 445/1000; 3.56 sec... \tStep: 107245... \tLoss: 1.030... \tVal Loss: 0.649 \tVal Frac: 0.7790\n",
      "\tEpoch: 450/1000; 3.41 sec... \tStep: 108450... \tLoss: 0.791... \tVal Loss: 0.645 \tVal Frac: 0.7879\n",
      "\tEpoch: 455/1000; 3.48 sec... \tStep: 109655... \tLoss: 0.475... \tVal Loss: 0.638 \tVal Frac: 0.7798\n",
      "\tEpoch: 460/1000; 3.54 sec... \tStep: 110860... \tLoss: 0.741... \tVal Loss: 0.650 \tVal Frac: 0.7805\n",
      "\tEpoch: 465/1000; 3.55 sec... \tStep: 112065... \tLoss: 0.913... \tVal Loss: 0.636 \tVal Frac: 0.8028\n",
      "\tEpoch: 470/1000; 3.46 sec... \tStep: 113270... \tLoss: 0.789... \tVal Loss: 0.658 \tVal Frac: 0.7879\n",
      "\tEpoch: 475/1000; 3.48 sec... \tStep: 114475... \tLoss: 0.686... \tVal Loss: 0.621 \tVal Frac: 0.7976\n",
      "\tValidation loss decreased (0.628 --> 0.621).  Saving model ...\n",
      "\tEpoch: 480/1000; 3.46 sec... \tStep: 115680... \tLoss: 0.957... \tVal Loss: 0.650 \tVal Frac: 0.7924\n",
      "\tEpoch: 485/1000; 3.46 sec... \tStep: 116885... \tLoss: 0.930... \tVal Loss: 0.653 \tVal Frac: 0.8010\n",
      "\tEpoch: 490/1000; 3.49 sec... \tStep: 118090... \tLoss: 0.819... \tVal Loss: 0.639 \tVal Frac: 0.7857\n",
      "\tEpoch: 495/1000; 3.34 sec... \tStep: 119295... \tLoss: 0.653... \tVal Loss: 0.647 \tVal Frac: 0.7853\n",
      "\tEpoch: 500/1000; 3.36 sec... \tStep: 120500... \tLoss: 0.605... \tVal Loss: 0.648 \tVal Frac: 0.7876\n",
      "\tEpoch: 505/1000; 3.54 sec... \tStep: 121705... \tLoss: 0.686... \tVal Loss: 0.658 \tVal Frac: 0.7898\n",
      "\tEpoch: 510/1000; 3.49 sec... \tStep: 122910... \tLoss: 0.805... \tVal Loss: 0.665 \tVal Frac: 0.7790\n",
      "\tEpoch: 515/1000; 3.59 sec... \tStep: 124115... \tLoss: 0.490... \tVal Loss: 0.628 \tVal Frac: 0.7924\n",
      "\tEpoch: 520/1000; 3.59 sec... \tStep: 125320... \tLoss: 0.515... \tVal Loss: 0.641 \tVal Frac: 0.7798\n",
      "\tEpoch: 525/1000; 3.46 sec... \tStep: 126525... \tLoss: 0.720... \tVal Loss: 0.633 \tVal Frac: 0.7961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 530/1000; 3.49 sec... \tStep: 127730... \tLoss: 0.714... \tVal Loss: 0.620 \tVal Frac: 0.7928\n",
      "\tValidation loss decreased (0.621 --> 0.620).  Saving model ...\n",
      "\tEpoch: 535/1000; 3.58 sec... \tStep: 128935... \tLoss: 0.652... \tVal Loss: 0.635 \tVal Frac: 0.7917\n",
      "\tEpoch: 540/1000; 3.46 sec... \tStep: 130140... \tLoss: 0.850... \tVal Loss: 0.651 \tVal Frac: 0.7861\n",
      "\tEpoch: 545/1000; 3.45 sec... \tStep: 131345... \tLoss: 0.784... \tVal Loss: 0.632 \tVal Frac: 0.7928\n",
      "\tEpoch: 550/1000; 3.51 sec... \tStep: 132550... \tLoss: 0.850... \tVal Loss: 0.630 \tVal Frac: 0.7827\n",
      "\tEpoch: 555/1000; 3.42 sec... \tStep: 133755... \tLoss: 1.168... \tVal Loss: 0.610 \tVal Frac: 0.8036\n",
      "\tValidation loss decreased (0.620 --> 0.610).  Saving model ...\n",
      "\tEpoch: 560/1000; 3.56 sec... \tStep: 134960... \tLoss: 0.785... \tVal Loss: 0.646 \tVal Frac: 0.7868\n",
      "\tEpoch: 565/1000; 3.55 sec... \tStep: 136165... \tLoss: 0.949... \tVal Loss: 0.640 \tVal Frac: 0.7943\n",
      "\tEpoch: 570/1000; 3.52 sec... \tStep: 137370... \tLoss: 0.850... \tVal Loss: 0.669 \tVal Frac: 0.7798\n",
      "\tEpoch: 575/1000; 3.56 sec... \tStep: 138575... \tLoss: 0.587... \tVal Loss: 0.641 \tVal Frac: 0.7943\n",
      "\tEpoch: 580/1000; 3.49 sec... \tStep: 139780... \tLoss: 0.459... \tVal Loss: 0.675 \tVal Frac: 0.7779\n",
      "\tEpoch: 585/1000; 3.41 sec... \tStep: 140985... \tLoss: 0.880... \tVal Loss: 0.659 \tVal Frac: 0.7760\n",
      "\tEpoch: 590/1000; 3.43 sec... \tStep: 142190... \tLoss: 0.930... \tVal Loss: 0.647 \tVal Frac: 0.7764\n",
      "\tEpoch: 595/1000; 3.54 sec... \tStep: 143395... \tLoss: 0.452... \tVal Loss: 0.639 \tVal Frac: 0.7935\n",
      "\tEpoch: 600/1000; 3.42 sec... \tStep: 144600... \tLoss: 0.719... \tVal Loss: 0.640 \tVal Frac: 0.7913\n",
      "\tEpoch: 605/1000; 3.48 sec... \tStep: 145805... \tLoss: 0.743... \tVal Loss: 0.647 \tVal Frac: 0.7958\n",
      "\tEpoch: 610/1000; 3.42 sec... \tStep: 147010... \tLoss: 0.844... \tVal Loss: 0.650 \tVal Frac: 0.7827\n",
      "\tEpoch: 615/1000; 3.56 sec... \tStep: 148215... \tLoss: 0.625... \tVal Loss: 0.652 \tVal Frac: 0.7842\n",
      "\tEpoch: 620/1000; 3.44 sec... \tStep: 149420... \tLoss: 0.566... \tVal Loss: 0.626 \tVal Frac: 0.7987\n",
      "\tEpoch: 625/1000; 3.48 sec... \tStep: 150625... \tLoss: 0.480... \tVal Loss: 0.648 \tVal Frac: 0.7991\n",
      "\tEpoch: 630/1000; 3.46 sec... \tStep: 151830... \tLoss: 0.668... \tVal Loss: 0.637 \tVal Frac: 0.7898\n",
      "\tEpoch: 635/1000; 3.45 sec... \tStep: 153035... \tLoss: 0.951... \tVal Loss: 0.637 \tVal Frac: 0.7928\n",
      "\tEpoch: 640/1000; 3.37 sec... \tStep: 154240... \tLoss: 0.612... \tVal Loss: 0.643 \tVal Frac: 0.7872\n",
      "\tEpoch: 645/1000; 3.35 sec... \tStep: 155445... \tLoss: 0.913... \tVal Loss: 0.640 \tVal Frac: 0.7894\n",
      "\tEpoch: 650/1000; 3.47 sec... \tStep: 156650... \tLoss: 0.911... \tVal Loss: 0.654 \tVal Frac: 0.7820\n",
      "\tEpoch: 655/1000; 3.41 sec... \tStep: 157855... \tLoss: 1.023... \tVal Loss: 0.632 \tVal Frac: 0.7839\n",
      "\tEpoch: 660/1000; 3.48 sec... \tStep: 159060... \tLoss: 0.632... \tVal Loss: 0.660 \tVal Frac: 0.7887\n",
      "\tEpoch: 665/1000; 3.42 sec... \tStep: 160265... \tLoss: 0.521... \tVal Loss: 0.648 \tVal Frac: 0.7846\n",
      "\tEpoch: 670/1000; 3.46 sec... \tStep: 161470... \tLoss: 0.437... \tVal Loss: 0.631 \tVal Frac: 0.7991\n",
      "\tEpoch: 675/1000; 3.44 sec... \tStep: 162675... \tLoss: 0.922... \tVal Loss: 0.652 \tVal Frac: 0.7872\n",
      "\tEpoch: 680/1000; 3.58 sec... \tStep: 163880... \tLoss: 0.805... \tVal Loss: 0.646 \tVal Frac: 0.7861\n",
      "\tEpoch: 685/1000; 3.44 sec... \tStep: 165085... \tLoss: 0.894... \tVal Loss: 0.642 \tVal Frac: 0.7883\n",
      "\tEpoch: 690/1000; 3.49 sec... \tStep: 166290... \tLoss: 0.821... \tVal Loss: 0.642 \tVal Frac: 0.7768\n",
      "\tEpoch: 695/1000; 3.46 sec... \tStep: 167495... \tLoss: 0.652... \tVal Loss: 0.652 \tVal Frac: 0.7842\n",
      "\tEpoch: 700/1000; 3.51 sec... \tStep: 168700... \tLoss: 0.435... \tVal Loss: 0.650 \tVal Frac: 0.7902\n",
      "\tEpoch: 705/1000; 3.48 sec... \tStep: 169905... \tLoss: 0.499... \tVal Loss: 0.633 \tVal Frac: 0.7909\n",
      "\tEpoch: 710/1000; 3.46 sec... \tStep: 171110... \tLoss: 0.580... \tVal Loss: 0.639 \tVal Frac: 0.7961\n",
      "\tEpoch: 715/1000; 3.56 sec... \tStep: 172315... \tLoss: 0.517... \tVal Loss: 0.650 \tVal Frac: 0.7935\n",
      "\tEpoch: 720/1000; 3.52 sec... \tStep: 173520... \tLoss: 0.590... \tVal Loss: 0.640 \tVal Frac: 0.7932\n",
      "\tEpoch: 725/1000; 3.48 sec... \tStep: 174725... \tLoss: 0.820... \tVal Loss: 0.644 \tVal Frac: 0.7861\n",
      "\tEpoch: 730/1000; 3.40 sec... \tStep: 175930... \tLoss: 0.977... \tVal Loss: 0.643 \tVal Frac: 0.7972\n",
      "\tEpoch: 735/1000; 3.43 sec... \tStep: 177135... \tLoss: 0.809... \tVal Loss: 0.680 \tVal Frac: 0.7920\n",
      "\tEpoch: 740/1000; 3.37 sec... \tStep: 178340... \tLoss: 0.819... \tVal Loss: 0.645 \tVal Frac: 0.8032\n",
      "\tEpoch: 745/1000; 3.41 sec... \tStep: 179545... \tLoss: 0.580... \tVal Loss: 0.640 \tVal Frac: 0.7943\n",
      "\tEpoch: 750/1000; 3.52 sec... \tStep: 180750... \tLoss: 0.643... \tVal Loss: 0.633 \tVal Frac: 0.8025\n",
      "\tEpoch: 755/1000; 3.54 sec... \tStep: 181955... \tLoss: 0.518... \tVal Loss: 0.624 \tVal Frac: 0.8043\n",
      "\tEpoch: 760/1000; 3.44 sec... \tStep: 183160... \tLoss: 0.739... \tVal Loss: 0.634 \tVal Frac: 0.7928\n",
      "\tEpoch: 765/1000; 3.41 sec... \tStep: 184365... \tLoss: 0.704... \tVal Loss: 0.634 \tVal Frac: 0.7876\n",
      "\tEpoch: 770/1000; 3.38 sec... \tStep: 185570... \tLoss: 0.554... \tVal Loss: 0.626 \tVal Frac: 0.7887\n",
      "\tEpoch: 775/1000; 3.46 sec... \tStep: 186775... \tLoss: 0.447... \tVal Loss: 0.634 \tVal Frac: 0.7913\n",
      "\tEpoch: 780/1000; 3.38 sec... \tStep: 187980... \tLoss: 0.612... \tVal Loss: 0.649 \tVal Frac: 0.7853\n",
      "\tEpoch: 785/1000; 3.44 sec... \tStep: 189185... \tLoss: 0.451... \tVal Loss: 0.632 \tVal Frac: 0.7872\n",
      "\tEpoch: 790/1000; 3.49 sec... \tStep: 190390... \tLoss: 0.609... \tVal Loss: 0.622 \tVal Frac: 0.7835\n",
      "\tEpoch: 795/1000; 3.49 sec... \tStep: 191595... \tLoss: 0.661... \tVal Loss: 0.665 \tVal Frac: 0.7656\n",
      "\tEpoch: 800/1000; 3.52 sec... \tStep: 192800... \tLoss: 0.413... \tVal Loss: 0.633 \tVal Frac: 0.7920\n",
      "\tEpoch: 805/1000; 3.42 sec... \tStep: 194005... \tLoss: 0.511... \tVal Loss: 0.636 \tVal Frac: 0.7790\n",
      "\tEpoch: 810/1000; 3.47 sec... \tStep: 195210... \tLoss: 0.259... \tVal Loss: 0.613 \tVal Frac: 0.8006\n",
      "\tEpoch: 815/1000; 3.62 sec... \tStep: 196415... \tLoss: 1.073... \tVal Loss: 0.631 \tVal Frac: 0.7887\n",
      "\tEpoch: 820/1000; 3.45 sec... \tStep: 197620... \tLoss: 0.620... \tVal Loss: 0.633 \tVal Frac: 0.7902\n",
      "\tEpoch: 825/1000; 3.39 sec... \tStep: 198825... \tLoss: 0.801... \tVal Loss: 0.642 \tVal Frac: 0.7906\n",
      "\tEpoch: 830/1000; 3.52 sec... \tStep: 200030... \tLoss: 0.485... \tVal Loss: 0.624 \tVal Frac: 0.7906\n",
      "\tEpoch: 835/1000; 3.49 sec... \tStep: 201235... \tLoss: 0.555... \tVal Loss: 0.627 \tVal Frac: 0.7894\n",
      "\tEpoch: 840/1000; 3.47 sec... \tStep: 202440... \tLoss: 1.131... \tVal Loss: 0.628 \tVal Frac: 0.7909\n",
      "\tEpoch: 845/1000; 3.46 sec... \tStep: 203645... \tLoss: 0.868... \tVal Loss: 0.621 \tVal Frac: 0.7909\n",
      "\tEpoch: 850/1000; 3.46 sec... \tStep: 204850... \tLoss: 0.810... \tVal Loss: 0.637 \tVal Frac: 0.7775\n",
      "\tEpoch: 855/1000; 3.49 sec... \tStep: 206055... \tLoss: 0.510... \tVal Loss: 0.640 \tVal Frac: 0.7835\n",
      "\tEpoch: 860/1000; 3.48 sec... \tStep: 207260... \tLoss: 0.718... \tVal Loss: 0.663 \tVal Frac: 0.7876\n",
      "\tEpoch: 865/1000; 3.52 sec... \tStep: 208465... \tLoss: 0.694... \tVal Loss: 0.645 \tVal Frac: 0.7786\n",
      "\tEpoch: 870/1000; 3.42 sec... \tStep: 209670... \tLoss: 0.761... \tVal Loss: 0.666 \tVal Frac: 0.7742\n",
      "\tEpoch: 875/1000; 3.52 sec... \tStep: 210875... \tLoss: 0.632... \tVal Loss: 0.670 \tVal Frac: 0.7850\n",
      "\tEpoch: 880/1000; 3.49 sec... \tStep: 212080... \tLoss: 0.408... \tVal Loss: 0.637 \tVal Frac: 0.7853\n",
      "\tEpoch: 885/1000; 3.63 sec... \tStep: 213285... \tLoss: 0.596... \tVal Loss: 0.629 \tVal Frac: 0.7972\n",
      "\tEpoch: 890/1000; 3.36 sec... \tStep: 214490... \tLoss: 1.157... \tVal Loss: 0.635 \tVal Frac: 0.7939\n",
      "\tEpoch: 895/1000; 3.44 sec... \tStep: 215695... \tLoss: 0.980... \tVal Loss: 0.692 \tVal Frac: 0.7623\n",
      "\tEpoch: 900/1000; 3.46 sec... \tStep: 216900... \tLoss: 0.696... \tVal Loss: 0.640 \tVal Frac: 0.7850\n",
      "\tEpoch: 905/1000; 3.41 sec... \tStep: 218105... \tLoss: 0.502... \tVal Loss: 0.643 \tVal Frac: 0.7987\n",
      "\tEpoch: 910/1000; 3.57 sec... \tStep: 219310... \tLoss: 0.709... \tVal Loss: 0.666 \tVal Frac: 0.7812\n",
      "\tEpoch: 915/1000; 3.56 sec... \tStep: 220515... \tLoss: 0.822... \tVal Loss: 0.660 \tVal Frac: 0.7842\n",
      "\tEpoch: 920/1000; 3.41 sec... \tStep: 221720... \tLoss: 0.909... \tVal Loss: 0.649 \tVal Frac: 0.7835\n",
      "\tEpoch: 925/1000; 3.39 sec... \tStep: 222925... \tLoss: 0.944... \tVal Loss: 0.617 \tVal Frac: 0.7972\n",
      "\tEpoch: 930/1000; 3.48 sec... \tStep: 224130... \tLoss: 0.787... \tVal Loss: 0.635 \tVal Frac: 0.7917\n",
      "\tEpoch: 935/1000; 3.46 sec... \tStep: 225335... \tLoss: 0.678... \tVal Loss: 0.643 \tVal Frac: 0.7898\n",
      "\tEpoch: 940/1000; 3.39 sec... \tStep: 226540... \tLoss: 0.660... \tVal Loss: 0.636 \tVal Frac: 0.7850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 945/1000; 3.48 sec... \tStep: 227745... \tLoss: 0.893... \tVal Loss: 0.644 \tVal Frac: 0.7790\n",
      "\tEpoch: 950/1000; 3.51 sec... \tStep: 228950... \tLoss: 0.465... \tVal Loss: 0.636 \tVal Frac: 0.7839\n",
      "\tEpoch: 955/1000; 3.43 sec... \tStep: 230155... \tLoss: 0.552... \tVal Loss: 0.636 \tVal Frac: 0.7868\n",
      "\tEpoch: 960/1000; 3.51 sec... \tStep: 231360... \tLoss: 0.961... \tVal Loss: 0.624 \tVal Frac: 0.7965\n",
      "\tEpoch: 965/1000; 3.45 sec... \tStep: 232565... \tLoss: 0.665... \tVal Loss: 0.626 \tVal Frac: 0.7961\n",
      "\tEpoch: 970/1000; 3.35 sec... \tStep: 233770... \tLoss: 0.685... \tVal Loss: 0.649 \tVal Frac: 0.7842\n",
      "\tEpoch: 975/1000; 3.50 sec... \tStep: 234975... \tLoss: 0.466... \tVal Loss: 0.652 \tVal Frac: 0.7865\n",
      "\tEpoch: 980/1000; 3.41 sec... \tStep: 236180... \tLoss: 0.728... \tVal Loss: 0.609 \tVal Frac: 0.8058\n",
      "\tValidation loss decreased (0.610 --> 0.609).  Saving model ...\n",
      "\tEpoch: 985/1000; 3.49 sec... \tStep: 237385... \tLoss: 0.806... \tVal Loss: 0.651 \tVal Frac: 0.7824\n",
      "\tEpoch: 990/1000; 3.41 sec... \tStep: 238590... \tLoss: 0.791... \tVal Loss: 0.646 \tVal Frac: 0.7839\n",
      "\tEpoch: 995/1000; 3.41 sec... \tStep: 239795... \tLoss: 1.003... \tVal Loss: 0.651 \tVal Frac: 0.7961\n",
      "\tEpoch: 1000/1000; 3.43 sec... \tStep: 241000... \tLoss: 0.525... \tVal Loss: 0.614 \tVal Frac: 0.7924\n",
      "Completed:  n_windows :  250 \n",
      "\tloss: 0.609275 \n",
      "\tfrac:0.805804\n"
     ]
    }
   ],
   "source": [
    "for key in sweeps.keys():\n",
    "    for param_val in sweeps[key]:\n",
    "        n.random.seed(2358)\n",
    "        torch.manual_seed(2358)\n",
    "        params = copy.copy(params_original)\n",
    "        params[key] = param_val\n",
    "        \n",
    "        print(\"\\n###########\\nTesting with \", key, \": \", param_val)\n",
    "        \n",
    "        params_result, model = test_model(params, datas, device, int(n.random.random()*1e9))\n",
    "        \n",
    "        results.append((copy.deepcopy(params_result), copy.deepcopy(model)))\n",
    "        \n",
    "        print(\"Completed: \", key, \": \", param_val, \"\\n\\tloss: {:.6f} \\n\\tfrac:{:.6f}\".format(params_result['valid_loss_min'], params_result['valid_frac_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "result loss: 0.617172 \n",
      "result frac:0.801838\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 10, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.6171717030160567, 'valid_frac_max': 0.8018382352941177, 'val_losses': [1.0371554816470427, 0.9868090201826657, 0.8191421533332152, 1.2353495829245624, 0.8405808357631459, 1.2118288362727445, 0.9042030025930966, 0.7698272445622613, 1.0254128364955677, 1.0769603785346535, 0.9242537239018609, 0.9490161913282731, 0.8094653206713059, 1.1695227756219753, 1.4808050323935116, 0.8298907511374529, 0.9736046454485725, 0.7815301930203157, 0.7439049420987859, 0.9772262187565074, 0.7770780458169825, 0.9203896964297575, 1.0271419784601996, 0.8474342451376073, 0.7344633074367748, 0.7323199777042164, 0.8473169319769915, 0.7586980237680323, 0.8205578453400556, 0.7870850892627941, 0.9184956424376544, 0.7158056613276986, 0.8901391337899601, 0.7890324371702531, 0.8431792890324312, 0.7379274767987868, 0.7411986529827118, 0.8345468608772053, 1.2194106284309836, 0.7414138548514422, 0.720219074978548, 0.7317240259226631, 0.7459379662485683, 0.6215860608746023, 0.8397167048033546, 0.8435065634110395, 0.8363493382930756, 0.8474602089208715, 0.7142530181828667, 0.7426736768554238, 0.8654574089190539, 0.6999588647309472, 0.7163677625796374, 0.7438632586423088, 0.7103065445142633, 0.8163168100749745, 0.7104431169874528, 0.8215773294953739, 0.8457250728326685, 0.7272545001086067, 0.7577859257950502, 0.7152448797927183, 0.6822367170277763, 0.7157107794986052, 0.796879289781346, 0.744409905461704, 0.6896284822155447, 0.8385816956267638, 0.7269851309411666, 0.7904100709101733, 0.7873519571388469, 0.8481996981536641, 0.9177358241642223, 0.8566252385868746, 0.7170803764287164, 0.7351051120197072, 1.033724453869988, 0.7390399666393505, 0.9284658523166881, 0.7183480108485503, 0.710452785912682, 0.7465657244710361, 0.6893252975800458, 0.7177348746972926, 0.7115860076511608, 0.7894765527809368, 0.7383559777456171, 0.688442471448113, 0.7037806900108562, 0.7610229600878323, 0.6705631417386672, 0.7108229268999661, 0.6865710710777956, 0.684550282534431, 0.6977831377702601, 0.7402087457039777, 0.8256459902314579, 0.7937603312380174, 0.6958734599982991, 0.7352984337245717, 0.7315708384794347, 0.7707412891528186, 0.6702263243058149, 0.6838271235718446, 0.7963381272905014, 0.6422309468774234, 0.7621213842840756, 0.9610318429329816, 0.7642759102232316, 0.6845208795631633, 0.696840528530233, 0.6879435332382426, 0.7108562395853155, 0.6903051344787373, 0.6800670511582319, 0.7636667262105381, 0.6669975368415608, 0.7141583088566276, 0.7649686501306646, 0.8945357897702385, 0.6800749813809114, 0.7178260284311632, 0.7309959036462447, 0.7249459168490242, 0.6554487028542687, 0.6730947566383025, 0.6518032783971114, 0.7058524489402771, 0.6683863590745365, 0.7832794533056371, 0.6596868083757512, 0.674513804211336, 0.8014653265476227, 0.8902495650684132, 1.147560057219337, 0.7858133438755485, 0.6815742398009581, 0.9285113229471095, 0.752722838345696, 0.8220778847441954, 0.8524277813294354, 0.7283003652797025, 0.7673916350392734, 0.8957940445226782, 0.833129574621425, 0.7565617820795845, 0.7300939917564392, 0.7233505234998815, 0.7671826317029841, 0.7673285151229186, 0.9521277413648718, 0.6602424407706541, 0.6793579522301169, 0.646178961501402, 0.642963981628418, 0.6525374088217231, 0.7171666885123533, 0.7717084218473995, 0.6698705799439374, 0.6211496873813517, 0.6761456186280531, 0.6773252620416529, 0.6698348939418792, 0.6613492106690126, 0.645948945424136, 0.6171717030160567, 0.648147430490045, 0.6966216437956866, 0.7253076995120329, 0.6422222161994261, 0.8333874520133523, 0.731617668095757, 0.7575140812817742, 0.7005605418892468, 0.6777587164850796, 0.6201893759124419, 0.6817701427375569, 0.7130250972859999, 0.8177561619702507, 0.7079809739309199, 0.7304688306415782, 0.679005991010105, 0.6743927643579595, 0.698384519184337, 0.676319915582152, 0.6729516081950244, 0.7031275198740118, 0.7680454040274901, 0.6883874987854677, 0.6798304119530846, 0.6804664699470295, 0.7386529165155747, 0.6956235857570873, 0.771515959150651, 0.9632547287379994, 0.6659873864229988, 0.6835588634014129, 0.7029816939550287, 0.6907408165581086, 0.6728217040791231], 'val_fracs': [0.7426470588235294, 0.7268382352941176, 0.7514705882352941, 0.7095588235294118, 0.7386029411764706, 0.7227941176470588, 0.7121323529411765, 0.7389705882352942, 0.7091911764705883, 0.7294117647058823, 0.7003676470588235, 0.7371323529411765, 0.7761029411764706, 0.42426470588235293, 0.3426470588235294, 0.7242647058823529, 0.6757352941176471, 0.7900735294117647, 0.7816176470588235, 0.7290441176470588, 0.7580882352941176, 0.7286764705882353, 0.7025735294117647, 0.7400735294117647, 0.7757352941176471, 0.7551470588235294, 0.7375, 0.7540441176470588, 0.7522058823529412, 0.7470588235294118, 0.7444852941176471, 0.7772058823529412, 0.711764705882353, 0.7544117647058823, 0.7375, 0.7819852941176471, 0.7716911764705883, 0.7555147058823529, 0.48823529411764705, 0.7522058823529412, 0.7838235294117647, 0.7617647058823529, 0.7419117647058824, 0.8018382352941177, 0.7481617647058824, 0.7544117647058823, 0.78125, 0.7834558823529412, 0.7886029411764706, 0.7863970588235294, 0.7433823529411765, 0.7841911764705882, 0.7805147058823529, 0.7878676470588235, 0.7904411764705882, 0.7488970588235294, 0.7871323529411764, 0.7647058823529411, 0.7448529411764706, 0.7786764705882353, 0.7834558823529412, 0.7463235294117647, 0.774264705882353, 0.7628676470588235, 0.7488970588235294, 0.7525735294117647, 0.7720588235294118, 0.7735294117647059, 0.7724264705882353, 0.7172794117647059, 0.7433823529411765, 0.7481617647058824, 0.7496323529411765, 0.7485294117647059, 0.7849264705882353, 0.7625, 0.7158088235294118, 0.7628676470588235, 0.7551470588235294, 0.7669117647058824, 0.7507352941176471, 0.7455882352941177, 0.7852941176470588, 0.7665441176470589, 0.7834558823529412, 0.7470588235294118, 0.774264705882353, 0.7871323529411764, 0.7761029411764706, 0.7658088235294118, 0.7863970588235294, 0.794485294117647, 0.7863970588235294, 0.7930147058823529, 0.7948529411764705, 0.7816176470588235, 0.7897058823529411, 0.7400735294117647, 0.7856617647058823, 0.7889705882352941, 0.7783088235294118, 0.7290441176470588, 0.7639705882352941, 0.7643382352941176, 0.7797794117647059, 0.7988970588235295, 0.7536764705882353, 0.694485294117647, 0.7919117647058823, 0.7926470588235294, 0.7915441176470588, 0.7886029411764706, 0.7599264705882353, 0.7841911764705882, 0.7926470588235294, 0.7408088235294118, 0.7981617647058824, 0.7577205882352941, 0.75, 0.6764705882352942, 0.7790441176470588, 0.7485294117647059, 0.7459558823529412, 0.7746323529411765, 0.79375, 0.7889705882352941, 0.7830882352941176, 0.7841911764705882, 0.7897058823529411, 0.7595588235294117, 0.7838235294117647, 0.7488970588235294, 0.7691176470588236, 0.7415441176470589, 0.4077205882352941, 0.7474264705882353, 0.7819852941176471, 0.75, 0.7893382352941176, 0.7672794117647059, 0.7636029411764705, 0.7731617647058824, 0.7838235294117647, 0.7253676470588235, 0.78125, 0.7867647058823529, 0.7948529411764705, 0.7922794117647058, 0.7930147058823529, 0.7617647058823529, 0.79375, 0.7941176470588235, 0.7900735294117647, 0.7963235294117647, 0.7867647058823529, 0.7922794117647058, 0.7860294117647059, 0.7727941176470589, 0.7547794117647059, 0.7959558823529411, 0.7915441176470588, 0.7875, 0.7819852941176471, 0.7911764705882353, 0.7911764705882353, 0.8, 0.7974264705882353, 0.7830882352941176, 0.7819852941176471, 0.8003676470588236, 0.7669117647058824, 0.7779411764705882, 0.7698529411764706, 0.7764705882352941, 0.7900735294117647, 0.7963235294117647, 0.7900735294117647, 0.7919117647058823, 0.7327205882352941, 0.7518382352941176, 0.768014705882353, 0.7911764705882353, 0.7746323529411765, 0.775, 0.7698529411764706, 0.7724264705882353, 0.7841911764705882, 0.7573529411764706, 0.7856617647058823, 0.7878676470588235, 0.7900735294117647, 0.768014705882353, 0.7753676470588236, 0.7797794117647059, 0.7628676470588235, 0.7933823529411764, 0.7727941176470589, 0.7654411764705882, 0.7665441176470589, 0.7856617647058823], 'pred_labels_best': array([7, 6, 6, 6, 6, 7, 6, 7, 6, 6, 6, 6, 6, 7, 7, 6, 6, 6, 6, 6, 7, 6,\n",
      "       3, 6, 6, 7, 6, 6, 6, 6, 3, 6], dtype=int64), 'true_labels_best': array([7, 6, 6, 6, 6, 7, 6, 2, 6, 6, 6, 6, 6, 7, 7, 6, 6, 6, 6, 6, 3, 6,\n",
      "       5, 6, 6, 7, 6, 6, 6, 6, 7, 6])}\n",
      "########\n",
      "result loss: 0.609183 \n",
      "result frac:0.809926\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 20, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.6091828984372756, 'valid_frac_max': 0.8099264705882353, 'val_losses': [1.062409574845258, 0.9088914997437421, 1.0584523705875173, 0.7194545970243567, 0.9021200246670666, 0.7628433451933019, 0.8853750951149885, 0.7705291369382072, 0.8796536214211408, 0.9110520552186405, 0.7129999118692735, 0.708727566985523, 0.6868811120005215, 0.7453036280239329, 0.718838674881879, 0.7680923921220443, 0.7555990822174969, 0.7134358739151674, 0.6461110514753005, 0.6719291325877694, 0.729844639932408, 0.7768485423396615, 0.9344224368824678, 0.705387513777789, 0.7711155312902788, 0.9407608782543856, 0.7167150718324324, 0.7090894208234899, 0.6353497340398676, 0.7296494245529175, 0.6679925778332878, 0.694084513538024, 0.6919694244861603, 0.6689090371131897, 0.8227083227213692, 0.6912397591506734, 0.7479768290239222, 0.7009724795818328, 0.8172850861268885, 0.6907756631865221, 0.6790295348447912, 0.7433013141155242, 0.7856103592059192, 0.6826255765031366, 0.7372361256795771, 0.68682201294338, 0.6828983280588599, 0.644962002249325, 0.6853835765053244, 0.659822282370399, 0.7258682976750767, 0.6252440065145493, 0.6688131903900819, 0.7735678728889016, 0.6430135397350087, 0.6429962298449348, 0.6828582809251897, 0.6592932711629307, 0.6273859883055968, 0.6119836779201732, 0.6478478684144862, 0.6588242076775607, 0.6239005809321123, 0.6753471788238077, 0.649536361063228, 0.6714701536823722, 0.6678379518144271, 0.6807512686533086, 0.6453257013769711, 0.7790544958675609, 0.8276879527989556, 0.6741905966225792, 0.651377513478784, 0.6163301858831854, 0.633363165750223, 0.7229446127134211, 0.6335352385745329, 0.6498762043083415, 0.6321906622718362, 0.6841365125249413, 0.6802349518327152, 0.837293251472361, 0.7107791385229896, 0.6730194950804991, 0.7186811776722178, 0.6918435797971838, 0.6590568829985226, 0.6849557248985066, 0.6561669475892011, 0.7153868945205912, 0.737221293589648, 0.7293009323232315, 0.6711482686154983, 0.6698943495750427, 0.6448684085817898, 0.7229085266590118, 0.7009551882743835, 0.6745456842815175, 0.6139156611526714, 0.6399694293737411, 0.6630067341467913, 0.6848621897837694, 0.8194967613500708, 0.6559624864774591, 0.6677035591181587, 0.6516321666100446, 0.644946117962108, 0.7313552397138933, 0.6840672564857146, 0.6664895815007826, 0.6836291011642007, 0.6826249490765964, 0.6488917413879843, 0.7001923590898513, 0.6492787894080667, 0.6679892541731105, 0.6458621494910296, 0.6790082181201261, 0.6373566855402554, 0.6386091277879827, 0.6856331983033348, 0.653149545543334, 0.6446734803564408, 0.6732392714304083, 0.662869259890388, 0.6552357254659429, 0.7010918922284071, 0.641107482068679, 0.6393804294221541, 0.6203469798845404, 0.7004467077115003, 0.6301578227211447, 0.6385366718558704, 0.622567279549206, 0.6353277644690345, 0.6518664044492385, 0.6550817296785467, 0.6299043122459861, 0.6165929047500386, 0.6688355975291308, 0.6469207539277918, 0.6257456767208436, 0.6222575359484729, 0.6382679998874664, 0.6484687815694248, 0.6438459031722125, 0.6219219481243806, 0.6329272399930393, 0.6444281521965476, 0.6409147592151866, 0.634610314579571, 0.6825849252588608, 0.6873260294689851, 0.6405121259829577, 0.6418306897668278, 0.6270543982000912, 0.6259049191194422, 0.6296981394290924, 0.7160233685198952, 0.6489772004239699, 0.638038960274528, 0.664549546732622, 0.661444247119567, 0.6705023292232962, 0.6893506404231576, 0.6647603273391723, 0.6770374476909637, 0.6209617236081292, 0.6740573995253619, 0.6253725690000197, 0.628428668660276, 0.6444229217136608, 0.648265162636252, 0.6350447570576387, 0.6715275760959176, 0.6460835744352902, 0.6304143176359289, 0.6163597485598395, 0.6237118170541875, 0.62792607125114, 0.6350268921431373, 0.6362989548374625, 0.7578165913329405, 0.6656562871792737, 0.6368147576556487, 0.6168345477651147, 0.6204910762169782, 0.6435302075217751, 0.6473029820358052, 0.6872400094481076, 0.6283992397434571, 0.6443374542629018, 0.6224033217219745, 0.6091828984372756, 0.657107168786666, 0.6562265869449166, 0.6364443617708543, 0.6346489937866435, 0.6477823983220493, 0.6201199135359596], 'val_fracs': [0.7345588235294118, 0.7290441176470588, 0.7290441176470588, 0.756985294117647, 0.7275735294117647, 0.7772058823529412, 0.725, 0.7672794117647059, 0.7544117647058823, 0.725, 0.7819852941176471, 0.7683823529411765, 0.7911764705882353, 0.7555147058823529, 0.7761029411764706, 0.7511029411764706, 0.7610294117647058, 0.7709558823529412, 0.7893382352941176, 0.7628676470588235, 0.775, 0.7488970588235294, 0.7444852941176471, 0.7889705882352941, 0.7584558823529411, 0.7525735294117647, 0.7801470588235294, 0.7783088235294118, 0.794485294117647, 0.7860294117647059, 0.7731617647058824, 0.7900735294117647, 0.7959558823529411, 0.7959558823529411, 0.7757352941176471, 0.7676470588235295, 0.75625, 0.7555147058823529, 0.7580882352941176, 0.7790441176470588, 0.7676470588235295, 0.7772058823529412, 0.7621323529411764, 0.7849264705882353, 0.7794117647058824, 0.7996323529411765, 0.756985294117647, 0.7753676470588236, 0.7481617647058824, 0.7886029411764706, 0.7625, 0.8029411764705883, 0.7577205882352941, 0.7786764705882353, 0.7849264705882353, 0.78125, 0.7783088235294118, 0.7731617647058824, 0.7941176470588235, 0.8018382352941177, 0.7816176470588235, 0.7797794117647059, 0.7933823529411764, 0.7904411764705882, 0.7779411764705882, 0.7790441176470588, 0.7893382352941176, 0.7926470588235294, 0.7919117647058823, 0.7525735294117647, 0.7753676470588236, 0.7650735294117647, 0.7724264705882353, 0.8066176470588236, 0.8025735294117647, 0.7856617647058823, 0.7713235294117647, 0.7897058823529411, 0.7911764705882353, 0.7683823529411765, 0.7823529411764706, 0.7577205882352941, 0.7985294117647059, 0.7783088235294118, 0.7731617647058824, 0.7816176470588235, 0.7908088235294117, 0.7933823529411764, 0.7613970588235294, 0.7727941176470589, 0.7386029411764706, 0.7588235294117647, 0.7779411764705882, 0.7768382352941177, 0.7841911764705882, 0.7408088235294118, 0.7606617647058823, 0.763235294117647, 0.7897058823529411, 0.7849264705882353, 0.7775735294117647, 0.7676470588235295, 0.7772058823529412, 0.7952205882352941, 0.7511029411764706, 0.7911764705882353, 0.7713235294117647, 0.7779411764705882, 0.7830882352941176, 0.7816176470588235, 0.7786764705882353, 0.7738970588235294, 0.7897058823529411, 0.75, 0.7783088235294118, 0.7878676470588235, 0.7915441176470588, 0.7889705882352941, 0.7908088235294117, 0.7779411764705882, 0.7738970588235294, 0.7904411764705882, 0.7613970588235294, 0.7584558823529411, 0.7867647058823529, 0.7930147058823529, 0.7323529411764705, 0.7988970588235295, 0.7970588235294118, 0.8099264705882353, 0.7669117647058824, 0.8011029411764706, 0.7841911764705882, 0.8, 0.7963235294117647, 0.7849264705882353, 0.7974264705882353, 0.7897058823529411, 0.8047794117647059, 0.7735294117647059, 0.7628676470588235, 0.7647058823529411, 0.7867647058823529, 0.7955882352941176, 0.7625, 0.7893382352941176, 0.7871323529411764, 0.7970588235294118, 0.7889705882352941, 0.7746323529411765, 0.7694852941176471, 0.7823529411764706, 0.7606617647058823, 0.7889705882352941, 0.7970588235294118, 0.7970588235294118, 0.8040441176470589, 0.8069852941176471, 0.7720588235294118, 0.8029411764705883, 0.794485294117647, 0.7845588235294118, 0.7805147058823529, 0.7738970588235294, 0.7636029411764705, 0.7522058823529412, 0.7599264705882353, 0.7966911764705882, 0.7790441176470588, 0.7952205882352941, 0.7933823529411764, 0.7863970588235294, 0.7808823529411765, 0.7849264705882353, 0.7838235294117647, 0.794485294117647, 0.7911764705882353, 0.7985294117647059, 0.7738970588235294, 0.7863970588235294, 0.7852941176470588, 0.7830882352941176, 0.7297794117647058, 0.7665441176470589, 0.7724264705882353, 0.7676470588235295, 0.7985294117647059, 0.7805147058823529, 0.7922794117647058, 0.7860294117647059, 0.79375, 0.8022058823529412, 0.8018382352941177, 0.8003676470588236, 0.7930147058823529, 0.7970588235294118, 0.7981617647058824, 0.8011029411764706, 0.7783088235294118, 0.7886029411764706], 'pred_labels_best': array([6, 6, 6, 2, 7, 6, 7, 7, 6, 7, 6, 7, 3, 6, 6, 6, 7, 6, 6, 6, 1, 7,\n",
      "       6, 2, 6, 6, 2, 6, 6, 6, 6, 6], dtype=int64), 'true_labels_best': array([6, 6, 6, 7, 7, 6, 7, 7, 6, 7, 7, 7, 2, 6, 4, 6, 7, 6, 6, 6, 7, 7,\n",
      "       6, 2, 6, 6, 5, 6, 6, 6, 6, 6])}\n",
      "########\n",
      "result loss: 0.545355 \n",
      "result frac:0.827941\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 100, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.5453551373061012, 'valid_frac_max': 0.8279411764705882, 'val_losses': [0.6683553650098688, 0.7029738664627075, 0.7143491408404182, 0.6712854388882132, 0.6275970490539775, 0.6477491906460594, 0.6376923943267149, 0.5877260215142194, 0.606463066795293, 0.6555101980181302, 0.6327571248306948, 0.7782106620423934, 0.6353795780855067, 0.6028084797017714, 0.6331694099832984, 0.6084229146733003, 0.6100835098939783, 0.6180876777452581, 0.6300643121494967, 0.5846407422248054, 0.6646193925072165, 0.5781076827469994, 0.5954851010266472, 0.5856840898008907, 0.6067777805468615, 0.6194981859010809, 0.6270240496186649, 0.6529547628234414, 0.5975703909116633, 0.5964713941602146, 0.6000556200742722, 0.591392384557163, 0.5945927451638614, 0.6084847313516281, 0.6057726200889139, 0.5794152380789027, 0.584867503713159, 0.6451304400668425, 0.5899561752291287, 0.5900744180468952, 0.5691823456217261, 0.6092736826223486, 0.587991814402973, 0.6017813123324338, 0.6039584287825752, 0.6142532513422124, 0.5804935921640957, 0.6166818552157458, 0.5931651325786815, 0.598270942884333, 0.5886964259778752, 0.5780350069789325, 0.606727892686339, 0.6289085454800549, 0.6479141382610096, 0.5793215036392212, 0.6022499785703771, 0.5883005808381473, 0.588825603092418, 0.6003353536128998, 0.6201237627688576, 0.6030906670233782, 0.5731632127481349, 0.6141476864323897, 0.5900881087078768, 0.5822580120142768, 0.5787476106601603, 0.5759565777638379, 0.5728238375747905, 0.5693058154162238, 0.5761904125704485, 0.5683391670970356, 0.5773797266623553, 0.5749723215313519, 0.6073195434668485, 0.6110740970162785, 0.5774628150112489, 0.5861348169691423, 0.6066752710763146, 0.5973598320256261, 0.6008884373833151, 0.6159652888774871, 0.5638908621142892, 0.6130172953886144, 0.5890041847439373, 0.5821623137768577, 0.5766616400550394, 0.5861889499075272, 0.5655829387552598, 0.5818217729820925, 0.5737695697475882, 0.5714357127161587, 0.5844001089825349, 0.6081099036861869, 0.5881433041656718, 0.5704329622142456, 0.5775141263709349, 0.6008714051807628, 0.572075454101843, 0.5828738787594964, 0.5678146174725365, 0.5595229611677282, 0.5796016810571446, 0.5688888511236976, 0.5862355042906369, 0.5944692229523378, 0.5866699111812255, 0.5881171983831069, 0.572833356611869, 0.5943397834020503, 0.580121358352549, 0.5502540604156606, 0.5507310958469616, 0.5715231674558976, 0.5585651162792654, 0.5649202075074701, 0.579200834386489, 0.5775498996762668, 0.5593009214190876, 0.5976814610116622, 0.5886588711948956, 0.5730821434189292, 0.5855531376950881, 0.5758857602582258, 0.5922376569579629, 0.6156085328144185, 0.5859024412491742, 0.5844645861317129, 0.5710982291137471, 0.5965450222001356, 0.5663711461950751, 0.5937977519105463, 0.5880478306728251, 0.5882176902364282, 0.6046837726060081, 0.5763190683196573, 0.595259156998466, 0.5833931514445473, 0.5918143374078414, 0.605072978664847, 0.5784430147970424, 0.5912289956036736, 0.5748194796197554, 0.5707038407816606, 0.5765234929673811, 0.5923673359786763, 0.5770836381351246, 0.5967699897639892, 0.5744711641003104, 0.561926193447674, 0.5651455877458348, 0.5944590449333191, 0.5692441733444438, 0.5746187357341542, 0.5615578619872823, 0.5840770244598389, 0.5724094618769253, 0.5738309316775378, 0.5693774447721593, 0.5678991408909069, 0.5455440140822354, 0.5647548216230729, 0.5866620919283698, 0.6177402634831036, 0.6124056556645562, 0.5743811353164561, 0.5727168989532134, 0.5744235624285305, 0.5979894245372099, 0.5877002141054939, 0.5865159471245373, 0.5871034424094593, 0.5851668971426347, 0.5872427179532893, 0.5875238877885481, 0.5991929911515292, 0.5708374749211704, 0.5665868934463052, 0.5826021983343013, 0.5898086546098484, 0.5575848687221022, 0.5595635138890322, 0.5572268103851992, 0.5652372793239706, 0.6019799640950034, 0.5749452233314514, 0.5930333205882241, 0.5642038057832157, 0.5866512978778166, 0.5696483576998991, 0.5717850432676428, 0.5805129543823354, 0.5531688034534454, 0.566186902102302, 0.5659729766495087, 0.5634775915566612, 0.5541924828992171, 0.55255106038907, 0.5467532147379482, 0.5453551373061012], 'val_fracs': [0.7878676470588235, 0.7753676470588236, 0.7808823529411765, 0.7970588235294118, 0.7908088235294117, 0.7786764705882353, 0.7926470588235294, 0.8073529411764706, 0.8084558823529412, 0.7955882352941176, 0.7849264705882353, 0.7893382352941176, 0.805514705882353, 0.8040441176470589, 0.7908088235294117, 0.8029411764705883, 0.8044117647058824, 0.8073529411764706, 0.7911764705882353, 0.8121323529411765, 0.7919117647058823, 0.8169117647058823, 0.805514705882353, 0.8106617647058824, 0.8125, 0.7970588235294118, 0.8117647058823529, 0.8040441176470589, 0.8106617647058824, 0.8147058823529412, 0.8091911764705882, 0.8051470588235294, 0.8011029411764706, 0.799264705882353, 0.8102941176470588, 0.8143382352941176, 0.8113970588235294, 0.8011029411764706, 0.8125, 0.8047794117647059, 0.8125, 0.8025735294117647, 0.8213235294117647, 0.8080882352941177, 0.8025735294117647, 0.7952205882352941, 0.8073529411764706, 0.8033088235294118, 0.8014705882352942, 0.8084558823529412, 0.8091911764705882, 0.8128676470588235, 0.8018382352941177, 0.7871323529411764, 0.7904411764705882, 0.8136029411764706, 0.7955882352941176, 0.8091911764705882, 0.8011029411764706, 0.8014705882352942, 0.8029411764705883, 0.8011029411764706, 0.8088235294117647, 0.7904411764705882, 0.8011029411764706, 0.7981617647058824, 0.8132352941176471, 0.8099264705882353, 0.8150735294117647, 0.8216911764705882, 0.8161764705882353, 0.8183823529411764, 0.8158088235294118, 0.8139705882352941, 0.8022058823529412, 0.8022058823529412, 0.8165441176470588, 0.80625, 0.7897058823529411, 0.79375, 0.7988970588235295, 0.7948529411764705, 0.8246323529411764, 0.8007352941176471, 0.8025735294117647, 0.8128676470588235, 0.8169117647058823, 0.8014705882352942, 0.8205882352941176, 0.8121323529411765, 0.8209558823529411, 0.8224264705882353, 0.8033088235294118, 0.8069852941176471, 0.8113970588235294, 0.8117647058823529, 0.8154411764705882, 0.8069852941176471, 0.8132352941176471, 0.7996323529411765, 0.8176470588235294, 0.8136029411764706, 0.8099264705882353, 0.8143382352941176, 0.8117647058823529, 0.8176470588235294, 0.8180147058823529, 0.8077205882352941, 0.8080882352941177, 0.8176470588235294, 0.8051470588235294, 0.8169117647058823, 0.8261029411764705, 0.8183823529411764, 0.8147058823529412, 0.8139705882352941, 0.8113970588235294, 0.8084558823529412, 0.8136029411764706, 0.8018382352941177, 0.8025735294117647, 0.8136029411764706, 0.8161764705882353, 0.8128676470588235, 0.8095588235294118, 0.7878676470588235, 0.8058823529411765, 0.8003676470588236, 0.7970588235294118, 0.7933823529411764, 0.8150735294117647, 0.8091911764705882, 0.8018382352941177, 0.80625, 0.8073529411764706, 0.8095588235294118, 0.8047794117647059, 0.8040441176470589, 0.8003676470588236, 0.799264705882353, 0.8025735294117647, 0.79375, 0.8088235294117647, 0.8117647058823529, 0.8058823529411765, 0.8040441176470589, 0.8213235294117647, 0.7926470588235294, 0.8143382352941176, 0.8275735294117647, 0.8125, 0.8033088235294118, 0.8022058823529412, 0.8091911764705882, 0.8099264705882353, 0.8007352941176471, 0.8136029411764706, 0.8161764705882353, 0.8209558823529411, 0.8132352941176471, 0.8279411764705882, 0.8128676470588235, 0.8018382352941177, 0.8150735294117647, 0.8025735294117647, 0.8110294117647059, 0.7933823529411764, 0.7988970588235295, 0.7933823529411764, 0.8003676470588236, 0.7970588235294118, 0.7966911764705882, 0.7977941176470589, 0.7996323529411765, 0.8150735294117647, 0.794485294117647, 0.8216911764705882, 0.8161764705882353, 0.8047794117647059, 0.8014705882352942, 0.8147058823529412, 0.8272058823529411, 0.8191176470588235, 0.8213235294117647, 0.8025735294117647, 0.7823529411764706, 0.8007352941176471, 0.8154411764705882, 0.8047794117647059, 0.8205882352941176, 0.8213235294117647, 0.8102941176470588, 0.825735294117647, 0.8242647058823529, 0.8161764705882353, 0.8183823529411764, 0.8183823529411764, 0.8238970588235294, 0.8268382352941176, 0.8246323529411764], 'pred_labels_best': array([6, 6, 3, 6, 6, 7, 3, 6, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 7, 7, 6, 6,\n",
      "       7, 6, 6, 7, 6, 6, 3, 6, 3, 6], dtype=int64), 'true_labels_best': array([6, 6, 5, 6, 6, 7, 7, 6, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 7, 7, 6, 6,\n",
      "       7, 6, 6, 7, 6, 6, 4, 6, 7, 6])}\n",
      "########\n",
      "result loss: 0.399848 \n",
      "result frac:0.886397\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 2, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.399848132361384, 'valid_frac_max': 0.8863970588235294, 'val_losses': [0.521930020346361, 0.4858308392412522, 0.471087762187509, 0.46218671816236834, 0.41509518412982715, 0.4336159835843479, 0.4062365865006166, 0.47331105996580686, 0.43915215011905223, 0.43294653515605364, 0.4081890770617653, 0.42601992593092075, 0.42029289892491173, 0.41471673241432977, 0.399848132361384, 0.4397423314697602, 0.4146135118954322, 0.46020638031118055, 0.4626955125261756, 0.40585533213966035, 0.4041930848184754, 0.455934001417721, 0.4469854918472907, 0.43758545360144446, 0.5357847758952309, 0.5291435350390041, 0.4833482309299357, 0.4386198618832757, 0.5674002324833589, 0.5457628134418936, 0.5475174739080317, 0.5299861227764803, 0.4664337281795109, 0.44842168127789217, 0.4617427713730756, 0.48439403789884905, 0.44392661040320114, 0.4057715315152617, 0.4329922302680857, 0.4413697354057256, 0.44067123944268505, 0.4751202576300677, 0.5001818351885852, 0.5588744245031301, 0.5012732068405432, 0.4693355321884155, 0.4421713190920213, 0.45362704150816974, 0.46268434364567784, 0.4993450257708045, 0.4867234519299339, 0.4517054870724678, 0.43930346790482017, 0.5045790674055324, 0.49227882566697456, 0.5044125881265191, 0.45090764138628453, 0.44083712214932724, 0.45651672035455704, 0.5452473878860473, 0.49434357758830577, 0.5724200569531497, 0.4639821706449284, 0.4678035364431493, 0.5673924568821402, 0.5785049252650317, 0.5658711748964647, 0.5181913042769712, 0.521725974538747, 0.5288705797756419, 0.6361070983550128, 0.5516383022069931, 0.5861281168811462, 0.6413473790182787, 0.6240936102236019, 0.6610912273911869, 0.6460202125065467, 0.6657684126321007, 0.6382655064849293, 0.634523034972303, 0.6739019656882567, 0.6901949079597698, 0.6442835038199144, 0.6853717674227322, 0.6668489165165845, 0.6918674844152787, 0.6667323529720306, 0.6656330084099489, 0.7837920883122612, 0.775700096873676, 0.8269237248336567, 0.7672236183110406, 0.7092932133113636, 0.6893823564052581, 0.7607156834181618, 0.9236174134647145, 0.8210725573932424, 0.7899884237962611, 0.7465838320115034, 0.9947463396717521, 0.973531970206429, 0.8444390244343701, 0.8063254598309012, 0.8113476230817682, 0.8076361203894896, 0.7801412824322196, 0.7869166889611412, 0.9227558325318729, 0.997711901103749, 0.9675962528761696, 0.9140744903508354, 0.8943699885817135, 0.8564326188143562, 0.776644515991211, 1.0778891801834107, 0.925228595383027, 0.8905459898359636, 0.8636223950806786, 0.9259162499624141, 0.8991610856617198, 0.8480694286963519, 0.771964559134315, 0.8982711490462808, 0.8071069552617914, 0.8531993988682242, 0.9613579287248499, 0.9048620476442225, 0.9060850062791039, 1.0569798217100255, 0.8536509510348825, 0.8614740627653459, 0.9297458809964797, 0.956653222616981, 0.9815611895392923, 0.9176557099117952, 0.9551602391635671, 0.8944745088324827, 1.0106090047780205, 1.025045784781961, 1.045664600765004, 0.993026790899389, 0.8981787506271811, 0.8925233343068291, 0.9705984662560856, 1.0206979043343487, 0.9415889231597676, 0.912909304745057, 0.979680036797243, 0.8922751840423135, 0.8915598438066594, 0.8762886012301726, 0.9938546292922076, 0.9215869524899651, 0.8588267641908982, 0.9325780861517963, 0.9903458693448235, 0.8477793567320879, 0.8364441468435175, 0.8404688140925239, 0.8949988014557783, 0.8383649962789872, 0.8368767969748553, 0.8812085765249589, 0.8627618102466359, 0.8142316993545083, 0.8419250512824339, 0.7970982099280638, 0.824234270698884, 0.8825477656196146, 0.8635229282519397, 0.7952055650598863, 0.8489874902893515, 0.7424965760287117, 0.7395431301173042, 0.7727417199050679, 0.7240808083730585, 0.7389807143632103, 0.7430694927187527, 0.8150141011266148, 0.6950209596577812, 0.6948221245232751, 0.6962210029363632, 0.8164962691419265, 0.7881565633942099, 0.8332464926383074, 0.8454526007175446, 0.7959798833903144, 0.8153706140377942, 0.7646419107913971, 0.7834691622677972, 0.787618484917809, 0.7295106789645027, 0.7354441053727094, 0.7399638509049135, 0.7263247833532446, 0.7404021263122559, 0.853072592090158, 0.8729149807901944, 0.8324614756247577, 0.7675471544265747], 'val_fracs': [0.8448529411764706, 0.8544117647058823, 0.8595588235294118, 0.8595588235294118, 0.8702205882352941, 0.8709558823529412, 0.8753676470588235, 0.8705882352941177, 0.8613970588235295, 0.8621323529411765, 0.8779411764705882, 0.8742647058823529, 0.8863970588235294, 0.8761029411764706, 0.8746323529411765, 0.8720588235294118, 0.8783088235294118, 0.868014705882353, 0.8661764705882353, 0.8808823529411764, 0.8779411764705882, 0.8643382352941177, 0.8613970588235295, 0.8661764705882353, 0.8264705882352941, 0.8514705882352941, 0.8547794117647058, 0.8580882352941176, 0.8246323529411764, 0.8286764705882353, 0.8514705882352941, 0.8430147058823529, 0.861764705882353, 0.8650735294117647, 0.8551470588235294, 0.8525735294117647, 0.8720588235294118, 0.8761029411764706, 0.8738970588235294, 0.8705882352941177, 0.8746323529411765, 0.8720588235294118, 0.8518382352941176, 0.8426470588235294, 0.850735294117647, 0.8639705882352942, 0.86875, 0.8731617647058824, 0.8628676470588236, 0.8573529411764705, 0.8580882352941176, 0.8694852941176471, 0.8621323529411765, 0.8382352941176471, 0.856985294117647, 0.8397058823529412, 0.8628676470588236, 0.8595588235294118, 0.8595588235294118, 0.8338235294117647, 0.8496323529411764, 0.8209558823529411, 0.8544117647058823, 0.8573529411764705, 0.80625, 0.8150735294117647, 0.8272058823529411, 0.8426470588235294, 0.8397058823529412, 0.8272058823529411, 0.8128676470588235, 0.8264705882352941, 0.8198529411764706, 0.8033088235294118, 0.8117647058823529, 0.794485294117647, 0.8095588235294118, 0.7955882352941176, 0.8007352941176471, 0.7988970588235295, 0.7878676470588235, 0.7819852941176471, 0.7823529411764706, 0.7757352941176471, 0.7827205882352941, 0.7845588235294118, 0.7871323529411764, 0.7959558823529411, 0.7533088235294118, 0.7419117647058824, 0.7327205882352941, 0.7481617647058824, 0.7577205882352941, 0.7669117647058824, 0.7602941176470588, 0.7139705882352941, 0.7386029411764706, 0.7573529411764706, 0.7492647058823529, 0.700735294117647, 0.6878676470588235, 0.7349264705882353, 0.7463235294117647, 0.7301470588235294, 0.7643382352941176, 0.7654411764705882, 0.7397058823529412, 0.7139705882352941, 0.6727941176470589, 0.6981617647058823, 0.7253676470588235, 0.7125, 0.7191176470588235, 0.7411764705882353, 0.618014705882353, 0.7036764705882353, 0.7253676470588235, 0.7330882352941176, 0.669485294117647, 0.725, 0.7441176470588236, 0.7522058823529412, 0.7066176470588236, 0.7341911764705882, 0.73125, 0.7011029411764705, 0.7308823529411764, 0.7375, 0.6650735294117647, 0.7371323529411765, 0.7316176470588235, 0.7106617647058824, 0.6959558823529411, 0.7202205882352941, 0.7419117647058824, 0.7158088235294118, 0.725735294117647, 0.7345588235294118, 0.7, 0.6360294117647058, 0.6672794117647058, 0.7227941176470588, 0.7308823529411764, 0.7073529411764706, 0.7253676470588235, 0.7375, 0.73125, 0.6974264705882353, 0.743014705882353, 0.7213235294117647, 0.7393382352941177, 0.7113970588235294, 0.7363970588235295, 0.74375, 0.7268382352941176, 0.7106617647058824, 0.7386029411764706, 0.7433823529411765, 0.7408088235294118, 0.7378676470588236, 0.7334558823529411, 0.7610294117647058, 0.7415441176470589, 0.7481617647058824, 0.7555147058823529, 0.7555147058823529, 0.7481617647058824, 0.7610294117647058, 0.7584558823529411, 0.7470588235294118, 0.7544117647058823, 0.7327205882352941, 0.7606617647058823, 0.768014705882353, 0.7566176470588235, 0.7694852941176471, 0.7555147058823529, 0.7470588235294118, 0.7676470588235295, 0.7738970588235294, 0.7654411764705882, 0.7573529411764706, 0.7183823529411765, 0.7345588235294118, 0.7169117647058824, 0.7139705882352941, 0.7224264705882353, 0.7088235294117647, 0.7459558823529412, 0.7422794117647059, 0.7382352941176471, 0.7492647058823529, 0.7522058823529412, 0.7363970588235295, 0.7378676470588236, 0.7485294117647059, 0.7191176470588235, 0.7216911764705882, 0.725735294117647, 0.75], 'pred_labels_best': array([6, 6, 6, 7, 6, 7, 7, 6, 6, 2, 7, 7, 7, 6, 6, 6, 6, 7, 7, 6, 7, 7,\n",
      "       6, 7, 7, 7, 7, 7, 6, 6, 6, 6], dtype=int64), 'true_labels_best': array([6, 6, 6, 2, 6, 4, 5, 6, 6, 2, 2, 7, 7, 6, 6, 6, 6, 7, 2, 6, 7, 7,\n",
      "       6, 7, 7, 2, 6, 7, 6, 6, 6, 6])}\n",
      "########\n",
      "result loss: 0.391459 \n",
      "result frac:0.894853\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 3, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.39145865151110815, 'valid_frac_max': 0.8948529411764706, 'val_losses': [0.5669143652214723, 0.4534521355348475, 0.4524130358415491, 0.4033321614651119, 0.5132564125692143, 0.4498239415533402, 0.41154446838533176, 0.399137000301305, 0.4518713522483321, 0.4639184958191917, 0.4090823926908128, 0.42384138455724013, 0.393668976075509, 0.4014609559055637, 0.39145865151110815, 0.4182007642353282, 0.4311814595671261, 0.4391858169261147, 0.4108576674671734, 0.44020992833025313, 0.4503583049072939, 0.44479433008853125, 0.40296087431557037, 0.4048305232516106, 0.40805912429795543, 0.438135653032976, 0.41795886807581956, 0.4210213734823115, 0.43385208652299995, 0.45270311700947147, 0.41189312040805814, 0.41611524460946814, 0.44936957223450436, 0.4260056476382648, 0.47143602279179236, 0.46411101492669654, 0.46081439747529873, 0.4577347510877778, 0.4707966472296154, 0.41260533091776513, 0.4098007658825201, 0.4174919819130617, 0.427731411969837, 0.3971209061496398, 0.42547392704907583, 0.42897044810302115, 0.42542721377137827, 0.41202446110546587, 0.4425434534383171, 0.47157942310852163, 0.46403045772629625, 0.42953554312972464, 0.49733727258794447, 0.5253174648565404, 0.46612279748215396, 0.6554292005651138, 0.7276679838404936, 0.6593857656506931, 0.5437161972417551, 0.5374518827480428, 0.4739379462073831, 0.5223091326215688, 0.5419751221642775, 0.5949831839870005, 0.6207828079952913, 0.6272440156515907, 0.5707755474483266, 0.5907690100810107, 0.5750360818470226, 0.7043854853686164, 0.7263181030750274, 0.8024171958951389, 0.7741084621233099, 0.9650003047550426, 0.6769438515691196, 1.044112556471544, 0.7960497133872089, 0.8494868969216066, 0.839411192781785, 0.7441665162058437, 0.8651061731226304, 1.1335707243751076, 0.8723643558866837, 1.0534972751841825, 1.0620175838470458, 0.9368903086465947, 1.1538402858902426, 0.7280229491346023, 0.7679929782362546, 0.7990820286905065, 0.700141807864694, 0.6674870252609253, 0.6618580327314489, 0.7171096437117632, 0.8143844415159787, 0.6449231004013735, 0.67112195632037, 0.6415272600510541, 0.8234263851362116, 0.974186955129399, 0.7845677035696367, 0.7806394457817077, 0.6850896230515312, 0.7369263266815859, 0.6974960239494548, 0.7406307918183944, 0.7735511749982834, 0.6765870465951808, 0.6775755922583973, 0.8023964994093951, 0.626204764141756, 0.7623082714922288, 0.8050356247845818, 0.6981855336357565, 0.7677603392040029, 0.810296993746477, 0.7906487636706409, 0.8268635202856625, 0.8291180473916671, 0.72221319640384, 0.6483745476778816, 0.844650042407653, 0.7473601779517005, 0.7017673292580773, 0.7074580579996109, 0.7519779408679289, 0.6016254747615141, 0.6498407404212391, 0.6599139981410083, 0.6605963060084511, 0.5643758805359111, 0.5619490148390041, 0.6982327401638031, 0.6078014417606241, 0.6173052615979139, 0.5525088552166434, 0.5879626931513057, 0.6025544746833689, 0.6630409994546105, 0.701551438780392, 0.6458459913730621, 0.6013621495050543, 0.5962998986244201, 0.6608126084594166, 0.6271944619276945, 0.717811305733288, 0.7435469802688149, 0.6745273853049559, 0.6634447693824768, 0.6768809695454204, 0.7513194571523105, 0.6947160447345061, 0.8438099457937128, 0.9787311953656813, 0.701256354416118, 0.6224080645862747, 0.7942196477861966, 0.6733416148844887, 0.7612504096592174, 0.8671244046267341, 0.7995657650863424, 0.8838739107636845, 0.87096827573636, 0.7272181854528539, 0.7718955699135276, 0.6886934678344165, 0.7695007050738615, 0.7656002812525805, 0.8106132479274974, 0.6901428829221165, 0.8973600776756511, 0.9437715737258687, 0.859411511000465, 0.8851743841872496, 0.8477490656516131, 0.9391844966832329, 0.912538309658275, 0.8788078416796291, 0.8706030705395866, 0.8384060070795172, 0.9265506414806142, 1.5012290169210996, 1.0708712539252112, 1.0592744105002458, 1.0002866352305693, 1.2963780929060544, 1.0482693293515373, 1.2029909140923445, 0.8827079815023086, 0.7768566345467287, 1.050897573372897, 0.7666205248411964, 0.939368462562561, 0.9945422572248123, 1.0247435198110693, 1.2222726751776303, 0.8973967285717235, 0.9595695814665626, 0.9526587591451757, 0.9923404034446267], 'val_fracs': [0.8308823529411765, 0.8610294117647059, 0.8705882352941177, 0.8841911764705882, 0.8536764705882353, 0.8727941176470588, 0.8764705882352941, 0.8845588235294117, 0.8764705882352941, 0.8738970588235294, 0.8878676470588235, 0.8764705882352941, 0.8900735294117647, 0.881985294117647, 0.8919117647058824, 0.8875, 0.8893382352941176, 0.8867647058823529, 0.8860294117647058, 0.8816176470588235, 0.8816176470588235, 0.88125, 0.8948529411764706, 0.8845588235294117, 0.8904411764705882, 0.8845588235294117, 0.8915441176470589, 0.8911764705882353, 0.8856617647058823, 0.8731617647058824, 0.89375, 0.8863970588235294, 0.8753676470588235, 0.8867647058823529, 0.8775735294117647, 0.8779411764705882, 0.8738970588235294, 0.8816176470588235, 0.8665441176470589, 0.8893382352941176, 0.893014705882353, 0.8875, 0.8860294117647058, 0.8878676470588235, 0.8786764705882353, 0.8852941176470588, 0.893014705882353, 0.8893382352941176, 0.8849264705882353, 0.8731617647058824, 0.8720588235294118, 0.8705882352941177, 0.8536764705882353, 0.8496323529411764, 0.8639705882352942, 0.8077205882352941, 0.7856617647058823, 0.8183823529411764, 0.8496323529411764, 0.8466911764705882, 0.8595588235294118, 0.8389705882352941, 0.8338235294117647, 0.8106617647058824, 0.80625, 0.8047794117647059, 0.8246323529411764, 0.8077205882352941, 0.8176470588235294, 0.7849264705882353, 0.7617647058823529, 0.75625, 0.7738970588235294, 0.7477941176470588, 0.8036764705882353, 0.7125, 0.7761029411764706, 0.7713235294117647, 0.7470588235294118, 0.7536764705882353, 0.7338235294117647, 0.6455882352941177, 0.7341911764705882, 0.6367647058823529, 0.7150735294117647, 0.6819852941176471, 0.5680147058823529, 0.7213235294117647, 0.7654411764705882, 0.7661764705882353, 0.7897058823529411, 0.8095588235294118, 0.8033088235294118, 0.7808823529411765, 0.7838235294117647, 0.8128676470588235, 0.7996323529411765, 0.8136029411764706, 0.7511029411764706, 0.7227941176470588, 0.7470588235294118, 0.7625, 0.7845588235294118, 0.7676470588235295, 0.7830882352941176, 0.7558823529411764, 0.7507352941176471, 0.7856617647058823, 0.7805147058823529, 0.768014705882353, 0.8040441176470589, 0.7830882352941176, 0.7625, 0.7735294117647059, 0.7716911764705883, 0.7525735294117647, 0.7797794117647059, 0.7452205882352941, 0.7558823529411764, 0.7643382352941176, 0.7922794117647058, 0.7625, 0.7610294117647058, 0.7878676470588235, 0.7852941176470588, 0.768014705882353, 0.8044117647058824, 0.8014705882352942, 0.8066176470588236, 0.805514705882353, 0.8319852941176471, 0.8316176470588236, 0.7959558823529411, 0.8110294117647059, 0.8025735294117647, 0.8246323529411764, 0.8088235294117647, 0.8044117647058824, 0.7977941176470589, 0.7540441176470588, 0.7735294117647059, 0.7966911764705882, 0.81875, 0.7900735294117647, 0.8047794117647059, 0.7808823529411765, 0.7764705882352941, 0.8040441176470589, 0.8003676470588236, 0.7805147058823529, 0.8040441176470589, 0.7886029411764706, 0.7305147058823529, 0.7194852941176471, 0.7952205882352941, 0.8099264705882353, 0.7786764705882353, 0.7948529411764705, 0.7584558823529411, 0.7158088235294118, 0.7209558823529412, 0.7183823529411765, 0.71875, 0.7628676470588235, 0.7588235294117647, 0.7643382352941176, 0.7481617647058824, 0.7658088235294118, 0.7422794117647059, 0.8011029411764706, 0.7268382352941176, 0.7227941176470588, 0.74375, 0.7069852941176471, 0.7014705882352941, 0.7110294117647059, 0.7283088235294117, 0.7275735294117647, 0.7371323529411765, 0.7566176470588235, 0.7110294117647059, 0.4511029411764706, 0.6503676470588236, 0.6621323529411764, 0.6871323529411765, 0.6235294117647059, 0.6974264705882353, 0.6911764705882353, 0.7441176470588236, 0.7775735294117647, 0.7128676470588236, 0.7801470588235294, 0.7025735294117647, 0.6996323529411764, 0.6808823529411765, 0.6018382352941176, 0.7227941176470588, 0.7301470588235294, 0.7213235294117647, 0.6827205882352941], 'pred_labels_best': array([6, 6, 7, 6, 6, 6, 2, 7, 6, 6, 6, 7, 7, 6, 7, 7, 7, 6, 6, 6, 6, 6,\n",
      "       7, 6, 7, 7, 6, 6, 6, 7, 6, 6], dtype=int64), 'true_labels_best': array([6, 6, 3, 6, 6, 6, 2, 2, 6, 6, 6, 7, 5, 6, 7, 7, 7, 6, 6, 6, 6, 6,\n",
      "       4, 6, 7, 4, 6, 6, 6, 7, 6, 6])}\n",
      "########\n",
      "result loss: 0.378481 \n",
      "result frac:0.895956\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 5, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.3784807978745769, 'valid_frac_max': 0.8959558823529412, 'val_losses': [0.6143916729618522, 0.48872053447891683, 0.46650502769386065, 0.43954831256585963, 0.4147457960773917, 0.4297228534432018, 0.397371331558508, 0.41460136779967477, 0.3963662815882879, 0.39835042927195047, 0.44869375000981726, 0.40117764551849927, 0.42929463991347483, 0.4049372918465558, 0.4464490112136392, 0.46731484199271484, 0.4463341775624191, 0.4221057454452795, 0.43197324302266626, 0.4102626298280323, 0.42309069576508856, 0.432623540610075, 0.3956961029154413, 0.40297000267926386, 0.45741968299536145, 0.4193692862987518, 0.43042859542457496, 0.4352532210595467, 0.3784807978745769, 0.4425944250734413, 0.39737257357029354, 0.3985409363227732, 0.48917662825654534, 0.42813802929047273, 0.4447139016845647, 0.4178225483964471, 0.38947663390461135, 0.47109392094699776, 0.40758830124841017, 0.39537615482421484, 0.4312973893740598, 0.430622255451539, 0.4231877444421544, 0.4174267683835591, 0.39982946059283087, 0.3985157247413607, 0.463524125253453, 0.4399195721920799, 0.42225119204205624, 0.3938372233992114, 0.4154646088643109, 0.4131288434652721, 0.4107587537344764, 0.42366503126481, 0.3974149105303428, 0.40508011632105884, 0.3920653307919993, 0.40086914226412773, 0.4163560405373573, 0.4631642679738648, 0.42242165023789685, 0.43118016281548666, 0.39003750944838805, 0.4427838831701699, 0.46119649401482415, 0.43846417691777734, 0.4726451360127505, 0.43934609442949296, 0.40651041571708285, 0.419312877931139, 0.44396684108411566, 0.4072964430731886, 0.4239705171655206, 0.41816904961186296, 0.4240316817865652, 0.4138692944365389, 0.42538187048014475, 0.45612949378350204, 0.41761426636401344, 0.4133257602944094, 0.4195348102818517, 0.44875161001349195, 0.4261323780697935, 0.4015316774739939, 0.4046206336687593, 0.40389414030839416, 0.42487860091907137, 0.39846770719570274, 0.40661131573074005, 0.4157134156893281, 0.39730688079315074, 0.46872686244109096, 0.4438602212159073, 0.42779562210335453, 0.4331917426603682, 0.4601030579822905, 0.43290536526371454, 0.4342189872089554, 0.4140404236229027, 0.42502969398218043, 0.44134272310663675, 0.4318031157421715, 0.42914488376939997, 0.42220798082211436, 0.4189233995974064, 0.4794505506315652, 0.43545731604099275, 0.4431122787735041, 0.4209225303548224, 0.4113136802526081, 0.4541698958067333, 0.41437737363226274, 0.3971046014305423, 0.42512291974442845, 0.40738520994782446, 0.4154976678245208, 0.3959937641725821, 0.45170871904667687, 0.4501609867109972, 0.3975301971330362, 0.40710666565334097, 0.4023700219743392, 0.3992967361913008, 0.42552455895963837, 0.393772071074037, 0.43410437299924737, 0.4338626243393211, 0.4129149174427285, 0.44749226815560283, 0.4591224573114339, 0.4587798675193506, 0.4561666479882072, 0.4575372122666415, 0.45099580962868296, 0.45145517500007853, 0.42536975105895714, 0.4255051721544827, 0.45541549105854595, 0.4282527228488642, 0.4210098690846387, 0.41199917262967894, 0.44302445439731375, 0.46696564042392896, 0.4720146196729997, 0.4394932236741571, 0.4376046158811625, 0.489465286889497, 0.45186153483741426, 0.4537891755647519, 0.4454079366782132, 0.4314019971910645, 0.467362941801548, 0.45417635265518636, 0.49958705437533996, 0.4926986853866016, 0.49095215034835477, 0.4785433025482823, 0.5602962467600318, 0.5649890019613154, 0.5388888383612913, 0.5234678042285582, 0.5176147770355729, 0.5604209893766572, 0.6318495156133876, 0.6361160250271067, 0.6457471237463109, 0.7649028890273151, 0.6922019807731404, 0.6798677272656385, 1.17441667248221, 0.6909956086207839, 0.6903256496962379, 0.7156208132996279, 0.7120286471703473, 0.7108253661323997, 0.7018751409123926, 0.8553908121936461, 0.7674584630657645, 1.4580574849072625, 1.295408668237574, 1.2187554590842302, 0.9540128834107343, 1.1167140077142155, 0.8228996476706336, 0.7845173506175771, 0.8702360114630531, 1.1655522318447338, 1.0898949552984798, 1.318753950736102, 1.3869758753215566, 1.4358974295503952, 1.5060949381660014, 1.4776158010258393, 1.4489202976226807, 1.4919809958514045, 1.4777140308828915, 1.4213860553853652, 1.3916776839424583, 1.340405035720152, 1.4212761724696439], 'val_fracs': [0.8275735294117647, 0.8430147058823529, 0.8632352941176471, 0.8676470588235294, 0.8716911764705882, 0.8761029411764706, 0.8786764705882353, 0.881985294117647, 0.8797794117647059, 0.8841911764705882, 0.8764705882352941, 0.8827205882352941, 0.8805147058823529, 0.8834558823529411, 0.8761029411764706, 0.881985294117647, 0.8830882352941176, 0.8772058823529412, 0.8886029411764705, 0.8845588235294117, 0.875, 0.8742647058823529, 0.8834558823529411, 0.8849264705882353, 0.8764705882352941, 0.8860294117647058, 0.8926470588235295, 0.881985294117647, 0.8933823529411765, 0.8860294117647058, 0.8878676470588235, 0.8908088235294118, 0.86875, 0.8871323529411764, 0.8875, 0.8841911764705882, 0.8908088235294118, 0.8713235294117647, 0.8933823529411765, 0.8889705882352941, 0.8827205882352941, 0.8838235294117647, 0.8863970588235294, 0.881985294117647, 0.8886029411764705, 0.888235294117647, 0.8757352941176471, 0.8794117647058823, 0.8860294117647058, 0.8886029411764705, 0.8908088235294118, 0.8871323529411764, 0.8893382352941176, 0.8838235294117647, 0.8886029411764705, 0.8845588235294117, 0.8845588235294117, 0.8897058823529411, 0.8904411764705882, 0.8860294117647058, 0.8897058823529411, 0.8863970588235294, 0.8919117647058824, 0.8878676470588235, 0.8823529411764706, 0.8878676470588235, 0.8856617647058823, 0.8860294117647058, 0.8897058823529411, 0.8852941176470588, 0.8908088235294118, 0.8893382352941176, 0.8915441176470589, 0.8915441176470589, 0.8959558823529412, 0.8952205882352942, 0.8904411764705882, 0.8794117647058823, 0.8900735294117647, 0.89375, 0.8926470588235295, 0.8779411764705882, 0.8908088235294118, 0.8878676470588235, 0.8875, 0.8886029411764705, 0.881985294117647, 0.8849264705882353, 0.8834558823529411, 0.8849264705882353, 0.8867647058823529, 0.8808823529411764, 0.8845588235294117, 0.8834558823529411, 0.8852941176470588, 0.8860294117647058, 0.8841911764705882, 0.8900735294117647, 0.88125, 0.8834558823529411, 0.8805147058823529, 0.8856617647058823, 0.8852941176470588, 0.8911764705882353, 0.8827205882352941, 0.8746323529411765, 0.8863970588235294, 0.8849264705882353, 0.8863970588235294, 0.8889705882352941, 0.8768382352941176, 0.8827205882352941, 0.8838235294117647, 0.8849264705882353, 0.8827205882352941, 0.881985294117647, 0.8897058823529411, 0.8786764705882353, 0.8871323529411764, 0.888235294117647, 0.8904411764705882, 0.8823529411764706, 0.8856617647058823, 0.8897058823529411, 0.8849264705882353, 0.8764705882352941, 0.8801470588235294, 0.881985294117647, 0.8801470588235294, 0.8823529411764706, 0.8790441176470588, 0.8797794117647059, 0.8786764705882353, 0.8790441176470588, 0.8830882352941176, 0.8834558823529411, 0.8841911764705882, 0.8830882352941176, 0.8845588235294117, 0.8867647058823529, 0.8845588235294117, 0.8786764705882353, 0.8742647058823529, 0.8735294117647059, 0.8779411764705882, 0.8735294117647059, 0.8702205882352941, 0.8786764705882353, 0.8746323529411765, 0.8764705882352941, 0.88125, 0.8772058823529412, 0.8808823529411764, 0.8713235294117647, 0.8713235294117647, 0.8621323529411765, 0.8643382352941177, 0.8397058823529412, 0.81875, 0.8382352941176471, 0.85, 0.8430147058823529, 0.8386029411764706, 0.8014705882352942, 0.8125, 0.7985294117647059, 0.7470588235294118, 0.7941176470588235, 0.7764705882352941, 0.5735294117647058, 0.7709558823529412, 0.7599264705882353, 0.7459558823529412, 0.7650735294117647, 0.7790441176470588, 0.7790441176470588, 0.7246323529411764, 0.7503676470588235, 0.3205882352941177, 0.5801470588235295, 0.6496323529411765, 0.7022058823529411, 0.6349264705882353, 0.7297794117647058, 0.7316176470588235, 0.7323529411764705, 0.6415441176470589, 0.6606617647058823, 0.5514705882352942, 0.4974264705882353, 0.46911764705882353, 0.3941176470588235, 0.42867647058823527, 0.4724264705882353, 0.4113970588235294, 0.4514705882352941, 0.4672794117647059, 0.5139705882352941, 0.5378676470588235, 0.48933823529411763], 'pred_labels_best': array([7, 5, 6, 7, 6, 7, 6, 6, 6, 6, 6, 6, 2, 6, 7, 6, 6, 6, 6, 6, 7, 6,\n",
      "       6, 6, 7, 6, 7, 2, 7, 7, 6, 7], dtype=int64), 'true_labels_best': array([7, 5, 6, 7, 6, 2, 6, 6, 6, 6, 6, 6, 2, 6, 7, 6, 6, 6, 6, 6, 7, 6,\n",
      "       6, 6, 5, 6, 5, 2, 7, 7, 6, 7])}\n",
      "########\n",
      "result loss: 0.493958 \n",
      "result frac:0.847059\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.001, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.49395847127718084, 'valid_frac_max': 0.8470588235294118, 'val_losses': [0.7828705303809222, 0.8038542638806736, 0.7708435552961687, 1.0277800861526938, 0.6857578814029693, 0.7190949440002441, 0.6636199930134942, 0.7216968732721666, 0.6732318071758046, 0.7087604694506702, 0.6908919909421135, 0.6704905127777773, 0.6350409101037419, 0.6568725866429946, 0.6428275939296274, 0.6426484269254348, 0.6487298765603233, 0.6128690219977323, 0.6325743130024741, 0.6274843061671538, 0.6388329162317163, 0.8204847602283254, 0.6820658859084634, 0.6417200316401089, 0.6830234513563268, 0.665751853760551, 0.6331434197285596, 0.6547699355027254, 0.6184506034149843, 0.6938778232125675, 0.6617232452420627, 0.7010677257004906, 0.6419517057783464, 0.6525983554475447, 0.6203132832751554, 0.6353742766029694, 0.6979331903597887, 0.6392123060191379, 0.6407034533865311, 0.6426042542738073, 0.6098233387750738, 0.640060525080737, 0.6644402560065774, 0.7038264607681948, 0.6479282735025181, 0.6233510360998266, 0.6226795035250047, 0.6175033309880424, 0.6400176675880657, 0.6644990840378929, 0.6236163248034085, 0.6720560449011186, 0.656643758801853, 0.6194376170635223, 0.6226562296642977, 0.7098880767822265, 0.7114145029993618, 0.6591502687510322, 0.6324806010021883, 0.6911815769532148, 0.6554779147400576, 0.6457171454149134, 0.8226929727722617, 0.7011252024594475, 0.652122941087274, 0.7331475107108846, 0.6406371540883008, 0.664819180614808, 0.6141391052919276, 0.6532852916156544, 0.6640943411518546, 0.6613613945596358, 0.6199412552749409, 0.6211964621263392, 0.7019778433968039, 0.7240825260386747, 0.6264581341953839, 0.6992160737514496, 0.6162560350754682, 0.6075923290322809, 0.6100911813623765, 0.614255390272421, 0.5938946120879229, 0.5892029565923355, 0.5846333084737554, 0.5998022987562067, 0.6126441615469316, 0.5857987619498197, 0.5700520632898106, 0.594815066281487, 0.5804829897249446, 0.613463263651904, 0.5923270812805961, 0.6232679410892374, 0.6057410348864163, 0.7086742276654524, 0.6186416906469009, 0.6183223945253036, 0.6468595462686876, 0.6172903774415746, 0.5983393947867787, 0.5954348352025537, 0.5965059504789465, 0.5847472806187237, 0.5637160578194786, 0.5510648918502471, 0.5678029225153082, 0.6117473844219656, 0.6311164764796986, 0.5662350558182773, 0.5760714490624035, 0.5986411764341242, 0.5629590681370567, 0.5734513517688302, 0.6152492239194758, 0.6140750781578176, 0.579779596363797, 0.5679635689539068, 0.5993298183469211, 0.5550014683428932, 0.5408037646728403, 0.5475583337685641, 0.5493760768105002, 0.5109712926780476, 0.6254397970788619, 0.5785362766069524, 0.5616861629135469, 0.5177327091203017, 0.5144802149604348, 0.5181309086434981, 0.49704988774131326, 0.5398285425761167, 0.5567793998648138, 0.5330268554827746, 0.572849908821723, 0.5202364160734064, 0.5300736667478786, 0.5229783384238973, 0.5285075782853015, 0.547203776065041, 0.5242380711085656, 0.5206296812085545, 0.5193129786673714, 0.510009268802755, 0.5993145158185679, 0.5171248344814077, 0.5446879560456557, 0.5057936060078004, 0.49783905218629276, 0.5154808005865883, 0.517446635926471, 0.534464377690764, 0.5285204289590612, 0.5340807255576638, 0.5142887145280838, 0.525143717492328, 0.5455552192295299, 0.5192968980354421, 0.5664956194512984, 0.5495565445984111, 0.5121957852559931, 0.4965043085462907, 0.5276346692267586, 0.5646701379733927, 0.5483497687998939, 0.5185544098124785, 0.5330435596844729, 0.5322288528961294, 0.5273551840992535, 0.5722105518859976, 0.5433621730874566, 0.5400975310627152, 0.5537535748061012, 0.539503200264538, 0.543334282671704, 0.5258693213848507, 0.5691221941919887, 0.5675100950633778, 0.588106409591787, 0.5831288455163731, 0.5365621584303238, 0.5290351168197744, 0.5186542486443239, 0.547406167493147, 0.5380642806782442, 0.5113602047457414, 0.5618239102994694, 0.5465395440073574, 0.5434892636011629, 0.5123684530749041, 0.5328745908596937, 0.522515038707677, 0.5124129209448309, 0.5206849348019151, 0.5267312423271291, 0.5282850744093166, 0.5387501096900772, 0.49395847127718084, 0.49505533102680654, 0.5180702216484967], 'val_fracs': [0.7511029411764706, 0.7845588235294118, 0.7595588235294117, 0.75625, 0.7727941176470589, 0.7628676470588235, 0.7694852941176471, 0.7533088235294118, 0.7746323529411765, 0.7375, 0.7525735294117647, 0.7794117647058824, 0.7893382352941176, 0.774264705882353, 0.7801470588235294, 0.7963235294117647, 0.7915441176470588, 0.7996323529411765, 0.8036764705882353, 0.8069852941176471, 0.7963235294117647, 0.7639705882352941, 0.8003676470588236, 0.8044117647058824, 0.7977941176470589, 0.7889705882352941, 0.7952205882352941, 0.7790441176470588, 0.7952205882352941, 0.775, 0.7819852941176471, 0.7875, 0.7606617647058823, 0.7764705882352941, 0.78125, 0.7922794117647058, 0.7827205882352941, 0.7731617647058824, 0.7691176470588236, 0.76875, 0.7856617647058823, 0.7886029411764706, 0.7691176470588236, 0.7731617647058824, 0.7738970588235294, 0.7805147058823529, 0.794485294117647, 0.7963235294117647, 0.7727941176470589, 0.7628676470588235, 0.7779411764705882, 0.7544117647058823, 0.7683823529411765, 0.7794117647058824, 0.7856617647058823, 0.774264705882353, 0.7536764705882353, 0.7783088235294118, 0.7797794117647059, 0.7606617647058823, 0.7827205882352941, 0.7827205882352941, 0.7286764705882353, 0.7555147058823529, 0.7805147058823529, 0.7610294117647058, 0.7790441176470588, 0.7683823529411765, 0.7834558823529412, 0.7727941176470589, 0.7738970588235294, 0.7875, 0.8, 0.8110294117647059, 0.788235294117647, 0.7613970588235294, 0.799264705882353, 0.7904411764705882, 0.8117647058823529, 0.8047794117647059, 0.799264705882353, 0.8066176470588236, 0.8036764705882353, 0.8169117647058823, 0.8143382352941176, 0.799264705882353, 0.788235294117647, 0.8066176470588236, 0.8180147058823529, 0.8018382352941177, 0.8084558823529412, 0.8040441176470589, 0.8069852941176471, 0.7988970588235295, 0.805514705882353, 0.7808823529411765, 0.8014705882352942, 0.79375, 0.7841911764705882, 0.8040441176470589, 0.8044117647058824, 0.8051470588235294, 0.8091911764705882, 0.8143382352941176, 0.8161764705882353, 0.8224264705882353, 0.8183823529411764, 0.8191176470588235, 0.799264705882353, 0.8139705882352941, 0.8183823529411764, 0.8125, 0.8242647058823529, 0.8191176470588235, 0.7952205882352941, 0.8117647058823529, 0.8121323529411765, 0.8169117647058823, 0.8091911764705882, 0.8301470588235295, 0.8253676470588235, 0.8297794117647059, 0.8301470588235295, 0.8360294117647059, 0.8073529411764706, 0.8231617647058823, 0.81875, 0.8352941176470589, 0.8330882352941177, 0.8319852941176471, 0.8371323529411765, 0.8176470588235294, 0.8363970588235294, 0.8279411764705882, 0.8169117647058823, 0.8308823529411765, 0.8327205882352942, 0.8327205882352942, 0.8224264705882353, 0.8272058823529411, 0.83125, 0.8297794117647059, 0.8301470588235295, 0.8301470588235295, 0.8158088235294118, 0.8301470588235295, 0.8253676470588235, 0.8349264705882353, 0.8415441176470588, 0.8356617647058824, 0.8371323529411765, 0.8238970588235294, 0.8268382352941176, 0.8286764705882353, 0.8319852941176471, 0.8308823529411765, 0.8279411764705882, 0.8279411764705882, 0.8264705882352941, 0.830514705882353, 0.8378676470588236, 0.8470588235294118, 0.8198529411764706, 0.8158088235294118, 0.81875, 0.8283088235294118, 0.8375, 0.8356617647058824, 0.8382352941176471, 0.8191176470588235, 0.8231617647058823, 0.8264705882352941, 0.825, 0.8227941176470588, 0.8224264705882353, 0.8264705882352941, 0.8172794117647059, 0.8283088235294118, 0.8220588235294117, 0.8110294117647059, 0.8231617647058823, 0.8316176470588236, 0.8327205882352942, 0.8297794117647059, 0.8341911764705883, 0.8286764705882353, 0.819485294117647, 0.8231617647058823, 0.8286764705882353, 0.8297794117647059, 0.8242647058823529, 0.8275735294117647, 0.8224264705882353, 0.8294117647058824, 0.8286764705882353, 0.8264705882352941, 0.8165441176470588, 0.8356617647058824, 0.8330882352941177, 0.8290441176470589], 'pred_labels_best': array([6, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 6,\n",
      "       6, 7, 6, 6, 3, 7, 6, 3, 6, 6], dtype=int64), 'true_labels_best': array([6, 6, 6, 6, 3, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 6,\n",
      "       6, 7, 6, 6, 2, 7, 6, 2, 6, 6])}\n",
      "########\n",
      "result loss: 0.569573 \n",
      "result frac:0.825735\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.01, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.5695733350865981, 'valid_frac_max': 0.825735294117647, 'val_losses': [0.818456473420648, 0.7651226033182705, 0.723929741452722, 0.7639149339760051, 0.644057371335871, 0.6577982800848344, 0.6668950533165651, 0.7361452256931978, 0.7494854446719674, 0.6940470811198739, 0.6333748722777647, 0.6622359409051783, 0.6665429330923978, 0.6388739964541267, 0.6411709704819848, 0.6588937999571071, 0.683490438671673, 0.6593189846066868, 0.6076216068337945, 0.6275418330641354, 0.6346174415420084, 0.589089247584343, 0.6033493304953855, 0.6345025181770325, 0.6183981657028198, 0.6347355211482328, 0.6026793557054856, 0.6190432453856749, 0.6245444483616773, 0.6639980561593, 0.591499442913953, 0.6188481195884592, 0.6158136248588562, 0.6023709379574832, 0.6329726583817425, 0.6191923665649751, 0.6146507459528306, 0.6032521310098031, 0.6083110605969149, 0.6091527789831161, 0.6294489737819223, 0.6226112230735666, 0.6346002908313976, 0.6093688274131102, 0.6183058877201641, 0.6096079735194936, 0.6022159527329838, 0.6195993023760179, 0.6231845161494087, 0.6017297423937742, 0.5745209185516134, 0.5956037728225484, 0.6018194780630224, 0.5936943113803863, 0.6055131920996835, 0.612014616412275, 0.6663814479813857, 0.6032026424127467, 0.595914680817548, 0.5962089216007905, 0.5946873103871065, 0.6025038003921509, 0.612649360649726, 0.5918642587521497, 0.6011253749623018, 0.6106752351802938, 0.620871709024205, 0.6002212033552282, 0.6124103281427832, 0.6123779472182779, 0.6211846723276027, 0.6106690066702226, 0.596061202357797, 0.5892013037905973, 0.570926717449637, 0.5817575998166028, 0.5782753748052261, 0.5785378007327809, 0.612574769819484, 0.6061636090278626, 0.5866478355491862, 0.6038528079495711, 0.5979318602996714, 0.5916647998725667, 0.5889546127880321, 0.6011214498211356, 0.6064217206309823, 0.6021753446144216, 0.5956337623736437, 0.6007861454697216, 0.6034818924525205, 0.5959306706400479, 0.6028381505433251, 0.5781151080832762, 0.5882893259034437, 0.6412520079051747, 0.62014299455811, 0.5918691754341125, 0.6156124644419726, 0.5965355953749488, 0.6070960416513331, 0.6220047375735115, 0.6084348331479466, 0.6137298713712132, 0.6074263821629917, 0.6002224089468227, 0.6061194181442261, 0.6048406692112194, 0.6073865119148704, 0.603573736373116, 0.6055403330746819, 0.5863572888514574, 0.6002389574752135, 0.5763844528618981, 0.6343759243979173, 0.6236797655329985, 0.5817326945417067, 0.6410664723199957, 0.5954843387884252, 0.6018665545126971, 0.6046577883117339, 0.6503200604635127, 0.6112607242429957, 0.5913820748820025, 0.5871734818991493, 0.6241732933942009, 0.5963848434826907, 0.5892306706484627, 0.6285788062740775, 0.6283292044611538, 0.6036482442827785, 0.6120325887904448, 0.5999668500002693, 0.6234685431508457, 0.6385022265069625, 0.591427156329155, 0.6087788618662778, 0.6037834486540626, 0.6235130055862315, 0.598766827583313, 0.5946213706451304, 0.6216523072298835, 0.599709445588729, 0.6005527676904903, 0.5830115793820689, 0.608231858996784, 0.590812986387926, 0.5972377845469643, 0.5998998063452103, 0.5988167492782368, 0.6144637248095344, 0.5966766360928031, 0.6101571219808916, 0.5972552636090447, 0.6093393301262575, 0.5900955159874524, 0.5707188003203448, 0.5999216556549072, 0.570892500351457, 0.6154066483764088, 0.6279876021777883, 0.6072942688184626, 0.6198911719462451, 0.5886436048675986, 0.5695733350865981, 0.609335064537385, 0.5881772917859694, 0.6084416815463234, 0.6195469977224575, 0.6273265617735245, 0.5871984690427781, 0.5938507011708092, 0.6072981287451351, 0.6000819619964151, 0.5970154078567729, 0.5870568757548051, 0.6097023595781887, 0.5743299584178364, 0.5979861985234654, 0.5920177103842006, 0.5803739197113935, 0.5914929079658845, 0.5862184044192819, 0.6018427010844736, 0.5928196526625578, 0.5857919163563672, 0.6052507014835582, 0.5883465987794539, 0.5900096160524032, 0.6007729537346784, 0.6181101157384761, 0.6069508598131292, 0.5964286576299106, 0.6005315098692389, 0.5922672564492506, 0.6015251694356694, 0.6066277084981694, 0.5957536034724291, 0.5968746171278112, 0.6213091899366939], 'val_fracs': [0.7628676470588235, 0.7352941176470589, 0.7606617647058823, 0.7709558823529412, 0.7886029411764706, 0.7996323529411765, 0.7985294117647059, 0.7533088235294118, 0.7555147058823529, 0.7643382352941176, 0.7797794117647059, 0.7783088235294118, 0.7672794117647059, 0.799264705882353, 0.7966911764705882, 0.7705882352941177, 0.7658088235294118, 0.7908088235294117, 0.8044117647058824, 0.7709558823529412, 0.7746323529411765, 0.7959558823529411, 0.805514705882353, 0.7974264705882353, 0.8, 0.8047794117647059, 0.8125, 0.7974264705882353, 0.7841911764705882, 0.7797794117647059, 0.8077205882352941, 0.7952205882352941, 0.794485294117647, 0.8, 0.7919117647058823, 0.7933823529411764, 0.7919117647058823, 0.7996323529411765, 0.788235294117647, 0.7970588235294118, 0.7753676470588236, 0.7845588235294118, 0.7886029411764706, 0.7816176470588235, 0.8003676470588236, 0.81875, 0.7988970588235295, 0.799264705882353, 0.799264705882353, 0.8183823529411764, 0.8231617647058823, 0.7985294117647059, 0.7952205882352941, 0.80625, 0.8051470588235294, 0.7948529411764705, 0.7573529411764706, 0.8003676470588236, 0.8, 0.7970588235294118, 0.7981617647058824, 0.8, 0.7761029411764706, 0.8106617647058824, 0.8003676470588236, 0.7952205882352941, 0.794485294117647, 0.7897058823529411, 0.8025735294117647, 0.7783088235294118, 0.7889705882352941, 0.7970588235294118, 0.7977941176470589, 0.7959558823529411, 0.8117647058823529, 0.7948529411764705, 0.8150735294117647, 0.7919117647058823, 0.7966911764705882, 0.7955882352941176, 0.8044117647058824, 0.7875, 0.7952205882352941, 0.7996323529411765, 0.8099264705882353, 0.7966911764705882, 0.799264705882353, 0.7974264705882353, 0.81875, 0.8172794117647059, 0.8113970588235294, 0.8117647058823529, 0.8036764705882353, 0.8080882352941177, 0.8165441176470588, 0.78125, 0.7977941176470589, 0.8040441176470589, 0.8102941176470588, 0.8154411764705882, 0.8040441176470589, 0.7886029411764706, 0.8110294117647059, 0.7970588235294118, 0.8077205882352941, 0.8044117647058824, 0.7985294117647059, 0.8088235294117647, 0.7886029411764706, 0.8077205882352941, 0.7856617647058823, 0.8080882352941177, 0.8125, 0.8224264705882353, 0.7753676470588236, 0.8007352941176471, 0.7981617647058824, 0.7808823529411765, 0.7911764705882353, 0.7933823529411764, 0.7904411764705882, 0.7768382352941177, 0.7860294117647059, 0.8069852941176471, 0.8147058823529412, 0.7963235294117647, 0.7933823529411764, 0.8011029411764706, 0.7746323529411765, 0.7735294117647059, 0.7955882352941176, 0.8058823529411765, 0.8, 0.8051470588235294, 0.774264705882353, 0.8058823529411765, 0.8044117647058824, 0.8066176470588236, 0.7981617647058824, 0.8051470588235294, 0.8022058823529412, 0.78125, 0.7886029411764706, 0.7845588235294118, 0.8110294117647059, 0.7834558823529412, 0.7985294117647059, 0.8044117647058824, 0.8007352941176471, 0.805514705882353, 0.8011029411764706, 0.7988970588235295, 0.7941176470588235, 0.8077205882352941, 0.8007352941176471, 0.79375, 0.8136029411764706, 0.8084558823529412, 0.8161764705882353, 0.8025735294117647, 0.8, 0.7926470588235294, 0.7955882352941176, 0.8051470588235294, 0.825735294117647, 0.799264705882353, 0.8128676470588235, 0.8073529411764706, 0.8036764705882353, 0.7922794117647058, 0.7941176470588235, 0.8084558823529412, 0.8136029411764706, 0.794485294117647, 0.80625, 0.8022058823529412, 0.794485294117647, 0.81875, 0.7981617647058824, 0.799264705882353, 0.8077205882352941, 0.8110294117647059, 0.8069852941176471, 0.8058823529411765, 0.8029411764705883, 0.8161764705882353, 0.7919117647058823, 0.8018382352941177, 0.7988970588235295, 0.7886029411764706, 0.7801470588235294, 0.7823529411764706, 0.79375, 0.8051470588235294, 0.8029411764705883, 0.799264705882353, 0.7948529411764705, 0.7966911764705882, 0.7834558823529412, 0.7691176470588236], 'pred_labels_best': array([6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 3, 7, 2, 6, 5, 6, 6, 7,\n",
      "       6, 7, 1, 6, 6, 7, 6, 6, 7, 6], dtype=int64), 'true_labels_best': array([6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 7, 7, 2, 6, 7, 6, 6, 7,\n",
      "       6, 7, 5, 6, 6, 2, 6, 6, 5, 6])}\n",
      "########\n",
      "result loss: 0.232972 \n",
      "result frac:0.925368\n",
      "{'n_windows': 1, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.23297204629463308, 'valid_frac_max': 0.9253676470588236, 'val_losses': [0.44163314892965205, 0.40366553050630233, 0.338313645475051, 0.3912277347024749, 0.33509482679998176, 0.3303147631094736, 0.3443913980441935, 0.32451736120616687, 0.3327410087865942, 0.33644616853664905, 0.3343535847523633, 0.2947696014581358, 0.31348893283044593, 0.31378346050486844, 0.302136083911447, 0.3006172558402314, 0.30908028369440754, 0.29123482818112656, 0.31038190801354015, 0.30323822138940587, 0.3018376202267759, 0.2901774188174921, 0.29548569976845207, 0.29807350981761427, 0.3044570123228957, 0.3022000908413354, 0.2929529401966754, 0.3065254568177111, 0.30303113609552385, 0.3010865941643715, 0.3024986442835892, 0.30633173795307383, 0.3358276112991221, 0.313317476519767, 0.31316976442056543, 0.30808729713453964, 0.3073485563783085, 0.30561727890196966, 0.3167033355902223, 0.3162675654186922, 0.31294857675538346, 0.33420096434214536, 0.3123953721540816, 0.3619880501838291, 0.3082494107239387, 0.32863920471247504, 0.3181074379121556, 0.31019963551970087, 0.2954377985614188, 0.3003810627495541, 0.3139567179714932, 0.2933275380774456, 0.2961643906638903, 0.27801772636525773, 0.28636389449238775, 0.2794331274488393, 0.29713138762642355, 0.27581332369762307, 0.28807185910203875, 0.29820705814396636, 0.2831346362390939, 0.2835252555415911, 0.2909709926913766, 0.2877578218631885, 0.29701682188931633, 0.29814961622743047, 0.29249951453769907, 0.30219397728934005, 0.291507778492044, 0.29143940407563657, 0.2856183830429526, 0.29717197803890005, 0.2896714730297818, 0.2835203809773221, 0.2736224036444636, 0.27642561806475413, 0.28401042755474065, 0.2829590874559739, 0.288871732441818, 0.2905220962184317, 0.29490473595831324, 0.2964793176773716, 0.2798570375670405, 0.2831672760931885, 0.2937051715657992, 0.28893516015480547, 0.27999034819795804, 0.2990309899344164, 0.29380067426930456, 0.3066935245605076, 0.30118876958594604, 0.2950273028191398, 0.28764372827375634, 0.2910860844833009, 0.2882486739579369, 0.2804013675188317, 0.2941063541699858, 0.29740311805816255, 0.30012149841469876, 0.30324510484933853, 0.2977830420522129, 0.27889424159246334, 0.29132814661544915, 0.3124255006804186, 0.30052894362631966, 0.2987964237875798, 0.2822854764978675, 0.26623536238775536, 0.27249965290812883, 0.271947608318399, 0.26708999751683543, 0.2530950803309679, 0.26012954742592925, 0.25744690579526563, 0.24520576840595287, 0.24260253286098732, 0.25129891476210425, 0.23297204629463308, 0.2503661501933547, 0.2732583091539495, 0.26054642743924084, 0.26123065508025534, 0.2680750482880017, 0.27194020736086016, 0.28346348733586424, 0.2791301925392712, 0.29232526684508603, 0.28625927273841467, 0.27980950768379603, 0.2723209913162624, 0.27259415537118914, 0.28552628747899744, 0.29222236203358454, 0.2966387622496661, 0.28387222002972573, 0.29025660209796006, 0.29113026156144983, 0.2969814350061557, 0.284685659671531, 0.2735711931305773, 0.2759194203597658, 0.27407818208722506, 0.2824073943802539, 0.2847981009413214, 0.28848965281949324, 0.2848427170339753, 0.2883625261814279, 0.2758270944523461, 0.28152391415308503, 0.29595629756941516, 0.2841973379254341, 0.2884202879579628, 0.28898886711720156, 0.29437742347226425, 0.2949902651941075, 0.2974763992516434, 0.2928061278865618, 0.29212380809819, 0.294810928995995, 0.2808760887450155, 0.2821365509839619, 0.2847077765228117, 0.2882281509392402, 0.2883230923291515, 0.28391446418621963, 0.2810188612955458, 0.2835511453449726, 0.274563359315781, 0.27553203850984576, 0.26466793632682634, 0.2633219255846651, 0.2542377069373341, 0.2602088134516688, 0.2644975411979591, 0.27879279054263056, 0.2709021653541747, 0.2876569460858317, 0.2922392423319466, 0.27885044427479017, 0.27844994278515084, 0.28752439350766296, 0.27131793961367184, 0.2697311393259203, 0.26535581440171774, 0.26915827113039353, 0.2808279996409136, 0.26834348768872374, 0.2897136082544046, 0.2838505696286173, 0.27269491448121913, 0.27438628134482046, 0.27945093924508374, 0.26805517011705565, 0.2689985978691017, 0.27402127170387436, 0.26526568391743827, 0.27094600796699525, 0.25604715732967154, 0.2722806136836024, 0.26772556063883446], 'val_fracs': [0.8797794117647059, 0.8808823529411764, 0.8926470588235295, 0.8871323529411764, 0.8948529411764706, 0.9073529411764706, 0.8977941176470589, 0.9, 0.9007352941176471, 0.8974264705882353, 0.893014705882353, 0.9051470588235294, 0.9047794117647059, 0.9044117647058824, 0.9117647058823529, 0.9095588235294118, 0.9058823529411765, 0.9077205882352941, 0.90625, 0.9099264705882353, 0.9099264705882353, 0.9066176470588235, 0.9033088235294118, 0.9080882352941176, 0.9018382352941177, 0.9040441176470588, 0.9047794117647059, 0.9003676470588236, 0.9095588235294118, 0.9077205882352941, 0.9022058823529412, 0.9047794117647059, 0.89375, 0.8996323529411765, 0.9047794117647059, 0.9022058823529412, 0.9003676470588236, 0.9011029411764706, 0.9018382352941177, 0.9029411764705882, 0.9066176470588235, 0.8944852941176471, 0.9066176470588235, 0.8919117647058824, 0.9040441176470588, 0.8904411764705882, 0.893014705882353, 0.8904411764705882, 0.9066176470588235, 0.9014705882352941, 0.8985294117647059, 0.9051470588235294, 0.9102941176470588, 0.9150735294117647, 0.9128676470588235, 0.9117647058823529, 0.9095588235294118, 0.9121323529411764, 0.913235294117647, 0.90625, 0.9091911764705882, 0.9099264705882353, 0.9069852941176471, 0.9033088235294118, 0.9099264705882353, 0.9066176470588235, 0.9110294117647059, 0.9018382352941177, 0.9040441176470588, 0.9077205882352941, 0.9110294117647059, 0.9102941176470588, 0.9143382352941176, 0.9088235294117647, 0.9113970588235294, 0.9113970588235294, 0.9136029411764706, 0.9099264705882353, 0.9106617647058823, 0.9095588235294118, 0.9091911764705882, 0.9139705882352941, 0.913235294117647, 0.9154411764705882, 0.9102941176470588, 0.9084558823529412, 0.9110294117647059, 0.9073529411764706, 0.9033088235294118, 0.9029411764705882, 0.9007352941176471, 0.9036764705882353, 0.9077205882352941, 0.9055147058823529, 0.9084558823529412, 0.9051470588235294, 0.9011029411764706, 0.9036764705882353, 0.9047794117647059, 0.8988970588235294, 0.9018382352941177, 0.9051470588235294, 0.9022058823529412, 0.9007352941176471, 0.9055147058823529, 0.9025735294117647, 0.9047794117647059, 0.9110294117647059, 0.9040441176470588, 0.9066176470588235, 0.9117647058823529, 0.9169117647058823, 0.9176470588235294, 0.9121323529411764, 0.9180147058823529, 0.9231617647058824, 0.9158088235294117, 0.9235294117647059, 0.9147058823529411, 0.9073529411764706, 0.9113970588235294, 0.9191176470588235, 0.9128676470588235, 0.9117647058823529, 0.9128676470588235, 0.9161764705882353, 0.9084558823529412, 0.9066176470588235, 0.9084558823529412, 0.9136029411764706, 0.9117647058823529, 0.9117647058823529, 0.9099264705882353, 0.9113970588235294, 0.9136029411764706, 0.9099264705882353, 0.9106617647058823, 0.9080882352941176, 0.9077205882352941, 0.9121323529411764, 0.9158088235294117, 0.9102941176470588, 0.9102941176470588, 0.9117647058823529, 0.9066176470588235, 0.9073529411764706, 0.9077205882352941, 0.9117647058823529, 0.9158088235294117, 0.9084558823529412, 0.9077205882352941, 0.9106617647058823, 0.9113970588235294, 0.9058823529411765, 0.9066176470588235, 0.9077205882352941, 0.9077205882352941, 0.9110294117647059, 0.9091911764705882, 0.9099264705882353, 0.9113970588235294, 0.9121323529411764, 0.9106617647058823, 0.9073529411764706, 0.9091911764705882, 0.9088235294117647, 0.9121323529411764, 0.9150735294117647, 0.9136029411764706, 0.9136029411764706, 0.9165441176470588, 0.9180147058823529, 0.9205882352941176, 0.9180147058823529, 0.9165441176470588, 0.9253676470588236, 0.9165441176470588, 0.9161764705882353, 0.9147058823529411, 0.9128676470588235, 0.9143382352941176, 0.9180147058823529, 0.9147058823529411, 0.919485294117647, 0.9191176470588235, 0.9147058823529411, 0.9143382352941176, 0.9058823529411765, 0.9080882352941176, 0.9095588235294118, 0.9077205882352941, 0.9121323529411764, 0.9154411764705882, 0.913235294117647, 0.913235294117647, 0.9191176470588235, 0.9117647058823529, 0.9213235294117647, 0.9150735294117647, 0.9158088235294117], 'pred_labels_best': array([7, 6, 6, 7, 5, 6, 6, 6, 6, 6, 2, 7, 6, 6, 6, 7, 6, 2, 6, 2, 5, 7,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 7], dtype=int64), 'true_labels_best': array([7, 6, 6, 1, 2, 6, 6, 6, 6, 6, 5, 7, 6, 6, 6, 4, 6, 2, 6, 5, 5, 7,\n",
      "       6, 6, 6, 6, 6, 6, 6, 7, 6, 7])}\n",
      "########\n",
      "result loss: 0.954318 \n",
      "result frac:0.779044\n",
      "{'n_windows': 20, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.9543183382819681, 'valid_frac_max': 0.7790441176470588, 'val_losses': [1.2978386570425595, 1.3444742567398968, 1.149190189557917, 1.2052485143437106, 1.2306637343238382, 1.2209833832348094, 1.2076086542185616, 0.9910573166959425, 1.277126218992121, 1.3392598572899312, 1.1506324894288007, 1.1847962926415836, 1.231356797498815, 1.1884565718033735, 1.1860121341312633, 1.2230916619300842, 1.1821392094387728, 1.0706567315494313, 1.2715188545339249, 1.2630772801006542, 1.257238256230074, 1.334991626178517, 1.2302534383886001, 1.0690174530534182, 1.2090379644842708, 1.2266071424764746, 1.1961453129263486, 1.1803571343421937, 1.2208927953944486, 1.217881014066584, 1.2462944675894345, 1.238925863013548, 1.220696868615992, 1.202168284444248, 1.1091025492724251, 1.15866542002734, 1.1911833791171804, 1.2036568760871886, 1.2405020138796639, 1.0970730101360995, 1.0406386045848621, 0.9543183382819681, 1.1818454749443952, 0.9565834753653583, 1.0339989956687479, 1.1966040386873134, 1.1850104114588569, 1.2044582591337316, 1.166540832379285, 1.159005088665906, 1.0681058091275832, 1.1608104867093703, 1.1380496859550475, 1.1657659684910493, 1.0467138732180876, 1.0941590673783246, 1.1460202413446763, 1.0366791493752423, 1.1147056663737578, 1.1793319225311278, 1.2054734356263104, 1.1591079992406508, 1.1841098736314213, 1.19346423219232, 1.1866284559754765, 1.166723370552063, 1.18508500141256, 1.1718868795563193, 1.18474301590639, 1.159290866992053, 1.0155869119307575, 1.133434821577633, 1.176599035543554, 1.1760529188548818, 1.1720778135692371, 1.1781853752977707, 1.1687449981184568, 1.1909358985283796, 1.1794970189823824, 1.2290311974637649, 1.1629208235179678, 1.1594147759325364, 1.1883072811014512, 1.2150566304431243, 1.2159427818130044, 1.2139095152125638, 1.2197866201400758, 1.2702214423347922, 1.172218785566442, 1.4043504055808573, 1.1821125507354737, 1.0734594948151532, 1.2046151890474208, 1.214692131210776, 1.1843103955773746, 1.2082688485874848, 1.176024018315708, 1.2123322374680463, 1.1848060790230246, 1.1922192517448873, 1.172163292239694, 1.1496362356578602, 1.1544131566496456, 1.2291249864241656, 1.2180264592170715, 1.1830824347103344, 1.186419056443607, 1.1541527593837064, 1.1722302226459278, 1.1824006788870867, 1.1893513791701373, 1.173913166102241, 1.1339351633015802, 1.1869501057793113, 1.162406948033501, 1.1810014577472912, 1.1470245908288395, 1.1620341364075155, 1.1712888822836034, 1.1561614387175616, 1.1568058427642374, 1.1545762994710136, 1.1943713531774633, 1.1174767382004682, 1.1206561221795923, 1.0762279741904315, 1.1835183466182035, 1.115854513645172, 1.1214435696601868, 1.1262702808660618, 1.1441188054926255, 1.0428760549601386, 1.2377390868523541, 1.119684533511891, 1.0437410957673017, 1.0401477498166702, 1.0578010453897364, 1.1655068895396063, 0.9932269678396337, 1.0316768288612366, 1.0805320066564224, 1.0377426315756406, 1.028958847242243, 1.0605817991144517, 1.0820379109943614, 1.0335950739243451, 1.1284837498384364, 1.051158838412341, 1.1176780560437372, 1.1204588574521681, 1.1156404901953305, 1.0145783915239222, 1.1616837052737965, 1.1404498492970185, 1.0152283584370332, 1.1433018733473386, 1.0617215829737046, 1.0953435049337499, 1.033142455886392, 0.9761860658140743, 1.1959402701433968, 1.1518203517969916, 1.155950080647188, 0.981599882069756, 1.1455176444614634, 1.147454169217278, 1.026205618942485, 1.0105553535854115, 1.1278750061988831, 1.128587715064778, 1.123681585227742, 1.1040662085308748, 1.1410140486324534, 1.1271257049897139, 1.1535155625904308, 1.1373105056145612, 1.1500252723693847, 1.132642644293168, 1.125056199466481, 1.1549475396380704, 1.1185883739415337, 1.106616551735822, 1.109702638317557, 1.1222118328599369, 1.1102522737839642, 1.1004194252631243, 1.0963045660187216, 1.1111169730915742, 1.1298447658033932, 1.114325554931865, 1.1389612436294556, 1.1197625356561998, 1.159438974015853, 1.167310759600471, 1.0849405807607315, 1.1183037806959713, 1.1006756095325245, 1.094757280630224, 1.1179490587290595, 1.103474212394041], 'val_fracs': [0.6610294117647059, 0.7334558823529411, 0.7404411764705883, 0.7389705882352942, 0.7422794117647059, 0.7378676470588236, 0.7139705882352941, 0.7573529411764706, 0.7231617647058823, 0.7025735294117647, 0.7136029411764706, 0.7297794117647058, 0.725, 0.45073529411764707, 0.7474264705882353, 0.7088235294117647, 0.7371323529411765, 0.7481617647058824, 0.4658088235294118, 0.40036764705882355, 0.7011029411764705, 0.06727941176470588, 0.05551470588235294, 0.7360294117647059, 0.06507352941176471, 0.4264705882352941, 0.27683823529411766, 0.7194852941176471, 0.7338235294117647, 0.75, 0.07389705882352941, 0.4488970588235294, 0.7375, 0.6860294117647059, 0.7444852941176471, 0.09852941176470588, 0.7301470588235294, 0.7272058823529411, 0.24227941176470588, 0.7231617647058823, 0.7261029411764706, 0.7242647058823529, 0.7051470588235295, 0.7246323529411764, 0.7352941176470589, 0.7301470588235294, 0.7297794117647058, 0.7216911764705882, 0.7341911764705882, 0.7275735294117647, 0.7555147058823529, 0.73125, 0.7400735294117647, 0.7731617647058824, 0.7415441176470589, 0.7198529411764706, 0.743014705882353, 0.7323529411764705, 0.7448529411764706, 0.07463235294117647, 0.08308823529411764, 0.7202205882352941, 0.06838235294117648, 0.07279411764705883, 0.7290441176470588, 0.7389705882352942, 0.7305147058823529, 0.6330882352941176, 0.7264705882352941, 0.7459558823529412, 0.7389705882352942, 0.7268382352941176, 0.7363970588235295, 0.41323529411764703, 0.7316176470588235, 0.6209558823529412, 0.6227941176470588, 0.06691176470588235, 0.6294117647058823, 0.5963235294117647, 0.4801470588235294, 0.7470588235294118, 0.07536764705882353, 0.6025735294117647, 0.5761029411764705, 0.6573529411764706, 0.65625, 0.15735294117647058, 0.6764705882352942, 0.18823529411764706, 0.7033088235294118, 0.7275735294117647, 0.6838235294117647, 0.7286764705882353, 0.7143382352941177, 0.7169117647058824, 0.08639705882352941, 0.07426470588235294, 0.7433823529411765, 0.7345588235294118, 0.7393382352941177, 0.6985294117647058, 0.7316176470588235, 0.6547794117647059, 0.725735294117647, 0.4345588235294118, 0.7161764705882353, 0.6988970588235294, 0.705514705882353, 0.6919117647058823, 0.06397058823529411, 0.6463235294117647, 0.4588235294117647, 0.7194852941176471, 0.7213235294117647, 0.6161764705882353, 0.7058823529411765, 0.7349264705882353, 0.7485294117647059, 0.09191176470588236, 0.7341911764705882, 0.74375, 0.6323529411764706, 0.7477941176470588, 0.7540441176470588, 0.7617647058823529, 0.7558823529411764, 0.7643382352941176, 0.7639705882352941, 0.6386029411764705, 0.23713235294117646, 0.7544117647058823, 0.09889705882352941, 0.7371323529411765, 0.7540441176470588, 0.7305147058823529, 0.7477941176470588, 0.30661764705882355, 0.7643382352941176, 0.7731617647058824, 0.7558823529411764, 0.7790441176470588, 0.7544117647058823, 0.7658088235294118, 0.7496323529411765, 0.7422794117647059, 0.5224264705882353, 0.7558823529411764, 0.5224264705882353, 0.5455882352941176, 0.6077205882352941, 0.7279411764705882, 0.1275735294117647, 0.4922794117647059, 0.7492647058823529, 0.5091911764705882, 0.7588235294117647, 0.7779411764705882, 0.74375, 0.7492647058823529, 0.5977941176470588, 0.7488970588235294, 0.48455882352941176, 0.7713235294117647, 0.768014705882353, 0.743014705882353, 0.7415441176470589, 0.7716911764705883, 0.7709558823529412, 0.11727941176470588, 0.0974264705882353, 0.7783088235294118, 0.7625, 0.7731617647058824, 0.7738970588235294, 0.30772058823529413, 0.6088235294117647, 0.7459558823529412, 0.7485294117647059, 0.5275735294117647, 0.7121323529411765, 0.7547794117647059, 0.6981617647058823, 0.7731617647058824, 0.7669117647058824, 0.7518382352941176, 0.12610294117647058, 0.40588235294117647, 0.10330882352941176, 0.11139705882352942, 0.7727941176470589, 0.7393382352941177, 0.43419117647058825, 0.09889705882352941, 0.763235294117647, 0.74375, 0.7650735294117647, 0.7724264705882353, 0.12095588235294118, 0.6930147058823529], 'pred_labels_best': array([6, 0, 6, 5, 3, 5, 7, 7, 3, 6, 6, 6, 6, 5, 6, 7, 6, 5, 6, 6, 6, 6,\n",
      "       0, 6, 6, 6, 6, 6, 6, 5, 5, 3], dtype=int64), 'true_labels_best': array([6, 7, 6, 7, 7, 2, 4, 7, 2, 6, 6, 6, 6, 7, 6, 5, 6, 7, 6, 6, 6, 6,\n",
      "       2, 6, 6, 6, 6, 6, 6, 7, 7, 5])}\n",
      "########\n",
      "result loss: 0.571452 \n",
      "result frac:0.816176\n",
      "{'n_windows': 50, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.5714520815540762, 'valid_frac_max': 0.8161764705882353, 'val_losses': [0.7971647374770221, 0.7048185271375319, 0.7616511309848112, 0.7883275179301992, 0.750008423538769, 0.7464160284575294, 0.6518447146696202, 0.6431755427051993, 0.6171796065919539, 0.6660938897553612, 0.6898247129776899, 0.6565630449968226, 0.7975223295828875, 0.7504533455652349, 0.6314637923941893, 0.7032611193025813, 0.6285944228663164, 0.6185996141503839, 0.621953164304004, 0.6657170390381533, 0.6242048919200898, 0.6777750492095947, 0.6279886286048328, 0.6592982754987828, 0.631867915391922, 0.6214810585274416, 0.6294540464878082, 0.6329147366916432, 0.6317921215997023, 0.6341729847823873, 0.5995228658704197, 0.6119674309211619, 0.6295438131865333, 0.5984135299921036, 0.629200316176695, 0.6312537379124585, 0.630159275672015, 0.586873388465713, 0.6364319699652055, 0.6593134198118659, 0.6048885447137496, 0.6632978022098541, 0.6313910340561586, 0.6327043890953064, 0.6035875884925618, 0.6769028072848039, 0.6129873822717106, 0.6214377024594475, 0.6135279855307411, 0.5983997742919361, 0.5886286016772775, 0.6180399505531087, 0.6099200658938464, 0.6061423094833598, 0.6191633192931905, 0.6189614904277465, 0.6140454316840452, 0.6561065400347991, 0.6211405606830821, 0.6450938896221273, 0.6473948355983286, 0.6554455729091868, 0.6315372952643563, 0.5777840926366694, 0.5997024469515857, 0.6209168325452243, 0.6063701405244715, 0.62785102725029, 0.602765105752384, 0.5881573547335232, 0.5909870731479981, 0.580944695893456, 0.5942491143941879, 0.6481746498276205, 0.5908547303255867, 0.6256884473211625, 0.6299238930730259, 0.6146143276901807, 0.6008394087062162, 0.5933940620983348, 0.5790732688763562, 0.6127288772779352, 0.6305516400757958, 0.6031046173151802, 0.6100890753900303, 0.6153439381543327, 0.6169808491187937, 0.6046261976746952, 0.5858896595590255, 0.6246393999632667, 0.6097941675606896, 0.6093734916518716, 0.6069203758940978, 0.6224893622538623, 0.5957544028759003, 0.5981746428153094, 0.6152747287469752, 0.593026640310007, 0.6169195732649635, 0.6017290623749003, 0.642131775266984, 0.6222425239927628, 0.6462179143639172, 0.6100944168427411, 0.5856330818989698, 0.6118767154567382, 0.6415327983744005, 0.6065741395249086, 0.5880436932339388, 0.5728656007963069, 0.585371445908266, 0.5892422889961916, 0.5844125134103438, 0.6173515053356395, 0.6162151117535198, 0.6237231009146746, 0.5856205540544847, 0.6123417040880988, 0.5855678640744265, 0.620093352654401, 0.608472927703577, 0.610666137933731, 0.6158349261564366, 0.5718073933440097, 0.5941877025015214, 0.6080779436756583, 0.6078031117425245, 0.6152498297831591, 0.6253246465150047, 0.6064850489882861, 0.5778463056858848, 0.5904110501794254, 0.6064824630232418, 0.6193776761784273, 0.5967170757405899, 0.5815226688104518, 0.6102103850420784, 0.610972035632414, 0.6033755456700044, 0.6090158052304212, 0.5918145179748535, 0.5942573545610204, 0.6040580868721008, 0.5891742858816595, 0.5857118069249041, 0.6083970979732626, 0.6088715749628404, 0.604172644720358, 0.6066814247299643, 0.5809845947167452, 0.5993949195917915, 0.6028150537434747, 0.6114114652661716, 0.604221348376835, 0.6093570057083578, 0.580037030227044, 0.5928454125628752, 0.6124756150385913, 0.5855028976412381, 0.5934351889526143, 0.5825627689852434, 0.5971174375099294, 0.6269797391751233, 0.5714520815540762, 0.5807685866075404, 0.5832072920659009, 0.6079185801393846, 0.5820375396924861, 0.5838480816167944, 0.6395266178776237, 0.586616582029006, 0.5997638171210008, 0.6002242745722042, 0.6069190498660593, 0.6136607085957246, 0.5901936820324729, 0.6017592240782345, 0.5909696582485647, 0.6065406122628381, 0.6131493882221334, 0.5773327206864076, 0.595852304907406, 0.6038167073446161, 0.5770861289080451, 0.5867144025423947, 0.6007225509952097, 0.59632029077586, 0.5809004454051747, 0.5754957114948945, 0.5776952322791604, 0.594240109184209, 0.5755353489342858, 0.5727993975667393, 0.5813545868677251, 0.6018298389280543, 0.582444579986965, 0.5955959626856973, 0.5957436551066005, 0.5850624957505395, 0.5766559578040067], 'val_fracs': [0.7235294117647059, 0.7790441176470588, 0.7621323529411764, 0.7797794117647059, 0.7772058823529412, 0.7724264705882353, 0.7650735294117647, 0.794485294117647, 0.8, 0.7823529411764706, 0.7713235294117647, 0.7808823529411765, 0.7555147058823529, 0.7551470588235294, 0.805514705882353, 0.7863970588235294, 0.7816176470588235, 0.7738970588235294, 0.805514705882353, 0.7919117647058823, 0.7988970588235295, 0.7897058823529411, 0.8011029411764706, 0.7775735294117647, 0.7996323529411765, 0.7985294117647059, 0.7952205882352941, 0.7959558823529411, 0.7816176470588235, 0.8058823529411765, 0.8003676470588236, 0.8051470588235294, 0.8, 0.7863970588235294, 0.7900735294117647, 0.7977941176470589, 0.79375, 0.8161764705882353, 0.8029411764705883, 0.7886029411764706, 0.8033088235294118, 0.7830882352941176, 0.7834558823529412, 0.7908088235294117, 0.805514705882353, 0.7889705882352941, 0.7966911764705882, 0.8022058823529412, 0.8051470588235294, 0.7955882352941176, 0.805514705882353, 0.788235294117647, 0.7779411764705882, 0.7981617647058824, 0.794485294117647, 0.7966911764705882, 0.7959558823529411, 0.7948529411764705, 0.7893382352941176, 0.7775735294117647, 0.7952205882352941, 0.7727941176470589, 0.7860294117647059, 0.8132352941176471, 0.8099264705882353, 0.7915441176470588, 0.8044117647058824, 0.7897058823529411, 0.805514705882353, 0.7919117647058823, 0.8014705882352942, 0.8117647058823529, 0.7963235294117647, 0.7941176470588235, 0.8125, 0.7871323529411764, 0.8044117647058824, 0.8047794117647059, 0.8091911764705882, 0.8080882352941177, 0.8084558823529412, 0.7834558823529412, 0.7849264705882353, 0.7963235294117647, 0.8011029411764706, 0.8, 0.7988970588235295, 0.7930147058823529, 0.79375, 0.7816176470588235, 0.7955882352941176, 0.7988970588235295, 0.8069852941176471, 0.7933823529411764, 0.8154411764705882, 0.8110294117647059, 0.7963235294117647, 0.7933823529411764, 0.7948529411764705, 0.7827205882352941, 0.7794117647058824, 0.7827205882352941, 0.7746323529411765, 0.7845588235294118, 0.8025735294117647, 0.7933823529411764, 0.7952205882352941, 0.7816176470588235, 0.799264705882353, 0.8099264705882353, 0.8113970588235294, 0.7977941176470589, 0.8095588235294118, 0.7926470588235294, 0.7941176470588235, 0.8029411764705883, 0.7948529411764705, 0.7911764705882353, 0.8080882352941177, 0.799264705882353, 0.7908088235294117, 0.7904411764705882, 0.7904411764705882, 0.7996323529411765, 0.8033088235294118, 0.8003676470588236, 0.7731617647058824, 0.7761029411764706, 0.7797794117647059, 0.7930147058823529, 0.8139705882352941, 0.8036764705882353, 0.7930147058823529, 0.7908088235294117, 0.8007352941176471, 0.7988970588235295, 0.7919117647058823, 0.7919117647058823, 0.7963235294117647, 0.7878676470588235, 0.79375, 0.7794117647058824, 0.7834558823529412, 0.7996323529411765, 0.794485294117647, 0.7819852941176471, 0.7911764705882353, 0.7830882352941176, 0.7889705882352941, 0.8106617647058824, 0.7926470588235294, 0.7827205882352941, 0.7863970588235294, 0.8014705882352942, 0.7911764705882353, 0.8099264705882353, 0.8069852941176471, 0.79375, 0.8121323529411765, 0.7985294117647059, 0.8073529411764706, 0.8102941176470588, 0.7875, 0.8110294117647059, 0.7988970588235295, 0.799264705882353, 0.8007352941176471, 0.8047794117647059, 0.8113970588235294, 0.7970588235294118, 0.8132352941176471, 0.8044117647058824, 0.7981617647058824, 0.7926470588235294, 0.7963235294117647, 0.8025735294117647, 0.7955882352941176, 0.8003676470588236, 0.78125, 0.7860294117647059, 0.7996323529411765, 0.8132352941176471, 0.7981617647058824, 0.7988970588235295, 0.8029411764705883, 0.7919117647058823, 0.8033088235294118, 0.8047794117647059, 0.8147058823529412, 0.8, 0.8047794117647059, 0.8077205882352941, 0.8011029411764706, 0.8113970588235294, 0.8025735294117647, 0.8095588235294118, 0.8095588235294118, 0.8029411764705883, 0.8022058823529412, 0.8110294117647059], 'pred_labels_best': array([7, 7, 7, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 7, 6, 6, 7, 7, 3, 7, 6], dtype=int64), 'true_labels_best': array([7, 5, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 7, 6, 6, 6, 6, 6,\n",
      "       6, 6, 7, 6, 6, 7, 7, 2, 2, 6])}\n",
      "########\n",
      "result loss: 1.094808 \n",
      "result frac:0.119048\n",
      "{'n_windows': 100, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 1.0948081080402647, 'valid_frac_max': 0.11904761904761904, 'val_losses': [1.323141329345249, 1.2887104522614252, 1.2795853983788263, 1.3160269288789659, 1.25577170082501, 1.2853015647048043, 1.2773540985016596, 1.1959322534856343, 1.1973742679471062, 1.21733154853185, 1.215633946515265, 1.256086270014445, 1.2260713435354686, 1.281270866592725, 1.2235974499157496, 1.2162753492593765, 1.2213670590094157, 1.2410688627333868, 1.2265990390664054, 1.238902291371709, 1.200973853468895, 1.2458035846551259, 1.2367930511633556, 1.1966688235600789, 1.2018267597470964, 1.1972445881082898, 1.212672002258755, 1.210623562335968, 1.1922481074219657, 1.1988012826158887, 1.196767957437606, 1.229397971715246, 1.205713609144801, 1.2130557561204547, 1.240860484895252, 1.1871888091166813, 1.2036403304054624, 1.173273803222747, 1.1990524842625572, 1.1818050727957772, 1.1985298962820143, 1.2112408550012679, 1.2034572071972347, 1.1998471262909116, 1.196062601747967, 1.1778423942270733, 1.1807498591286796, 1.159356490487144, 1.2098107160556884, 1.161804246050971, 1.1584679221823102, 1.1951066972244353, 1.184248333175977, 1.1484731428679966, 1.177972979488827, 1.1587279389301937, 1.1962989817063014, 1.1670147081216176, 1.15553484360377, 1.1521764674357005, 1.1633721206869398, 1.1477079526299523, 1.1367883362940379, 1.1439113304728554, 1.1648179917108445, 1.1719526577563513, 1.1456820205563591, 1.176515369188218, 1.1591476514225914, 1.1559933459474927, 1.1464518521513258, 1.148259930667423, 1.1464034922066189, 1.1418756345907848, 1.1388181753101803, 1.152682296576954, 1.1486193970555352, 1.1894539083753313, 1.156962513923645, 1.1589525796118236, 1.1336973288229533, 1.1555301596721013, 1.1475556336698078, 1.1540878266096115, 1.1658264526299067, 1.1584129014185496, 1.1354750260001136, 1.1412679843959355, 1.177836027883348, 1.12359362414905, 1.124848960410981, 1.1383274694283803, 1.137836183820452, 1.1471671093077886, 1.132422163372948, 1.14971946818488, 1.1487208562237876, 1.1521144076472236, 1.1463669006313597, 1.1597792186907359, 1.1466664636418933, 1.1431128907771337, 1.145200904636156, 1.1420040691182727, 1.174581562479337, 1.1254636212473823, 1.1466008446046285, 1.1227326222828455, 1.1348931406225478, 1.1318306568123044, 1.125122097276506, 1.135503495023364, 1.1374036236887886, 1.1234351836499714, 1.1257787949982143, 1.1480634269260226, 1.136846423858688, 1.1233192476488294, 1.1299301797435397, 1.1145636325790769, 1.1121777934687478, 1.1474541610195523, 1.1249587819689797, 1.1282656483706974, 1.136385393284616, 1.115877423967634, 1.1191636281354087, 1.146922114349547, 1.1153561536754881, 1.1351707265490578, 1.121403089591435, 1.140692799573853, 1.1697366833686829, 1.1033643320912407, 1.1066308347951799, 1.1147882817756563, 1.1255163380077906, 1.1130270482528777, 1.1013822789703096, 1.1039354595400037, 1.114034089304152, 1.1436993862901415, 1.1164546878564925, 1.119809627532959, 1.1394343844481878, 1.1363005957433157, 1.1421641012032826, 1.1408682273966926, 1.1197677439167386, 1.1199674712760108, 1.1192304832594735, 1.1280049255916051, 1.1290295464651925, 1.134325114034471, 1.1252502713884627, 1.1234700097924186, 1.1076015326238813, 1.1053841227576846, 1.147718948267755, 1.1002634643089204, 1.1188836445411046, 1.120042425535974, 1.1164339851765406, 1.1395241447857447, 1.1220234355756216, 1.106760547274635, 1.1074889728001185, 1.1155915778307688, 1.0956940005222957, 1.151829508798463, 1.125221753404254, 1.099324637225696, 1.1159436092490242, 1.1041934937238693, 1.1160367663417543, 1.1355101955788476, 1.145354505096163, 1.1184271354050863, 1.0948081080402647, 1.116505042428062, 1.1192197735820497, 1.1081428393012, 1.1093582411607106, 1.1000139841011591, 1.1197826301767713, 1.1075393373057956, 1.1193453783080691, 1.1397869686285655, 1.1213928794577008, 1.1060755508286613, 1.1173675195092247, 1.1004575043916702, 1.1104260675963902, 1.1085584922915412, 1.1079645391021455, 1.1091748454741068, 1.118842689763932, 1.1148126316922051, 1.1053388828323, 1.1360919922590256], 'val_fracs': [0.07626488095238096, 0.050967261904761904, 0.05357142857142857, 0.043154761904761904, 0.0625, 0.07105654761904762, 0.07775297619047619, 0.05654761904761905, 0.05803571428571429, 0.08370535714285714, 0.07105654761904762, 0.05282738095238095, 0.0546875, 0.0818452380952381, 0.10193452380952381, 0.06733630952380952, 0.04799107142857143, 0.046130952380952384, 0.05282738095238095, 0.05505952380952381, 0.055431547619047616, 0.04352678571428571, 0.07738095238095238, 0.049479166666666664, 0.05282738095238095, 0.049479166666666664, 0.05319940476190476, 0.046130952380952384, 0.04724702380952381, 0.08035714285714286, 0.09226190476190477, 0.0900297619047619, 0.08668154761904762, 0.08296130952380952, 0.06212797619047619, 0.07886904761904762, 0.07775297619047619, 0.07998511904761904, 0.05766369047619048, 0.09895833333333333, 0.09300595238095238, 0.08370535714285714, 0.056919642857142856, 0.07700892857142858, 0.07552083333333333, 0.055431547619047616, 0.07552083333333333, 0.08333333333333333, 0.05431547619047619, 0.08742559523809523, 0.09375, 0.050595238095238096, 0.0625, 0.06882440476190477, 0.061755952380952384, 0.08630952380952381, 0.0662202380952381, 0.05431547619047619, 0.09486607142857142, 0.057291666666666664, 0.08779761904761904, 0.06696428571428571, 0.06845238095238096, 0.07068452380952381, 0.08370535714285714, 0.07068452380952381, 0.0974702380952381, 0.07552083333333333, 0.0822172619047619, 0.07291666666666667, 0.09858630952380952, 0.10974702380952381, 0.11086309523809523, 0.11123511904761904, 0.10081845238095238, 0.10081845238095238, 0.0974702380952381, 0.06956845238095238, 0.0546875, 0.0665922619047619, 0.07663690476190477, 0.0665922619047619, 0.08482142857142858, 0.0625, 0.07254464285714286, 0.06287202380952381, 0.06547619047619048, 0.058779761904761904, 0.06584821428571429, 0.07105654761904762, 0.09821428571428571, 0.07589285714285714, 0.06510416666666667, 0.09598214285714286, 0.08147321428571429, 0.07626488095238096, 0.06733630952380952, 0.07328869047619048, 0.07477678571428571, 0.06473214285714286, 0.05766369047619048, 0.07589285714285714, 0.06473214285714286, 0.07366071428571429, 0.057291666666666664, 0.10825892857142858, 0.0744047619047619, 0.07924107142857142, 0.05952380952380952, 0.07998511904761904, 0.06956845238095238, 0.0703125, 0.06398809523809523, 0.07514880952380952, 0.06101190476190476, 0.057291666666666664, 0.07998511904761904, 0.07663690476190477, 0.0822172619047619, 0.07738095238095238, 0.09561011904761904, 0.07589285714285714, 0.06063988095238095, 0.06138392857142857, 0.09300595238095238, 0.10342261904761904, 0.06770833333333333, 0.06547619047619048, 0.09263392857142858, 0.06994047619047619, 0.06919642857142858, 0.07849702380952381, 0.09300595238095238, 0.09895833333333333, 0.09523809523809523, 0.11421130952380952, 0.09114583333333333, 0.07589285714285714, 0.07291666666666667, 0.07700892857142858, 0.10639880952380952, 0.08891369047619048, 0.0822172619047619, 0.10342261904761904, 0.07998511904761904, 0.07738095238095238, 0.06436011904761904, 0.06696428571428571, 0.05915178571428571, 0.09337797619047619, 0.08556547619047619, 0.09226190476190477, 0.0822172619047619, 0.07254464285714286, 0.08035714285714286, 0.0665922619047619, 0.07217261904761904, 0.0744047619047619, 0.0744047619047619, 0.0900297619047619, 0.06733630952380952, 0.09412202380952381, 0.11383928571428571, 0.11904761904761904, 0.08110119047619048, 0.10714285714285714, 0.09635416666666667, 0.09895833333333333, 0.07477678571428571, 0.06547619047619048, 0.07700892857142858, 0.0740327380952381, 0.09970238095238096, 0.08444940476190477, 0.07924107142857142, 0.07105654761904762, 0.07626488095238096, 0.07924107142857142, 0.06956845238095238, 0.0974702380952381, 0.08333333333333333, 0.10639880952380952, 0.08705357142857142, 0.08258928571428571, 0.08928571428571429, 0.08668154761904762, 0.07849702380952381, 0.1056547619047619, 0.11495535714285714, 0.0744047619047619, 0.07366071428571429, 0.11160714285714286, 0.10677083333333333, 0.09561011904761904, 0.06808035714285714, 0.06882440476190477, 0.050595238095238096, 0.10974702380952381, 0.11569940476190477, 0.08779761904761904], 'pred_labels_best': array([3, 4, 7, 4, 4, 4, 4, 7, 4, 0, 5, 4, 4, 4, 7, 4, 4, 4, 4, 5, 5, 7,\n",
      "       4, 6, 4, 4, 7, 5, 4, 4, 4, 7], dtype=int64), 'true_labels_best': array([7, 6, 7, 6, 6, 6, 6, 7, 6, 2, 7, 6, 6, 6, 7, 6, 6, 6, 6, 7, 7, 7,\n",
      "       6, 7, 6, 6, 7, 5, 6, 6, 6, 7])}\n",
      "########\n",
      "result loss: 0.609275 \n",
      "result frac:0.805804\n",
      "{'n_windows': 250, 'n_stride': 20, 'shuffle': True, 'dropout_prob': 0.3, 'batch_size': 32, 'hidden_dim': 50, 'n_layers_LSTM': 1, 'output_dim': 8, 'input_dim': 40, 'lr': 0.005, 'N_epochs': 1000, 'print_every': 5, 'clip': 5, 'valid_loss_min': 0.609274994404543, 'valid_frac_max': 0.8058035714285714, 'val_losses': [1.0076339819601603, 0.8553503120229358, 0.758206290503343, 0.8498947570721308, 0.7331951971919763, 0.7284545085969425, 0.7455238366410846, 0.6740676618757702, 0.7125151027880964, 0.6806891475405011, 0.7085605565281141, 0.713947502275308, 0.6891786374506497, 0.6949419932706016, 0.6679967588612011, 0.6935279511270069, 0.6799243387012255, 0.6956196622479529, 0.7012265512631053, 0.7070015743374825, 0.7232609068353971, 0.674400809620108, 0.697235933017163, 0.6942739923085485, 0.6599675126018978, 0.7147360475999969, 0.6579796349008878, 0.7421494250496229, 0.6766746909845442, 0.7497165345010304, 0.6751680352858135, 0.6438470112071151, 0.6666817317406336, 0.6694991602784112, 0.6611557175360975, 0.6717637418991044, 0.6637440561538651, 0.6785493556942258, 0.6774131710685435, 0.6626715475604648, 0.6773739254900387, 0.6682420103322892, 0.672431285182635, 0.6707440486976078, 0.6526674731146722, 0.6777468266941252, 0.6393006743774527, 0.6377614970718112, 0.6594855278020814, 0.686248446504275, 0.6613288386946633, 0.6320912768798215, 0.6776148703481469, 0.6780010813048908, 0.7003465461589041, 0.655120890764963, 0.648697762617043, 0.6516395640515146, 0.6379073780207407, 0.6752266028807277, 0.6719298412402471, 0.6515256847654071, 0.6343879036250568, 0.6350817836466289, 0.663384389309656, 0.6659394876942748, 0.6616849211000261, 0.650769021539461, 0.6500123810200464, 0.6542674182426362, 0.6277913986926987, 0.6435096746399289, 0.6614850337306658, 0.6547493917778844, 0.6343057879379818, 0.6602924962838491, 0.6530645558876651, 0.6718040488305546, 0.6306498966046742, 0.662349093528021, 0.681580200791359, 0.6893420205229804, 0.6546618725572314, 0.6562472826668194, 0.6586145779916218, 0.6880217879301026, 0.6670138197285789, 0.666973729573545, 0.6494144641217732, 0.6454972160004434, 0.6375798462402253, 0.6504716784471557, 0.6356042518856979, 0.6580253106852373, 0.6214175266878945, 0.6502504996245816, 0.6525060289672443, 0.6385595502243155, 0.646507026893752, 0.6475910288946969, 0.6575593072034064, 0.6652383694336528, 0.6281485993947301, 0.6409780894007001, 0.6328610288245338, 0.6199416358556066, 0.6350612659894285, 0.6512160084786869, 0.6319223080007803, 0.6296095145600182, 0.6103253135723727, 0.6460839103729952, 0.6401229788150106, 0.6691399387837875, 0.6414579821839219, 0.6753737972605796, 0.6586128444898696, 0.6473168732509726, 0.6388413824495816, 0.6402939174856458, 0.6466002868754523, 0.6498986816122418, 0.6523862976048674, 0.6262651596750531, 0.6482800788113049, 0.636513009312607, 0.6365087717061951, 0.6428432273013251, 0.6402998651776995, 0.6535368473047302, 0.632431599976761, 0.6603104500543504, 0.648393102699802, 0.6305787616542408, 0.6523824660550981, 0.6461946059550557, 0.6419373307199705, 0.6423326469957829, 0.6518739600266729, 0.6496848705269042, 0.6325110005480903, 0.6385881757097585, 0.6500126660934517, 0.6398478388076737, 0.6438438636916024, 0.6429150908121041, 0.6803665884903499, 0.6452968163149697, 0.6397306734607333, 0.6328634552302814, 0.6236933998408771, 0.6336610881345612, 0.6340347698756627, 0.6256852589902424, 0.6339191684410685, 0.6494660753579367, 0.6319223926180885, 0.6218025585015615, 0.6648517669666381, 0.6328752718511081, 0.6363128842342467, 0.6134847760910079, 0.6311378665268421, 0.6330252428139959, 0.6418331776346479, 0.6242492784346853, 0.6274826590503965, 0.6275143985237394, 0.6212991082242557, 0.6367045243581136, 0.6402673256539163, 0.6632645555904934, 0.6445121032496294, 0.6663315347617581, 0.6704244184352103, 0.6368687685046878, 0.6292802460846447, 0.6350987940317109, 0.6915538772231057, 0.6400619477388405, 0.6426320395299366, 0.6656246447847003, 0.659925347282773, 0.648646624847537, 0.617448723032361, 0.6354486328505334, 0.6431357370955604, 0.6363265794657526, 0.6436972522309848, 0.636186477151655, 0.6357912314789635, 0.6239572770538784, 0.625714089898836, 0.648760281148411, 0.651906590731371, 0.609274994404543, 0.6505266873254663, 0.6457964451540084, 0.6513292745110535, 0.6135233658410254], 'val_fracs': [0.7433035714285714, 0.7555803571428571, 0.7633928571428571, 0.7239583333333334, 0.7593005952380952, 0.7630208333333334, 0.7191220238095238, 0.7641369047619048, 0.7689732142857143, 0.7689732142857143, 0.7626488095238095, 0.765625, 0.7674851190476191, 0.7619047619047619, 0.7741815476190477, 0.7872023809523809, 0.7667410714285714, 0.7767857142857143, 0.7555803571428571, 0.7764136904761905, 0.7533482142857143, 0.7823660714285714, 0.7689732142857143, 0.7708333333333334, 0.7767857142857143, 0.7630208333333334, 0.7909226190476191, 0.7596726190476191, 0.7667410714285714, 0.7555803571428571, 0.7775297619047619, 0.7842261904761905, 0.7805059523809523, 0.7760416666666666, 0.7920386904761905, 0.7860863095238095, 0.7845982142857143, 0.7671130952380952, 0.7741815476190477, 0.7860863095238095, 0.7752976190476191, 0.7760416666666666, 0.7760416666666666, 0.7816220238095238, 0.7693452380952381, 0.7633928571428571, 0.7886904761904762, 0.7953869047619048, 0.7879464285714286, 0.7745535714285714, 0.7749255952380952, 0.7879464285714286, 0.765625, 0.7715773809523809, 0.7734375, 0.7886904761904762, 0.7972470238095238, 0.7890625, 0.7983630952380952, 0.7842261904761905, 0.7805059523809523, 0.7860863095238095, 0.7875744047619048, 0.7883184523809523, 0.7860863095238095, 0.7816220238095238, 0.7756696428571429, 0.7808779761904762, 0.7872023809523809, 0.78125, 0.7946428571428571, 0.7819940476190477, 0.7682291666666666, 0.7767857142857143, 0.7801339285714286, 0.7827380952380952, 0.7831101190476191, 0.7771577380952381, 0.7864583333333334, 0.7853422619047619, 0.7738095238095238, 0.7697172619047619, 0.7879464285714286, 0.7834821428571429, 0.7898065476190477, 0.7715773809523809, 0.7805059523809523, 0.7808779761904762, 0.7790178571428571, 0.7879464285714286, 0.7797619047619048, 0.7805059523809523, 0.8028273809523809, 0.7879464285714286, 0.7976190476190477, 0.7924107142857143, 0.8009672619047619, 0.7857142857142857, 0.7853422619047619, 0.7875744047619048, 0.7898065476190477, 0.7790178571428571, 0.7924107142857143, 0.7797619047619048, 0.7961309523809523, 0.7927827380952381, 0.7916666666666666, 0.7860863095238095, 0.7927827380952381, 0.7827380952380952, 0.8035714285714286, 0.7868303571428571, 0.7942708333333334, 0.7797619047619048, 0.7942708333333334, 0.7779017857142857, 0.7760416666666666, 0.7764136904761905, 0.7935267857142857, 0.7912946428571429, 0.7957589285714286, 0.7827380952380952, 0.7842261904761905, 0.7987351190476191, 0.7991071428571429, 0.7898065476190477, 0.7927827380952381, 0.7872023809523809, 0.7894345238095238, 0.7819940476190477, 0.7838541666666666, 0.7886904761904762, 0.7845982142857143, 0.7991071428571429, 0.7872023809523809, 0.7860863095238095, 0.7883184523809523, 0.7767857142857143, 0.7842261904761905, 0.7901785714285714, 0.7909226190476191, 0.7961309523809523, 0.7935267857142857, 0.7931547619047619, 0.7860863095238095, 0.7972470238095238, 0.7920386904761905, 0.8031994047619048, 0.7942708333333334, 0.8024553571428571, 0.8043154761904762, 0.7927827380952381, 0.7875744047619048, 0.7886904761904762, 0.7912946428571429, 0.7853422619047619, 0.7872023809523809, 0.7834821428571429, 0.765625, 0.7920386904761905, 0.7790178571428571, 0.8005952380952381, 0.7886904761904762, 0.7901785714285714, 0.7905505952380952, 0.7905505952380952, 0.7894345238095238, 0.7909226190476191, 0.7909226190476191, 0.7775297619047619, 0.7834821428571429, 0.7875744047619048, 0.7786458333333334, 0.7741815476190477, 0.7849702380952381, 0.7853422619047619, 0.7972470238095238, 0.7938988095238095, 0.7622767857142857, 0.7849702380952381, 0.7987351190476191, 0.78125, 0.7842261904761905, 0.7834821428571429, 0.7972470238095238, 0.7916666666666666, 0.7898065476190477, 0.7849702380952381, 0.7790178571428571, 0.7838541666666666, 0.7868303571428571, 0.7965029761904762, 0.7961309523809523, 0.7842261904761905, 0.7864583333333334, 0.8058035714285714, 0.7823660714285714, 0.7838541666666666, 0.7961309523809523, 0.7924107142857143], 'pred_labels_best': array([6, 6, 3, 7, 6, 6, 6, 6, 2, 7, 6, 7, 7, 6, 6, 6, 6, 3, 6, 7, 6, 6,\n",
      "       3, 6, 3, 7, 6, 6, 6, 6, 6, 7], dtype=int64), 'true_labels_best': array([6, 6, 5, 7, 6, 4, 6, 6, 7, 7, 6, 7, 5, 6, 6, 6, 6, 2, 6, 7, 6, 6,\n",
      "       7, 6, 2, 7, 6, 6, 6, 6, 6, 7])}\n"
     ]
    }
   ],
   "source": [
    "for result in results: \n",
    "    print(\"########\\nresult loss: {:.6f} \\nresult frac:{:.6f}\".format(result[0]['valid_loss_min'], result[0]['valid_frac_max']))\n",
    "    print(result[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(b\"test.res\",\"wb\")\n",
    "pickle.dump(result[-1],filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
