{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy.lib.recfunctions as rfn\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000, only_rat1 = False):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    if not only_rat1:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    else:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_1_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_1_vel_labels])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, splits= (0.7, 0.2, 0.1), separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = data_len // chunk_size\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = splits[0]*num_chunks\n",
    "    val_chunk_idx_bound = (splits[0]+splits[1])*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    features_val = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    targets_val = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        elif i < val_chunk_idx_bound:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('val')\n",
    "            features_val.append(curr_chunk)\n",
    "            targets_val.append(curr_chunk_t)\n",
    "\n",
    "#     print(len(features_separate))\n",
    "#     print(len(targets_separate))\n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    features_val = n.concatenate(features_val, axis=0)\n",
    "    \n",
    "    targets_val = n.concatenate(targets_val)\n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, features_val, targets_train, targets_test, targets_val\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_val, features_separate,\\\n",
    "                targets_train, targets_test, targets_val, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / n.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return (n.arccos(n.clip(n.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = ['Female1']#list(all_data.keys())\n",
    "datas = []\n",
    "# for key in all_data.keys():\n",
    "#     all_data[key] = calculate_velocity(all_data[key])\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels, None, False)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 40)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_points = n.zeros((features_all.shape[0], int(features_all.shape[1]/2), 2))\n",
    "# t = 1000\n",
    "angles = []\n",
    "for t in range(features_all.shape[0]):\n",
    "    points = features_all[t]\n",
    "    # x, y format\n",
    "    anchor_point = n.array([points[1], points[0]])\n",
    "    second_point = n.array([points[3], points[2]])\n",
    "    anchor_vector = second_point - anchor_point\n",
    "    angle_to_rotate = -angle_between(anchor_vector, n.array((1,0)))\n",
    "    if n.all(anchor_vector[1] < 0): angle_to_rotate *= -1\n",
    "    angles.append(angle_to_rotate)\n",
    "#     if angle_to_rotate < 0: angle_to_rotate += 2*n.pi\n",
    "    # given vector (x1, y1), rotating it by A around origin gives:\n",
    "    # x2 = cosA x1 - sinA y1\n",
    "    # y2 = sinA x1 + cosA y1\n",
    "    num_points = int(len(points)/2)\n",
    "    new_points = n.zeros((num_points,2))\n",
    "    for points_idx in range(1,num_points):\n",
    "        second_point = n.array([points[points_idx*2+1], points[points_idx*2]])\n",
    "        vector = second_point - anchor_point\n",
    "        new_x = n.cos(angle_to_rotate) * vector[0] - n.sin(angle_to_rotate) * vector[1]\n",
    "        new_y = n.sin(angle_to_rotate) * vector[0] + n.cos(angle_to_rotate) * vector[1]\n",
    "\n",
    "        ego_points[t][points_idx] = [new_x, new_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 18000\n",
    "scale = 100\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(ego_points[t,i,0], ego_points[t,i,1])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-scale,scale)\n",
    "plt.xlim(-scale,scale)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(features_all[t][i*2+1], features_all[t][i*2])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-scale,scale)\n",
    "plt.xlim(-scale,scale)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\videos\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\data\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\results\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\model\"\n",
      "1  videos from the directory C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1 were added to the project.\n",
      "Copying the videos \n",
      "\n",
      "A VAME project has been created. \n",
      "\n",
      "Next use vame.create_trainset(config) to split your data into a train and test set. \n",
      "Afterwards you can use vame.rnn_model() to train the model on your data.\n"
     ]
    }
   ],
   "source": [
    "config = vame.init_new_project(project='single-mouse-f1', videos = ['C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1'], \n",
    "                      working_directory='C:/Users/Neuropixel/AH-EN/swc-en-social/vame', videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\single-mouse-f1-Nov20-2020\\\\config.yaml'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 10, 2)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(, 'w+') as f:\n",
    "f = \"C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\single-mouse-f1-Nov20-2020\\\\data\\\\Female1\\\\Female1-PE-seq.npy\"\n",
    "n.save(f, ego_points.reshape(-1, 20).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset.\n",
      "Lenght of train data: 24216\n",
      "Lenght of test data: 6054\n"
     ]
    }
   ],
   "source": [
    "trainset = vame.create_trainset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RNN model!\n",
      "Using CUDA\n",
      "GPU active: True\n",
      "GPU used: Quadro P620\n",
      "Latent Dimensions: 30, Beta: 1, lr: 0.0005\n",
      "Compute mean and std for temporal dataset.\n",
      "Initialize train data. Datapoints 24216\n",
      "Initialize test data. Datapoints 6054\n",
      "Epoch: 1\n",
      "Train: \n",
      "Epoch: 1.  loss: 240607.7500\n",
      "Average Train loss: 45083.7238, MSE-Loss: 28401.3596, MSE-Future-Loss 16682.3641, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 2242.4663, MSE-Loss: 2242.4663, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 2\n",
      "Train: \n",
      "Epoch: 2.  loss: 20272.3301\n",
      "Average Train loss: 19168.1105, MSE-Loss: 11154.4096, MSE-Future-Loss 8013.7009, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 1694.9520, MSE-Loss: 1694.9520, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 3\n",
      "Train: \n",
      "Epoch: 3.  loss: 18419.5059\n",
      "Average Train loss: 15205.0624, MSE-Loss: 8588.5394, MSE-Future-Loss 6616.5230, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 1333.4067, MSE-Loss: 1333.4067, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 4\n",
      "Train: \n",
      "Epoch: 4.  loss: 13919.4775\n",
      "Average Train loss: 12936.9534, MSE-Loss: 7025.1373, MSE-Future-Loss 5903.0632, KL-Loss: 5.2824,  Kmeans-Loss: 3.4705, weigt: 0.5000\n",
      "Test: \n",
      "Average Test loss: 1123.5923, MSE-Loss: 1115.6178, KL-Loss: 5.6659, Kmeans-Loss: 2.3086\n",
      "lr: 0.0005\n",
      "Epoch: 5\n",
      "Train: \n",
      "Epoch: 5.  loss: 10919.7246\n",
      "Average Train loss: 11917.4542, MSE-Loss: 6187.9039, MSE-Future-Loss 5716.5309, KL-Loss: 8.2965,  Kmeans-Loss: 4.7229, weigt: 0.6250\n",
      "Test: \n",
      "Average Test loss: 1021.9699, MSE-Loss: 1010.2224, KL-Loss: 8.5156, Kmeans-Loss: 3.2320\n",
      "lr: 0.0005\n",
      "Epoch: 6\n",
      "Train: \n",
      "Epoch: 6.  loss: 9640.6240\n",
      "Average Train loss: 10968.6813, MSE-Loss: 5458.8559, MSE-Future-Loss 5491.9125, KL-Loss: 11.8305,  Kmeans-Loss: 6.0825, weigt: 0.7500\n",
      "Test: \n",
      "Average Test loss: 900.1999, MSE-Loss: 884.1543, KL-Loss: 11.8048, Kmeans-Loss: 4.2408\n",
      "lr: 0.0005\n",
      "Epoch: 7\n",
      "Train: \n",
      "Epoch: 7.  loss: 11003.0742\n",
      "Average Train loss: 10238.0558, MSE-Loss: 4965.2872, MSE-Future-Loss 5249.6406, KL-Loss: 15.5939,  Kmeans-Loss: 7.5341, weigt: 0.8750\n",
      "Test: \n",
      "Average Test loss: 780.9250, MSE-Loss: 760.1892, KL-Loss: 15.3693, Kmeans-Loss: 5.3665\n",
      "lr: 0.0005\n",
      "Epoch: 8\n",
      "Train: \n",
      "Epoch: 8.  loss: 9435.9287\n",
      "Average Train loss: 9638.3636, MSE-Loss: 4465.4086, MSE-Future-Loss 5144.0141, KL-Loss: 19.8470,  Kmeans-Loss: 9.0938, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 749.2237, MSE-Loss: 723.0848, KL-Loss: 19.5198, Kmeans-Loss: 6.6191\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 9\n",
      "Train: \n",
      "Epoch: 9.  loss: 9329.5068\n",
      "Average Train loss: 9220.4676, MSE-Loss: 4228.6595, MSE-Future-Loss 4960.5878, KL-Loss: 21.6768,  Kmeans-Loss: 9.5435, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 718.6952, MSE-Loss: 690.5878, KL-Loss: 21.0847, Kmeans-Loss: 7.0227\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 10\n",
      "Train: \n",
      "Epoch: 10.  loss: 8982.4746\n",
      "Average Train loss: 8804.3826, MSE-Loss: 3926.5726, MSE-Future-Loss 4844.7238, KL-Loss: 23.1654,  Kmeans-Loss: 9.9208, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 673.3208, MSE-Loss: 644.0805, KL-Loss: 21.9621, Kmeans-Loss: 7.2782\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 11\n",
      "Train: \n",
      "Epoch: 11.  loss: 7759.4263\n",
      "Average Train loss: 8511.2123, MSE-Loss: 3674.9094, MSE-Future-Loss 4800.9995, KL-Loss: 24.9922,  Kmeans-Loss: 10.3112, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 650.2838, MSE-Loss: 618.5625, KL-Loss: 24.0034, Kmeans-Loss: 7.7180\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 12\n",
      "Train: \n",
      "Epoch: 12.  loss: 8600.6006\n",
      "Average Train loss: 8135.1934, MSE-Loss: 3424.2547, MSE-Future-Loss 4673.6046, KL-Loss: 26.6779,  Kmeans-Loss: 10.6562, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 655.2769, MSE-Loss: 621.2802, KL-Loss: 25.9483, Kmeans-Loss: 8.0484\n",
      "lr: 0.0005\n",
      "Epoch: 13\n",
      "Train: \n",
      "Epoch: 13.  loss: 8133.7847\n",
      "Average Train loss: 7993.7957, MSE-Loss: 3430.0685, MSE-Future-Loss 4524.5875, KL-Loss: 28.1773,  Kmeans-Loss: 10.9625, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 593.8340, MSE-Loss: 558.8780, KL-Loss: 26.7100, Kmeans-Loss: 8.2460\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 14\n",
      "Train: \n",
      "Epoch: 14.  loss: 7877.2700\n",
      "Average Train loss: 7640.3998, MSE-Loss: 3222.5716, MSE-Future-Loss 4376.9366, KL-Loss: 29.6706,  Kmeans-Loss: 11.2210, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 600.3789, MSE-Loss: 563.1553, KL-Loss: 28.5947, Kmeans-Loss: 8.6289\n",
      "lr: 0.0005\n",
      "Epoch: 15\n",
      "Train: \n",
      "Epoch: 15.  loss: 7824.0034\n",
      "Average Train loss: 7481.8780, MSE-Loss: 3150.1004, MSE-Future-Loss 4289.5145, KL-Loss: 30.7846,  Kmeans-Loss: 11.4785, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 613.5966, MSE-Loss: 576.1397, KL-Loss: 28.7273, Kmeans-Loss: 8.7296\n",
      "lr: 0.0005\n",
      "Epoch: 16\n",
      "Train: \n",
      "Epoch: 16.  loss: 7273.0571\n",
      "Average Train loss: 7261.6879, MSE-Loss: 3015.4448, MSE-Future-Loss 4202.7746, KL-Loss: 31.7736,  Kmeans-Loss: 11.6948, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 550.9567, MSE-Loss: 512.4247, KL-Loss: 29.6357, Kmeans-Loss: 8.8962\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 17\n",
      "Train: \n",
      "Epoch: 17.  loss: 7818.1855\n",
      "Average Train loss: 7048.9870, MSE-Loss: 2983.6234, MSE-Future-Loss 4020.6059, KL-Loss: 32.8221,  Kmeans-Loss: 11.9357, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 543.5142, MSE-Loss: 503.6278, KL-Loss: 30.6699, Kmeans-Loss: 9.2165\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 18\n",
      "Train: \n",
      "Epoch: 18.  loss: 7738.3813\n",
      "Average Train loss: 6972.2946, MSE-Loss: 2890.0522, MSE-Future-Loss 4036.4555, KL-Loss: 33.6614,  Kmeans-Loss: 12.1256, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 527.6534, MSE-Loss: 486.8173, KL-Loss: 31.4909, Kmeans-Loss: 9.3453\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 19\n",
      "Train: \n",
      "Epoch: 19.  loss: 6318.7573\n",
      "Average Train loss: 6758.6990, MSE-Loss: 2871.6210, MSE-Future-Loss 3840.1587, KL-Loss: 34.6043,  Kmeans-Loss: 12.3151, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 525.2751, MSE-Loss: 484.3605, KL-Loss: 31.4984, Kmeans-Loss: 9.4162\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 20\n",
      "Train: \n",
      "Epoch: 20.  loss: 6355.7056\n",
      "Average Train loss: 6531.5352, MSE-Loss: 2773.3821, MSE-Future-Loss 3710.2607, KL-Loss: 35.3723,  Kmeans-Loss: 12.5201, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 498.2609, MSE-Loss: 455.8102, KL-Loss: 32.8338, Kmeans-Loss: 9.6170\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 21\n",
      "Train: \n",
      "Epoch: 21.  loss: 5504.0864\n",
      "Average Train loss: 6496.3106, MSE-Loss: 2725.8641, MSE-Future-Loss 3721.3990, KL-Loss: 36.3179,  Kmeans-Loss: 12.7296, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 501.4061, MSE-Loss: 458.5295, KL-Loss: 33.0692, Kmeans-Loss: 9.8074\n",
      "lr: 0.0005\n",
      "Epoch: 22\n",
      "Train: \n",
      "Epoch: 22.  loss: 5667.3809\n",
      "Average Train loss: 6334.3668, MSE-Loss: 2664.6662, MSE-Future-Loss 3619.9314, KL-Loss: 36.8766,  Kmeans-Loss: 12.8926, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 511.7521, MSE-Loss: 467.7684, KL-Loss: 34.0427, Kmeans-Loss: 9.9410\n",
      "lr: 0.0005\n",
      "Epoch: 23\n",
      "Train: \n",
      "Epoch: 23.  loss: 6566.8071\n",
      "Average Train loss: 6150.0054, MSE-Loss: 2668.9892, MSE-Future-Loss 3430.2355, KL-Loss: 37.7039,  Kmeans-Loss: 13.0768, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 490.2865, MSE-Loss: 445.5562, KL-Loss: 34.6186, Kmeans-Loss: 10.1117\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 24\n",
      "Train: \n",
      "Epoch: 24.  loss: 6254.4185\n",
      "Average Train loss: 6083.4652, MSE-Loss: 2596.6489, MSE-Future-Loss 3434.9869, KL-Loss: 38.5716,  Kmeans-Loss: 13.2578, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 494.4017, MSE-Loss: 448.8386, KL-Loss: 35.2676, Kmeans-Loss: 10.2954\n",
      "lr: 0.0005\n",
      "Epoch: 25\n",
      "Train: \n",
      "Epoch: 25.  loss: 7023.8755\n",
      "Average Train loss: 5895.8900, MSE-Loss: 2552.3771, MSE-Future-Loss 3290.8827, KL-Loss: 39.2075,  Kmeans-Loss: 13.4226, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 487.7902, MSE-Loss: 441.2724, KL-Loss: 36.0496, Kmeans-Loss: 10.4682\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 26\n",
      "Train: \n",
      "Epoch: 26.  loss: 5982.5605\n",
      "Average Train loss: 5843.4120, MSE-Loss: 2552.7461, MSE-Future-Loss 3237.1974, KL-Loss: 39.8602,  Kmeans-Loss: 13.6083, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 483.8322, MSE-Loss: 437.7875, KL-Loss: 35.5436, Kmeans-Loss: 10.5012\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 27\n",
      "Train: \n",
      "Epoch: 27.  loss: 5668.4062\n",
      "Average Train loss: 5635.1895, MSE-Loss: 2461.9891, MSE-Future-Loss 3119.3356, KL-Loss: 40.1503,  Kmeans-Loss: 13.7144, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 483.0468, MSE-Loss: 436.7704, KL-Loss: 35.6639, Kmeans-Loss: 10.6124\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 28\n",
      "Train: \n",
      "Epoch: 28.  loss: 5212.9102\n",
      "Average Train loss: 5449.7989, MSE-Loss: 2379.2307, MSE-Future-Loss 3016.1108, KL-Loss: 40.6042,  Kmeans-Loss: 13.8533, weigt: 1.0000\n",
      "Test: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 480.6234, MSE-Loss: 433.5298, KL-Loss: 36.3212, Kmeans-Loss: 10.7724\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 29\n",
      "Train: \n",
      "Epoch: 29.  loss: 5331.4399\n",
      "Average Train loss: 5378.4647, MSE-Loss: 2384.6454, MSE-Future-Loss 2938.5734, KL-Loss: 41.2276,  Kmeans-Loss: 14.0183, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 489.5567, MSE-Loss: 441.0815, KL-Loss: 37.4461, Kmeans-Loss: 11.0291\n",
      "lr: 0.0005\n",
      "Epoch: 30\n",
      "Train: \n",
      "Epoch: 30.  loss: 5077.9932\n",
      "Average Train loss: 5236.8491, MSE-Loss: 2301.8510, MSE-Future-Loss 2879.0088, KL-Loss: 41.8048,  Kmeans-Loss: 14.1845, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 491.8839, MSE-Loss: 443.3529, KL-Loss: 37.4714, Kmeans-Loss: 11.0595\n",
      "lr: 0.0005\n",
      "Epoch: 31\n",
      "Train: \n",
      "Epoch: 31.  loss: 5059.1431\n",
      "Average Train loss: 5132.6219, MSE-Loss: 2278.4973, MSE-Future-Loss 2797.8666, KL-Loss: 41.9854,  Kmeans-Loss: 14.2725, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 463.1592, MSE-Loss: 413.7404, KL-Loss: 38.1983, Kmeans-Loss: 11.2205\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 32\n",
      "Train: \n",
      "Epoch: 32.  loss: 4729.0615\n",
      "Average Train loss: 5021.4497, MSE-Loss: 2227.1582, MSE-Future-Loss 2737.2906, KL-Loss: 42.5958,  Kmeans-Loss: 14.4051, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 485.1872, MSE-Loss: 435.1253, KL-Loss: 38.7909, Kmeans-Loss: 11.2710\n",
      "lr: 0.0005\n",
      "Epoch: 33\n",
      "Train: \n",
      "Epoch: 33.  loss: 4928.7554\n",
      "Average Train loss: 5015.8640, MSE-Loss: 2280.9925, MSE-Future-Loss 2677.3435, KL-Loss: 42.9776,  Kmeans-Loss: 14.5503, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 461.4305, MSE-Loss: 411.1105, KL-Loss: 38.7987, Kmeans-Loss: 11.5213\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 34\n",
      "Train: \n",
      "Epoch: 34.  loss: 4861.5220\n",
      "Average Train loss: 4775.7723, MSE-Loss: 2152.8257, MSE-Future-Loss 2564.9242, KL-Loss: 43.3529,  Kmeans-Loss: 14.6694, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 449.0610, MSE-Loss: 397.5497, KL-Loss: 39.8448, Kmeans-Loss: 11.6665\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 35\n",
      "Train: \n",
      "Epoch: 35.  loss: 5372.5996\n",
      "Average Train loss: 4658.4705, MSE-Loss: 2136.1952, MSE-Future-Loss 2463.4390, KL-Loss: 43.9950,  Kmeans-Loss: 14.8411, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 450.4447, MSE-Loss: 398.4970, KL-Loss: 40.1397, Kmeans-Loss: 11.8081\n",
      "lr: 0.0005\n",
      "Epoch: 36\n",
      "Train: \n",
      "Epoch: 36.  loss: 4228.3354\n",
      "Average Train loss: 4604.6672, MSE-Loss: 2095.3140, MSE-Future-Loss 2450.0180, KL-Loss: 44.3938,  Kmeans-Loss: 14.9414, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 435.4710, MSE-Loss: 383.0261, KL-Loss: 40.6260, Kmeans-Loss: 11.8189\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 37\n",
      "Train: \n",
      "Epoch: 37.  loss: 4331.4609\n",
      "Average Train loss: 4531.6889, MSE-Loss: 2097.1164, MSE-Future-Loss 2374.5797, KL-Loss: 44.9247,  Kmeans-Loss: 15.0681, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 456.3710, MSE-Loss: 403.1995, KL-Loss: 41.2029, Kmeans-Loss: 11.9686\n",
      "lr: 0.0005\n",
      "Epoch: 38\n",
      "Train: \n",
      "Epoch: 38.  loss: 4466.4429\n",
      "Average Train loss: 4507.1228, MSE-Loss: 2126.0309, MSE-Future-Loss 2320.9412, KL-Loss: 45.0120,  Kmeans-Loss: 15.1387, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 454.1327, MSE-Loss: 401.5116, KL-Loss: 40.6530, Kmeans-Loss: 11.9681\n",
      "lr: 0.0005\n",
      "Epoch: 39\n",
      "Train: \n",
      "Epoch: 39.  loss: 3810.5166\n",
      "Average Train loss: 4375.9104, MSE-Loss: 2046.2234, MSE-Future-Loss 2269.3795, KL-Loss: 45.1165,  Kmeans-Loss: 15.1911, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 446.5925, MSE-Loss: 392.4058, KL-Loss: 41.9886, Kmeans-Loss: 12.1981\n",
      "lr: 0.0005\n",
      "Epoch: 40\n",
      "Train: \n",
      "Epoch: 40.  loss: 4656.3945\n",
      "Average Train loss: 4259.3993, MSE-Loss: 2022.5968, MSE-Future-Loss 2175.3878, KL-Loss: 46.0263,  Kmeans-Loss: 15.3884, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 452.7581, MSE-Loss: 398.8756, KL-Loss: 41.6722, Kmeans-Loss: 12.2103\n",
      "lr: 0.0005\n",
      "Epoch: 41\n",
      "Train: \n",
      "Epoch: 41.  loss: 4109.0015\n",
      "Average Train loss: 4331.2780, MSE-Loss: 2049.9188, MSE-Future-Loss 2219.6509, KL-Loss: 46.2427,  Kmeans-Loss: 15.4657, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 486.4922, MSE-Loss: 431.1438, KL-Loss: 42.9496, Kmeans-Loss: 12.3988\n",
      "lr: 0.0005\n",
      "Epoch: 42\n",
      "Train: \n",
      "Epoch: 42.  loss: 4248.7114\n",
      "Average Train loss: 4179.1275, MSE-Loss: 2014.7118, MSE-Future-Loss 2102.1873, KL-Loss: 46.6661,  Kmeans-Loss: 15.5624, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 438.7648, MSE-Loss: 383.6255, KL-Loss: 42.7063, Kmeans-Loss: 12.4330\n",
      "lr: 0.0005\n",
      "Epoch: 43\n",
      "Train: \n",
      "Epoch: 43.  loss: 3625.3643\n",
      "Average Train loss: 4091.0649, MSE-Loss: 1996.2163, MSE-Future-Loss 2032.2554, KL-Loss: 46.9593,  Kmeans-Loss: 15.6340, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 431.9088, MSE-Loss: 376.3618, KL-Loss: 43.0596, Kmeans-Loss: 12.4873\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 44\n",
      "Train: \n",
      "Epoch: 44.  loss: 3709.6697\n",
      "Average Train loss: 3941.3862, MSE-Loss: 1909.1291, MSE-Future-Loss 1969.2563, KL-Loss: 47.2721,  Kmeans-Loss: 15.7288, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 431.1518, MSE-Loss: 375.1119, KL-Loss: 43.4385, Kmeans-Loss: 12.6013\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 45\n",
      "Train: \n",
      "Epoch: 45.  loss: 4770.7021\n",
      "Average Train loss: 3924.6574, MSE-Loss: 1921.9973, MSE-Future-Loss 1939.2884, KL-Loss: 47.5773,  Kmeans-Loss: 15.7945, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 444.0299, MSE-Loss: 387.6129, KL-Loss: 43.7882, Kmeans-Loss: 12.6288\n",
      "lr: 0.0005\n",
      "Epoch: 46\n",
      "Train: \n",
      "Epoch: 46.  loss: 3696.7146\n",
      "Average Train loss: 3940.1694, MSE-Loss: 1923.0460, MSE-Future-Loss 1953.0914, KL-Loss: 48.1288,  Kmeans-Loss: 15.9032, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 436.3710, MSE-Loss: 379.4260, KL-Loss: 44.1684, Kmeans-Loss: 12.7767\n",
      "lr: 0.0005\n",
      "Epoch: 47\n",
      "Train: \n",
      "Epoch: 47.  loss: 3908.5986\n",
      "Average Train loss: 3853.3818, MSE-Loss: 1917.8000, MSE-Future-Loss 1870.9924, KL-Loss: 48.5560,  Kmeans-Loss: 16.0334, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 443.6932, MSE-Loss: 386.6742, KL-Loss: 44.1426, Kmeans-Loss: 12.8764\n",
      "lr: 0.0005\n",
      "Epoch: 48\n",
      "Train: \n",
      "Epoch: 48.  loss: 3628.5781\n"
     ]
    }
   ],
   "source": [
    "vame.rnn_model(config, model_name='VAME', pretrained_weights=False, pretrained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
