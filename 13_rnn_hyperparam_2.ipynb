{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as n\n",
    "import numpy.lib.recfunctions as rfn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, split = 0.8, separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = int(data_len // chunk_size)\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = split*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        \n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    \n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, targets_train, targets_test\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_separate,\\\n",
    "                targets_train, targets_test, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, architecture):\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.D_in = self.architecture['D_in']\n",
    "        self.D_out = self.architecture['D_out']\n",
    "        self.hidden_dims = self.architecture['hidden_dims']\n",
    "        \n",
    "        self.layers = []\n",
    "        prev_dim = self.architecture['D_in']\n",
    "        for dim in self.architecture['hidden_dims']:\n",
    "            self.layers += [torch.nn.Linear(prev_dim, dim),\n",
    "                           torch.nn.ReLU()]\n",
    "            prev_dim = dim\n",
    "        self.layers += [torch.nn.Linear(prev_dim, self.D_out)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.trackers = {}\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.model.apply(init_weights)\n",
    "   \n",
    "    def start_trackers(self,track, reset_trackers=True):\n",
    "        if reset_trackers: self.trackers = {}\n",
    "        for t in track:\n",
    "            self.trackers[t] = []\n",
    "    \n",
    "    def track(self):\n",
    "        for variable in self.trackers.keys():\n",
    "            self.trackers[variable].append(copy.deepcopy(getattr(self, variable)))\n",
    "        \n",
    "    def learn(self,learning, training_set, test_set, reset_trackers=True, verbose=True):\n",
    "        \n",
    "        #self.init_weights()\n",
    "        \n",
    "        # set up variables\n",
    "        N_batch = learning['N_batch']\n",
    "        N_epochs = learning['N_epochs']\n",
    "        loss_fn = learning['loss_fn']\n",
    "        print_interval = learning['print_interval']\n",
    "        track = learning['track']\n",
    "        learning_rate = learning['learning_rate']\n",
    "        \n",
    "        self.learning = learning\n",
    "        \n",
    "        optimizer = learning['optimizer'](self.model.parameters(), learning_rate)\n",
    "        self.start_trackers(track, reset_trackers)\n",
    "        \n",
    "        # load data\n",
    "        x_train, y_train = training_set\n",
    "        x_test, y_test = test_set\n",
    "\n",
    "        N_training = len(y_train)\n",
    "        N_test = len(y_test)\n",
    "    \n",
    "        self.t = 0\n",
    "        tic = time.time()\n",
    "        end = False\n",
    "        for self.epoch_idx in range(N_epochs):\n",
    "            if verbose: print(\"### EPOCH {:2d} ###\".format(self.epoch_idx))\n",
    "                \n",
    "            # randomize batches\n",
    "            indices = n.random.choice(range(N_training), N_training, False)\n",
    "            num_batches = len(indices) // N_batch + 1\n",
    "\n",
    "            for self.batch_idx in range(num_batches):\n",
    "                # load batch\n",
    "                b_idx = self.batch_idx\n",
    "                x_train_batch = x_train[indices[b_idx*N_batch :(b_idx+1)*N_batch]]\n",
    "                y_train_batch = y_train[indices[b_idx*N_batch : (b_idx+1)*N_batch]]\n",
    "\n",
    "                \n",
    "                # predict, loss and learn\n",
    "                y_train_batch_pred = self.model(x_train_batch)\n",
    "                loss = loss_fn(y_train_batch_pred, y_train_batch)\n",
    "                self.train_loss = loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.t += 1\n",
    "\n",
    "                if self.batch_idx % print_interval == 0:\n",
    "                    \n",
    "#                     y_train_pred = self.model(x_train)\n",
    "#                     self.train_loss = loss_fn(y_train_pred, y_train).item()\n",
    "#                     pred_labels = n.argmax(y_train_pred.detach().numpy(),axis=1)\n",
    "#                     true_labels = y_train.detach().numpy()\n",
    "#                     correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.train_frac_correct = 0# n.mean(correct_preds)\n",
    "                    \n",
    "                    y_test_pred = self.model(x_test)\n",
    "                    self.test_loss = loss_fn(y_test_pred, y_test).item()\n",
    "                    pred_labels = n.argmax(y_test_pred.detach().numpy(),axis=1)\n",
    "                    true_labels = y_test.detach().numpy()\n",
    "                    correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.test_frac_correct = n.mean(correct_preds)\n",
    "\n",
    "                    toc = time.time()\n",
    "                    delta = toc - tic\n",
    "                    tic = toc\n",
    "                    print(\"Time: {:4.2f}, Batch {:3d}, Train Loss (for batch): {:4.2f}, Test Loss: {:4.2f}, Test Correct Frac: {:.3f}\".format(\\\n",
    "                                       delta, self.batch_idx, self.train_loss, self.test_loss, self.test_frac_correct))\n",
    "            \n",
    "                    self.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_net_results(net, x_test, y_test):\n",
    "    plt.plot(net.trackers['t'],net.trackers['train_loss'], label='train')\n",
    "    plt.plot(net.trackers['t'],net.trackers['test_loss'], label='test')\n",
    "    plt.title(\"Cross Entropy Loss through training\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,10)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(net.trackers['t'], net.trackers['test_frac_correct'])\n",
    "    plt.title(\"Fraction of correct labels on test set through training\")\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.show()\n",
    "    \n",
    "    max_perf_ind = n.argmax(net.trackers['test_frac_correct'])\n",
    "    min_loss_ind = n.argmin(net.trackers['test_loss'])\n",
    "    max_perf_model = net.trackers['model'][max_perf_ind]\n",
    "    \n",
    "    prediction_test = max_perf_model(x_test)\n",
    "    pred = n.argmax(prediction_test.detach().numpy(),axis=1)\n",
    "    true = y_test.detach().numpy()\n",
    "    confmat = confusion_matrix(true, pred, normalize='true')\n",
    "    f, ax = plt.subplots(figsize=(10,10))\n",
    "    m = ax.matshow(confmat, cmap='Blues', vmin=0,  vmax=1)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_xticks(list(range(len(categories))))\n",
    "    ax.set_xticklabels(categories, rotation=45)\n",
    "    ax.set_yticks(list(range(len(categories))))\n",
    "    ax.set_yticklabels(categories, rotation=45)\n",
    "    f.colorbar(m)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,output_dim, n_layers_LSTM, dropout_prob):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers_LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, \n",
    "                            num_layers  = n_layers_LSTM, dropout = dropout_prob,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.out_func = nn.Softmax(dim=1)\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "#         x = x.long()\n",
    "#         print(x.shape)\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(x)\n",
    "        else: \n",
    "            lstm_out, hidden = self.lstm(x,hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(batch_size, self.output_dim, -1)[:,:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(datas, n_window, n_stride, test_vids=[3,4],batch_size=32, shuffle=True):\n",
    "    datapoints = []\n",
    "    labels = []\n",
    "\n",
    "    trial_bounds = []\n",
    "\n",
    "    trial_idx = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for data in datas:  \n",
    "        trial_bounds.append(idx)\n",
    "        for i in range(int(data.shape[1]/n_stride)):\n",
    "            if (i*n_stride + n_window) < data.shape[1]:\n",
    "                datapoints.append(data[:,i*n_stride : i*n_stride + n_window])\n",
    "                new_labels = [all_data[trial_keys[trial_idx]]['ann'][i*n_stride + int(n_window/2)]]\n",
    "                new_labels = [target_map[l] for l in new_labels]\n",
    "                labels.append(new_labels)\n",
    "                idx += 1\n",
    "        trial_idx += 1\n",
    "    trial_bounds.append(idx)  \n",
    "    \n",
    "    Xs = n.stack(datapoints)\n",
    "    Ys = n.stack(labels)\n",
    "    num_features = Xs.shape[1]\n",
    "    \n",
    "    Xs_train = []; Ys_train = []; Xs_test = []; Ys_test = []\n",
    "    for i in range(len(trial_bounds)-1):\n",
    "        if i in test_vids:\n",
    "            Xs_test.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_test.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "        else:\n",
    "            Xs_train.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_train.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "#     print(Xs_train[0].shape)\n",
    "    Xs_train = n.concatenate(Xs_train,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_train = n.concatenate(Ys_train,axis=0).reshape(-1)\n",
    "    Xs_test = n.concatenate(Xs_test,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_test = n.concatenate(Ys_test,axis=0).reshape(-1)\n",
    "    \n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_train), torch.from_numpy(Ys_train))\n",
    "    test_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_test), torch.from_numpy(Ys_test))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    \n",
    "    return train_data, train_loader, test_data, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(params, datas, device, random_val):\n",
    "    train_data, train_loader, val_data, val_loader \\\n",
    "            = extract_data(datas, params['n_windows'], params['n_stride'],\n",
    "                        batch_size=params['batch_size'],shuffle=params['shuffle'])\n",
    "        \n",
    "    model = LSTM_net(params['input_dim'], params['hidden_dim'], params['output_dim'], params['n_layers_LSTM'], params['dropout_prob'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    params['val_losses'] = []\n",
    "    params['val_fracs'] = []\n",
    "    \n",
    "    counter = 0 \n",
    "    tic = time.time()\n",
    "    for i in range(params['N_epochs']):\n",
    "        h = model.init_hidden(params['batch_size'])\n",
    "        for inputs, labels in train_loader:\n",
    "            if inputs.shape[0] != params['batch_size']: continue\n",
    "            counter += 1\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs.float(), (h[0].float(), h[1].float()))\n",
    "            loss = criterion(output.squeeze(), labels[:].long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), params['clip'])\n",
    "            optimizer.step()\n",
    "                        \n",
    "        toc = time.time() - tic\n",
    "        tic = time.time()\n",
    "        \n",
    "        if counter%params['print_every'] == 0:\n",
    "            val_h = model.init_hidden(params['batch_size'])\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            fracs = []\n",
    "            for inp, lab in val_loader:\n",
    "                if inp.shape[0] != params['batch_size']: continue\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), [val_h[0].float(), val_h[1].float()])\n",
    "                val_loss = criterion(out.squeeze(), lab[:].long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred_labels = n.argmax(out.cpu().detach().numpy(),axis=1)\n",
    "                true_labels = lab.cpu().detach().numpy()\n",
    "                correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                fracs.append(n.mean(correct_preds))\n",
    "                \n",
    "            frac_correct_val = (n.mean(fracs))\n",
    "            params['val_losses'].append(n.mean(val_losses))\n",
    "            params['val_fracs'].append(frac_correct_val)\n",
    "            model.train()\n",
    "            print(\"\\tEpoch: {}/{}; {:.2f} sec...\".format(i+1, params['N_epochs'],toc),\n",
    "                  \"\\tLoss: {:.3f}...\".format(loss.item()),\n",
    "                  \"\\tVal Loss: {:.3f}\".format(n.mean(val_losses)),\n",
    "                  \"\\tVal Frac: {:.4f}\".format(frac_correct_val))\n",
    "            if n.mean(val_losses) <= params['valid_loss_min']:\n",
    "                torch.save(model.state_dict(), './state_dict' + str(random_val) + '.pt')\n",
    "                print('\\tValidation loss decreased ({:.3f} --> {:.3f}).  Saving model ...'.format(params['valid_loss_min'],n.mean(val_losses)))\n",
    "                params['valid_loss_min'] = n.mean(val_losses)\n",
    "            if frac_correct_val >= params['valid_frac_max']:\n",
    "                params['valid_frac_max'] = frac_correct_val\n",
    "                params['pred_labels_best'] = pred_labels.copy()\n",
    "                params['true_labels_best'] = true_labels.copy()\n",
    "\n",
    "                \n",
    "    return params, model\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n",
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = list(all_data.keys())\n",
    "datas = []\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)\n",
    "input_dim = datas[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_original = {\n",
    "    'n_windows'      : 10,\n",
    "    'n_stride'       : 5,\n",
    "    'shuffle'        : True,\n",
    "    'dropout_prob'   : 0.3,\n",
    "    'batch_size'     : 32,\n",
    "    'hidden_dim'     : 50,\n",
    "    'n_layers_LSTM'  : 3, \n",
    "    'output_dim'     : N_categories,\n",
    "    'input_dim'      : input_dim,\n",
    "    'lr'             : 0.005,\n",
    "    'N_epochs'       : 300,\n",
    "    'print_every'    : 10,\n",
    "    'clip'           : 5,\n",
    "    'valid_loss_min' : n.Inf,\n",
    "    'valid_frac_max' : 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweeps = {\n",
    "    'shuffle'         : [True, False],\n",
    "    'n_windows'       : [1,  5, 20, 50],\n",
    "    'lr'              : [0.001, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###########\n",
      "Testing with  shuffle :  True\n",
      "\tEpoch: 5/300; 7.53 sec... \tLoss: 0.600... \tVal Loss: 0.438 \tVal Frac: 0.8659\n",
      "\tValidation loss decreased (inf --> 0.438).  Saving model ...\n",
      "\tEpoch: 10/300; 7.33 sec... \tLoss: 0.395... \tVal Loss: 0.403 \tVal Frac: 0.8808\n",
      "\tValidation loss decreased (0.438 --> 0.403).  Saving model ...\n",
      "\tEpoch: 15/300; 7.53 sec... \tLoss: 0.140... \tVal Loss: 0.405 \tVal Frac: 0.8923\n",
      "\tEpoch: 20/300; 7.52 sec... \tLoss: 0.715... \tVal Loss: 0.401 \tVal Frac: 0.8894\n",
      "\tValidation loss decreased (0.403 --> 0.401).  Saving model ...\n",
      "\tEpoch: 25/300; 8.15 sec... \tLoss: 0.398... \tVal Loss: 0.361 \tVal Frac: 0.8939\n",
      "\tValidation loss decreased (0.401 --> 0.361).  Saving model ...\n",
      "\tEpoch: 30/300; 7.39 sec... \tLoss: 0.321... \tVal Loss: 0.385 \tVal Frac: 0.8897\n",
      "\tEpoch: 35/300; 7.36 sec... \tLoss: 0.149... \tVal Loss: 0.377 \tVal Frac: 0.8998\n",
      "\tEpoch: 40/300; 7.42 sec... \tLoss: 0.314... \tVal Loss: 0.381 \tVal Frac: 0.8896\n",
      "\tEpoch: 45/300; 7.30 sec... \tLoss: 0.309... \tVal Loss: 0.338 \tVal Frac: 0.9016\n",
      "\tValidation loss decreased (0.361 --> 0.338).  Saving model ...\n",
      "\tEpoch: 50/300; 7.46 sec... \tLoss: 0.572... \tVal Loss: 0.396 \tVal Frac: 0.8962\n",
      "\tEpoch: 55/300; 7.53 sec... \tLoss: 0.283... \tVal Loss: 0.359 \tVal Frac: 0.8921\n",
      "\tEpoch: 60/300; 7.45 sec... \tLoss: 0.248... \tVal Loss: 0.352 \tVal Frac: 0.9032\n",
      "\tEpoch: 65/300; 7.28 sec... \tLoss: 0.358... \tVal Loss: 0.362 \tVal Frac: 0.8980\n",
      "\tEpoch: 70/300; 7.53 sec... \tLoss: 0.159... \tVal Loss: 0.366 \tVal Frac: 0.8995\n",
      "\tEpoch: 75/300; 8.45 sec... \tLoss: 0.252... \tVal Loss: 0.381 \tVal Frac: 0.8872\n",
      "\tEpoch: 80/300; 7.46 sec... \tLoss: 0.363... \tVal Loss: 0.385 \tVal Frac: 0.8893\n",
      "\tEpoch: 85/300; 7.28 sec... \tLoss: 0.277... \tVal Loss: 0.346 \tVal Frac: 0.9001\n",
      "\tEpoch: 90/300; 7.53 sec... \tLoss: 0.689... \tVal Loss: 0.345 \tVal Frac: 0.9009\n",
      "\tEpoch: 95/300; 7.59 sec... \tLoss: 0.338... \tVal Loss: 0.362 \tVal Frac: 0.9072\n",
      "\tEpoch: 100/300; 7.42 sec... \tLoss: 0.137... \tVal Loss: 0.354 \tVal Frac: 0.9009\n",
      "\tEpoch: 105/300; 7.37 sec... \tLoss: 0.655... \tVal Loss: 0.358 \tVal Frac: 0.9003\n",
      "\tEpoch: 110/300; 7.55 sec... \tLoss: 0.202... \tVal Loss: 0.316 \tVal Frac: 0.9062\n",
      "\tValidation loss decreased (0.338 --> 0.316).  Saving model ...\n",
      "\tEpoch: 115/300; 7.42 sec... \tLoss: 0.144... \tVal Loss: 0.363 \tVal Frac: 0.9002\n",
      "\tEpoch: 120/300; 7.50 sec... \tLoss: 0.332... \tVal Loss: 0.338 \tVal Frac: 0.9042\n",
      "\tEpoch: 125/300; 7.64 sec... \tLoss: 0.402... \tVal Loss: 0.348 \tVal Frac: 0.9057\n",
      "\tEpoch: 130/300; 7.49 sec... \tLoss: 0.754... \tVal Loss: 0.324 \tVal Frac: 0.9106\n",
      "\tEpoch: 135/300; 7.40 sec... \tLoss: 0.091... \tVal Loss: 0.353 \tVal Frac: 0.9014\n",
      "\tEpoch: 140/300; 7.43 sec... \tLoss: 0.261... \tVal Loss: 0.339 \tVal Frac: 0.8965\n",
      "\tEpoch: 145/300; 7.49 sec... \tLoss: 0.710... \tVal Loss: 0.325 \tVal Frac: 0.9037\n",
      "\tEpoch: 150/300; 7.47 sec... \tLoss: 0.265... \tVal Loss: 0.342 \tVal Frac: 0.9031\n",
      "\tEpoch: 155/300; 7.50 sec... \tLoss: 0.374... \tVal Loss: 0.362 \tVal Frac: 0.9008\n",
      "\tEpoch: 160/300; 7.39 sec... \tLoss: 0.198... \tVal Loss: 0.383 \tVal Frac: 0.8951\n",
      "\tEpoch: 165/300; 7.40 sec... \tLoss: 0.341... \tVal Loss: 0.346 \tVal Frac: 0.9082\n",
      "\tEpoch: 170/300; 7.42 sec... \tLoss: 0.222... \tVal Loss: 0.326 \tVal Frac: 0.9012\n",
      "\tEpoch: 175/300; 7.38 sec... \tLoss: 0.196... \tVal Loss: 0.344 \tVal Frac: 0.9078\n",
      "\tEpoch: 180/300; 7.46 sec... \tLoss: 0.369... \tVal Loss: 0.326 \tVal Frac: 0.9068\n",
      "\tEpoch: 185/300; 7.32 sec... \tLoss: 0.308... \tVal Loss: 0.313 \tVal Frac: 0.9076\n",
      "\tValidation loss decreased (0.316 --> 0.313).  Saving model ...\n",
      "\tEpoch: 190/300; 7.48 sec... \tLoss: 0.243... \tVal Loss: 0.351 \tVal Frac: 0.8974\n",
      "\tEpoch: 195/300; 7.35 sec... \tLoss: 0.443... \tVal Loss: 0.364 \tVal Frac: 0.8936\n",
      "\tEpoch: 200/300; 7.50 sec... \tLoss: 0.420... \tVal Loss: 0.316 \tVal Frac: 0.9103\n",
      "\tEpoch: 205/300; 7.46 sec... \tLoss: 0.542... \tVal Loss: 0.372 \tVal Frac: 0.9052\n",
      "\tEpoch: 210/300; 7.57 sec... \tLoss: 0.568... \tVal Loss: 0.353 \tVal Frac: 0.9041\n",
      "\tEpoch: 215/300; 7.42 sec... \tLoss: 0.467... \tVal Loss: 0.318 \tVal Frac: 0.9080\n",
      "\tEpoch: 220/300; 7.50 sec... \tLoss: 0.292... \tVal Loss: 0.350 \tVal Frac: 0.9097\n",
      "\tEpoch: 225/300; 7.55 sec... \tLoss: 0.432... \tVal Loss: 0.345 \tVal Frac: 0.9088\n",
      "\tEpoch: 230/300; 7.46 sec... \tLoss: 0.253... \tVal Loss: 0.386 \tVal Frac: 0.8910\n",
      "\tEpoch: 235/300; 7.40 sec... \tLoss: 0.351... \tVal Loss: 0.366 \tVal Frac: 0.8988\n",
      "\tEpoch: 240/300; 7.48 sec... \tLoss: 0.224... \tVal Loss: 0.335 \tVal Frac: 0.9059\n",
      "\tEpoch: 245/300; 7.36 sec... \tLoss: 0.253... \tVal Loss: 0.351 \tVal Frac: 0.9018\n",
      "\tEpoch: 250/300; 7.45 sec... \tLoss: 0.260... \tVal Loss: 0.356 \tVal Frac: 0.9010\n",
      "\tEpoch: 255/300; 7.47 sec... \tLoss: 0.432... \tVal Loss: 0.364 \tVal Frac: 0.8971\n",
      "\tEpoch: 260/300; 7.52 sec... \tLoss: 0.587... \tVal Loss: 0.404 \tVal Frac: 0.8986\n",
      "\tEpoch: 265/300; 7.42 sec... \tLoss: 0.509... \tVal Loss: 0.363 \tVal Frac: 0.8964\n",
      "\tEpoch: 270/300; 7.55 sec... \tLoss: 0.722... \tVal Loss: 0.362 \tVal Frac: 0.8935\n",
      "\tEpoch: 275/300; 7.49 sec... \tLoss: 0.376... \tVal Loss: 0.362 \tVal Frac: 0.8981\n",
      "\tEpoch: 280/300; 7.60 sec... \tLoss: 0.464... \tVal Loss: 0.342 \tVal Frac: 0.9037\n",
      "\tEpoch: 285/300; 7.39 sec... \tLoss: 0.393... \tVal Loss: 0.347 \tVal Frac: 0.8964\n",
      "\tEpoch: 290/300; 7.33 sec... \tLoss: 0.428... \tVal Loss: 0.332 \tVal Frac: 0.9008\n",
      "\tEpoch: 295/300; 7.44 sec... \tLoss: 0.355... \tVal Loss: 0.347 \tVal Frac: 0.9023\n",
      "\tEpoch: 300/300; 7.40 sec... \tLoss: 0.382... \tVal Loss: 0.331 \tVal Frac: 0.9022\n",
      "Completed: shuffle: True\n",
      "\tloss: 0.312817 \n",
      "\tfrac:0.910557\n",
      "b'run_645243078.res'\n",
      "\n",
      "###########\n",
      "Testing with  shuffle :  False\n",
      "\tEpoch: 5/300; 7.30 sec... \tLoss: 0.805... \tVal Loss: 1.568 \tVal Frac: 0.6976\n",
      "\tValidation loss decreased (inf --> 1.568).  Saving model ...\n",
      "\tEpoch: 10/300; 7.39 sec... \tLoss: 0.001... \tVal Loss: 1.931 \tVal Frac: 0.7056\n",
      "\tEpoch: 15/300; 7.47 sec... \tLoss: 0.023... \tVal Loss: 1.341 \tVal Frac: 0.6921\n",
      "\tValidation loss decreased (1.568 --> 1.341).  Saving model ...\n",
      "\tEpoch: 20/300; 7.36 sec... \tLoss: 0.007... \tVal Loss: 0.933 \tVal Frac: 0.6963\n",
      "\tValidation loss decreased (1.341 --> 0.933).  Saving model ...\n",
      "\tEpoch: 25/300; 7.60 sec... \tLoss: 0.002... \tVal Loss: 0.673 \tVal Frac: 0.7426\n",
      "\tValidation loss decreased (0.933 --> 0.673).  Saving model ...\n",
      "\tEpoch: 30/300; 7.42 sec... \tLoss: 0.020... \tVal Loss: 0.543 \tVal Frac: 0.7992\n",
      "\tValidation loss decreased (0.673 --> 0.543).  Saving model ...\n",
      "\tEpoch: 35/300; 7.39 sec... \tLoss: 0.001... \tVal Loss: 0.438 \tVal Frac: 0.8755\n",
      "\tValidation loss decreased (0.543 --> 0.438).  Saving model ...\n",
      "\tEpoch: 40/300; 7.50 sec... \tLoss: 0.000... \tVal Loss: 0.409 \tVal Frac: 0.8751\n",
      "\tValidation loss decreased (0.438 --> 0.409).  Saving model ...\n",
      "\tEpoch: 45/300; 7.47 sec... \tLoss: 0.000... \tVal Loss: 0.434 \tVal Frac: 0.8706\n",
      "\tEpoch: 50/300; 7.52 sec... \tLoss: 0.000... \tVal Loss: 0.395 \tVal Frac: 0.8854\n",
      "\tValidation loss decreased (0.409 --> 0.395).  Saving model ...\n",
      "\tEpoch: 55/300; 7.47 sec... \tLoss: 0.001... \tVal Loss: 0.367 \tVal Frac: 0.8846\n",
      "\tValidation loss decreased (0.395 --> 0.367).  Saving model ...\n",
      "\tEpoch: 60/300; 7.52 sec... \tLoss: 0.001... \tVal Loss: 0.362 \tVal Frac: 0.8877\n",
      "\tValidation loss decreased (0.367 --> 0.362).  Saving model ...\n",
      "\tEpoch: 65/300; 7.47 sec... \tLoss: 0.000... \tVal Loss: 0.382 \tVal Frac: 0.8860\n",
      "\tEpoch: 70/300; 7.36 sec... \tLoss: 0.001... \tVal Loss: 0.360 \tVal Frac: 0.8932\n",
      "\tValidation loss decreased (0.362 --> 0.360).  Saving model ...\n",
      "\tEpoch: 75/300; 7.42 sec... \tLoss: 0.001... \tVal Loss: 0.338 \tVal Frac: 0.8986\n",
      "\tValidation loss decreased (0.360 --> 0.338).  Saving model ...\n",
      "\tEpoch: 80/300; 7.46 sec... \tLoss: 0.000... \tVal Loss: 0.390 \tVal Frac: 0.8819\n",
      "\tEpoch: 85/300; 7.32 sec... \tLoss: 0.001... \tVal Loss: 0.368 \tVal Frac: 0.8895\n",
      "\tEpoch: 90/300; 7.39 sec... \tLoss: 0.001... \tVal Loss: 0.346 \tVal Frac: 0.8897\n",
      "\tEpoch: 95/300; 8.35 sec... \tLoss: 0.002... \tVal Loss: 0.352 \tVal Frac: 0.8909\n",
      "\tEpoch: 100/300; 7.48 sec... \tLoss: 0.002... \tVal Loss: 0.361 \tVal Frac: 0.8895\n",
      "\tEpoch: 105/300; 7.36 sec... \tLoss: 0.000... \tVal Loss: 0.363 \tVal Frac: 0.8899\n",
      "\tEpoch: 110/300; 7.59 sec... \tLoss: 0.000... \tVal Loss: 0.393 \tVal Frac: 0.8737\n",
      "\tEpoch: 115/300; 7.47 sec... \tLoss: 0.001... \tVal Loss: 0.372 \tVal Frac: 0.8882\n",
      "\tEpoch: 120/300; 7.33 sec... \tLoss: 0.001... \tVal Loss: 0.384 \tVal Frac: 0.8881\n",
      "\tEpoch: 125/300; 7.53 sec... \tLoss: 0.000... \tVal Loss: 0.366 \tVal Frac: 0.8938\n",
      "\tEpoch: 130/300; 7.44 sec... \tLoss: 0.000... \tVal Loss: 0.372 \tVal Frac: 0.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 135/300; 7.45 sec... \tLoss: 0.000... \tVal Loss: 0.365 \tVal Frac: 0.8906\n",
      "\tEpoch: 140/300; 7.33 sec... \tLoss: 0.000... \tVal Loss: 0.415 \tVal Frac: 0.8746\n",
      "\tEpoch: 145/300; 7.40 sec... \tLoss: 0.000... \tVal Loss: 0.436 \tVal Frac: 0.8796\n",
      "\tEpoch: 150/300; 7.30 sec... \tLoss: 0.000... \tVal Loss: 0.359 \tVal Frac: 0.8940\n",
      "\tEpoch: 155/300; 7.39 sec... \tLoss: 0.000... \tVal Loss: 0.359 \tVal Frac: 0.8998\n",
      "\tEpoch: 160/300; 7.45 sec... \tLoss: 0.000... \tVal Loss: 0.361 \tVal Frac: 0.8969\n",
      "\tEpoch: 165/300; 7.32 sec... \tLoss: 0.000... \tVal Loss: 0.364 \tVal Frac: 0.8897\n",
      "\tEpoch: 170/300; 7.47 sec... \tLoss: 0.000... \tVal Loss: 0.361 \tVal Frac: 0.8954\n",
      "\tEpoch: 175/300; 7.38 sec... \tLoss: 0.000... \tVal Loss: 0.340 \tVal Frac: 0.8934\n",
      "\tEpoch: 180/300; 7.26 sec... \tLoss: 0.000... \tVal Loss: 0.385 \tVal Frac: 0.8860\n",
      "\tEpoch: 185/300; 7.49 sec... \tLoss: 0.001... \tVal Loss: 0.388 \tVal Frac: 0.8737\n",
      "\tEpoch: 190/300; 7.47 sec... \tLoss: 0.000... \tVal Loss: 0.433 \tVal Frac: 0.8676\n",
      "\tEpoch: 195/300; 7.49 sec... \tLoss: 0.000... \tVal Loss: 0.398 \tVal Frac: 0.8743\n",
      "\tEpoch: 200/300; 8.37 sec... \tLoss: 0.001... \tVal Loss: 0.363 \tVal Frac: 0.8933\n",
      "\tEpoch: 205/300; 7.42 sec... \tLoss: 0.000... \tVal Loss: 0.376 \tVal Frac: 0.8828\n",
      "\tEpoch: 210/300; 7.29 sec... \tLoss: 0.000... \tVal Loss: 0.351 \tVal Frac: 0.9009\n",
      "\tEpoch: 215/300; 7.42 sec... \tLoss: 0.000... \tVal Loss: 0.340 \tVal Frac: 0.8975\n",
      "\tEpoch: 220/300; 7.59 sec... \tLoss: 0.003... \tVal Loss: 0.402 \tVal Frac: 0.8825\n",
      "\tEpoch: 225/300; 7.59 sec... \tLoss: 0.000... \tVal Loss: 0.380 \tVal Frac: 0.8878\n",
      "\tEpoch: 230/300; 7.28 sec... \tLoss: 0.000... \tVal Loss: 0.398 \tVal Frac: 0.8756\n",
      "\tEpoch: 235/300; 7.32 sec... \tLoss: 0.000... \tVal Loss: 0.359 \tVal Frac: 0.8868\n",
      "\tEpoch: 240/300; 7.32 sec... \tLoss: 0.001... \tVal Loss: 0.357 \tVal Frac: 0.8895\n",
      "\tEpoch: 245/300; 7.48 sec... \tLoss: 0.000... \tVal Loss: 0.369 \tVal Frac: 0.8935\n",
      "\tEpoch: 250/300; 7.36 sec... \tLoss: 0.000... \tVal Loss: 0.357 \tVal Frac: 0.8917\n",
      "\tEpoch: 255/300; 7.43 sec... \tLoss: 0.000... \tVal Loss: 0.354 \tVal Frac: 0.8962\n",
      "\tEpoch: 260/300; 8.78 sec... \tLoss: 0.003... \tVal Loss: 0.388 \tVal Frac: 0.8808\n",
      "\tEpoch: 265/300; 7.39 sec... \tLoss: 0.000... \tVal Loss: 0.362 \tVal Frac: 0.8940\n",
      "\tEpoch: 270/300; 7.33 sec... \tLoss: 0.001... \tVal Loss: 0.379 \tVal Frac: 0.8881\n",
      "\tEpoch: 275/300; 7.46 sec... \tLoss: 0.001... \tVal Loss: 0.396 \tVal Frac: 0.8814\n",
      "\tEpoch: 280/300; 7.33 sec... \tLoss: 0.000... \tVal Loss: 0.366 \tVal Frac: 0.8849\n",
      "\tEpoch: 285/300; 7.31 sec... \tLoss: 0.005... \tVal Loss: 0.370 \tVal Frac: 0.8936\n",
      "\tEpoch: 290/300; 7.42 sec... \tLoss: 0.009... \tVal Loss: 0.387 \tVal Frac: 0.8883\n",
      "\tEpoch: 295/300; 7.42 sec... \tLoss: 0.001... \tVal Loss: 0.378 \tVal Frac: 0.8891\n",
      "\tEpoch: 300/300; 7.40 sec... \tLoss: 0.001... \tVal Loss: 0.368 \tVal Frac: 0.8930\n",
      "Completed: shuffle: False\n",
      "\tloss: 0.337507 \n",
      "\tfrac:0.900935\n",
      "b'run_599438457.res'\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  1\n",
      "\tEpoch: 5/300; 7.15 sec... \tLoss: 0.403... \tVal Loss: 0.379 \tVal Frac: 0.8894\n",
      "\tValidation loss decreased (inf --> 0.379).  Saving model ...\n",
      "\tEpoch: 10/300; 6.57 sec... \tLoss: 0.478... \tVal Loss: 0.316 \tVal Frac: 0.9086\n",
      "\tValidation loss decreased (0.379 --> 0.316).  Saving model ...\n",
      "\tEpoch: 15/300; 6.66 sec... \tLoss: 0.639... \tVal Loss: 0.337 \tVal Frac: 0.8912\n",
      "\tEpoch: 20/300; 6.57 sec... \tLoss: 0.377... \tVal Loss: 0.358 \tVal Frac: 0.8857\n",
      "\tEpoch: 25/300; 6.49 sec... \tLoss: 0.537... \tVal Loss: 0.313 \tVal Frac: 0.8987\n",
      "\tValidation loss decreased (0.316 --> 0.313).  Saving model ...\n",
      "\tEpoch: 30/300; 6.56 sec... \tLoss: 0.358... \tVal Loss: 0.385 \tVal Frac: 0.8748\n",
      "\tEpoch: 35/300; 6.57 sec... \tLoss: 0.376... \tVal Loss: 0.312 \tVal Frac: 0.8991\n",
      "\tValidation loss decreased (0.313 --> 0.312).  Saving model ...\n",
      "\tEpoch: 40/300; 6.40 sec... \tLoss: 0.269... \tVal Loss: 0.370 \tVal Frac: 0.8853\n",
      "\tEpoch: 45/300; 6.64 sec... \tLoss: 0.505... \tVal Loss: 0.356 \tVal Frac: 0.8880\n",
      "\tEpoch: 50/300; 6.52 sec... \tLoss: 0.303... \tVal Loss: 0.308 \tVal Frac: 0.9035\n",
      "\tValidation loss decreased (0.312 --> 0.308).  Saving model ...\n",
      "\tEpoch: 55/300; 6.65 sec... \tLoss: 0.258... \tVal Loss: 0.349 \tVal Frac: 0.8881\n",
      "\tEpoch: 60/300; 6.52 sec... \tLoss: 0.672... \tVal Loss: 0.303 \tVal Frac: 0.8985\n",
      "\tValidation loss decreased (0.308 --> 0.303).  Saving model ...\n",
      "\tEpoch: 65/300; 6.43 sec... \tLoss: 0.405... \tVal Loss: 0.311 \tVal Frac: 0.8991\n",
      "\tEpoch: 70/300; 6.59 sec... \tLoss: 0.269... \tVal Loss: 0.299 \tVal Frac: 0.9024\n",
      "\tValidation loss decreased (0.303 --> 0.299).  Saving model ...\n",
      "\tEpoch: 75/300; 6.42 sec... \tLoss: 0.538... \tVal Loss: 0.305 \tVal Frac: 0.9052\n",
      "\tEpoch: 80/300; 6.52 sec... \tLoss: 0.493... \tVal Loss: 0.294 \tVal Frac: 0.9081\n",
      "\tValidation loss decreased (0.299 --> 0.294).  Saving model ...\n",
      "\tEpoch: 85/300; 6.48 sec... \tLoss: 0.308... \tVal Loss: 0.315 \tVal Frac: 0.8987\n",
      "\tEpoch: 90/300; 6.65 sec... \tLoss: 0.445... \tVal Loss: 0.313 \tVal Frac: 0.9015\n",
      "\tEpoch: 95/300; 6.53 sec... \tLoss: 0.667... \tVal Loss: 0.321 \tVal Frac: 0.8986\n",
      "\tEpoch: 100/300; 6.33 sec... \tLoss: 0.170... \tVal Loss: 0.348 \tVal Frac: 0.8907\n",
      "\tEpoch: 105/300; 6.50 sec... \tLoss: 0.283... \tVal Loss: 0.319 \tVal Frac: 0.9012\n",
      "\tEpoch: 110/300; 6.46 sec... \tLoss: 0.194... \tVal Loss: 0.310 \tVal Frac: 0.8985\n",
      "\tEpoch: 115/300; 6.49 sec... \tLoss: 0.422... \tVal Loss: 0.296 \tVal Frac: 0.9051\n",
      "\tEpoch: 120/300; 7.47 sec... \tLoss: 0.499... \tVal Loss: 0.283 \tVal Frac: 0.9100\n",
      "\tValidation loss decreased (0.294 --> 0.283).  Saving model ...\n",
      "\tEpoch: 125/300; 6.52 sec... \tLoss: 0.338... \tVal Loss: 0.293 \tVal Frac: 0.9070\n",
      "\tEpoch: 130/300; 6.62 sec... \tLoss: 0.373... \tVal Loss: 0.300 \tVal Frac: 0.9066\n",
      "\tEpoch: 135/300; 6.51 sec... \tLoss: 0.353... \tVal Loss: 0.311 \tVal Frac: 0.9017\n",
      "\tEpoch: 140/300; 6.43 sec... \tLoss: 0.081... \tVal Loss: 0.300 \tVal Frac: 0.9010\n",
      "\tEpoch: 145/300; 6.56 sec... \tLoss: 0.392... \tVal Loss: 0.299 \tVal Frac: 0.9040\n",
      "\tEpoch: 150/300; 6.52 sec... \tLoss: 0.275... \tVal Loss: 0.295 \tVal Frac: 0.9067\n",
      "\tEpoch: 155/300; 6.55 sec... \tLoss: 0.155... \tVal Loss: 0.295 \tVal Frac: 0.9061\n",
      "\tEpoch: 160/300; 6.63 sec... \tLoss: 0.494... \tVal Loss: 0.297 \tVal Frac: 0.9062\n",
      "\tEpoch: 165/300; 6.47 sec... \tLoss: 0.384... \tVal Loss: 0.287 \tVal Frac: 0.9088\n",
      "\tEpoch: 170/300; 6.60 sec... \tLoss: 0.363... \tVal Loss: 0.279 \tVal Frac: 0.9139\n",
      "\tValidation loss decreased (0.283 --> 0.279).  Saving model ...\n",
      "\tEpoch: 175/300; 6.44 sec... \tLoss: 0.455... \tVal Loss: 0.286 \tVal Frac: 0.9086\n",
      "\tEpoch: 180/300; 6.54 sec... \tLoss: 0.455... \tVal Loss: 0.276 \tVal Frac: 0.9095\n",
      "\tValidation loss decreased (0.279 --> 0.276).  Saving model ...\n",
      "\tEpoch: 185/300; 7.60 sec... \tLoss: 0.227... \tVal Loss: 0.283 \tVal Frac: 0.9086\n",
      "\tEpoch: 190/300; 6.62 sec... \tLoss: 0.424... \tVal Loss: 0.282 \tVal Frac: 0.9102\n",
      "\tEpoch: 195/300; 6.65 sec... \tLoss: 0.169... \tVal Loss: 0.307 \tVal Frac: 0.9034\n",
      "\tEpoch: 200/300; 6.42 sec... \tLoss: 0.379... \tVal Loss: 0.313 \tVal Frac: 0.9017\n",
      "\tEpoch: 205/300; 6.43 sec... \tLoss: 0.351... \tVal Loss: 0.300 \tVal Frac: 0.9031\n",
      "\tEpoch: 210/300; 6.59 sec... \tLoss: 0.371... \tVal Loss: 0.283 \tVal Frac: 0.9126\n",
      "\tEpoch: 215/300; 6.61 sec... \tLoss: 0.422... \tVal Loss: 0.326 \tVal Frac: 0.9018\n",
      "\tEpoch: 220/300; 6.52 sec... \tLoss: 0.277... \tVal Loss: 0.305 \tVal Frac: 0.9073\n",
      "\tEpoch: 225/300; 6.43 sec... \tLoss: 0.501... \tVal Loss: 0.276 \tVal Frac: 0.9150\n",
      "\tEpoch: 230/300; 6.54 sec... \tLoss: 0.186... \tVal Loss: 0.290 \tVal Frac: 0.9060\n",
      "\tEpoch: 235/300; 6.61 sec... \tLoss: 0.458... \tVal Loss: 0.305 \tVal Frac: 0.9021\n",
      "\tEpoch: 240/300; 6.49 sec... \tLoss: 0.082... \tVal Loss: 0.303 \tVal Frac: 0.9034\n",
      "\tEpoch: 245/300; 6.40 sec... \tLoss: 0.261... \tVal Loss: 0.327 \tVal Frac: 0.8943\n",
      "\tEpoch: 250/300; 6.56 sec... \tLoss: 0.381... \tVal Loss: 0.278 \tVal Frac: 0.9111\n",
      "\tEpoch: 255/300; 6.50 sec... \tLoss: 0.310... \tVal Loss: 0.295 \tVal Frac: 0.9049\n",
      "\tEpoch: 260/300; 6.57 sec... \tLoss: 0.286... \tVal Loss: 0.290 \tVal Frac: 0.9049\n",
      "\tEpoch: 265/300; 6.40 sec... \tLoss: 0.089... \tVal Loss: 0.292 \tVal Frac: 0.9021\n",
      "\tEpoch: 270/300; 6.38 sec... \tLoss: 0.419... \tVal Loss: 0.306 \tVal Frac: 0.8955\n",
      "\tEpoch: 275/300; 6.35 sec... \tLoss: 0.092... \tVal Loss: 0.315 \tVal Frac: 0.8980\n",
      "\tEpoch: 280/300; 6.50 sec... \tLoss: 0.517... \tVal Loss: 0.312 \tVal Frac: 0.9031\n",
      "\tEpoch: 285/300; 6.50 sec... \tLoss: 0.441... \tVal Loss: 0.317 \tVal Frac: 0.9012\n",
      "\tEpoch: 290/300; 6.37 sec... \tLoss: 0.459... \tVal Loss: 0.295 \tVal Frac: 0.9095\n",
      "\tEpoch: 295/300; 6.40 sec... \tLoss: 0.381... \tVal Loss: 0.281 \tVal Frac: 0.9103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 300/300; 6.55 sec... \tLoss: 0.828... \tVal Loss: 0.296 \tVal Frac: 0.9074\n",
      "Completed: n_windows: 1\n",
      "\tloss: 0.275791 \n",
      "\tfrac:0.915048\n",
      "b'run_471023216.res'\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  5\n",
      "\tEpoch: 5/300; 6.80 sec... \tLoss: 0.343... \tVal Loss: 0.377 \tVal Frac: 0.8765\n",
      "\tValidation loss decreased (inf --> 0.377).  Saving model ...\n",
      "\tEpoch: 10/300; 7.00 sec... \tLoss: 0.492... \tVal Loss: 0.353 \tVal Frac: 0.8911\n",
      "\tValidation loss decreased (0.377 --> 0.353).  Saving model ...\n",
      "\tEpoch: 15/300; 7.05 sec... \tLoss: 0.272... \tVal Loss: 0.331 \tVal Frac: 0.9048\n",
      "\tValidation loss decreased (0.353 --> 0.331).  Saving model ...\n",
      "\tEpoch: 20/300; 7.02 sec... \tLoss: 0.213... \tVal Loss: 0.293 \tVal Frac: 0.9051\n",
      "\tValidation loss decreased (0.331 --> 0.293).  Saving model ...\n",
      "\tEpoch: 25/300; 7.01 sec... \tLoss: 0.305... \tVal Loss: 0.293 \tVal Frac: 0.9096\n",
      "\tValidation loss decreased (0.293 --> 0.293).  Saving model ...\n",
      "\tEpoch: 30/300; 7.12 sec... \tLoss: 0.289... \tVal Loss: 0.291 \tVal Frac: 0.9093\n",
      "\tValidation loss decreased (0.293 --> 0.291).  Saving model ...\n",
      "\tEpoch: 35/300; 6.92 sec... \tLoss: 0.475... \tVal Loss: 0.298 \tVal Frac: 0.9034\n",
      "\tEpoch: 40/300; 6.89 sec... \tLoss: 0.234... \tVal Loss: 0.320 \tVal Frac: 0.9028\n",
      "\tEpoch: 45/300; 6.97 sec... \tLoss: 0.205... \tVal Loss: 0.299 \tVal Frac: 0.9041\n",
      "\tEpoch: 50/300; 6.84 sec... \tLoss: 0.439... \tVal Loss: 0.284 \tVal Frac: 0.9109\n",
      "\tValidation loss decreased (0.291 --> 0.284).  Saving model ...\n",
      "\tEpoch: 55/300; 7.09 sec... \tLoss: 0.264... \tVal Loss: 0.303 \tVal Frac: 0.9012\n",
      "\tEpoch: 60/300; 6.95 sec... \tLoss: 0.132... \tVal Loss: 0.308 \tVal Frac: 0.8997\n",
      "\tEpoch: 65/300; 6.90 sec... \tLoss: 0.324... \tVal Loss: 0.283 \tVal Frac: 0.9084\n",
      "\tValidation loss decreased (0.284 --> 0.283).  Saving model ...\n",
      "\tEpoch: 70/300; 6.90 sec... \tLoss: 0.103... \tVal Loss: 0.288 \tVal Frac: 0.9085\n",
      "\tEpoch: 75/300; 6.99 sec... \tLoss: 0.293... \tVal Loss: 0.287 \tVal Frac: 0.9111\n",
      "\tEpoch: 80/300; 7.00 sec... \tLoss: 0.268... \tVal Loss: 0.296 \tVal Frac: 0.9072\n",
      "\tEpoch: 85/300; 6.95 sec... \tLoss: 0.324... \tVal Loss: 0.286 \tVal Frac: 0.9083\n",
      "\tEpoch: 90/300; 6.93 sec... \tLoss: 0.307... \tVal Loss: 0.276 \tVal Frac: 0.9108\n",
      "\tValidation loss decreased (0.283 --> 0.276).  Saving model ...\n",
      "\tEpoch: 95/300; 6.89 sec... \tLoss: 0.308... \tVal Loss: 0.309 \tVal Frac: 0.9024\n",
      "\tEpoch: 100/300; 7.05 sec... \tLoss: 0.437... \tVal Loss: 0.324 \tVal Frac: 0.9023\n",
      "\tEpoch: 105/300; 6.97 sec... \tLoss: 0.198... \tVal Loss: 0.284 \tVal Frac: 0.9123\n",
      "\tEpoch: 110/300; 6.90 sec... \tLoss: 0.389... \tVal Loss: 0.291 \tVal Frac: 0.9106\n",
      "\tEpoch: 115/300; 7.00 sec... \tLoss: 0.174... \tVal Loss: 0.272 \tVal Frac: 0.9108\n",
      "\tValidation loss decreased (0.276 --> 0.272).  Saving model ...\n",
      "\tEpoch: 120/300; 6.92 sec... \tLoss: 0.457... \tVal Loss: 0.275 \tVal Frac: 0.9120\n",
      "\tEpoch: 125/300; 7.02 sec... \tLoss: 0.344... \tVal Loss: 0.260 \tVal Frac: 0.9173\n",
      "\tValidation loss decreased (0.272 --> 0.260).  Saving model ...\n",
      "\tEpoch: 130/300; 7.65 sec... \tLoss: 0.442... \tVal Loss: 0.270 \tVal Frac: 0.9073\n",
      "\tEpoch: 135/300; 6.93 sec... \tLoss: 0.112... \tVal Loss: 0.260 \tVal Frac: 0.9149\n",
      "\tValidation loss decreased (0.260 --> 0.260).  Saving model ...\n",
      "\tEpoch: 140/300; 6.90 sec... \tLoss: 0.060... \tVal Loss: 0.261 \tVal Frac: 0.9150\n",
      "\tEpoch: 145/300; 6.90 sec... \tLoss: 0.334... \tVal Loss: 0.278 \tVal Frac: 0.9074\n",
      "\tEpoch: 150/300; 6.93 sec... \tLoss: 0.446... \tVal Loss: 0.269 \tVal Frac: 0.9144\n",
      "\tEpoch: 155/300; 7.16 sec... \tLoss: 0.153... \tVal Loss: 0.267 \tVal Frac: 0.9117\n",
      "\tEpoch: 160/300; 6.92 sec... \tLoss: 0.192... \tVal Loss: 0.280 \tVal Frac: 0.9102\n",
      "\tEpoch: 165/300; 6.93 sec... \tLoss: 0.284... \tVal Loss: 0.279 \tVal Frac: 0.9131\n",
      "\tEpoch: 170/300; 6.99 sec... \tLoss: 0.221... \tVal Loss: 0.277 \tVal Frac: 0.9158\n",
      "\tEpoch: 175/300; 6.92 sec... \tLoss: 0.046... \tVal Loss: 0.274 \tVal Frac: 0.9141\n",
      "\tEpoch: 180/300; 6.97 sec... \tLoss: 0.573... \tVal Loss: 0.257 \tVal Frac: 0.9178\n",
      "\tValidation loss decreased (0.260 --> 0.257).  Saving model ...\n",
      "\tEpoch: 185/300; 6.97 sec... \tLoss: 0.652... \tVal Loss: 0.259 \tVal Frac: 0.9176\n",
      "\tEpoch: 190/300; 7.03 sec... \tLoss: 0.115... \tVal Loss: 0.251 \tVal Frac: 0.9204\n",
      "\tValidation loss decreased (0.257 --> 0.251).  Saving model ...\n",
      "\tEpoch: 195/300; 6.97 sec... \tLoss: 0.192... \tVal Loss: 0.263 \tVal Frac: 0.9150\n",
      "\tEpoch: 200/300; 7.00 sec... \tLoss: 0.071... \tVal Loss: 0.267 \tVal Frac: 0.9210\n",
      "\tEpoch: 205/300; 6.95 sec... \tLoss: 0.251... \tVal Loss: 0.279 \tVal Frac: 0.9137\n",
      "\tEpoch: 210/300; 6.99 sec... \tLoss: 0.411... \tVal Loss: 0.261 \tVal Frac: 0.9154\n",
      "\tEpoch: 215/300; 6.96 sec... \tLoss: 0.201... \tVal Loss: 0.253 \tVal Frac: 0.9179\n",
      "\tEpoch: 220/300; 7.03 sec... \tLoss: 0.390... \tVal Loss: 0.270 \tVal Frac: 0.9156\n",
      "\tEpoch: 225/300; 7.02 sec... \tLoss: 0.128... \tVal Loss: 0.270 \tVal Frac: 0.9170\n",
      "\tEpoch: 230/300; 7.04 sec... \tLoss: 0.290... \tVal Loss: 0.272 \tVal Frac: 0.9177\n",
      "\tEpoch: 235/300; 6.97 sec... \tLoss: 0.152... \tVal Loss: 0.266 \tVal Frac: 0.9164\n",
      "\tEpoch: 240/300; 6.82 sec... \tLoss: 0.228... \tVal Loss: 0.274 \tVal Frac: 0.9137\n",
      "\tEpoch: 245/300; 6.93 sec... \tLoss: 0.185... \tVal Loss: 0.260 \tVal Frac: 0.9172\n",
      "\tEpoch: 250/300; 7.00 sec... \tLoss: 0.225... \tVal Loss: 0.257 \tVal Frac: 0.9163\n",
      "\tEpoch: 255/300; 6.85 sec... \tLoss: 0.222... \tVal Loss: 0.273 \tVal Frac: 0.9127\n",
      "\tEpoch: 260/300; 7.06 sec... \tLoss: 0.330... \tVal Loss: 0.289 \tVal Frac: 0.9101\n",
      "\tEpoch: 265/300; 6.97 sec... \tLoss: 0.409... \tVal Loss: 0.272 \tVal Frac: 0.9122\n",
      "\tEpoch: 270/300; 6.99 sec... \tLoss: 0.283... \tVal Loss: 0.274 \tVal Frac: 0.9123\n",
      "\tEpoch: 275/300; 6.90 sec... \tLoss: 0.101... \tVal Loss: 0.269 \tVal Frac: 0.9132\n",
      "\tEpoch: 280/300; 7.07 sec... \tLoss: 0.214... \tVal Loss: 0.280 \tVal Frac: 0.9136\n",
      "\tEpoch: 285/300; 7.00 sec... \tLoss: 0.849... \tVal Loss: 0.264 \tVal Frac: 0.9150\n",
      "\tEpoch: 290/300; 7.14 sec... \tLoss: 0.116... \tVal Loss: 0.280 \tVal Frac: 0.9103\n",
      "\tEpoch: 295/300; 6.87 sec... \tLoss: 0.322... \tVal Loss: 0.250 \tVal Frac: 0.9213\n",
      "\tValidation loss decreased (0.251 --> 0.250).  Saving model ...\n",
      "\tEpoch: 300/300; 6.97 sec... \tLoss: 0.439... \tVal Loss: 0.271 \tVal Frac: 0.9145\n",
      "Completed: n_windows: 5\n",
      "\tloss: 0.250215 \n",
      "\tfrac:0.921279\n",
      "b'run_871545604.res'\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  20\n",
      "\tEpoch: 5/300; 8.24 sec... \tLoss: 0.522... \tVal Loss: 0.430 \tVal Frac: 0.8715\n",
      "\tValidation loss decreased (inf --> 0.430).  Saving model ...\n",
      "\tEpoch: 10/300; 7.98 sec... \tLoss: 0.251... \tVal Loss: 0.391 \tVal Frac: 0.8811\n",
      "\tValidation loss decreased (0.430 --> 0.391).  Saving model ...\n",
      "\tEpoch: 15/300; 8.06 sec... \tLoss: 0.277... \tVal Loss: 0.401 \tVal Frac: 0.8858\n",
      "\tEpoch: 20/300; 8.34 sec... \tLoss: 0.284... \tVal Loss: 0.392 \tVal Frac: 0.8869\n",
      "\tEpoch: 25/300; 8.18 sec... \tLoss: 0.540... \tVal Loss: 0.388 \tVal Frac: 0.8882\n",
      "\tValidation loss decreased (0.391 --> 0.388).  Saving model ...\n",
      "\tEpoch: 30/300; 8.05 sec... \tLoss: 0.606... \tVal Loss: 0.394 \tVal Frac: 0.8938\n",
      "\tEpoch: 35/300; 8.29 sec... \tLoss: 0.273... \tVal Loss: 0.402 \tVal Frac: 0.8853\n",
      "\tEpoch: 40/300; 8.06 sec... \tLoss: 0.299... \tVal Loss: 0.396 \tVal Frac: 0.8910\n",
      "\tEpoch: 45/300; 7.96 sec... \tLoss: 0.349... \tVal Loss: 0.418 \tVal Frac: 0.8880\n",
      "\tEpoch: 50/300; 8.14 sec... \tLoss: 0.187... \tVal Loss: 0.420 \tVal Frac: 0.8909\n",
      "\tEpoch: 55/300; 8.09 sec... \tLoss: 0.477... \tVal Loss: 0.418 \tVal Frac: 0.8860\n",
      "\tEpoch: 60/300; 8.02 sec... \tLoss: 0.342... \tVal Loss: 0.431 \tVal Frac: 0.8884\n",
      "\tEpoch: 65/300; 8.21 sec... \tLoss: 0.186... \tVal Loss: 0.380 \tVal Frac: 0.8926\n",
      "\tValidation loss decreased (0.388 --> 0.380).  Saving model ...\n",
      "\tEpoch: 70/300; 8.11 sec... \tLoss: 0.290... \tVal Loss: 0.424 \tVal Frac: 0.8860\n",
      "\tEpoch: 75/300; 8.05 sec... \tLoss: 0.447... \tVal Loss: 0.392 \tVal Frac: 0.8872\n",
      "\tEpoch: 80/300; 8.15 sec... \tLoss: 0.310... \tVal Loss: 0.408 \tVal Frac: 0.8899\n",
      "\tEpoch: 85/300; 8.08 sec... \tLoss: 0.370... \tVal Loss: 0.409 \tVal Frac: 0.8883\n",
      "\tEpoch: 90/300; 7.97 sec... \tLoss: 0.425... \tVal Loss: 0.411 \tVal Frac: 0.8827\n",
      "\tEpoch: 95/300; 8.13 sec... \tLoss: 0.324... \tVal Loss: 0.436 \tVal Frac: 0.8879\n",
      "\tEpoch: 100/300; 8.11 sec... \tLoss: 0.073... \tVal Loss: 0.431 \tVal Frac: 0.8892\n",
      "\tEpoch: 105/300; 8.14 sec... \tLoss: 0.287... \tVal Loss: 0.392 \tVal Frac: 0.8908\n",
      "\tEpoch: 110/300; 8.11 sec... \tLoss: 0.268... \tVal Loss: 0.420 \tVal Frac: 0.8866\n",
      "\tEpoch: 115/300; 8.02 sec... \tLoss: 1.592... \tVal Loss: 1.785 \tVal Frac: 0.2780\n",
      "\tEpoch: 120/300; 8.02 sec... \tLoss: 0.535... \tVal Loss: 0.450 \tVal Frac: 0.8845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 125/300; 8.09 sec... \tLoss: 0.451... \tVal Loss: 0.423 \tVal Frac: 0.8835\n",
      "\tEpoch: 130/300; 8.35 sec... \tLoss: 0.255... \tVal Loss: 0.410 \tVal Frac: 0.8834\n",
      "\tEpoch: 135/300; 8.32 sec... \tLoss: 0.285... \tVal Loss: 0.405 \tVal Frac: 0.8873\n",
      "\tEpoch: 140/300; 8.09 sec... \tLoss: 0.354... \tVal Loss: 0.440 \tVal Frac: 0.8784\n",
      "\tEpoch: 145/300; 8.08 sec... \tLoss: 0.198... \tVal Loss: 0.397 \tVal Frac: 0.8930\n",
      "\tEpoch: 150/300; 8.22 sec... \tLoss: 0.498... \tVal Loss: 0.431 \tVal Frac: 0.8826\n",
      "\tEpoch: 155/300; 8.08 sec... \tLoss: 0.387... \tVal Loss: 0.488 \tVal Frac: 0.8710\n",
      "\tEpoch: 160/300; 8.17 sec... \tLoss: 0.652... \tVal Loss: 0.418 \tVal Frac: 0.8830\n",
      "\tEpoch: 165/300; 8.24 sec... \tLoss: 0.574... \tVal Loss: 0.421 \tVal Frac: 0.8821\n",
      "\tEpoch: 170/300; 8.02 sec... \tLoss: 0.525... \tVal Loss: 0.446 \tVal Frac: 0.8823\n",
      "\tEpoch: 175/300; 8.14 sec... \tLoss: 0.609... \tVal Loss: 0.432 \tVal Frac: 0.8808\n",
      "\tEpoch: 180/300; 8.11 sec... \tLoss: 0.464... \tVal Loss: 0.429 \tVal Frac: 0.8861\n",
      "\tEpoch: 185/300; 8.02 sec... \tLoss: 0.298... \tVal Loss: 0.426 \tVal Frac: 0.8826\n",
      "\tEpoch: 190/300; 8.00 sec... \tLoss: 0.715... \tVal Loss: 0.406 \tVal Frac: 0.8847\n",
      "\tEpoch: 195/300; 8.86 sec... \tLoss: 0.455... \tVal Loss: 0.444 \tVal Frac: 0.8821\n",
      "\tEpoch: 200/300; 8.12 sec... \tLoss: 0.176... \tVal Loss: 0.402 \tVal Frac: 0.8811\n",
      "\tEpoch: 205/300; 8.28 sec... \tLoss: 0.474... \tVal Loss: 0.417 \tVal Frac: 0.8820\n",
      "\tEpoch: 210/300; 8.11 sec... \tLoss: 0.273... \tVal Loss: 0.417 \tVal Frac: 0.8881\n",
      "\tEpoch: 215/300; 8.05 sec... \tLoss: 0.380... \tVal Loss: 0.407 \tVal Frac: 0.8826\n",
      "\tEpoch: 220/300; 8.41 sec... \tLoss: 0.276... \tVal Loss: 0.458 \tVal Frac: 0.8733\n",
      "\tEpoch: 225/300; 8.18 sec... \tLoss: 0.316... \tVal Loss: 0.442 \tVal Frac: 0.8767\n",
      "\tEpoch: 230/300; 8.12 sec... \tLoss: 0.536... \tVal Loss: 0.421 \tVal Frac: 0.8827\n",
      "\tEpoch: 235/300; 8.09 sec... \tLoss: 0.293... \tVal Loss: 0.427 \tVal Frac: 0.8846\n",
      "\tEpoch: 240/300; 8.19 sec... \tLoss: 0.263... \tVal Loss: 0.420 \tVal Frac: 0.8803\n",
      "\tEpoch: 245/300; 8.11 sec... \tLoss: 0.221... \tVal Loss: 0.406 \tVal Frac: 0.8804\n",
      "\tEpoch: 250/300; 8.09 sec... \tLoss: 0.406... \tVal Loss: 0.447 \tVal Frac: 0.8728\n",
      "\tEpoch: 255/300; 8.41 sec... \tLoss: 0.371... \tVal Loss: 0.459 \tVal Frac: 0.8742\n",
      "\tEpoch: 260/300; 8.21 sec... \tLoss: 0.346... \tVal Loss: 0.439 \tVal Frac: 0.8732\n",
      "\tEpoch: 265/300; 8.67 sec... \tLoss: 0.302... \tVal Loss: 0.417 \tVal Frac: 0.8739\n",
      "\tEpoch: 270/300; 8.39 sec... \tLoss: 0.572... \tVal Loss: 0.463 \tVal Frac: 0.8682\n",
      "\tEpoch: 275/300; 8.09 sec... \tLoss: 0.543... \tVal Loss: 0.442 \tVal Frac: 0.8735\n",
      "\tEpoch: 280/300; 8.29 sec... \tLoss: 0.127... \tVal Loss: 0.452 \tVal Frac: 0.8764\n",
      "\tEpoch: 285/300; 8.11 sec... \tLoss: 0.639... \tVal Loss: 0.428 \tVal Frac: 0.8699\n",
      "\tEpoch: 290/300; 8.25 sec... \tLoss: 0.382... \tVal Loss: 0.484 \tVal Frac: 0.8619\n",
      "\tEpoch: 295/300; 8.17 sec... \tLoss: 0.637... \tVal Loss: 0.468 \tVal Frac: 0.8649\n",
      "\tEpoch: 300/300; 8.25 sec... \tLoss: 0.519... \tVal Loss: 0.503 \tVal Frac: 0.8551\n",
      "Completed: n_windows: 20\n",
      "\tloss: 0.379757 \n",
      "\tfrac:0.893842\n",
      "b'run_293708342.res'\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  50\n",
      "\tEpoch: 10/300; 12.73 sec... \tLoss: 0.359... \tVal Loss: 0.433 \tVal Frac: 0.8751\n",
      "\tValidation loss decreased (inf --> 0.433).  Saving model ...\n",
      "\tEpoch: 20/300; 12.73 sec... \tLoss: 0.360... \tVal Loss: 0.440 \tVal Frac: 0.8748\n",
      "\tEpoch: 30/300; 12.72 sec... \tLoss: 0.307... \tVal Loss: 0.435 \tVal Frac: 0.8774\n",
      "\tEpoch: 40/300; 12.95 sec... \tLoss: 0.377... \tVal Loss: 0.420 \tVal Frac: 0.8879\n",
      "\tValidation loss decreased (0.433 --> 0.420).  Saving model ...\n",
      "\tEpoch: 50/300; 12.75 sec... \tLoss: 0.316... \tVal Loss: 0.372 \tVal Frac: 0.8953\n",
      "\tValidation loss decreased (0.420 --> 0.372).  Saving model ...\n",
      "\tEpoch: 60/300; 12.76 sec... \tLoss: 0.272... \tVal Loss: 0.442 \tVal Frac: 0.8789\n",
      "\tEpoch: 70/300; 13.15 sec... \tLoss: 0.307... \tVal Loss: 0.412 \tVal Frac: 0.8943\n",
      "\tEpoch: 80/300; 12.86 sec... \tLoss: 0.302... \tVal Loss: 0.475 \tVal Frac: 0.8801\n",
      "\tEpoch: 90/300; 12.65 sec... \tLoss: 0.183... \tVal Loss: 0.404 \tVal Frac: 0.8896\n",
      "\tEpoch: 100/300; 12.76 sec... \tLoss: 0.447... \tVal Loss: 0.454 \tVal Frac: 0.8841\n",
      "\tEpoch: 110/300; 12.99 sec... \tLoss: 0.371... \tVal Loss: 0.400 \tVal Frac: 0.8869\n",
      "\tEpoch: 120/300; 12.84 sec... \tLoss: 0.398... \tVal Loss: 0.534 \tVal Frac: 0.8509\n",
      "\tEpoch: 130/300; 12.97 sec... \tLoss: 0.558... \tVal Loss: 0.680 \tVal Frac: 0.7854\n",
      "\tEpoch: 140/300; 13.05 sec... \tLoss: 0.736... \tVal Loss: 0.963 \tVal Frac: 0.7327\n",
      "\tEpoch: 150/300; 13.15 sec... \tLoss: 0.884... \tVal Loss: 0.893 \tVal Frac: 0.7176\n",
      "\tEpoch: 160/300; 13.33 sec... \tLoss: 1.140... \tVal Loss: 0.786 \tVal Frac: 0.7406\n",
      "\tEpoch: 170/300; 13.15 sec... \tLoss: 1.003... \tVal Loss: 0.901 \tVal Frac: 0.7498\n",
      "\tEpoch: 180/300; 13.33 sec... \tLoss: 0.636... \tVal Loss: 0.759 \tVal Frac: 0.7517\n",
      "\tEpoch: 190/300; 13.13 sec... \tLoss: 0.830... \tVal Loss: 0.764 \tVal Frac: 0.7639\n",
      "\tEpoch: 200/300; 13.07 sec... \tLoss: 0.960... \tVal Loss: 0.755 \tVal Frac: 0.7654\n",
      "\tEpoch: 210/300; 12.97 sec... \tLoss: 0.813... \tVal Loss: 0.889 \tVal Frac: 0.7386\n",
      "\tEpoch: 220/300; 13.08 sec... \tLoss: 0.617... \tVal Loss: 0.938 \tVal Frac: 0.7199\n",
      "\tEpoch: 230/300; 13.15 sec... \tLoss: 1.309... \tVal Loss: 1.361 \tVal Frac: 0.4866\n",
      "\tEpoch: 240/300; 13.17 sec... \tLoss: 0.629... \tVal Loss: 1.415 \tVal Frac: 0.4828\n",
      "\tEpoch: 250/300; 13.12 sec... \tLoss: 1.295... \tVal Loss: 1.254 \tVal Frac: 0.5796\n",
      "\tEpoch: 260/300; 13.12 sec... \tLoss: 1.324... \tVal Loss: 1.029 \tVal Frac: 0.6878\n",
      "\tEpoch: 270/300; 13.25 sec... \tLoss: 0.688... \tVal Loss: 0.784 \tVal Frac: 0.7642\n",
      "\tEpoch: 280/300; 13.52 sec... \tLoss: 1.414... \tVal Loss: 1.268 \tVal Frac: 0.5698\n",
      "\tEpoch: 290/300; 13.09 sec... \tLoss: 1.055... \tVal Loss: 1.302 \tVal Frac: 0.5717\n",
      "\tEpoch: 300/300; 13.23 sec... \tLoss: 1.231... \tVal Loss: 0.867 \tVal Frac: 0.7281\n",
      "Completed: n_windows: 50\n",
      "\tloss: 0.372239 \n",
      "\tfrac:0.895312\n",
      "b'run_747768436.res'\n",
      "\n",
      "###########\n",
      "Testing with  lr :  0.001\n",
      "\tEpoch: 5/300; 7.63 sec... \tLoss: 0.354... \tVal Loss: 0.394 \tVal Frac: 0.8852\n",
      "\tValidation loss decreased (inf --> 0.394).  Saving model ...\n",
      "\tEpoch: 10/300; 7.57 sec... \tLoss: 0.533... \tVal Loss: 0.364 \tVal Frac: 0.9031\n",
      "\tValidation loss decreased (0.394 --> 0.364).  Saving model ...\n",
      "\tEpoch: 15/300; 7.57 sec... \tLoss: 0.474... \tVal Loss: 0.298 \tVal Frac: 0.9103\n",
      "\tValidation loss decreased (0.364 --> 0.298).  Saving model ...\n",
      "\tEpoch: 20/300; 7.55 sec... \tLoss: 0.413... \tVal Loss: 0.311 \tVal Frac: 0.9108\n",
      "\tEpoch: 25/300; 7.70 sec... \tLoss: 0.256... \tVal Loss: 0.308 \tVal Frac: 0.9155\n",
      "\tEpoch: 30/300; 7.36 sec... \tLoss: 0.204... \tVal Loss: 0.317 \tVal Frac: 0.9067\n",
      "\tEpoch: 35/300; 7.47 sec... \tLoss: 0.513... \tVal Loss: 0.314 \tVal Frac: 0.9161\n",
      "\tEpoch: 40/300; 7.36 sec... \tLoss: 0.811... \tVal Loss: 0.316 \tVal Frac: 0.9087\n",
      "\tEpoch: 45/300; 7.48 sec... \tLoss: 0.635... \tVal Loss: 0.294 \tVal Frac: 0.9122\n",
      "\tValidation loss decreased (0.298 --> 0.294).  Saving model ...\n",
      "\tEpoch: 50/300; 7.32 sec... \tLoss: 0.286... \tVal Loss: 0.300 \tVal Frac: 0.9146\n",
      "\tEpoch: 55/300; 7.60 sec... \tLoss: 0.170... \tVal Loss: 0.290 \tVal Frac: 0.9226\n",
      "\tValidation loss decreased (0.294 --> 0.290).  Saving model ...\n",
      "\tEpoch: 60/300; 7.54 sec... \tLoss: 0.292... \tVal Loss: 0.314 \tVal Frac: 0.9128\n",
      "\tEpoch: 65/300; 7.43 sec... \tLoss: 0.235... \tVal Loss: 0.285 \tVal Frac: 0.9205\n",
      "\tValidation loss decreased (0.290 --> 0.285).  Saving model ...\n",
      "\tEpoch: 70/300; 7.45 sec... \tLoss: 0.248... \tVal Loss: 0.273 \tVal Frac: 0.9235\n",
      "\tValidation loss decreased (0.285 --> 0.273).  Saving model ...\n",
      "\tEpoch: 75/300; 7.40 sec... \tLoss: 0.505... \tVal Loss: 0.287 \tVal Frac: 0.9193\n",
      "\tEpoch: 80/300; 7.40 sec... \tLoss: 0.045... \tVal Loss: 0.304 \tVal Frac: 0.9107\n",
      "\tEpoch: 85/300; 7.35 sec... \tLoss: 0.228... \tVal Loss: 0.296 \tVal Frac: 0.9240\n",
      "\tEpoch: 90/300; 7.59 sec... \tLoss: 0.246... \tVal Loss: 0.285 \tVal Frac: 0.9225\n",
      "\tEpoch: 95/300; 7.40 sec... \tLoss: 0.060... \tVal Loss: 0.302 \tVal Frac: 0.9163\n",
      "\tEpoch: 100/300; 7.43 sec... \tLoss: 0.108... \tVal Loss: 0.285 \tVal Frac: 0.9225\n",
      "\tEpoch: 105/300; 7.36 sec... \tLoss: 0.155... \tVal Loss: 0.299 \tVal Frac: 0.9174\n",
      "\tEpoch: 110/300; 7.53 sec... \tLoss: 0.272... \tVal Loss: 0.310 \tVal Frac: 0.9139\n",
      "\tEpoch: 115/300; 7.42 sec... \tLoss: 0.475... \tVal Loss: 0.323 \tVal Frac: 0.9148\n",
      "\tEpoch: 120/300; 7.62 sec... \tLoss: 0.094... \tVal Loss: 0.283 \tVal Frac: 0.9232\n",
      "\tEpoch: 125/300; 7.45 sec... \tLoss: 0.198... \tVal Loss: 0.311 \tVal Frac: 0.9242\n",
      "\tEpoch: 130/300; 7.49 sec... \tLoss: 0.173... \tVal Loss: 0.297 \tVal Frac: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 135/300; 7.42 sec... \tLoss: 0.172... \tVal Loss: 0.327 \tVal Frac: 0.9172\n",
      "\tEpoch: 140/300; 7.47 sec... \tLoss: 0.060... \tVal Loss: 0.310 \tVal Frac: 0.9185\n",
      "\tEpoch: 145/300; 7.53 sec... \tLoss: 0.286... \tVal Loss: 0.316 \tVal Frac: 0.9150\n",
      "\tEpoch: 150/300; 7.39 sec... \tLoss: 0.133... \tVal Loss: 0.300 \tVal Frac: 0.9225\n",
      "\tEpoch: 155/300; 7.52 sec... \tLoss: 0.287... \tVal Loss: 0.322 \tVal Frac: 0.9186\n",
      "\tEpoch: 160/300; 7.36 sec... \tLoss: 0.322... \tVal Loss: 0.339 \tVal Frac: 0.9146\n",
      "\tEpoch: 165/300; 7.52 sec... \tLoss: 0.336... \tVal Loss: 0.318 \tVal Frac: 0.9181\n",
      "\tEpoch: 170/300; 7.40 sec... \tLoss: 0.246... \tVal Loss: 0.313 \tVal Frac: 0.9219\n",
      "\tEpoch: 175/300; 7.47 sec... \tLoss: 0.064... \tVal Loss: 0.312 \tVal Frac: 0.9208\n",
      "\tEpoch: 180/300; 7.43 sec... \tLoss: 0.165... \tVal Loss: 0.329 \tVal Frac: 0.9160\n",
      "\tEpoch: 185/300; 7.54 sec... \tLoss: 0.151... \tVal Loss: 0.317 \tVal Frac: 0.9169\n",
      "\tEpoch: 190/300; 7.53 sec... \tLoss: 0.136... \tVal Loss: 0.339 \tVal Frac: 0.9109\n",
      "\tEpoch: 195/300; 7.48 sec... \tLoss: 0.192... \tVal Loss: 0.338 \tVal Frac: 0.9153\n",
      "\tEpoch: 200/300; 7.42 sec... \tLoss: 0.131... \tVal Loss: 0.307 \tVal Frac: 0.9210\n",
      "\tEpoch: 205/300; 7.58 sec... \tLoss: 0.028... \tVal Loss: 0.319 \tVal Frac: 0.9155\n",
      "\tEpoch: 210/300; 7.37 sec... \tLoss: 0.121... \tVal Loss: 0.325 \tVal Frac: 0.9150\n",
      "\tEpoch: 215/300; 7.62 sec... \tLoss: 0.114... \tVal Loss: 0.300 \tVal Frac: 0.9192\n",
      "\tEpoch: 220/300; 7.50 sec... \tLoss: 0.118... \tVal Loss: 0.324 \tVal Frac: 0.9147\n",
      "\tEpoch: 225/300; 7.47 sec... \tLoss: 0.259... \tVal Loss: 0.318 \tVal Frac: 0.9169\n",
      "\tEpoch: 230/300; 7.45 sec... \tLoss: 0.217... \tVal Loss: 0.320 \tVal Frac: 0.9161\n",
      "\tEpoch: 235/300; 7.40 sec... \tLoss: 0.165... \tVal Loss: 0.325 \tVal Frac: 0.9179\n",
      "\tEpoch: 240/300; 7.42 sec... \tLoss: 0.116... \tVal Loss: 0.324 \tVal Frac: 0.9141\n",
      "\tEpoch: 245/300; 7.40 sec... \tLoss: 0.188... \tVal Loss: 0.323 \tVal Frac: 0.9184\n",
      "\tEpoch: 250/300; 7.47 sec... \tLoss: 0.291... \tVal Loss: 0.311 \tVal Frac: 0.9197\n",
      "\tEpoch: 255/300; 7.33 sec... \tLoss: 0.022... \tVal Loss: 0.323 \tVal Frac: 0.9199\n",
      "\tEpoch: 260/300; 7.47 sec... \tLoss: 0.031... \tVal Loss: 0.313 \tVal Frac: 0.9171\n",
      "\tEpoch: 265/300; 7.39 sec... \tLoss: 0.165... \tVal Loss: 0.310 \tVal Frac: 0.9203\n",
      "\tEpoch: 270/300; 7.43 sec... \tLoss: 0.301... \tVal Loss: 0.297 \tVal Frac: 0.9205\n",
      "\tEpoch: 275/300; 7.54 sec... \tLoss: 0.014... \tVal Loss: 0.318 \tVal Frac: 0.9201\n",
      "\tEpoch: 280/300; 7.45 sec... \tLoss: 0.249... \tVal Loss: 0.312 \tVal Frac: 0.9185\n",
      "\tEpoch: 285/300; 7.47 sec... \tLoss: 0.057... \tVal Loss: 0.327 \tVal Frac: 0.9176\n",
      "\tEpoch: 290/300; 7.37 sec... \tLoss: 0.182... \tVal Loss: 0.306 \tVal Frac: 0.9214\n",
      "\tEpoch: 295/300; 7.32 sec... \tLoss: 0.423... \tVal Loss: 0.309 \tVal Frac: 0.9186\n",
      "\tEpoch: 300/300; 7.60 sec... \tLoss: 0.063... \tVal Loss: 0.320 \tVal Frac: 0.9209\n",
      "Completed: lr: 0.001\n",
      "\tloss: 0.273055 \n",
      "\tfrac:0.924212\n",
      "b'run_995395769.res'\n",
      "\n",
      "###########\n",
      "Testing with  lr :  0.01\n",
      "\tEpoch: 5/300; 7.42 sec... \tLoss: 0.562... \tVal Loss: 0.510 \tVal Frac: 0.8480\n",
      "\tValidation loss decreased (inf --> 0.510).  Saving model ...\n",
      "\tEpoch: 10/300; 7.45 sec... \tLoss: 0.361... \tVal Loss: 0.451 \tVal Frac: 0.8596\n",
      "\tValidation loss decreased (0.510 --> 0.451).  Saving model ...\n",
      "\tEpoch: 15/300; 7.49 sec... \tLoss: 0.565... \tVal Loss: 0.436 \tVal Frac: 0.8697\n",
      "\tValidation loss decreased (0.451 --> 0.436).  Saving model ...\n",
      "\tEpoch: 20/300; 7.49 sec... \tLoss: 0.419... \tVal Loss: 0.445 \tVal Frac: 0.8657\n",
      "\tEpoch: 25/300; 7.55 sec... \tLoss: 0.326... \tVal Loss: 0.495 \tVal Frac: 0.8511\n",
      "\tEpoch: 30/300; 7.47 sec... \tLoss: 0.405... \tVal Loss: 0.462 \tVal Frac: 0.8575\n",
      "\tEpoch: 35/300; 7.47 sec... \tLoss: 0.507... \tVal Loss: 0.477 \tVal Frac: 0.8591\n",
      "\tEpoch: 40/300; 7.45 sec... \tLoss: 0.301... \tVal Loss: 0.468 \tVal Frac: 0.8607\n",
      "\tEpoch: 45/300; 7.28 sec... \tLoss: 0.384... \tVal Loss: 0.469 \tVal Frac: 0.8596\n",
      "\tEpoch: 50/300; 7.36 sec... \tLoss: 0.330... \tVal Loss: 0.437 \tVal Frac: 0.8640\n",
      "\tEpoch: 55/300; 7.40 sec... \tLoss: 0.431... \tVal Loss: 0.425 \tVal Frac: 0.8687\n",
      "\tValidation loss decreased (0.436 --> 0.425).  Saving model ...\n",
      "\tEpoch: 60/300; 7.38 sec... \tLoss: 0.565... \tVal Loss: 0.438 \tVal Frac: 0.8652\n",
      "\tEpoch: 65/300; 7.47 sec... \tLoss: 0.553... \tVal Loss: 0.508 \tVal Frac: 0.8556\n",
      "\tEpoch: 70/300; 7.53 sec... \tLoss: 0.323... \tVal Loss: 0.443 \tVal Frac: 0.8545\n",
      "\tEpoch: 75/300; 7.53 sec... \tLoss: 0.301... \tVal Loss: 0.434 \tVal Frac: 0.8617\n",
      "\tEpoch: 80/300; 7.30 sec... \tLoss: 0.360... \tVal Loss: 0.503 \tVal Frac: 0.8568\n",
      "\tEpoch: 85/300; 7.33 sec... \tLoss: 0.656... \tVal Loss: 0.441 \tVal Frac: 0.8633\n",
      "\tEpoch: 90/300; 7.42 sec... \tLoss: 0.921... \tVal Loss: 0.431 \tVal Frac: 0.8738\n",
      "\tEpoch: 95/300; 7.50 sec... \tLoss: 0.163... \tVal Loss: 0.409 \tVal Frac: 0.8745\n",
      "\tValidation loss decreased (0.425 --> 0.409).  Saving model ...\n",
      "\tEpoch: 100/300; 7.39 sec... \tLoss: 0.811... \tVal Loss: 0.427 \tVal Frac: 0.8712\n",
      "\tEpoch: 105/300; 7.42 sec... \tLoss: 0.462... \tVal Loss: 0.434 \tVal Frac: 0.8748\n",
      "\tEpoch: 110/300; 7.52 sec... \tLoss: 0.408... \tVal Loss: 0.438 \tVal Frac: 0.8645\n",
      "\tEpoch: 115/300; 7.38 sec... \tLoss: 0.810... \tVal Loss: 0.487 \tVal Frac: 0.8618\n",
      "\tEpoch: 120/300; 7.57 sec... \tLoss: 1.220... \tVal Loss: 0.458 \tVal Frac: 0.8626\n",
      "\tEpoch: 125/300; 7.42 sec... \tLoss: 0.508... \tVal Loss: 0.450 \tVal Frac: 0.8626\n",
      "\tEpoch: 130/300; 7.47 sec... \tLoss: 0.911... \tVal Loss: 0.455 \tVal Frac: 0.8627\n",
      "\tEpoch: 135/300; 7.50 sec... \tLoss: 0.923... \tVal Loss: 0.457 \tVal Frac: 0.8673\n",
      "\tEpoch: 140/300; 7.47 sec... \tLoss: 0.719... \tVal Loss: 0.467 \tVal Frac: 0.8650\n",
      "\tEpoch: 145/300; 7.38 sec... \tLoss: 0.471... \tVal Loss: 0.491 \tVal Frac: 0.8571\n",
      "\tEpoch: 150/300; 7.35 sec... \tLoss: 0.174... \tVal Loss: 0.415 \tVal Frac: 0.8723\n",
      "\tEpoch: 155/300; 7.42 sec... \tLoss: 0.768... \tVal Loss: 0.438 \tVal Frac: 0.8711\n",
      "\tEpoch: 160/300; 7.26 sec... \tLoss: 0.512... \tVal Loss: 0.455 \tVal Frac: 0.8668\n",
      "\tEpoch: 165/300; 7.47 sec... \tLoss: 0.430... \tVal Loss: 0.451 \tVal Frac: 0.8726\n",
      "\tEpoch: 170/300; 7.36 sec... \tLoss: 0.619... \tVal Loss: 0.451 \tVal Frac: 0.8697\n",
      "\tEpoch: 175/300; 7.53 sec... \tLoss: 0.468... \tVal Loss: 0.434 \tVal Frac: 0.8686\n",
      "\tEpoch: 180/300; 7.46 sec... \tLoss: 0.572... \tVal Loss: 0.462 \tVal Frac: 0.8646\n",
      "\tEpoch: 185/300; 7.37 sec... \tLoss: 0.460... \tVal Loss: 0.458 \tVal Frac: 0.8633\n",
      "\tEpoch: 190/300; 7.39 sec... \tLoss: 0.792... \tVal Loss: 0.473 \tVal Frac: 0.8614\n",
      "\tEpoch: 195/300; 7.35 sec... \tLoss: 0.391... \tVal Loss: 0.457 \tVal Frac: 0.8629\n",
      "\tEpoch: 200/300; 7.42 sec... \tLoss: 0.906... \tVal Loss: 0.459 \tVal Frac: 0.8657\n",
      "\tEpoch: 205/300; 7.45 sec... \tLoss: 0.291... \tVal Loss: 0.467 \tVal Frac: 0.8635\n",
      "\tEpoch: 210/300; 7.39 sec... \tLoss: 0.481... \tVal Loss: 0.457 \tVal Frac: 0.8635\n",
      "\tEpoch: 215/300; 7.52 sec... \tLoss: 0.510... \tVal Loss: 0.457 \tVal Frac: 0.8612\n",
      "\tEpoch: 220/300; 7.45 sec... \tLoss: 0.724... \tVal Loss: 0.438 \tVal Frac: 0.8761\n",
      "\tEpoch: 225/300; 7.59 sec... \tLoss: 0.582... \tVal Loss: 0.470 \tVal Frac: 0.8709\n",
      "\tEpoch: 230/300; 7.36 sec... \tLoss: 0.197... \tVal Loss: 0.428 \tVal Frac: 0.8633\n",
      "\tEpoch: 235/300; 7.52 sec... \tLoss: 0.545... \tVal Loss: 0.513 \tVal Frac: 0.8324\n",
      "\tEpoch: 240/300; 7.37 sec... \tLoss: 0.158... \tVal Loss: 0.491 \tVal Frac: 0.8429\n",
      "\tEpoch: 245/300; 7.36 sec... \tLoss: 0.633... \tVal Loss: 0.472 \tVal Frac: 0.8622\n",
      "\tEpoch: 250/300; 7.39 sec... \tLoss: 0.385... \tVal Loss: 0.457 \tVal Frac: 0.8613\n",
      "\tEpoch: 255/300; 7.45 sec... \tLoss: 0.713... \tVal Loss: 0.472 \tVal Frac: 0.8664\n",
      "\tEpoch: 260/300; 7.43 sec... \tLoss: 0.862... \tVal Loss: 0.514 \tVal Frac: 0.8486\n",
      "\tEpoch: 265/300; 7.74 sec... \tLoss: 0.414... \tVal Loss: 0.477 \tVal Frac: 0.8620\n",
      "\tEpoch: 270/300; 7.36 sec... \tLoss: 0.232... \tVal Loss: 0.481 \tVal Frac: 0.8652\n",
      "\tEpoch: 275/300; 7.38 sec... \tLoss: 0.678... \tVal Loss: 0.468 \tVal Frac: 0.8574\n",
      "\tEpoch: 280/300; 7.56 sec... \tLoss: 0.473... \tVal Loss: 0.460 \tVal Frac: 0.8501\n",
      "\tEpoch: 285/300; 7.37 sec... \tLoss: 0.333... \tVal Loss: 0.432 \tVal Frac: 0.8654\n",
      "\tEpoch: 290/300; 7.42 sec... \tLoss: 0.811... \tVal Loss: 0.421 \tVal Frac: 0.8708\n",
      "\tEpoch: 295/300; 7.53 sec... \tLoss: 0.438... \tVal Loss: 0.500 \tVal Frac: 0.8580\n",
      "\tEpoch: 300/300; 7.59 sec... \tLoss: 0.210... \tVal Loss: 0.464 \tVal Frac: 0.8671\n",
      "Completed: lr: 0.01\n",
      "\tloss: 0.409473 \n",
      "\tfrac:0.876100\n",
      "b'run_131342301.res'\n"
     ]
    }
   ],
   "source": [
    "n.random.seed(2358)\n",
    "torch.manual_seed(2358)\n",
    "\n",
    "results = []\n",
    "for key in sweeps.keys():\n",
    "    for param_val in sweeps[key]:\n",
    "        params = copy.copy(params_original)\n",
    "        params[key] = param_val\n",
    "        \n",
    "        print(\"\\n###########\\nTesting with \", key, \": \", param_val)\n",
    "        random_idx = int(n.random.random()*1e9)\n",
    "        params_result, model = test_model(params, datas, device, random_idx)\n",
    "        \n",
    "        \n",
    "        result_string = \"Completed: \" + str(key) + \": \" + str(param_val) + \"\\n\\tloss: {:.6f} \\n\\tfrac:{:.6f}\".format(params_result['valid_loss_min'], params_result['valid_frac_max'])\n",
    "        \n",
    "        results.append((copy.deepcopy(params_result), copy.deepcopy(model), result_string))\n",
    "        print(result_string)\n",
    "        \n",
    "        filename = b\"run_\" + bytes(str(random_idx),'ascii') + b\".res\"\n",
    "        print(filename)\n",
    "        filehandler = open(filename,\"wb\")\n",
    "        pickle.dump(results[-1],filehandler)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
