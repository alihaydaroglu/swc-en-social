{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as n\n",
    "import numpy.lib.recfunctions as rfn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, split = 0.8, separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = int(data_len // chunk_size)\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = split*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        \n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    \n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, targets_train, targets_test\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_separate,\\\n",
    "                targets_train, targets_test, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, architecture):\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.D_in = self.architecture['D_in']\n",
    "        self.D_out = self.architecture['D_out']\n",
    "        self.hidden_dims = self.architecture['hidden_dims']\n",
    "        \n",
    "        self.layers = []\n",
    "        prev_dim = self.architecture['D_in']\n",
    "        for dim in self.architecture['hidden_dims']:\n",
    "            self.layers += [torch.nn.Linear(prev_dim, dim),\n",
    "                           torch.nn.ReLU()]\n",
    "            prev_dim = dim\n",
    "        self.layers += [torch.nn.Linear(prev_dim, self.D_out)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.trackers = {}\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.model.apply(init_weights)\n",
    "   \n",
    "    def start_trackers(self,track, reset_trackers=True):\n",
    "        if reset_trackers: self.trackers = {}\n",
    "        for t in track:\n",
    "            self.trackers[t] = []\n",
    "    \n",
    "    def track(self):\n",
    "        for variable in self.trackers.keys():\n",
    "            self.trackers[variable].append(copy.deepcopy(getattr(self, variable)))\n",
    "        \n",
    "    def learn(self,learning, training_set, test_set, reset_trackers=True, verbose=True):\n",
    "        \n",
    "        #self.init_weights()\n",
    "        \n",
    "        # set up variables\n",
    "        N_batch = learning['N_batch']\n",
    "        N_epochs = learning['N_epochs']\n",
    "        loss_fn = learning['loss_fn']\n",
    "        print_interval = learning['print_interval']\n",
    "        track = learning['track']\n",
    "        learning_rate = learning['learning_rate']\n",
    "        \n",
    "        self.learning = learning\n",
    "        \n",
    "        optimizer = learning['optimizer'](self.model.parameters(), learning_rate)\n",
    "        self.start_trackers(track, reset_trackers)\n",
    "        \n",
    "        # load data\n",
    "        x_train, y_train = training_set\n",
    "        x_test, y_test = test_set\n",
    "\n",
    "        N_training = len(y_train)\n",
    "        N_test = len(y_test)\n",
    "    \n",
    "        self.t = 0\n",
    "        tic = time.time()\n",
    "        end = False\n",
    "        for self.epoch_idx in range(N_epochs):\n",
    "            if verbose: print(\"### EPOCH {:2d} ###\".format(self.epoch_idx))\n",
    "                \n",
    "            # randomize batches\n",
    "            indices = n.random.choice(range(N_training), N_training, False)\n",
    "            num_batches = len(indices) // N_batch + 1\n",
    "\n",
    "            for self.batch_idx in range(num_batches):\n",
    "                # load batch\n",
    "                b_idx = self.batch_idx\n",
    "                x_train_batch = x_train[indices[b_idx*N_batch :(b_idx+1)*N_batch]]\n",
    "                y_train_batch = y_train[indices[b_idx*N_batch : (b_idx+1)*N_batch]]\n",
    "\n",
    "                \n",
    "                # predict, loss and learn\n",
    "                y_train_batch_pred = self.model(x_train_batch)\n",
    "                loss = loss_fn(y_train_batch_pred, y_train_batch)\n",
    "                self.train_loss = loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.t += 1\n",
    "\n",
    "                if self.batch_idx % print_interval == 0:\n",
    "                    \n",
    "#                     y_train_pred = self.model(x_train)\n",
    "#                     self.train_loss = loss_fn(y_train_pred, y_train).item()\n",
    "#                     pred_labels = n.argmax(y_train_pred.detach().numpy(),axis=1)\n",
    "#                     true_labels = y_train.detach().numpy()\n",
    "#                     correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.train_frac_correct = 0# n.mean(correct_preds)\n",
    "                    \n",
    "                    y_test_pred = self.model(x_test)\n",
    "                    self.test_loss = loss_fn(y_test_pred, y_test).item()\n",
    "                    pred_labels = n.argmax(y_test_pred.detach().numpy(),axis=1)\n",
    "                    true_labels = y_test.detach().numpy()\n",
    "                    correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.test_frac_correct = n.mean(correct_preds)\n",
    "\n",
    "                    toc = time.time()\n",
    "                    delta = toc - tic\n",
    "                    tic = toc\n",
    "                    print(\"Time: {:4.2f}, Batch {:3d}, Train Loss (for batch): {:4.2f}, Test Loss: {:4.2f}, Test Correct Frac: {:.3f}\".format(\\\n",
    "                                       delta, self.batch_idx, self.train_loss, self.test_loss, self.test_frac_correct))\n",
    "            \n",
    "                    self.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_net_results(net, x_test, y_test):\n",
    "    plt.plot(net.trackers['t'],net.trackers['train_loss'], label='train')\n",
    "    plt.plot(net.trackers['t'],net.trackers['test_loss'], label='test')\n",
    "    plt.title(\"Cross Entropy Loss through training\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,10)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(net.trackers['t'], net.trackers['test_frac_correct'])\n",
    "    plt.title(\"Fraction of correct labels on test set through training\")\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.show()\n",
    "    \n",
    "    max_perf_ind = n.argmax(net.trackers['test_frac_correct'])\n",
    "    min_loss_ind = n.argmin(net.trackers['test_loss'])\n",
    "    max_perf_model = net.trackers['model'][max_perf_ind]\n",
    "    \n",
    "    prediction_test = max_perf_model(x_test)\n",
    "    pred = n.argmax(prediction_test.detach().numpy(),axis=1)\n",
    "    true = y_test.detach().numpy()\n",
    "    confmat = confusion_matrix(true, pred, normalize='true')\n",
    "    f, ax = plt.subplots(figsize=(10,10))\n",
    "    m = ax.matshow(confmat, cmap='Blues', vmin=0,  vmax=1)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_xticks(list(range(len(categories))))\n",
    "    ax.set_xticklabels(categories, rotation=45)\n",
    "    ax.set_yticks(list(range(len(categories))))\n",
    "    ax.set_yticklabels(categories, rotation=45)\n",
    "    f.colorbar(m)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,output_dim, n_layers_LSTM, dropout_prob):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers_LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, \n",
    "                            num_layers  = n_layers_LSTM, dropout = dropout_prob,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.out_func = nn.Softmax(dim=1)\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "#         x = x.long()\n",
    "#         print(x.shape)\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(x)\n",
    "        else: \n",
    "            lstm_out, hidden = self.lstm(x,hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(batch_size, self.output_dim, -1)[:,:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(datas, n_window, n_stride, test_vids=[3,4],batch_size=32, shuffle=True):\n",
    "    datapoints = []\n",
    "    labels = []\n",
    "\n",
    "    trial_bounds = []\n",
    "\n",
    "    trial_idx = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for data in datas:  \n",
    "        trial_bounds.append(idx)\n",
    "        for i in range(int(data.shape[1]/n_stride)):\n",
    "            if (i*n_stride + n_window) < data.shape[1]:\n",
    "                datapoints.append(data[:,i*n_stride : i*n_stride + n_window])\n",
    "                new_labels = [all_data[trial_keys[trial_idx]]['ann'][i*n_stride + int(n_window/2)]]\n",
    "                new_labels = [target_map[l] for l in new_labels]\n",
    "                labels.append(new_labels)\n",
    "                idx += 1\n",
    "        trial_idx += 1\n",
    "    trial_bounds.append(idx)  \n",
    "    \n",
    "    Xs = n.stack(datapoints)\n",
    "    Ys = n.stack(labels)\n",
    "    num_features = Xs.shape[1]\n",
    "    \n",
    "    Xs_train = []; Ys_train = []; Xs_test = []; Ys_test = []\n",
    "    for i in range(len(trial_bounds)-1):\n",
    "        if i in test_vids:\n",
    "            Xs_test.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_test.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "        else:\n",
    "            Xs_train.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_train.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "#     print(Xs_train[0].shape)\n",
    "    Xs_train = n.concatenate(Xs_train,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_train = n.concatenate(Ys_train,axis=0).reshape(-1)\n",
    "    Xs_test = n.concatenate(Xs_test,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_test = n.concatenate(Ys_test,axis=0).reshape(-1)\n",
    "    \n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_train), torch.from_numpy(Ys_train))\n",
    "    test_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_test), torch.from_numpy(Ys_test))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    \n",
    "    return train_data, train_loader, test_data, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(params, datas, device, random_val):\n",
    "    train_data, train_loader, val_data, val_loader \\\n",
    "            = extract_data(datas, params['n_windows'], params['n_stride'],\n",
    "                        batch_size=params['batch_size'],shuffle=params['shuffle'])\n",
    "        \n",
    "    model = LSTM_net(params['input_dim'], params['hidden_dim'], params['output_dim'], params['n_layers_LSTM'], params['dropout_prob'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    counter = 0 \n",
    "    tic = time.time()\n",
    "    for i in range(params['N_epochs']):\n",
    "        h = model.init_hidden(params['batch_size'])\n",
    "        for inputs, labels in train_loader:\n",
    "            if inputs.shape[0] != params['batch_size']: continue\n",
    "            counter += 1\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs.float(), (h[0].float(), h[1].float()))\n",
    "            loss = criterion(output.squeeze(), labels[:].long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), params['clip'])\n",
    "            optimizer.step()\n",
    "                        \n",
    "        toc = time.time() - tic\n",
    "        tic = time.time()\n",
    "        \n",
    "        if counter%params['print_every'] == 0:\n",
    "            val_h = model.init_hidden(params['batch_size'])\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            fracs = []\n",
    "            for inp, lab in val_loader:\n",
    "                if inp.shape[0] != params['batch_size']: continue\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), [val_h[0].float(), val_h[1].float()])\n",
    "                val_loss = criterion(out.squeeze(), lab[:].long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred_labels = n.argmax(out.detach().numpy(),axis=1)\n",
    "                true_labels = lab.detach().numpy()\n",
    "                correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                fracs.append(n.mean(correct_preds))\n",
    "                \n",
    "            frac_correct_val = (n.mean(fracs))\n",
    "            \n",
    "            model.train()\n",
    "            print(\"\\tEpoch: {}/{}; {:.2f} sec...\".format(i+1, params['N_epochs'],toc),\n",
    "                  \"\\tStep: {}...\".format(counter),\n",
    "                  \"\\tLoss: {:.3f}...\".format(loss.item()),\n",
    "                  \"\\tVal Loss: {:.3f}\".format(n.mean(val_losses)),\n",
    "                  \"\\tVal Frac: {:.4f}\".format(frac_correct_val))\n",
    "            if n.mean(val_losses) <= params['valid_loss_min']:\n",
    "                torch.save(model.state_dict(), './state_dict' + str(random_val) + '.pt')\n",
    "                print('\\tValidation loss decreased ({:.3f} --> {:.3f}).  Saving model ...'.format(params['valid_loss_min'],n.mean(val_losses)))\n",
    "                params['valid_loss_min'] = n.mean(val_losses)\n",
    "            if frac_correct_val >= params['valid_frac_max']:\n",
    "                params['valid_frac_max'] = frac_correct_val\n",
    "                params['pred_labels_best'] = pred_labels.copy()\n",
    "                params['true_labels_best'] = true_labels.copy()\n",
    "\n",
    "                \n",
    "    return params, model\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = '/mnt/d/Ali/Google Drive/SWC/Experimental Neuroscience/social and affective systems/rat_social_beh'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n",
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = list(all_data.keys())\n",
    "datas = []\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)\n",
    "input_dim = datas[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_original = {\n",
    "    'n_windows'      : 100,\n",
    "    'n_stride'       : 20,\n",
    "    'shuffle'        : True,\n",
    "    'dropout_prob'   : 0.3,\n",
    "    'batch_size'     : 32,\n",
    "    'hidden_dim'     : 50,\n",
    "    'n_layers_LSTM'  : 1, \n",
    "    'output_dim'     : N_categories,\n",
    "    'input_dim'      : input_dim,\n",
    "    'lr'             : 0.005,\n",
    "    'N_epochs'       : 1,\n",
    "    'print_every'    : 1,\n",
    "    'clip'           : 5,\n",
    "    'valid_loss_min' : n.Inf,\n",
    "    'valid_frac_max' : 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sweeps = {\n",
    "    'n_windows'       : [2, 20], #, 50, 100, 500],\n",
    "    'n_stride'        : [5, 20],# 50, 100],\n",
    "    'dropout_prob'    : [0, 0.3],# 0.5],\n",
    "    'batch_size'      : [8, 32],# 64, 128],\n",
    "    'hidden_dim'      : [10, 20], # 50, 100],\n",
    "    'n_layers_LSTM'   : [1, 2],# 3,5],\n",
    "    'lr'              : [0.001, 0.005],# 0.01]\n",
    "}\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###########\n",
      "Testing with  n_windows :  2\n",
      "\tEpoch: 1/1; 0.96 sec... \tStep: 243... \tLoss: 0.751... \tVal Loss: 0.673 \tVal Frac: 0.8059\n",
      "\tValidation loss decreased (inf --> 0.673).  Saving model ...\n",
      "\n",
      "Completed.\n",
      "\tloss: 0.672899 \n",
      "\tfrac:0.805882\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-19500ffc5f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n###########\\nTesting with \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\": \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mparams_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-546ac2633268>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(params, datas, device, random_val)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key in sweeps.keys():\n",
    "    for param_val in sweeps[key]:\n",
    "        n.random.seed(2358)\n",
    "        torch.manual_seed(2358)\n",
    "        params = copy.copy(params_original)\n",
    "        params[key] = param_val\n",
    "        \n",
    "        print(\"\\n###########\\nTesting with \", key, \": \", param_val)\n",
    "        \n",
    "        params_result, model = test_model(params, datas, device, int(n.random.random()*1e9))\n",
    "        \n",
    "        results.append((copy.deepcopy(params_result), copy.deepcopy(model)))\n",
    "        \n",
    "        print(\"Completed.\\n\\tloss: {:.6f} \\n\\tfrac:{:.6f}\".format(params_result['valid_loss_min'], params_result['valid_frac_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gp]",
   "language": "python",
   "name": "conda-env-gp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
