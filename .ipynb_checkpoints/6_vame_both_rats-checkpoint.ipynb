{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy.lib.recfunctions as rfn\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000, only_rat1 = False):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    if not only_rat1:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    else:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_1_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_1_vel_labels])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, splits= (0.7, 0.2, 0.1), separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = data_len // chunk_size\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = splits[0]*num_chunks\n",
    "    val_chunk_idx_bound = (splits[0]+splits[1])*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    features_val = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    targets_val = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        elif i < val_chunk_idx_bound:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('val')\n",
    "            features_val.append(curr_chunk)\n",
    "            targets_val.append(curr_chunk_t)\n",
    "\n",
    "#     print(len(features_separate))\n",
    "#     print(len(targets_separate))\n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    features_val = n.concatenate(features_val, axis=0)\n",
    "    \n",
    "    targets_val = n.concatenate(targets_val)\n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, features_val, targets_train, targets_test, targets_val\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_val, features_separate,\\\n",
    "                targets_train, targets_test, targets_val, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / n.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return (n.arccos(n.clip(n.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = ['Female1']#list(all_data.keys())\n",
    "datas = []\n",
    "# for key in all_data.keys():\n",
    "#     all_data[key] = calculate_velocity(all_data[key])\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels, None, True)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4866346588828399"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_to_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.34558105, 51.51176453])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.69473821698646"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7763568394002505e-15"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_points = n.zeros((features_all.shape[0], int(features_all.shape[1]/2), 2))\n",
    "# t = 1000\n",
    "angles = []\n",
    "for t in range(features_all.shape[0]):\n",
    "    points = features_all[t]\n",
    "    # x, y format\n",
    "    anchor_point = n.array([points[1], points[0]])\n",
    "    second_point = n.array([points[3], points[2]])\n",
    "    anchor_vector = second_point - anchor_point\n",
    "    angle_to_rotate = -angle_between(anchor_vector, n.array((1,0)))\n",
    "    if n.all(anchor_vector[1] < 0): angle_to_rotate *= -1\n",
    "    angles.append(angle_to_rotate)\n",
    "#     if angle_to_rotate < 0: angle_to_rotate += 2*n.pi\n",
    "    # given vector (x1, y1), rotating it by A around origin gives:\n",
    "    # x2 = cosA x1 - sinA y1\n",
    "    # y2 = sinA x1 + cosA y1\n",
    "    num_points = int(len(points)/2)\n",
    "    new_points = n.zeros((num_points,2))\n",
    "    for points_idx in range(1,num_points):\n",
    "        second_point = n.array([points[points_idx*2+1], points[points_idx*2]])\n",
    "        vector = second_point - anchor_point\n",
    "        new_x = n.cos(angle_to_rotate) * vector[0] - n.sin(angle_to_rotate) * vector[1]\n",
    "        new_y = n.sin(angle_to_rotate) * vector[0] + n.cos(angle_to_rotate) * vector[1]\n",
    "\n",
    "        ego_points[t][points_idx] = [new_x, new_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 10000\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(ego_points[t,i,0], ego_points[t,i,1])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-200,200)\n",
    "plt.xlim(-200,200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(features_all[t][i*2+1], features_all[t][i*2])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-200,200)\n",
    "plt.xlim(-200,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\videos\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\data\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\results\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\model\"\n",
      "1  videos from the directory C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1 were added to the project.\n",
      "Copying the videos \n",
      "\n",
      "A VAME project has been created. \n",
      "\n",
      "Next use vame.create_trainset(config) to split your data into a train and test set. \n",
      "Afterwards you can use vame.rnn_model() to train the model on your data.\n"
     ]
    }
   ],
   "source": [
    "config = vame.init_new_project(project='single-mouse-f1', videos = ['C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1'], \n",
    "                      working_directory='C:/Users/Neuropixel/AH-EN/swc-en-social/vame', videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\single-mouse-f1-Nov20-2020\\\\config.yaml'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 10, 2)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(, 'w+') as f:\n",
    "f = \"C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\single-mouse-f1-Nov20-2020\\\\data\\\\Female1\\\\Female1-PE-seq.npy\"\n",
    "n.save(f, ego_points.reshape(-1, 20).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset.\n",
      "Lenght of train data: 24216\n",
      "Lenght of test data: 6054\n"
     ]
    }
   ],
   "source": [
    "trainset = vame.create_trainset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RNN model!\n",
      "Using CUDA\n",
      "GPU active: True\n",
      "GPU used: Quadro P620\n",
      "Latent Dimensions: 30, Beta: 1, lr: 0.0005\n",
      "Compute mean and std for temporal dataset.\n",
      "Initialize train data. Datapoints 24216\n",
      "Initialize test data. Datapoints 6054\n",
      "Epoch: 1\n",
      "Train: \n",
      "Epoch: 1.  loss: 240607.7500\n",
      "Average Train loss: 45083.7238, MSE-Loss: 28401.3596, MSE-Future-Loss 16682.3641, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 2242.4663, MSE-Loss: 2242.4663, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 2\n",
      "Train: \n",
      "Epoch: 2.  loss: 20272.3301\n",
      "Average Train loss: 19168.1105, MSE-Loss: 11154.4096, MSE-Future-Loss 8013.7009, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 1694.9520, MSE-Loss: 1694.9520, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 3\n",
      "Train: \n",
      "Epoch: 3.  loss: 18419.5059\n",
      "Average Train loss: 15205.0624, MSE-Loss: 8588.5394, MSE-Future-Loss 6616.5230, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 1333.4067, MSE-Loss: 1333.4067, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 4\n",
      "Train: \n",
      "Epoch: 4.  loss: 13919.4775\n",
      "Average Train loss: 12936.9534, MSE-Loss: 7025.1373, MSE-Future-Loss 5903.0632, KL-Loss: 5.2824,  Kmeans-Loss: 3.4705, weigt: 0.5000\n",
      "Test: \n",
      "Average Test loss: 1123.5923, MSE-Loss: 1115.6178, KL-Loss: 5.6659, Kmeans-Loss: 2.3086\n",
      "lr: 0.0005\n",
      "Epoch: 5\n",
      "Train: \n",
      "Epoch: 5.  loss: 10919.7246\n",
      "Average Train loss: 11917.4542, MSE-Loss: 6187.9039, MSE-Future-Loss 5716.5309, KL-Loss: 8.2965,  Kmeans-Loss: 4.7229, weigt: 0.6250\n",
      "Test: \n",
      "Average Test loss: 1021.9699, MSE-Loss: 1010.2224, KL-Loss: 8.5156, Kmeans-Loss: 3.2320\n",
      "lr: 0.0005\n",
      "Epoch: 6\n",
      "Train: \n",
      "Epoch: 6.  loss: 9640.6240\n",
      "Average Train loss: 10968.6813, MSE-Loss: 5458.8559, MSE-Future-Loss 5491.9125, KL-Loss: 11.8305,  Kmeans-Loss: 6.0825, weigt: 0.7500\n",
      "Test: \n",
      "Average Test loss: 900.1999, MSE-Loss: 884.1543, KL-Loss: 11.8048, Kmeans-Loss: 4.2408\n",
      "lr: 0.0005\n",
      "Epoch: 7\n",
      "Train: \n",
      "Epoch: 7.  loss: 11003.0742\n",
      "Average Train loss: 10238.0558, MSE-Loss: 4965.2872, MSE-Future-Loss 5249.6406, KL-Loss: 15.5939,  Kmeans-Loss: 7.5341, weigt: 0.8750\n",
      "Test: \n",
      "Average Test loss: 780.9250, MSE-Loss: 760.1892, KL-Loss: 15.3693, Kmeans-Loss: 5.3665\n",
      "lr: 0.0005\n",
      "Epoch: 8\n",
      "Train: \n",
      "Epoch: 8.  loss: 9435.9287\n",
      "Average Train loss: 9638.3636, MSE-Loss: 4465.4086, MSE-Future-Loss 5144.0141, KL-Loss: 19.8470,  Kmeans-Loss: 9.0938, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 749.2237, MSE-Loss: 723.0848, KL-Loss: 19.5198, Kmeans-Loss: 6.6191\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 9\n",
      "Train: \n",
      "Epoch: 9.  loss: 9329.5068\n",
      "Average Train loss: 9220.4676, MSE-Loss: 4228.6595, MSE-Future-Loss 4960.5878, KL-Loss: 21.6768,  Kmeans-Loss: 9.5435, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 718.6952, MSE-Loss: 690.5878, KL-Loss: 21.0847, Kmeans-Loss: 7.0227\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 10\n",
      "Train: \n",
      "Epoch: 10.  loss: 8982.4746\n",
      "Average Train loss: 8804.3826, MSE-Loss: 3926.5726, MSE-Future-Loss 4844.7238, KL-Loss: 23.1654,  Kmeans-Loss: 9.9208, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 673.3208, MSE-Loss: 644.0805, KL-Loss: 21.9621, Kmeans-Loss: 7.2782\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 11\n",
      "Train: \n",
      "Epoch: 11.  loss: 7759.4263\n",
      "Average Train loss: 8511.2123, MSE-Loss: 3674.9094, MSE-Future-Loss 4800.9995, KL-Loss: 24.9922,  Kmeans-Loss: 10.3112, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 650.2838, MSE-Loss: 618.5625, KL-Loss: 24.0034, Kmeans-Loss: 7.7180\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 12\n",
      "Train: \n",
      "Epoch: 12.  loss: 8600.6006\n",
      "Average Train loss: 8135.1934, MSE-Loss: 3424.2547, MSE-Future-Loss 4673.6046, KL-Loss: 26.6779,  Kmeans-Loss: 10.6562, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 655.2769, MSE-Loss: 621.2802, KL-Loss: 25.9483, Kmeans-Loss: 8.0484\n",
      "lr: 0.0005\n",
      "Epoch: 13\n",
      "Train: \n",
      "Epoch: 13.  loss: 8133.7847\n",
      "Average Train loss: 7993.7957, MSE-Loss: 3430.0685, MSE-Future-Loss 4524.5875, KL-Loss: 28.1773,  Kmeans-Loss: 10.9625, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 593.8340, MSE-Loss: 558.8780, KL-Loss: 26.7100, Kmeans-Loss: 8.2460\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 14\n",
      "Train: \n",
      "Epoch: 14.  loss: 7877.2700\n",
      "Average Train loss: 7640.3998, MSE-Loss: 3222.5716, MSE-Future-Loss 4376.9366, KL-Loss: 29.6706,  Kmeans-Loss: 11.2210, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 600.3789, MSE-Loss: 563.1553, KL-Loss: 28.5947, Kmeans-Loss: 8.6289\n",
      "lr: 0.0005\n",
      "Epoch: 15\n",
      "Train: \n",
      "Epoch: 15.  loss: 7824.0034\n",
      "Average Train loss: 7481.8780, MSE-Loss: 3150.1004, MSE-Future-Loss 4289.5145, KL-Loss: 30.7846,  Kmeans-Loss: 11.4785, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 613.5966, MSE-Loss: 576.1397, KL-Loss: 28.7273, Kmeans-Loss: 8.7296\n",
      "lr: 0.0005\n",
      "Epoch: 16\n",
      "Train: \n",
      "Epoch: 16.  loss: 7273.0571\n",
      "Average Train loss: 7261.6879, MSE-Loss: 3015.4448, MSE-Future-Loss 4202.7746, KL-Loss: 31.7736,  Kmeans-Loss: 11.6948, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 550.9567, MSE-Loss: 512.4247, KL-Loss: 29.6357, Kmeans-Loss: 8.8962\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 17\n",
      "Train: \n",
      "Epoch: 17.  loss: 7818.1855\n",
      "Average Train loss: 7048.9870, MSE-Loss: 2983.6234, MSE-Future-Loss 4020.6059, KL-Loss: 32.8221,  Kmeans-Loss: 11.9357, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 543.5142, MSE-Loss: 503.6278, KL-Loss: 30.6699, Kmeans-Loss: 9.2165\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 18\n",
      "Train: \n",
      "Epoch: 18.  loss: 7738.3813\n",
      "Average Train loss: 6972.2946, MSE-Loss: 2890.0522, MSE-Future-Loss 4036.4555, KL-Loss: 33.6614,  Kmeans-Loss: 12.1256, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 527.6534, MSE-Loss: 486.8173, KL-Loss: 31.4909, Kmeans-Loss: 9.3453\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 19\n",
      "Train: \n",
      "Epoch: 19.  loss: 6318.7573\n",
      "Average Train loss: 6758.6990, MSE-Loss: 2871.6210, MSE-Future-Loss 3840.1587, KL-Loss: 34.6043,  Kmeans-Loss: 12.3151, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 525.2751, MSE-Loss: 484.3605, KL-Loss: 31.4984, Kmeans-Loss: 9.4162\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 20\n",
      "Train: \n",
      "Epoch: 20.  loss: 6355.7056\n",
      "Average Train loss: 6531.5352, MSE-Loss: 2773.3821, MSE-Future-Loss 3710.2607, KL-Loss: 35.3723,  Kmeans-Loss: 12.5201, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 498.2609, MSE-Loss: 455.8102, KL-Loss: 32.8338, Kmeans-Loss: 9.6170\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 21\n",
      "Train: \n",
      "Epoch: 21.  loss: 5504.0864\n",
      "Average Train loss: 6496.3106, MSE-Loss: 2725.8641, MSE-Future-Loss 3721.3990, KL-Loss: 36.3179,  Kmeans-Loss: 12.7296, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 501.4061, MSE-Loss: 458.5295, KL-Loss: 33.0692, Kmeans-Loss: 9.8074\n",
      "lr: 0.0005\n",
      "Epoch: 22\n",
      "Train: \n",
      "Epoch: 22.  loss: 5667.3809\n",
      "Average Train loss: 6334.3668, MSE-Loss: 2664.6662, MSE-Future-Loss 3619.9314, KL-Loss: 36.8766,  Kmeans-Loss: 12.8926, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 511.7521, MSE-Loss: 467.7684, KL-Loss: 34.0427, Kmeans-Loss: 9.9410\n",
      "lr: 0.0005\n",
      "Epoch: 23\n",
      "Train: \n",
      "Epoch: 23.  loss: 6566.8071\n",
      "Average Train loss: 6150.0054, MSE-Loss: 2668.9892, MSE-Future-Loss 3430.2355, KL-Loss: 37.7039,  Kmeans-Loss: 13.0768, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 490.2865, MSE-Loss: 445.5562, KL-Loss: 34.6186, Kmeans-Loss: 10.1117\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 24\n",
      "Train: \n",
      "Epoch: 24.  loss: 6254.4185\n",
      "Average Train loss: 6083.4652, MSE-Loss: 2596.6489, MSE-Future-Loss 3434.9869, KL-Loss: 38.5716,  Kmeans-Loss: 13.2578, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 494.4017, MSE-Loss: 448.8386, KL-Loss: 35.2676, Kmeans-Loss: 10.2954\n",
      "lr: 0.0005\n",
      "Epoch: 25\n",
      "Train: \n",
      "Epoch: 25.  loss: 7023.8755\n",
      "Average Train loss: 5895.8900, MSE-Loss: 2552.3771, MSE-Future-Loss 3290.8827, KL-Loss: 39.2075,  Kmeans-Loss: 13.4226, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 487.7902, MSE-Loss: 441.2724, KL-Loss: 36.0496, Kmeans-Loss: 10.4682\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 26\n",
      "Train: \n",
      "Epoch: 26.  loss: 5982.5605\n",
      "Average Train loss: 5843.4120, MSE-Loss: 2552.7461, MSE-Future-Loss 3237.1974, KL-Loss: 39.8602,  Kmeans-Loss: 13.6083, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 483.8322, MSE-Loss: 437.7875, KL-Loss: 35.5436, Kmeans-Loss: 10.5012\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 27\n",
      "Train: \n",
      "Epoch: 27.  loss: 5668.4062\n",
      "Average Train loss: 5635.1895, MSE-Loss: 2461.9891, MSE-Future-Loss 3119.3356, KL-Loss: 40.1503,  Kmeans-Loss: 13.7144, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 483.0468, MSE-Loss: 436.7704, KL-Loss: 35.6639, Kmeans-Loss: 10.6124\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 28\n",
      "Train: \n",
      "Epoch: 28.  loss: 5212.9102\n",
      "Average Train loss: 5449.7989, MSE-Loss: 2379.2307, MSE-Future-Loss 3016.1108, KL-Loss: 40.6042,  Kmeans-Loss: 13.8533, weigt: 1.0000\n",
      "Test: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 480.6234, MSE-Loss: 433.5298, KL-Loss: 36.3212, Kmeans-Loss: 10.7724\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 29\n",
      "Train: \n",
      "Epoch: 29.  loss: 5331.4399\n",
      "Average Train loss: 5378.4647, MSE-Loss: 2384.6454, MSE-Future-Loss 2938.5734, KL-Loss: 41.2276,  Kmeans-Loss: 14.0183, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 489.5567, MSE-Loss: 441.0815, KL-Loss: 37.4461, Kmeans-Loss: 11.0291\n",
      "lr: 0.0005\n",
      "Epoch: 30\n",
      "Train: \n",
      "Epoch: 30.  loss: 5077.9932\n",
      "Average Train loss: 5236.8491, MSE-Loss: 2301.8510, MSE-Future-Loss 2879.0088, KL-Loss: 41.8048,  Kmeans-Loss: 14.1845, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 491.8839, MSE-Loss: 443.3529, KL-Loss: 37.4714, Kmeans-Loss: 11.0595\n",
      "lr: 0.0005\n",
      "Epoch: 31\n",
      "Train: \n",
      "Epoch: 31.  loss: 5059.1431\n",
      "Average Train loss: 5132.6219, MSE-Loss: 2278.4973, MSE-Future-Loss 2797.8666, KL-Loss: 41.9854,  Kmeans-Loss: 14.2725, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 463.1592, MSE-Loss: 413.7404, KL-Loss: 38.1983, Kmeans-Loss: 11.2205\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 32\n",
      "Train: \n",
      "Epoch: 32.  loss: 4729.0615\n",
      "Average Train loss: 5021.4497, MSE-Loss: 2227.1582, MSE-Future-Loss 2737.2906, KL-Loss: 42.5958,  Kmeans-Loss: 14.4051, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 485.1872, MSE-Loss: 435.1253, KL-Loss: 38.7909, Kmeans-Loss: 11.2710\n",
      "lr: 0.0005\n",
      "Epoch: 33\n",
      "Train: \n",
      "Epoch: 33.  loss: 4928.7554\n",
      "Average Train loss: 5015.8640, MSE-Loss: 2280.9925, MSE-Future-Loss 2677.3435, KL-Loss: 42.9776,  Kmeans-Loss: 14.5503, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 461.4305, MSE-Loss: 411.1105, KL-Loss: 38.7987, Kmeans-Loss: 11.5213\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 34\n",
      "Train: \n",
      "Epoch: 34.  loss: 4861.5220\n",
      "Average Train loss: 4775.7723, MSE-Loss: 2152.8257, MSE-Future-Loss 2564.9242, KL-Loss: 43.3529,  Kmeans-Loss: 14.6694, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 449.0610, MSE-Loss: 397.5497, KL-Loss: 39.8448, Kmeans-Loss: 11.6665\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 35\n",
      "Train: \n",
      "Epoch: 35.  loss: 5372.5996\n",
      "Average Train loss: 4658.4705, MSE-Loss: 2136.1952, MSE-Future-Loss 2463.4390, KL-Loss: 43.9950,  Kmeans-Loss: 14.8411, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 450.4447, MSE-Loss: 398.4970, KL-Loss: 40.1397, Kmeans-Loss: 11.8081\n",
      "lr: 0.0005\n",
      "Epoch: 36\n",
      "Train: \n",
      "Epoch: 36.  loss: 4228.3354\n",
      "Average Train loss: 4604.6672, MSE-Loss: 2095.3140, MSE-Future-Loss 2450.0180, KL-Loss: 44.3938,  Kmeans-Loss: 14.9414, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 435.4710, MSE-Loss: 383.0261, KL-Loss: 40.6260, Kmeans-Loss: 11.8189\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 37\n",
      "Train: \n",
      "Epoch: 37.  loss: 4331.4609\n",
      "Average Train loss: 4531.6889, MSE-Loss: 2097.1164, MSE-Future-Loss 2374.5797, KL-Loss: 44.9247,  Kmeans-Loss: 15.0681, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 456.3710, MSE-Loss: 403.1995, KL-Loss: 41.2029, Kmeans-Loss: 11.9686\n",
      "lr: 0.0005\n",
      "Epoch: 38\n",
      "Train: \n",
      "Epoch: 38.  loss: 4466.4429\n",
      "Average Train loss: 4507.1228, MSE-Loss: 2126.0309, MSE-Future-Loss 2320.9412, KL-Loss: 45.0120,  Kmeans-Loss: 15.1387, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 454.1327, MSE-Loss: 401.5116, KL-Loss: 40.6530, Kmeans-Loss: 11.9681\n",
      "lr: 0.0005\n",
      "Epoch: 39\n",
      "Train: \n",
      "Epoch: 39.  loss: 3810.5166\n",
      "Average Train loss: 4375.9104, MSE-Loss: 2046.2234, MSE-Future-Loss 2269.3795, KL-Loss: 45.1165,  Kmeans-Loss: 15.1911, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 446.5925, MSE-Loss: 392.4058, KL-Loss: 41.9886, Kmeans-Loss: 12.1981\n",
      "lr: 0.0005\n",
      "Epoch: 40\n",
      "Train: \n",
      "Epoch: 40.  loss: 4656.3945\n",
      "Average Train loss: 4259.3993, MSE-Loss: 2022.5968, MSE-Future-Loss 2175.3878, KL-Loss: 46.0263,  Kmeans-Loss: 15.3884, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 452.7581, MSE-Loss: 398.8756, KL-Loss: 41.6722, Kmeans-Loss: 12.2103\n",
      "lr: 0.0005\n",
      "Epoch: 41\n",
      "Train: \n",
      "Epoch: 41.  loss: 4109.0015\n",
      "Average Train loss: 4331.2780, MSE-Loss: 2049.9188, MSE-Future-Loss 2219.6509, KL-Loss: 46.2427,  Kmeans-Loss: 15.4657, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 486.4922, MSE-Loss: 431.1438, KL-Loss: 42.9496, Kmeans-Loss: 12.3988\n",
      "lr: 0.0005\n",
      "Epoch: 42\n",
      "Train: \n",
      "Epoch: 42.  loss: 4248.7114\n",
      "Average Train loss: 4179.1275, MSE-Loss: 2014.7118, MSE-Future-Loss 2102.1873, KL-Loss: 46.6661,  Kmeans-Loss: 15.5624, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 438.7648, MSE-Loss: 383.6255, KL-Loss: 42.7063, Kmeans-Loss: 12.4330\n",
      "lr: 0.0005\n",
      "Epoch: 43\n",
      "Train: \n",
      "Epoch: 43.  loss: 3625.3643\n",
      "Average Train loss: 4091.0649, MSE-Loss: 1996.2163, MSE-Future-Loss 2032.2554, KL-Loss: 46.9593,  Kmeans-Loss: 15.6340, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 431.9088, MSE-Loss: 376.3618, KL-Loss: 43.0596, Kmeans-Loss: 12.4873\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 44\n",
      "Train: \n",
      "Epoch: 44.  loss: 3709.6697\n",
      "Average Train loss: 3941.3862, MSE-Loss: 1909.1291, MSE-Future-Loss 1969.2563, KL-Loss: 47.2721,  Kmeans-Loss: 15.7288, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 431.1518, MSE-Loss: 375.1119, KL-Loss: 43.4385, Kmeans-Loss: 12.6013\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 45\n",
      "Train: \n",
      "Epoch: 45.  loss: 4770.7021\n",
      "Average Train loss: 3924.6574, MSE-Loss: 1921.9973, MSE-Future-Loss 1939.2884, KL-Loss: 47.5773,  Kmeans-Loss: 15.7945, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 444.0299, MSE-Loss: 387.6129, KL-Loss: 43.7882, Kmeans-Loss: 12.6288\n",
      "lr: 0.0005\n",
      "Epoch: 46\n",
      "Train: \n",
      "Epoch: 46.  loss: 3696.7146\n",
      "Average Train loss: 3940.1694, MSE-Loss: 1923.0460, MSE-Future-Loss 1953.0914, KL-Loss: 48.1288,  Kmeans-Loss: 15.9032, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 436.3710, MSE-Loss: 379.4260, KL-Loss: 44.1684, Kmeans-Loss: 12.7767\n",
      "lr: 0.0005\n",
      "Epoch: 47\n",
      "Train: \n",
      "Epoch: 47.  loss: 3908.5986\n",
      "Average Train loss: 3853.3818, MSE-Loss: 1917.8000, MSE-Future-Loss 1870.9924, KL-Loss: 48.5560,  Kmeans-Loss: 16.0334, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 443.6932, MSE-Loss: 386.6742, KL-Loss: 44.1426, Kmeans-Loss: 12.8764\n",
      "lr: 0.0005\n",
      "Epoch: 48\n",
      "Train: \n",
      "Epoch: 48.  loss: 3628.5781\n"
     ]
    }
   ],
   "source": [
    "vame.rnn_model(config, model_name='VAME', pretrained_weights=False, pretrained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
