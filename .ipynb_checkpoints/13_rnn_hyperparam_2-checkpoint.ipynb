{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as n\n",
    "import numpy.lib.recfunctions as rfn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, split = 0.8, separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = int(data_len // chunk_size)\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = split*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        \n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    \n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, targets_train, targets_test\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_separate,\\\n",
    "                targets_train, targets_test, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, architecture):\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.D_in = self.architecture['D_in']\n",
    "        self.D_out = self.architecture['D_out']\n",
    "        self.hidden_dims = self.architecture['hidden_dims']\n",
    "        \n",
    "        self.layers = []\n",
    "        prev_dim = self.architecture['D_in']\n",
    "        for dim in self.architecture['hidden_dims']:\n",
    "            self.layers += [torch.nn.Linear(prev_dim, dim),\n",
    "                           torch.nn.ReLU()]\n",
    "            prev_dim = dim\n",
    "        self.layers += [torch.nn.Linear(prev_dim, self.D_out)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.trackers = {}\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.model.apply(init_weights)\n",
    "   \n",
    "    def start_trackers(self,track, reset_trackers=True):\n",
    "        if reset_trackers: self.trackers = {}\n",
    "        for t in track:\n",
    "            self.trackers[t] = []\n",
    "    \n",
    "    def track(self):\n",
    "        for variable in self.trackers.keys():\n",
    "            self.trackers[variable].append(copy.deepcopy(getattr(self, variable)))\n",
    "        \n",
    "    def learn(self,learning, training_set, test_set, reset_trackers=True, verbose=True):\n",
    "        \n",
    "        #self.init_weights()\n",
    "        \n",
    "        # set up variables\n",
    "        N_batch = learning['N_batch']\n",
    "        N_epochs = learning['N_epochs']\n",
    "        loss_fn = learning['loss_fn']\n",
    "        print_interval = learning['print_interval']\n",
    "        track = learning['track']\n",
    "        learning_rate = learning['learning_rate']\n",
    "        \n",
    "        self.learning = learning\n",
    "        \n",
    "        optimizer = learning['optimizer'](self.model.parameters(), learning_rate)\n",
    "        self.start_trackers(track, reset_trackers)\n",
    "        \n",
    "        # load data\n",
    "        x_train, y_train = training_set\n",
    "        x_test, y_test = test_set\n",
    "\n",
    "        N_training = len(y_train)\n",
    "        N_test = len(y_test)\n",
    "    \n",
    "        self.t = 0\n",
    "        tic = time.time()\n",
    "        end = False\n",
    "        for self.epoch_idx in range(N_epochs):\n",
    "            if verbose: print(\"### EPOCH {:2d} ###\".format(self.epoch_idx))\n",
    "                \n",
    "            # randomize batches\n",
    "            indices = n.random.choice(range(N_training), N_training, False)\n",
    "            num_batches = len(indices) // N_batch + 1\n",
    "\n",
    "            for self.batch_idx in range(num_batches):\n",
    "                # load batch\n",
    "                b_idx = self.batch_idx\n",
    "                x_train_batch = x_train[indices[b_idx*N_batch :(b_idx+1)*N_batch]]\n",
    "                y_train_batch = y_train[indices[b_idx*N_batch : (b_idx+1)*N_batch]]\n",
    "\n",
    "                \n",
    "                # predict, loss and learn\n",
    "                y_train_batch_pred = self.model(x_train_batch)\n",
    "                loss = loss_fn(y_train_batch_pred, y_train_batch)\n",
    "                self.train_loss = loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.t += 1\n",
    "\n",
    "                if self.batch_idx % print_interval == 0:\n",
    "                    \n",
    "#                     y_train_pred = self.model(x_train)\n",
    "#                     self.train_loss = loss_fn(y_train_pred, y_train).item()\n",
    "#                     pred_labels = n.argmax(y_train_pred.detach().numpy(),axis=1)\n",
    "#                     true_labels = y_train.detach().numpy()\n",
    "#                     correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.train_frac_correct = 0# n.mean(correct_preds)\n",
    "                    \n",
    "                    y_test_pred = self.model(x_test)\n",
    "                    self.test_loss = loss_fn(y_test_pred, y_test).item()\n",
    "                    pred_labels = n.argmax(y_test_pred.detach().numpy(),axis=1)\n",
    "                    true_labels = y_test.detach().numpy()\n",
    "                    correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.test_frac_correct = n.mean(correct_preds)\n",
    "\n",
    "                    toc = time.time()\n",
    "                    delta = toc - tic\n",
    "                    tic = toc\n",
    "                    print(\"Time: {:4.2f}, Batch {:3d}, Train Loss (for batch): {:4.2f}, Test Loss: {:4.2f}, Test Correct Frac: {:.3f}\".format(\\\n",
    "                                       delta, self.batch_idx, self.train_loss, self.test_loss, self.test_frac_correct))\n",
    "            \n",
    "                    self.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_net_results(net, x_test, y_test):\n",
    "    plt.plot(net.trackers['t'],net.trackers['train_loss'], label='train')\n",
    "    plt.plot(net.trackers['t'],net.trackers['test_loss'], label='test')\n",
    "    plt.title(\"Cross Entropy Loss through training\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,10)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(net.trackers['t'], net.trackers['test_frac_correct'])\n",
    "    plt.title(\"Fraction of correct labels on test set through training\")\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.show()\n",
    "    \n",
    "    max_perf_ind = n.argmax(net.trackers['test_frac_correct'])\n",
    "    min_loss_ind = n.argmin(net.trackers['test_loss'])\n",
    "    max_perf_model = net.trackers['model'][max_perf_ind]\n",
    "    \n",
    "    prediction_test = max_perf_model(x_test)\n",
    "    pred = n.argmax(prediction_test.detach().numpy(),axis=1)\n",
    "    true = y_test.detach().numpy()\n",
    "    confmat = confusion_matrix(true, pred, normalize='true')\n",
    "    f, ax = plt.subplots(figsize=(10,10))\n",
    "    m = ax.matshow(confmat, cmap='Blues', vmin=0,  vmax=1)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_xticks(list(range(len(categories))))\n",
    "    ax.set_xticklabels(categories, rotation=45)\n",
    "    ax.set_yticks(list(range(len(categories))))\n",
    "    ax.set_yticklabels(categories, rotation=45)\n",
    "    f.colorbar(m)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,output_dim, n_layers_LSTM, dropout_prob):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers_LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, \n",
    "                            num_layers  = n_layers_LSTM, dropout = dropout_prob,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.out_func = nn.Softmax(dim=1)\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "#         x = x.long()\n",
    "#         print(x.shape)\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(x)\n",
    "        else: \n",
    "            lstm_out, hidden = self.lstm(x,hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(batch_size, self.output_dim, -1)[:,:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(datas, n_window, n_stride, test_vids=[3,4],batch_size=32, shuffle=True):\n",
    "    datapoints = []\n",
    "    labels = []\n",
    "\n",
    "    trial_bounds = []\n",
    "\n",
    "    trial_idx = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for data in datas:  \n",
    "        trial_bounds.append(idx)\n",
    "        for i in range(int(data.shape[1]/n_stride)):\n",
    "            if (i*n_stride + n_window) < data.shape[1]:\n",
    "                datapoints.append(data[:,i*n_stride : i*n_stride + n_window])\n",
    "                new_labels = [all_data[trial_keys[trial_idx]]['ann'][i*n_stride + int(n_window/2)]]\n",
    "                new_labels = [target_map[l] for l in new_labels]\n",
    "                labels.append(new_labels)\n",
    "                idx += 1\n",
    "        trial_idx += 1\n",
    "    trial_bounds.append(idx)  \n",
    "    \n",
    "    Xs = n.stack(datapoints)\n",
    "    Ys = n.stack(labels)\n",
    "    num_features = Xs.shape[1]\n",
    "    \n",
    "    Xs_train = []; Ys_train = []; Xs_test = []; Ys_test = []\n",
    "    for i in range(len(trial_bounds)-1):\n",
    "        if i in test_vids:\n",
    "            Xs_test.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_test.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "        else:\n",
    "            Xs_train.append(Xs[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "            Ys_train.append(Ys[trial_bounds[i]:trial_bounds[i+1]-1])\n",
    "#     print(Xs_train[0].shape)\n",
    "    Xs_train = n.concatenate(Xs_train,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_train = n.concatenate(Ys_train,axis=0).reshape(-1)\n",
    "    Xs_test = n.concatenate(Xs_test,axis=0).reshape(-1, n_window, num_features)\n",
    "    Ys_test = n.concatenate(Ys_test,axis=0).reshape(-1)\n",
    "    \n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_train), torch.from_numpy(Ys_train))\n",
    "    test_data = torch.utils.data.TensorDataset(torch.from_numpy(Xs_test), torch.from_numpy(Ys_test))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, shuffle=shuffle, batch_size=batch_size)\n",
    "    \n",
    "    return train_data, train_loader, test_data, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(params, datas, device, random_val):\n",
    "    train_data, train_loader, val_data, val_loader \\\n",
    "            = extract_data(datas, params['n_windows'], params['n_stride'],\n",
    "                        batch_size=params['batch_size'],shuffle=params['shuffle'])\n",
    "        \n",
    "    model = LSTM_net(params['input_dim'], params['hidden_dim'], params['output_dim'], params['n_layers_LSTM'], params['dropout_prob'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    params['val_losses'] = []\n",
    "    params['val_fracs'] = []\n",
    "    \n",
    "    counter = 0 \n",
    "    tic = time.time()\n",
    "    for i in range(params['N_epochs']):\n",
    "        h = model.init_hidden(params['batch_size'])\n",
    "        for inputs, labels in train_loader:\n",
    "            if inputs.shape[0] != params['batch_size']: continue\n",
    "            counter += 1\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs.float(), (h[0].float(), h[1].float()))\n",
    "            loss = criterion(output.squeeze(), labels[:].long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), params['clip'])\n",
    "            optimizer.step()\n",
    "                        \n",
    "        toc = time.time() - tic\n",
    "        tic = time.time()\n",
    "        \n",
    "        if counter%params['print_every'] == 0:\n",
    "            val_h = model.init_hidden(params['batch_size'])\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            fracs = []\n",
    "            for inp, lab in val_loader:\n",
    "                if inp.shape[0] != params['batch_size']: continue\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp.float(), [val_h[0].float(), val_h[1].float()])\n",
    "                val_loss = criterion(out.squeeze(), lab[:].long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred_labels = n.argmax(out.cpu().detach().numpy(),axis=1)\n",
    "                true_labels = lab.cpu().detach().numpy()\n",
    "                correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                fracs.append(n.mean(correct_preds))\n",
    "                \n",
    "            frac_correct_val = (n.mean(fracs))\n",
    "            params['val_losses'].append(n.mean(val_losses))\n",
    "            params['val_fracs'].append(frac_correct_val)\n",
    "            model.train()\n",
    "            print(\"\\tEpoch: {}/{}; {:.2f} sec...\".format(i+1, params['N_epochs'],toc),\n",
    "                  \"\\tLoss: {:.3f}...\".format(loss.item()),\n",
    "                  \"\\tVal Loss: {:.3f}\".format(n.mean(val_losses)),\n",
    "                  \"\\tVal Frac: {:.4f}\".format(frac_correct_val))\n",
    "            if n.mean(val_losses) <= params['valid_loss_min']:\n",
    "                torch.save(model.state_dict(), './state_dict' + str(random_val) + '.pt')\n",
    "                print('\\tValidation loss decreased ({:.3f} --> {:.3f}).  Saving model ...'.format(params['valid_loss_min'],n.mean(val_losses)))\n",
    "                params['valid_loss_min'] = n.mean(val_losses)\n",
    "            if frac_correct_val >= params['valid_frac_max']:\n",
    "                params['valid_frac_max'] = frac_correct_val\n",
    "                params['pred_labels_best'] = pred_labels.copy()\n",
    "                params['true_labels_best'] = true_labels.copy()\n",
    "\n",
    "                \n",
    "    return params, model\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n",
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = list(all_data.keys())\n",
    "datas = []\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)\n",
    "input_dim = datas[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_original = {\n",
    "    'n_windows'      : 50,\n",
    "    'n_stride'       : 20,\n",
    "    'shuffle'        : True,\n",
    "    'dropout_prob'   : 0.3,\n",
    "    'batch_size'     : 32,\n",
    "    'hidden_dim'     : 50,\n",
    "    'n_layers_LSTM'  : 1, \n",
    "    'output_dim'     : N_categories,\n",
    "    'input_dim'      : input_dim,\n",
    "    'lr'             : 0.005,\n",
    "    'N_epochs'       : 1000,\n",
    "    'print_every'    : 5,\n",
    "    'clip'           : 5,\n",
    "    'valid_loss_min' : n.Inf,\n",
    "    'valid_frac_max' : 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweeps = {\n",
    "#     'n_stride'        : [5,50],\n",
    "#     'dropout_prob'    : [0, 0.1, 0.5],\n",
    "#     'batch_size'      : [8, 64, 128],\n",
    "    'hidden_dim'      : [10, 20, 100],\n",
    "    'n_layers_LSTM'   : [2, 3, 5],\n",
    "    'lr'              : [0.001, 0.01],\n",
    "    'n_windows'       : [1, 20, 50,100, 250],\n",
    "}\n",
    "# add shuffle\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###########\n",
      "Testing with  hidden_dim :  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neuropixel\\.conda\\envs\\en\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 5/1000; 1.50 sec... \tStep: 1215... \tLoss: 1.600... \tVal Loss: 1.037 \tVal Frac: 0.7426\n",
      "\tValidation loss decreased (inf --> 1.037).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.39 sec... \tStep: 2430... \tLoss: 1.135... \tVal Loss: 0.987 \tVal Frac: 0.7268\n",
      "\tValidation loss decreased (1.037 --> 0.987).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.52 sec... \tStep: 3645... \tLoss: 0.879... \tVal Loss: 0.819 \tVal Frac: 0.7515\n",
      "\tValidation loss decreased (0.987 --> 0.819).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.40 sec... \tStep: 4860... \tLoss: 1.326... \tVal Loss: 1.235 \tVal Frac: 0.7096\n",
      "\tEpoch: 25/1000; 1.54 sec... \tStep: 6075... \tLoss: 1.077... \tVal Loss: 0.841 \tVal Frac: 0.7386\n",
      "\tEpoch: 30/1000; 1.47 sec... \tStep: 7290... \tLoss: 1.358... \tVal Loss: 1.212 \tVal Frac: 0.7228\n",
      "\tEpoch: 35/1000; 1.70 sec... \tStep: 8505... \tLoss: 1.428... \tVal Loss: 0.904 \tVal Frac: 0.7121\n",
      "\tEpoch: 40/1000; 1.54 sec... \tStep: 9720... \tLoss: 0.861... \tVal Loss: 0.770 \tVal Frac: 0.7390\n",
      "\tValidation loss decreased (0.819 --> 0.770).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.50 sec... \tStep: 10935... \tLoss: 1.156... \tVal Loss: 1.025 \tVal Frac: 0.7092\n",
      "\tEpoch: 50/1000; 1.56 sec... \tStep: 12150... \tLoss: 0.890... \tVal Loss: 1.077 \tVal Frac: 0.7294\n",
      "\tEpoch: 55/1000; 1.50 sec... \tStep: 13365... \tLoss: 1.332... \tVal Loss: 0.924 \tVal Frac: 0.7004\n",
      "\tEpoch: 60/1000; 1.49 sec... \tStep: 14580... \tLoss: 1.480... \tVal Loss: 0.949 \tVal Frac: 0.7371\n",
      "\tEpoch: 65/1000; 1.55 sec... \tStep: 15795... \tLoss: 0.866... \tVal Loss: 0.809 \tVal Frac: 0.7761\n",
      "\tEpoch: 70/1000; 1.82 sec... \tStep: 17010... \tLoss: 0.756... \tVal Loss: 1.170 \tVal Frac: 0.4243\n",
      "\tEpoch: 75/1000; 1.86 sec... \tStep: 18225... \tLoss: 1.573... \tVal Loss: 1.481 \tVal Frac: 0.3426\n",
      "\tEpoch: 80/1000; 1.62 sec... \tStep: 19440... \tLoss: 0.916... \tVal Loss: 0.830 \tVal Frac: 0.7243\n",
      "\tEpoch: 85/1000; 1.57 sec... \tStep: 20655... \tLoss: 1.255... \tVal Loss: 0.974 \tVal Frac: 0.6757\n",
      "\tEpoch: 90/1000; 1.37 sec... \tStep: 21870... \tLoss: 0.827... \tVal Loss: 0.782 \tVal Frac: 0.7901\n",
      "\tEpoch: 95/1000; 1.50 sec... \tStep: 23085... \tLoss: 0.887... \tVal Loss: 0.744 \tVal Frac: 0.7816\n",
      "\tValidation loss decreased (0.770 --> 0.744).  Saving model ...\n",
      "\tEpoch: 100/1000; 1.62 sec... \tStep: 24300... \tLoss: 1.163... \tVal Loss: 0.977 \tVal Frac: 0.7290\n",
      "\tEpoch: 105/1000; 1.57 sec... \tStep: 25515... \tLoss: 0.857... \tVal Loss: 0.777 \tVal Frac: 0.7581\n",
      "\tEpoch: 110/1000; 1.64 sec... \tStep: 26730... \tLoss: 0.835... \tVal Loss: 0.920 \tVal Frac: 0.7287\n",
      "\tEpoch: 115/1000; 1.48 sec... \tStep: 27945... \tLoss: 1.324... \tVal Loss: 1.027 \tVal Frac: 0.7026\n",
      "\tEpoch: 120/1000; 1.53 sec... \tStep: 29160... \tLoss: 1.085... \tVal Loss: 0.847 \tVal Frac: 0.7401\n",
      "\tEpoch: 125/1000; 1.48 sec... \tStep: 30375... \tLoss: 0.910... \tVal Loss: 0.734 \tVal Frac: 0.7757\n",
      "\tValidation loss decreased (0.744 --> 0.734).  Saving model ...\n",
      "\tEpoch: 130/1000; 1.49 sec... \tStep: 31590... \tLoss: 0.993... \tVal Loss: 0.732 \tVal Frac: 0.7551\n",
      "\tValidation loss decreased (0.734 --> 0.732).  Saving model ...\n",
      "\tEpoch: 135/1000; 1.62 sec... \tStep: 32805... \tLoss: 0.962... \tVal Loss: 0.847 \tVal Frac: 0.7375\n",
      "\tEpoch: 140/1000; 1.63 sec... \tStep: 34020... \tLoss: 0.956... \tVal Loss: 0.759 \tVal Frac: 0.7540\n",
      "\tEpoch: 145/1000; 1.50 sec... \tStep: 35235... \tLoss: 0.811... \tVal Loss: 0.821 \tVal Frac: 0.7522\n",
      "\tEpoch: 150/1000; 1.43 sec... \tStep: 36450... \tLoss: 0.745... \tVal Loss: 0.787 \tVal Frac: 0.7471\n",
      "\tEpoch: 155/1000; 1.30 sec... \tStep: 37665... \tLoss: 1.163... \tVal Loss: 0.918 \tVal Frac: 0.7445\n",
      "\tEpoch: 160/1000; 1.63 sec... \tStep: 38880... \tLoss: 1.159... \tVal Loss: 0.716 \tVal Frac: 0.7772\n",
      "\tValidation loss decreased (0.732 --> 0.716).  Saving model ...\n",
      "\tEpoch: 165/1000; 1.56 sec... \tStep: 40095... \tLoss: 0.565... \tVal Loss: 0.890 \tVal Frac: 0.7118\n",
      "\tEpoch: 170/1000; 1.52 sec... \tStep: 41310... \tLoss: 0.835... \tVal Loss: 0.789 \tVal Frac: 0.7544\n",
      "\tEpoch: 175/1000; 1.50 sec... \tStep: 42525... \tLoss: 1.082... \tVal Loss: 0.843 \tVal Frac: 0.7375\n",
      "\tEpoch: 180/1000; 1.52 sec... \tStep: 43740... \tLoss: 0.948... \tVal Loss: 0.738 \tVal Frac: 0.7820\n",
      "\tEpoch: 185/1000; 1.44 sec... \tStep: 44955... \tLoss: 0.779... \tVal Loss: 0.741 \tVal Frac: 0.7717\n",
      "\tEpoch: 190/1000; 1.37 sec... \tStep: 46170... \tLoss: 0.925... \tVal Loss: 0.835 \tVal Frac: 0.7555\n",
      "\tEpoch: 195/1000; 1.64 sec... \tStep: 47385... \tLoss: 1.341... \tVal Loss: 1.219 \tVal Frac: 0.4882\n",
      "\tEpoch: 200/1000; 1.74 sec... \tStep: 48600... \tLoss: 0.742... \tVal Loss: 0.741 \tVal Frac: 0.7522\n",
      "\tEpoch: 205/1000; 1.50 sec... \tStep: 49815... \tLoss: 0.882... \tVal Loss: 0.720 \tVal Frac: 0.7838\n",
      "\tEpoch: 210/1000; 1.57 sec... \tStep: 51030... \tLoss: 1.076... \tVal Loss: 0.732 \tVal Frac: 0.7618\n",
      "\tEpoch: 215/1000; 1.60 sec... \tStep: 52245... \tLoss: 1.007... \tVal Loss: 0.746 \tVal Frac: 0.7419\n",
      "\tEpoch: 220/1000; 1.44 sec... \tStep: 53460... \tLoss: 0.738... \tVal Loss: 0.622 \tVal Frac: 0.8018\n",
      "\tValidation loss decreased (0.716 --> 0.622).  Saving model ...\n",
      "\tEpoch: 225/1000; 1.57 sec... \tStep: 54675... \tLoss: 0.893... \tVal Loss: 0.840 \tVal Frac: 0.7482\n",
      "\tEpoch: 230/1000; 1.60 sec... \tStep: 55890... \tLoss: 1.376... \tVal Loss: 0.844 \tVal Frac: 0.7544\n",
      "\tEpoch: 235/1000; 1.60 sec... \tStep: 57105... \tLoss: 1.150... \tVal Loss: 0.836 \tVal Frac: 0.7812\n",
      "\tEpoch: 240/1000; 1.47 sec... \tStep: 58320... \tLoss: 1.168... \tVal Loss: 0.847 \tVal Frac: 0.7835\n",
      "\tEpoch: 245/1000; 1.70 sec... \tStep: 59535... \tLoss: 1.016... \tVal Loss: 0.714 \tVal Frac: 0.7886\n",
      "\tEpoch: 250/1000; 1.49 sec... \tStep: 60750... \tLoss: 1.411... \tVal Loss: 0.743 \tVal Frac: 0.7864\n",
      "\tEpoch: 255/1000; 1.47 sec... \tStep: 61965... \tLoss: 0.957... \tVal Loss: 0.865 \tVal Frac: 0.7434\n",
      "\tEpoch: 260/1000; 1.56 sec... \tStep: 63180... \tLoss: 1.431... \tVal Loss: 0.700 \tVal Frac: 0.7842\n",
      "\tEpoch: 265/1000; 1.37 sec... \tStep: 64395... \tLoss: 0.702... \tVal Loss: 0.716 \tVal Frac: 0.7805\n",
      "\tEpoch: 270/1000; 1.52 sec... \tStep: 65610... \tLoss: 0.901... \tVal Loss: 0.744 \tVal Frac: 0.7879\n",
      "\tEpoch: 275/1000; 1.39 sec... \tStep: 66825... \tLoss: 0.999... \tVal Loss: 0.710 \tVal Frac: 0.7904\n",
      "\tEpoch: 280/1000; 1.45 sec... \tStep: 68040... \tLoss: 1.096... \tVal Loss: 0.816 \tVal Frac: 0.7489\n",
      "\tEpoch: 285/1000; 1.43 sec... \tStep: 69255... \tLoss: 0.582... \tVal Loss: 0.710 \tVal Frac: 0.7871\n",
      "\tEpoch: 290/1000; 1.60 sec... \tStep: 70470... \tLoss: 0.955... \tVal Loss: 0.822 \tVal Frac: 0.7647\n",
      "\tEpoch: 295/1000; 1.49 sec... \tStep: 71685... \tLoss: 0.708... \tVal Loss: 0.846 \tVal Frac: 0.7449\n",
      "\tEpoch: 300/1000; 1.45 sec... \tStep: 72900... \tLoss: 1.102... \tVal Loss: 0.727 \tVal Frac: 0.7787\n",
      "\tEpoch: 305/1000; 1.67 sec... \tStep: 74115... \tLoss: 1.006... \tVal Loss: 0.758 \tVal Frac: 0.7835\n",
      "\tEpoch: 310/1000; 1.53 sec... \tStep: 75330... \tLoss: 0.998... \tVal Loss: 0.715 \tVal Frac: 0.7463\n",
      "\tEpoch: 315/1000; 1.45 sec... \tStep: 76545... \tLoss: 1.036... \tVal Loss: 0.682 \tVal Frac: 0.7743\n",
      "\tEpoch: 320/1000; 1.59 sec... \tStep: 77760... \tLoss: 0.794... \tVal Loss: 0.716 \tVal Frac: 0.7629\n",
      "\tEpoch: 325/1000; 1.57 sec... \tStep: 78975... \tLoss: 1.044... \tVal Loss: 0.797 \tVal Frac: 0.7489\n",
      "\tEpoch: 330/1000; 1.63 sec... \tStep: 80190... \tLoss: 0.738... \tVal Loss: 0.744 \tVal Frac: 0.7526\n",
      "\tEpoch: 335/1000; 1.53 sec... \tStep: 81405... \tLoss: 0.866... \tVal Loss: 0.690 \tVal Frac: 0.7721\n",
      "\tEpoch: 340/1000; 1.65 sec... \tStep: 82620... \tLoss: 1.327... \tVal Loss: 0.839 \tVal Frac: 0.7735\n",
      "\tEpoch: 345/1000; 1.42 sec... \tStep: 83835... \tLoss: 1.041... \tVal Loss: 0.727 \tVal Frac: 0.7724\n",
      "\tEpoch: 350/1000; 1.57 sec... \tStep: 85050... \tLoss: 1.111... \tVal Loss: 0.790 \tVal Frac: 0.7173\n",
      "\tEpoch: 355/1000; 1.51 sec... \tStep: 86265... \tLoss: 0.788... \tVal Loss: 0.787 \tVal Frac: 0.7434\n",
      "\tEpoch: 360/1000; 1.53 sec... \tStep: 87480... \tLoss: 0.858... \tVal Loss: 0.848 \tVal Frac: 0.7482\n",
      "\tEpoch: 365/1000; 1.63 sec... \tStep: 88695... \tLoss: 1.126... \tVal Loss: 0.918 \tVal Frac: 0.7496\n",
      "\tEpoch: 370/1000; 1.66 sec... \tStep: 89910... \tLoss: 0.972... \tVal Loss: 0.857 \tVal Frac: 0.7485\n",
      "\tEpoch: 375/1000; 1.64 sec... \tStep: 91125... \tLoss: 0.832... \tVal Loss: 0.717 \tVal Frac: 0.7849\n",
      "\tEpoch: 380/1000; 1.50 sec... \tStep: 92340... \tLoss: 0.351... \tVal Loss: 0.735 \tVal Frac: 0.7625\n",
      "\tEpoch: 385/1000; 1.48 sec... \tStep: 93555... \tLoss: 1.011... \tVal Loss: 1.034 \tVal Frac: 0.7158\n",
      "\tEpoch: 390/1000; 1.56 sec... \tStep: 94770... \tLoss: 1.069... \tVal Loss: 0.739 \tVal Frac: 0.7629\n",
      "\tEpoch: 395/1000; 1.42 sec... \tStep: 95985... \tLoss: 1.303... \tVal Loss: 0.928 \tVal Frac: 0.7551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 400/1000; 1.50 sec... \tStep: 97200... \tLoss: 0.907... \tVal Loss: 0.718 \tVal Frac: 0.7669\n",
      "\tEpoch: 405/1000; 1.57 sec... \tStep: 98415... \tLoss: 0.749... \tVal Loss: 0.710 \tVal Frac: 0.7507\n",
      "\tEpoch: 410/1000; 1.42 sec... \tStep: 99630... \tLoss: 1.492... \tVal Loss: 0.747 \tVal Frac: 0.7456\n",
      "\tEpoch: 415/1000; 1.46 sec... \tStep: 100845... \tLoss: 0.908... \tVal Loss: 0.689 \tVal Frac: 0.7853\n",
      "\tEpoch: 420/1000; 1.53 sec... \tStep: 102060... \tLoss: 0.854... \tVal Loss: 0.718 \tVal Frac: 0.7665\n",
      "\tEpoch: 425/1000; 1.52 sec... \tStep: 103275... \tLoss: 0.666... \tVal Loss: 0.712 \tVal Frac: 0.7835\n",
      "\tEpoch: 430/1000; 1.50 sec... \tStep: 104490... \tLoss: 1.398... \tVal Loss: 0.789 \tVal Frac: 0.7471\n",
      "\tEpoch: 435/1000; 1.55 sec... \tStep: 105705... \tLoss: 1.179... \tVal Loss: 0.738 \tVal Frac: 0.7743\n",
      "\tEpoch: 440/1000; 1.60 sec... \tStep: 106920... \tLoss: 1.161... \tVal Loss: 0.688 \tVal Frac: 0.7871\n",
      "\tEpoch: 445/1000; 1.52 sec... \tStep: 108135... \tLoss: 0.967... \tVal Loss: 0.704 \tVal Frac: 0.7761\n",
      "\tEpoch: 450/1000; 1.55 sec... \tStep: 109350... \tLoss: 1.050... \tVal Loss: 0.761 \tVal Frac: 0.7658\n",
      "\tEpoch: 455/1000; 1.52 sec... \tStep: 110565... \tLoss: 0.601... \tVal Loss: 0.671 \tVal Frac: 0.7864\n",
      "\tEpoch: 460/1000; 1.59 sec... \tStep: 111780... \tLoss: 0.740... \tVal Loss: 0.711 \tVal Frac: 0.7945\n",
      "\tEpoch: 465/1000; 1.57 sec... \tStep: 112995... \tLoss: 0.910... \tVal Loss: 0.687 \tVal Frac: 0.7864\n",
      "\tEpoch: 470/1000; 1.37 sec... \tStep: 114210... \tLoss: 0.816... \tVal Loss: 0.685 \tVal Frac: 0.7930\n",
      "\tEpoch: 475/1000; 1.73 sec... \tStep: 115425... \tLoss: 0.890... \tVal Loss: 0.698 \tVal Frac: 0.7949\n",
      "\tEpoch: 480/1000; 1.57 sec... \tStep: 116640... \tLoss: 0.804... \tVal Loss: 0.740 \tVal Frac: 0.7816\n",
      "\tEpoch: 485/1000; 1.52 sec... \tStep: 117855... \tLoss: 1.643... \tVal Loss: 0.826 \tVal Frac: 0.7897\n",
      "\tEpoch: 490/1000; 1.52 sec... \tStep: 119070... \tLoss: 0.816... \tVal Loss: 0.794 \tVal Frac: 0.7401\n",
      "\tEpoch: 495/1000; 1.67 sec... \tStep: 120285... \tLoss: 0.620... \tVal Loss: 0.696 \tVal Frac: 0.7857\n",
      "\tEpoch: 500/1000; 1.52 sec... \tStep: 121500... \tLoss: 0.742... \tVal Loss: 0.735 \tVal Frac: 0.7890\n",
      "\tEpoch: 505/1000; 1.46 sec... \tStep: 122715... \tLoss: 0.996... \tVal Loss: 0.732 \tVal Frac: 0.7783\n",
      "\tEpoch: 510/1000; 1.43 sec... \tStep: 123930... \tLoss: 0.811... \tVal Loss: 0.771 \tVal Frac: 0.7290\n",
      "\tEpoch: 515/1000; 1.62 sec... \tStep: 125145... \tLoss: 1.059... \tVal Loss: 0.670 \tVal Frac: 0.7640\n",
      "\tEpoch: 520/1000; 1.55 sec... \tStep: 126360... \tLoss: 0.595... \tVal Loss: 0.684 \tVal Frac: 0.7643\n",
      "\tEpoch: 525/1000; 1.70 sec... \tStep: 127575... \tLoss: 0.987... \tVal Loss: 0.796 \tVal Frac: 0.7798\n",
      "\tEpoch: 530/1000; 1.42 sec... \tStep: 128790... \tLoss: 0.531... \tVal Loss: 0.642 \tVal Frac: 0.7989\n",
      "\tEpoch: 535/1000; 1.42 sec... \tStep: 130005... \tLoss: 0.924... \tVal Loss: 0.762 \tVal Frac: 0.7537\n",
      "\tEpoch: 540/1000; 1.49 sec... \tStep: 131220... \tLoss: 1.772... \tVal Loss: 0.961 \tVal Frac: 0.6945\n",
      "\tEpoch: 545/1000; 1.47 sec... \tStep: 132435... \tLoss: 0.995... \tVal Loss: 0.764 \tVal Frac: 0.7919\n",
      "\tEpoch: 550/1000; 1.53 sec... \tStep: 133650... \tLoss: 0.934... \tVal Loss: 0.685 \tVal Frac: 0.7926\n",
      "\tEpoch: 555/1000; 1.53 sec... \tStep: 134865... \tLoss: 0.798... \tVal Loss: 0.697 \tVal Frac: 0.7915\n",
      "\tEpoch: 560/1000; 1.52 sec... \tStep: 136080... \tLoss: 0.715... \tVal Loss: 0.688 \tVal Frac: 0.7886\n",
      "\tEpoch: 565/1000; 1.67 sec... \tStep: 137295... \tLoss: 1.088... \tVal Loss: 0.711 \tVal Frac: 0.7599\n",
      "\tEpoch: 570/1000; 1.53 sec... \tStep: 138510... \tLoss: 1.029... \tVal Loss: 0.690 \tVal Frac: 0.7842\n",
      "\tEpoch: 575/1000; 1.60 sec... \tStep: 139725... \tLoss: 0.790... \tVal Loss: 0.680 \tVal Frac: 0.7926\n",
      "\tEpoch: 580/1000; 1.43 sec... \tStep: 140940... \tLoss: 1.140... \tVal Loss: 0.764 \tVal Frac: 0.7408\n",
      "\tEpoch: 585/1000; 1.50 sec... \tStep: 142155... \tLoss: 1.172... \tVal Loss: 0.667 \tVal Frac: 0.7982\n",
      "\tEpoch: 590/1000; 1.62 sec... \tStep: 143370... \tLoss: 0.733... \tVal Loss: 0.714 \tVal Frac: 0.7577\n",
      "\tEpoch: 595/1000; 1.77 sec... \tStep: 144585... \tLoss: 0.934... \tVal Loss: 0.765 \tVal Frac: 0.7500\n",
      "\tEpoch: 600/1000; 1.42 sec... \tStep: 145800... \tLoss: 1.179... \tVal Loss: 0.895 \tVal Frac: 0.6765\n",
      "\tEpoch: 605/1000; 1.66 sec... \tStep: 147015... \tLoss: 1.123... \tVal Loss: 0.680 \tVal Frac: 0.7790\n",
      "\tEpoch: 610/1000; 1.85 sec... \tStep: 148230... \tLoss: 0.828... \tVal Loss: 0.718 \tVal Frac: 0.7485\n",
      "\tEpoch: 615/1000; 1.62 sec... \tStep: 149445... \tLoss: 0.981... \tVal Loss: 0.731 \tVal Frac: 0.7460\n",
      "\tEpoch: 620/1000; 1.50 sec... \tStep: 150660... \tLoss: 0.879... \tVal Loss: 0.725 \tVal Frac: 0.7746\n",
      "\tEpoch: 625/1000; 1.55 sec... \tStep: 151875... \tLoss: 0.704... \tVal Loss: 0.655 \tVal Frac: 0.7937\n",
      "\tEpoch: 630/1000; 1.43 sec... \tStep: 153090... \tLoss: 0.629... \tVal Loss: 0.673 \tVal Frac: 0.7890\n",
      "\tEpoch: 635/1000; 1.63 sec... \tStep: 154305... \tLoss: 0.749... \tVal Loss: 0.652 \tVal Frac: 0.7831\n",
      "\tEpoch: 640/1000; 1.60 sec... \tStep: 155520... \tLoss: 0.634... \tVal Loss: 0.706 \tVal Frac: 0.7842\n",
      "\tEpoch: 645/1000; 1.59 sec... \tStep: 156735... \tLoss: 0.975... \tVal Loss: 0.668 \tVal Frac: 0.7897\n",
      "\tEpoch: 650/1000; 1.54 sec... \tStep: 157950... \tLoss: 0.933... \tVal Loss: 0.783 \tVal Frac: 0.7596\n",
      "\tEpoch: 655/1000; 1.58 sec... \tStep: 159165... \tLoss: 1.050... \tVal Loss: 0.660 \tVal Frac: 0.7838\n",
      "\tEpoch: 660/1000; 1.47 sec... \tStep: 160380... \tLoss: 1.015... \tVal Loss: 0.675 \tVal Frac: 0.7489\n",
      "\tEpoch: 665/1000; 1.48 sec... \tStep: 161595... \tLoss: 0.984... \tVal Loss: 0.801 \tVal Frac: 0.7691\n",
      "\tEpoch: 670/1000; 1.45 sec... \tStep: 162810... \tLoss: 1.052... \tVal Loss: 0.890 \tVal Frac: 0.7415\n",
      "\tEpoch: 675/1000; 1.57 sec... \tStep: 164025... \tLoss: 1.223... \tVal Loss: 1.148 \tVal Frac: 0.4077\n",
      "\tEpoch: 680/1000; 1.53 sec... \tStep: 165240... \tLoss: 0.998... \tVal Loss: 0.786 \tVal Frac: 0.7474\n",
      "\tEpoch: 685/1000; 1.45 sec... \tStep: 166455... \tLoss: 0.796... \tVal Loss: 0.682 \tVal Frac: 0.7820\n",
      "\tEpoch: 690/1000; 1.64 sec... \tStep: 167670... \tLoss: 0.909... \tVal Loss: 0.929 \tVal Frac: 0.7500\n",
      "\tEpoch: 695/1000; 1.75 sec... \tStep: 168885... \tLoss: 1.014... \tVal Loss: 0.753 \tVal Frac: 0.7893\n",
      "\tEpoch: 700/1000; 1.86 sec... \tStep: 170100... \tLoss: 0.491... \tVal Loss: 0.822 \tVal Frac: 0.7673\n",
      "\tEpoch: 705/1000; 1.74 sec... \tStep: 171315... \tLoss: 1.078... \tVal Loss: 0.852 \tVal Frac: 0.7636\n",
      "\tEpoch: 710/1000; 1.53 sec... \tStep: 172530... \tLoss: 0.727... \tVal Loss: 0.728 \tVal Frac: 0.7732\n",
      "\tEpoch: 715/1000; 1.44 sec... \tStep: 173745... \tLoss: 1.145... \tVal Loss: 0.767 \tVal Frac: 0.7838\n",
      "\tEpoch: 720/1000; 1.52 sec... \tStep: 174960... \tLoss: 0.965... \tVal Loss: 0.896 \tVal Frac: 0.7254\n",
      "\tEpoch: 725/1000; 1.45 sec... \tStep: 176175... \tLoss: 1.078... \tVal Loss: 0.833 \tVal Frac: 0.7812\n",
      "\tEpoch: 730/1000; 1.49 sec... \tStep: 177390... \tLoss: 0.563... \tVal Loss: 0.757 \tVal Frac: 0.7868\n",
      "\tEpoch: 735/1000; 1.52 sec... \tStep: 178605... \tLoss: 1.047... \tVal Loss: 0.730 \tVal Frac: 0.7949\n",
      "\tEpoch: 740/1000; 1.65 sec... \tStep: 179820... \tLoss: 0.566... \tVal Loss: 0.723 \tVal Frac: 0.7923\n",
      "\tEpoch: 745/1000; 1.60 sec... \tStep: 181035... \tLoss: 0.848... \tVal Loss: 0.767 \tVal Frac: 0.7930\n",
      "\tEpoch: 750/1000; 1.57 sec... \tStep: 182250... \tLoss: 1.167... \tVal Loss: 0.767 \tVal Frac: 0.7618\n",
      "\tEpoch: 755/1000; 1.60 sec... \tStep: 183465... \tLoss: 1.319... \tVal Loss: 0.952 \tVal Frac: 0.7937\n",
      "\tEpoch: 760/1000; 1.40 sec... \tStep: 184680... \tLoss: 0.753... \tVal Loss: 0.660 \tVal Frac: 0.7941\n",
      "\tEpoch: 765/1000; 1.50 sec... \tStep: 185895... \tLoss: 0.557... \tVal Loss: 0.679 \tVal Frac: 0.7901\n",
      "\tEpoch: 770/1000; 1.70 sec... \tStep: 187110... \tLoss: 0.580... \tVal Loss: 0.646 \tVal Frac: 0.7963\n",
      "\tEpoch: 775/1000; 1.47 sec... \tStep: 188325... \tLoss: 0.808... \tVal Loss: 0.643 \tVal Frac: 0.7868\n",
      "\tEpoch: 780/1000; 1.45 sec... \tStep: 189540... \tLoss: 0.822... \tVal Loss: 0.653 \tVal Frac: 0.7923\n",
      "\tEpoch: 785/1000; 1.57 sec... \tStep: 190755... \tLoss: 0.821... \tVal Loss: 0.717 \tVal Frac: 0.7860\n",
      "\tEpoch: 790/1000; 1.61 sec... \tStep: 191970... \tLoss: 1.332... \tVal Loss: 0.772 \tVal Frac: 0.7728\n",
      "\tEpoch: 795/1000; 1.54 sec... \tStep: 193185... \tLoss: 0.636... \tVal Loss: 0.670 \tVal Frac: 0.7548\n",
      "\tEpoch: 800/1000; 1.57 sec... \tStep: 194400... \tLoss: 0.470... \tVal Loss: 0.621 \tVal Frac: 0.7960\n",
      "\tValidation loss decreased (0.622 --> 0.621).  Saving model ...\n",
      "\tEpoch: 805/1000; 1.64 sec... \tStep: 195615... \tLoss: 0.803... \tVal Loss: 0.676 \tVal Frac: 0.7915\n",
      "\tEpoch: 810/1000; 1.67 sec... \tStep: 196830... \tLoss: 0.706... \tVal Loss: 0.677 \tVal Frac: 0.7875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 815/1000; 1.62 sec... \tStep: 198045... \tLoss: 0.793... \tVal Loss: 0.670 \tVal Frac: 0.7820\n",
      "\tEpoch: 820/1000; 1.70 sec... \tStep: 199260... \tLoss: 0.701... \tVal Loss: 0.661 \tVal Frac: 0.7912\n",
      "\tEpoch: 825/1000; 1.45 sec... \tStep: 200475... \tLoss: 0.794... \tVal Loss: 0.646 \tVal Frac: 0.7912\n",
      "\tEpoch: 830/1000; 1.52 sec... \tStep: 201690... \tLoss: 0.533... \tVal Loss: 0.617 \tVal Frac: 0.8000\n",
      "\tValidation loss decreased (0.621 --> 0.617).  Saving model ...\n",
      "\tEpoch: 835/1000; 1.59 sec... \tStep: 202905... \tLoss: 0.958... \tVal Loss: 0.648 \tVal Frac: 0.7974\n",
      "\tEpoch: 840/1000; 1.47 sec... \tStep: 204120... \tLoss: 0.942... \tVal Loss: 0.697 \tVal Frac: 0.7831\n",
      "\tEpoch: 845/1000; 1.37 sec... \tStep: 205335... \tLoss: 0.849... \tVal Loss: 0.725 \tVal Frac: 0.7820\n",
      "\tEpoch: 850/1000; 1.69 sec... \tStep: 206550... \tLoss: 0.930... \tVal Loss: 0.642 \tVal Frac: 0.8004\n",
      "\tEpoch: 855/1000; 1.45 sec... \tStep: 207765... \tLoss: 0.921... \tVal Loss: 0.833 \tVal Frac: 0.7669\n",
      "\tEpoch: 860/1000; 1.43 sec... \tStep: 208980... \tLoss: 0.773... \tVal Loss: 0.732 \tVal Frac: 0.7779\n",
      "\tEpoch: 865/1000; 1.37 sec... \tStep: 210195... \tLoss: 0.625... \tVal Loss: 0.758 \tVal Frac: 0.7699\n",
      "\tEpoch: 870/1000; 1.43 sec... \tStep: 211410... \tLoss: 1.140... \tVal Loss: 0.701 \tVal Frac: 0.7765\n",
      "\tEpoch: 875/1000; 1.37 sec... \tStep: 212625... \tLoss: 0.791... \tVal Loss: 0.678 \tVal Frac: 0.7901\n",
      "\tEpoch: 880/1000; 1.37 sec... \tStep: 213840... \tLoss: 0.624... \tVal Loss: 0.620 \tVal Frac: 0.7963\n",
      "\tEpoch: 885/1000; 1.46 sec... \tStep: 215055... \tLoss: 0.557... \tVal Loss: 0.682 \tVal Frac: 0.7901\n",
      "\tEpoch: 890/1000; 1.40 sec... \tStep: 216270... \tLoss: 0.994... \tVal Loss: 0.713 \tVal Frac: 0.7919\n",
      "\tEpoch: 895/1000; 1.27 sec... \tStep: 217485... \tLoss: 1.261... \tVal Loss: 0.818 \tVal Frac: 0.7327\n",
      "\tEpoch: 900/1000; 1.39 sec... \tStep: 218700... \tLoss: 0.727... \tVal Loss: 0.708 \tVal Frac: 0.7518\n",
      "\tEpoch: 905/1000; 1.29 sec... \tStep: 219915... \tLoss: 0.993... \tVal Loss: 0.730 \tVal Frac: 0.7680\n",
      "\tEpoch: 910/1000; 1.35 sec... \tStep: 221130... \tLoss: 1.009... \tVal Loss: 0.679 \tVal Frac: 0.7912\n",
      "\tEpoch: 915/1000; 1.34 sec... \tStep: 222345... \tLoss: 0.738... \tVal Loss: 0.674 \tVal Frac: 0.7746\n",
      "\tEpoch: 920/1000; 1.30 sec... \tStep: 223560... \tLoss: 0.749... \tVal Loss: 0.698 \tVal Frac: 0.7750\n",
      "\tEpoch: 925/1000; 1.35 sec... \tStep: 224775... \tLoss: 0.535... \tVal Loss: 0.676 \tVal Frac: 0.7699\n",
      "\tEpoch: 930/1000; 1.34 sec... \tStep: 225990... \tLoss: 0.921... \tVal Loss: 0.673 \tVal Frac: 0.7724\n",
      "\tEpoch: 935/1000; 1.42 sec... \tStep: 227205... \tLoss: 0.589... \tVal Loss: 0.703 \tVal Frac: 0.7842\n",
      "\tEpoch: 940/1000; 1.30 sec... \tStep: 228420... \tLoss: 0.869... \tVal Loss: 0.768 \tVal Frac: 0.7574\n",
      "\tEpoch: 945/1000; 1.29 sec... \tStep: 229635... \tLoss: 0.920... \tVal Loss: 0.688 \tVal Frac: 0.7857\n",
      "\tEpoch: 950/1000; 1.50 sec... \tStep: 230850... \tLoss: 0.639... \tVal Loss: 0.680 \tVal Frac: 0.7879\n",
      "\tEpoch: 955/1000; 1.59 sec... \tStep: 232065... \tLoss: 0.888... \tVal Loss: 0.680 \tVal Frac: 0.7901\n",
      "\tEpoch: 960/1000; 1.22 sec... \tStep: 233280... \tLoss: 0.708... \tVal Loss: 0.739 \tVal Frac: 0.7680\n",
      "\tEpoch: 965/1000; 1.38 sec... \tStep: 234495... \tLoss: 0.731... \tVal Loss: 0.696 \tVal Frac: 0.7754\n",
      "\tEpoch: 970/1000; 1.30 sec... \tStep: 235710... \tLoss: 1.444... \tVal Loss: 0.772 \tVal Frac: 0.7798\n",
      "\tEpoch: 975/1000; 1.35 sec... \tStep: 236925... \tLoss: 0.987... \tVal Loss: 0.963 \tVal Frac: 0.7629\n",
      "\tEpoch: 980/1000; 1.34 sec... \tStep: 238140... \tLoss: 0.687... \tVal Loss: 0.666 \tVal Frac: 0.7934\n",
      "\tEpoch: 985/1000; 1.29 sec... \tStep: 239355... \tLoss: 0.932... \tVal Loss: 0.684 \tVal Frac: 0.7728\n",
      "\tEpoch: 990/1000; 1.32 sec... \tStep: 240570... \tLoss: 0.739... \tVal Loss: 0.703 \tVal Frac: 0.7654\n",
      "\tEpoch: 995/1000; 1.52 sec... \tStep: 241785... \tLoss: 0.725... \tVal Loss: 0.691 \tVal Frac: 0.7665\n",
      "\tEpoch: 1000/1000; 1.43 sec... \tStep: 243000... \tLoss: 0.567... \tVal Loss: 0.673 \tVal Frac: 0.7857\n",
      "Completed:  hidden_dim :  10 \n",
      "\tloss: 0.617172 \n",
      "\tfrac:0.801838\n",
      "\n",
      "###########\n",
      "Testing with  hidden_dim :  20\n",
      "\tEpoch: 5/1000; 1.33 sec... \tStep: 1215... \tLoss: 1.281... \tVal Loss: 1.062 \tVal Frac: 0.7346\n",
      "\tValidation loss decreased (inf --> 1.062).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.43 sec... \tStep: 2430... \tLoss: 1.335... \tVal Loss: 0.909 \tVal Frac: 0.7290\n",
      "\tValidation loss decreased (1.062 --> 0.909).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.36 sec... \tStep: 3645... \tLoss: 0.934... \tVal Loss: 1.058 \tVal Frac: 0.7290\n",
      "\tEpoch: 20/1000; 1.46 sec... \tStep: 4860... \tLoss: 0.981... \tVal Loss: 0.719 \tVal Frac: 0.7570\n",
      "\tValidation loss decreased (0.909 --> 0.719).  Saving model ...\n",
      "\tEpoch: 25/1000; 1.42 sec... \tStep: 6075... \tLoss: 0.900... \tVal Loss: 0.902 \tVal Frac: 0.7276\n",
      "\tEpoch: 30/1000; 1.42 sec... \tStep: 7290... \tLoss: 0.762... \tVal Loss: 0.763 \tVal Frac: 0.7772\n",
      "\tEpoch: 35/1000; 1.30 sec... \tStep: 8505... \tLoss: 0.903... \tVal Loss: 0.885 \tVal Frac: 0.7250\n",
      "\tEpoch: 40/1000; 1.26 sec... \tStep: 9720... \tLoss: 0.552... \tVal Loss: 0.771 \tVal Frac: 0.7673\n",
      "\tEpoch: 45/1000; 1.33 sec... \tStep: 10935... \tLoss: 1.388... \tVal Loss: 0.880 \tVal Frac: 0.7544\n",
      "\tEpoch: 50/1000; 1.26 sec... \tStep: 12150... \tLoss: 0.897... \tVal Loss: 0.911 \tVal Frac: 0.7250\n",
      "\tEpoch: 55/1000; 1.37 sec... \tStep: 13365... \tLoss: 0.679... \tVal Loss: 0.713 \tVal Frac: 0.7820\n",
      "\tValidation loss decreased (0.719 --> 0.713).  Saving model ...\n",
      "\tEpoch: 60/1000; 1.39 sec... \tStep: 14580... \tLoss: 0.753... \tVal Loss: 0.709 \tVal Frac: 0.7684\n",
      "\tValidation loss decreased (0.713 --> 0.709).  Saving model ...\n",
      "\tEpoch: 65/1000; 1.40 sec... \tStep: 15795... \tLoss: 1.036... \tVal Loss: 0.687 \tVal Frac: 0.7912\n",
      "\tValidation loss decreased (0.709 --> 0.687).  Saving model ...\n",
      "\tEpoch: 70/1000; 1.23 sec... \tStep: 17010... \tLoss: 1.234... \tVal Loss: 0.745 \tVal Frac: 0.7555\n",
      "\tEpoch: 75/1000; 1.35 sec... \tStep: 18225... \tLoss: 0.638... \tVal Loss: 0.719 \tVal Frac: 0.7761\n",
      "\tEpoch: 80/1000; 1.37 sec... \tStep: 19440... \tLoss: 0.757... \tVal Loss: 0.768 \tVal Frac: 0.7511\n",
      "\tEpoch: 85/1000; 1.32 sec... \tStep: 20655... \tLoss: 1.683... \tVal Loss: 0.756 \tVal Frac: 0.7610\n",
      "\tEpoch: 90/1000; 1.33 sec... \tStep: 21870... \tLoss: 0.781... \tVal Loss: 0.713 \tVal Frac: 0.7710\n",
      "\tEpoch: 95/1000; 1.33 sec... \tStep: 23085... \tLoss: 0.872... \tVal Loss: 0.646 \tVal Frac: 0.7893\n",
      "\tValidation loss decreased (0.687 --> 0.646).  Saving model ...\n",
      "\tEpoch: 100/1000; 1.33 sec... \tStep: 24300... \tLoss: 0.892... \tVal Loss: 0.672 \tVal Frac: 0.7629\n",
      "\tEpoch: 105/1000; 1.43 sec... \tStep: 25515... \tLoss: 0.893... \tVal Loss: 0.730 \tVal Frac: 0.7750\n",
      "\tEpoch: 110/1000; 1.44 sec... \tStep: 26730... \tLoss: 0.537... \tVal Loss: 0.777 \tVal Frac: 0.7489\n",
      "\tEpoch: 115/1000; 1.32 sec... \tStep: 27945... \tLoss: 1.255... \tVal Loss: 0.934 \tVal Frac: 0.7445\n",
      "\tEpoch: 120/1000; 1.36 sec... \tStep: 29160... \tLoss: 0.660... \tVal Loss: 0.705 \tVal Frac: 0.7890\n",
      "\tEpoch: 125/1000; 1.37 sec... \tStep: 30375... \tLoss: 1.077... \tVal Loss: 0.771 \tVal Frac: 0.7585\n",
      "\tEpoch: 130/1000; 1.37 sec... \tStep: 31590... \tLoss: 0.904... \tVal Loss: 0.941 \tVal Frac: 0.7526\n",
      "\tEpoch: 135/1000; 1.78 sec... \tStep: 32805... \tLoss: 1.025... \tVal Loss: 0.717 \tVal Frac: 0.7801\n",
      "\tEpoch: 140/1000; 1.52 sec... \tStep: 34020... \tLoss: 0.949... \tVal Loss: 0.709 \tVal Frac: 0.7783\n",
      "\tEpoch: 145/1000; 1.54 sec... \tStep: 35235... \tLoss: 0.617... \tVal Loss: 0.635 \tVal Frac: 0.7945\n",
      "\tValidation loss decreased (0.646 --> 0.635).  Saving model ...\n",
      "\tEpoch: 150/1000; 1.37 sec... \tStep: 36450... \tLoss: 0.794... \tVal Loss: 0.730 \tVal Frac: 0.7860\n",
      "\tEpoch: 155/1000; 1.43 sec... \tStep: 37665... \tLoss: 0.999... \tVal Loss: 0.668 \tVal Frac: 0.7732\n",
      "\tEpoch: 160/1000; 1.33 sec... \tStep: 38880... \tLoss: 0.709... \tVal Loss: 0.694 \tVal Frac: 0.7901\n",
      "\tEpoch: 165/1000; 1.30 sec... \tStep: 40095... \tLoss: 0.910... \tVal Loss: 0.692 \tVal Frac: 0.7960\n",
      "\tEpoch: 170/1000; 1.39 sec... \tStep: 41310... \tLoss: 0.609... \tVal Loss: 0.669 \tVal Frac: 0.7960\n",
      "\tEpoch: 175/1000; 1.35 sec... \tStep: 42525... \tLoss: 0.958... \tVal Loss: 0.823 \tVal Frac: 0.7757\n",
      "\tEpoch: 180/1000; 1.33 sec... \tStep: 43740... \tLoss: 1.270... \tVal Loss: 0.691 \tVal Frac: 0.7676\n",
      "\tEpoch: 185/1000; 1.32 sec... \tStep: 44955... \tLoss: 0.889... \tVal Loss: 0.748 \tVal Frac: 0.7562\n",
      "\tEpoch: 190/1000; 1.42 sec... \tStep: 46170... \tLoss: 0.706... \tVal Loss: 0.701 \tVal Frac: 0.7555\n",
      "\tEpoch: 195/1000; 1.30 sec... \tStep: 47385... \tLoss: 1.277... \tVal Loss: 0.817 \tVal Frac: 0.7581\n",
      "\tEpoch: 200/1000; 1.37 sec... \tStep: 48600... \tLoss: 0.606... \tVal Loss: 0.691 \tVal Frac: 0.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 205/1000; 1.35 sec... \tStep: 49815... \tLoss: 0.769... \tVal Loss: 0.679 \tVal Frac: 0.7676\n",
      "\tEpoch: 210/1000; 1.36 sec... \tStep: 51030... \tLoss: 0.990... \tVal Loss: 0.743 \tVal Frac: 0.7772\n",
      "\tEpoch: 215/1000; 1.85 sec... \tStep: 52245... \tLoss: 1.284... \tVal Loss: 0.786 \tVal Frac: 0.7621\n",
      "\tEpoch: 220/1000; 1.30 sec... \tStep: 53460... \tLoss: 0.580... \tVal Loss: 0.683 \tVal Frac: 0.7849\n",
      "\tEpoch: 225/1000; 1.37 sec... \tStep: 54675... \tLoss: 0.925... \tVal Loss: 0.737 \tVal Frac: 0.7794\n",
      "\tEpoch: 230/1000; 1.42 sec... \tStep: 55890... \tLoss: 0.718... \tVal Loss: 0.687 \tVal Frac: 0.7996\n",
      "\tEpoch: 235/1000; 1.39 sec... \tStep: 57105... \tLoss: 1.165... \tVal Loss: 0.683 \tVal Frac: 0.7570\n",
      "\tEpoch: 240/1000; 1.34 sec... \tStep: 58320... \tLoss: 0.931... \tVal Loss: 0.645 \tVal Frac: 0.7754\n",
      "\tEpoch: 245/1000; 1.44 sec... \tStep: 59535... \tLoss: 0.824... \tVal Loss: 0.685 \tVal Frac: 0.7482\n",
      "\tEpoch: 250/1000; 1.45 sec... \tStep: 60750... \tLoss: 0.664... \tVal Loss: 0.660 \tVal Frac: 0.7886\n",
      "\tEpoch: 255/1000; 1.47 sec... \tStep: 61965... \tLoss: 0.988... \tVal Loss: 0.726 \tVal Frac: 0.7625\n",
      "\tEpoch: 260/1000; 1.39 sec... \tStep: 63180... \tLoss: 0.681... \tVal Loss: 0.625 \tVal Frac: 0.8029\n",
      "\tValidation loss decreased (0.635 --> 0.625).  Saving model ...\n",
      "\tEpoch: 265/1000; 1.36 sec... \tStep: 64395... \tLoss: 0.715... \tVal Loss: 0.669 \tVal Frac: 0.7577\n",
      "\tEpoch: 270/1000; 1.39 sec... \tStep: 65610... \tLoss: 1.053... \tVal Loss: 0.774 \tVal Frac: 0.7787\n",
      "\tEpoch: 275/1000; 1.40 sec... \tStep: 66825... \tLoss: 0.692... \tVal Loss: 0.643 \tVal Frac: 0.7849\n",
      "\tEpoch: 280/1000; 1.42 sec... \tStep: 68040... \tLoss: 0.804... \tVal Loss: 0.643 \tVal Frac: 0.7812\n",
      "\tEpoch: 285/1000; 1.42 sec... \tStep: 69255... \tLoss: 0.443... \tVal Loss: 0.683 \tVal Frac: 0.7783\n",
      "\tEpoch: 290/1000; 1.29 sec... \tStep: 70470... \tLoss: 1.118... \tVal Loss: 0.659 \tVal Frac: 0.7732\n",
      "\tEpoch: 295/1000; 1.36 sec... \tStep: 71685... \tLoss: 1.117... \tVal Loss: 0.627 \tVal Frac: 0.7941\n",
      "\tEpoch: 300/1000; 1.30 sec... \tStep: 72900... \tLoss: 0.754... \tVal Loss: 0.612 \tVal Frac: 0.8018\n",
      "\tValidation loss decreased (0.625 --> 0.612).  Saving model ...\n",
      "\tEpoch: 305/1000; 1.32 sec... \tStep: 74115... \tLoss: 0.601... \tVal Loss: 0.648 \tVal Frac: 0.7816\n",
      "\tEpoch: 310/1000; 1.37 sec... \tStep: 75330... \tLoss: 0.619... \tVal Loss: 0.659 \tVal Frac: 0.7798\n",
      "\tEpoch: 315/1000; 1.43 sec... \tStep: 76545... \tLoss: 0.785... \tVal Loss: 0.624 \tVal Frac: 0.7934\n",
      "\tEpoch: 320/1000; 1.30 sec... \tStep: 77760... \tLoss: 1.034... \tVal Loss: 0.675 \tVal Frac: 0.7904\n",
      "\tEpoch: 325/1000; 1.41 sec... \tStep: 78975... \tLoss: 0.812... \tVal Loss: 0.650 \tVal Frac: 0.7779\n",
      "\tEpoch: 330/1000; 1.38 sec... \tStep: 80190... \tLoss: 1.133... \tVal Loss: 0.671 \tVal Frac: 0.7790\n",
      "\tEpoch: 335/1000; 1.39 sec... \tStep: 81405... \tLoss: 0.635... \tVal Loss: 0.668 \tVal Frac: 0.7893\n",
      "\tEpoch: 340/1000; 1.29 sec... \tStep: 82620... \tLoss: 0.484... \tVal Loss: 0.681 \tVal Frac: 0.7926\n",
      "\tEpoch: 345/1000; 1.37 sec... \tStep: 83835... \tLoss: 0.555... \tVal Loss: 0.645 \tVal Frac: 0.7919\n",
      "\tEpoch: 350/1000; 1.33 sec... \tStep: 85050... \tLoss: 0.693... \tVal Loss: 0.779 \tVal Frac: 0.7526\n",
      "\tEpoch: 355/1000; 1.39 sec... \tStep: 86265... \tLoss: 0.851... \tVal Loss: 0.828 \tVal Frac: 0.7754\n",
      "\tEpoch: 360/1000; 1.29 sec... \tStep: 87480... \tLoss: 0.790... \tVal Loss: 0.674 \tVal Frac: 0.7651\n",
      "\tEpoch: 365/1000; 1.44 sec... \tStep: 88695... \tLoss: 0.615... \tVal Loss: 0.651 \tVal Frac: 0.7724\n",
      "\tEpoch: 370/1000; 1.26 sec... \tStep: 89910... \tLoss: 0.667... \tVal Loss: 0.616 \tVal Frac: 0.8066\n",
      "\tEpoch: 375/1000; 1.42 sec... \tStep: 91125... \tLoss: 0.410... \tVal Loss: 0.633 \tVal Frac: 0.8026\n",
      "\tEpoch: 380/1000; 1.29 sec... \tStep: 92340... \tLoss: 0.626... \tVal Loss: 0.723 \tVal Frac: 0.7857\n",
      "\tEpoch: 385/1000; 1.30 sec... \tStep: 93555... \tLoss: 0.577... \tVal Loss: 0.634 \tVal Frac: 0.7713\n",
      "\tEpoch: 390/1000; 1.42 sec... \tStep: 94770... \tLoss: 0.434... \tVal Loss: 0.650 \tVal Frac: 0.7897\n",
      "\tEpoch: 395/1000; 1.36 sec... \tStep: 95985... \tLoss: 0.636... \tVal Loss: 0.632 \tVal Frac: 0.7912\n",
      "\tEpoch: 400/1000; 1.37 sec... \tStep: 97200... \tLoss: 1.243... \tVal Loss: 0.684 \tVal Frac: 0.7684\n",
      "\tEpoch: 405/1000; 1.40 sec... \tStep: 98415... \tLoss: 0.738... \tVal Loss: 0.680 \tVal Frac: 0.7824\n",
      "\tEpoch: 410/1000; 1.39 sec... \tStep: 99630... \tLoss: 1.002... \tVal Loss: 0.837 \tVal Frac: 0.7577\n",
      "\tEpoch: 415/1000; 1.50 sec... \tStep: 100845... \tLoss: 0.579... \tVal Loss: 0.711 \tVal Frac: 0.7985\n",
      "\tEpoch: 420/1000; 1.37 sec... \tStep: 102060... \tLoss: 0.724... \tVal Loss: 0.673 \tVal Frac: 0.7783\n",
      "\tEpoch: 425/1000; 1.38 sec... \tStep: 103275... \tLoss: 0.756... \tVal Loss: 0.719 \tVal Frac: 0.7732\n",
      "\tEpoch: 430/1000; 1.30 sec... \tStep: 104490... \tLoss: 0.638... \tVal Loss: 0.692 \tVal Frac: 0.7816\n",
      "\tEpoch: 435/1000; 1.33 sec... \tStep: 105705... \tLoss: 1.103... \tVal Loss: 0.659 \tVal Frac: 0.7908\n",
      "\tEpoch: 440/1000; 1.40 sec... \tStep: 106920... \tLoss: 1.014... \tVal Loss: 0.685 \tVal Frac: 0.7934\n",
      "\tEpoch: 445/1000; 1.40 sec... \tStep: 108135... \tLoss: 0.957... \tVal Loss: 0.656 \tVal Frac: 0.7614\n",
      "\tEpoch: 450/1000; 1.46 sec... \tStep: 109350... \tLoss: 0.710... \tVal Loss: 0.715 \tVal Frac: 0.7728\n",
      "\tEpoch: 455/1000; 1.35 sec... \tStep: 110565... \tLoss: 0.717... \tVal Loss: 0.737 \tVal Frac: 0.7386\n",
      "\tEpoch: 460/1000; 1.33 sec... \tStep: 111780... \tLoss: 0.970... \tVal Loss: 0.729 \tVal Frac: 0.7588\n",
      "\tEpoch: 465/1000; 1.29 sec... \tStep: 112995... \tLoss: 0.494... \tVal Loss: 0.671 \tVal Frac: 0.7779\n",
      "\tEpoch: 470/1000; 1.36 sec... \tStep: 114210... \tLoss: 1.241... \tVal Loss: 0.670 \tVal Frac: 0.7768\n",
      "\tEpoch: 475/1000; 1.30 sec... \tStep: 115425... \tLoss: 0.641... \tVal Loss: 0.645 \tVal Frac: 0.7842\n",
      "\tEpoch: 480/1000; 1.37 sec... \tStep: 116640... \tLoss: 1.317... \tVal Loss: 0.723 \tVal Frac: 0.7408\n",
      "\tEpoch: 485/1000; 1.37 sec... \tStep: 117855... \tLoss: 0.726... \tVal Loss: 0.701 \tVal Frac: 0.7607\n",
      "\tEpoch: 490/1000; 1.34 sec... \tStep: 119070... \tLoss: 0.747... \tVal Loss: 0.675 \tVal Frac: 0.7632\n",
      "\tEpoch: 495/1000; 1.50 sec... \tStep: 120285... \tLoss: 0.860... \tVal Loss: 0.614 \tVal Frac: 0.7897\n",
      "\tEpoch: 500/1000; 1.40 sec... \tStep: 121500... \tLoss: 0.733... \tVal Loss: 0.640 \tVal Frac: 0.7849\n",
      "\tEpoch: 505/1000; 1.34 sec... \tStep: 122715... \tLoss: 0.593... \tVal Loss: 0.663 \tVal Frac: 0.7776\n",
      "\tEpoch: 510/1000; 1.33 sec... \tStep: 123930... \tLoss: 0.712... \tVal Loss: 0.685 \tVal Frac: 0.7676\n",
      "\tEpoch: 515/1000; 1.45 sec... \tStep: 125145... \tLoss: 0.965... \tVal Loss: 0.819 \tVal Frac: 0.7772\n",
      "\tEpoch: 520/1000; 1.30 sec... \tStep: 126360... \tLoss: 0.478... \tVal Loss: 0.656 \tVal Frac: 0.7952\n",
      "\tEpoch: 525/1000; 1.30 sec... \tStep: 127575... \tLoss: 0.669... \tVal Loss: 0.668 \tVal Frac: 0.7511\n",
      "\tEpoch: 530/1000; 1.39 sec... \tStep: 128790... \tLoss: 0.707... \tVal Loss: 0.652 \tVal Frac: 0.7912\n",
      "\tEpoch: 535/1000; 1.27 sec... \tStep: 130005... \tLoss: 0.921... \tVal Loss: 0.645 \tVal Frac: 0.7713\n",
      "\tEpoch: 540/1000; 1.33 sec... \tStep: 131220... \tLoss: 0.476... \tVal Loss: 0.731 \tVal Frac: 0.7779\n",
      "\tEpoch: 545/1000; 1.37 sec... \tStep: 132435... \tLoss: 0.608... \tVal Loss: 0.684 \tVal Frac: 0.7831\n",
      "\tEpoch: 550/1000; 1.34 sec... \tStep: 133650... \tLoss: 1.050... \tVal Loss: 0.666 \tVal Frac: 0.7816\n",
      "\tEpoch: 555/1000; 1.26 sec... \tStep: 134865... \tLoss: 1.133... \tVal Loss: 0.684 \tVal Frac: 0.7787\n",
      "\tEpoch: 560/1000; 1.64 sec... \tStep: 136080... \tLoss: 0.779... \tVal Loss: 0.683 \tVal Frac: 0.7739\n",
      "\tEpoch: 565/1000; 1.54 sec... \tStep: 137295... \tLoss: 0.533... \tVal Loss: 0.649 \tVal Frac: 0.7897\n",
      "\tEpoch: 570/1000; 1.66 sec... \tStep: 138510... \tLoss: 0.555... \tVal Loss: 0.700 \tVal Frac: 0.7500\n",
      "\tEpoch: 575/1000; 1.29 sec... \tStep: 139725... \tLoss: 0.764... \tVal Loss: 0.649 \tVal Frac: 0.7783\n",
      "\tEpoch: 580/1000; 1.27 sec... \tStep: 140940... \tLoss: 0.739... \tVal Loss: 0.668 \tVal Frac: 0.7879\n",
      "\tEpoch: 585/1000; 1.40 sec... \tStep: 142155... \tLoss: 0.811... \tVal Loss: 0.646 \tVal Frac: 0.7915\n",
      "\tEpoch: 590/1000; 1.27 sec... \tStep: 143370... \tLoss: 1.096... \tVal Loss: 0.679 \tVal Frac: 0.7890\n",
      "\tEpoch: 595/1000; 1.38 sec... \tStep: 144585... \tLoss: 0.934... \tVal Loss: 0.637 \tVal Frac: 0.7908\n",
      "\tEpoch: 600/1000; 1.49 sec... \tStep: 145800... \tLoss: 1.112... \tVal Loss: 0.639 \tVal Frac: 0.7779\n",
      "\tEpoch: 605/1000; 1.29 sec... \tStep: 147015... \tLoss: 0.940... \tVal Loss: 0.686 \tVal Frac: 0.7739\n",
      "\tEpoch: 610/1000; 1.37 sec... \tStep: 148230... \tLoss: 0.390... \tVal Loss: 0.653 \tVal Frac: 0.7904\n",
      "\tEpoch: 615/1000; 1.34 sec... \tStep: 149445... \tLoss: 0.704... \tVal Loss: 0.645 \tVal Frac: 0.7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 620/1000; 1.35 sec... \tStep: 150660... \tLoss: 1.134... \tVal Loss: 0.673 \tVal Frac: 0.7585\n",
      "\tEpoch: 625/1000; 1.26 sec... \tStep: 151875... \tLoss: 0.568... \tVal Loss: 0.663 \tVal Frac: 0.7868\n",
      "\tEpoch: 630/1000; 1.43 sec... \tStep: 153090... \tLoss: 0.556... \tVal Loss: 0.655 \tVal Frac: 0.7930\n",
      "\tEpoch: 635/1000; 1.23 sec... \tStep: 154305... \tLoss: 1.479... \tVal Loss: 0.701 \tVal Frac: 0.7324\n",
      "\tEpoch: 640/1000; 1.37 sec... \tStep: 155520... \tLoss: 1.187... \tVal Loss: 0.641 \tVal Frac: 0.7989\n",
      "\tEpoch: 645/1000; 1.34 sec... \tStep: 156735... \tLoss: 0.595... \tVal Loss: 0.639 \tVal Frac: 0.7971\n",
      "\tEpoch: 650/1000; 1.29 sec... \tStep: 157950... \tLoss: 0.444... \tVal Loss: 0.620 \tVal Frac: 0.8099\n",
      "\tEpoch: 655/1000; 1.36 sec... \tStep: 159165... \tLoss: 0.953... \tVal Loss: 0.700 \tVal Frac: 0.7669\n",
      "\tEpoch: 660/1000; 1.35 sec... \tStep: 160380... \tLoss: 0.298... \tVal Loss: 0.630 \tVal Frac: 0.8011\n",
      "\tEpoch: 665/1000; 1.38 sec... \tStep: 161595... \tLoss: 0.682... \tVal Loss: 0.639 \tVal Frac: 0.7842\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 1.131... \tVal Loss: 0.623 \tVal Frac: 0.8000\n",
      "\tEpoch: 675/1000; 1.40 sec... \tStep: 164025... \tLoss: 0.780... \tVal Loss: 0.635 \tVal Frac: 0.7963\n",
      "\tEpoch: 680/1000; 1.54 sec... \tStep: 165240... \tLoss: 0.957... \tVal Loss: 0.652 \tVal Frac: 0.7849\n",
      "\tEpoch: 685/1000; 1.38 sec... \tStep: 166455... \tLoss: 0.746... \tVal Loss: 0.655 \tVal Frac: 0.7974\n",
      "\tEpoch: 690/1000; 1.39 sec... \tStep: 167670... \tLoss: 0.649... \tVal Loss: 0.630 \tVal Frac: 0.7897\n",
      "\tEpoch: 695/1000; 1.39 sec... \tStep: 168885... \tLoss: 0.564... \tVal Loss: 0.617 \tVal Frac: 0.8048\n",
      "\tEpoch: 700/1000; 1.32 sec... \tStep: 170100... \tLoss: 1.109... \tVal Loss: 0.669 \tVal Frac: 0.7735\n",
      "\tEpoch: 705/1000; 1.43 sec... \tStep: 171315... \tLoss: 1.068... \tVal Loss: 0.647 \tVal Frac: 0.7629\n",
      "\tEpoch: 710/1000; 1.35 sec... \tStep: 172530... \tLoss: 0.839... \tVal Loss: 0.626 \tVal Frac: 0.7647\n",
      "\tEpoch: 715/1000; 1.24 sec... \tStep: 173745... \tLoss: 0.677... \tVal Loss: 0.622 \tVal Frac: 0.7868\n",
      "\tEpoch: 720/1000; 1.35 sec... \tStep: 174960... \tLoss: 0.880... \tVal Loss: 0.638 \tVal Frac: 0.7956\n",
      "\tEpoch: 725/1000; 1.37 sec... \tStep: 176175... \tLoss: 0.438... \tVal Loss: 0.648 \tVal Frac: 0.7625\n",
      "\tEpoch: 730/1000; 1.47 sec... \tStep: 177390... \tLoss: 0.726... \tVal Loss: 0.644 \tVal Frac: 0.7893\n",
      "\tEpoch: 735/1000; 1.30 sec... \tStep: 178605... \tLoss: 0.352... \tVal Loss: 0.622 \tVal Frac: 0.7871\n",
      "\tEpoch: 740/1000; 1.30 sec... \tStep: 179820... \tLoss: 0.675... \tVal Loss: 0.633 \tVal Frac: 0.7971\n",
      "\tEpoch: 745/1000; 1.32 sec... \tStep: 181035... \tLoss: 0.937... \tVal Loss: 0.644 \tVal Frac: 0.7890\n",
      "\tEpoch: 750/1000; 1.37 sec... \tStep: 182250... \tLoss: 0.919... \tVal Loss: 0.641 \tVal Frac: 0.7746\n",
      "\tEpoch: 755/1000; 1.37 sec... \tStep: 183465... \tLoss: 0.598... \tVal Loss: 0.635 \tVal Frac: 0.7695\n",
      "\tEpoch: 760/1000; 1.45 sec... \tStep: 184680... \tLoss: 1.130... \tVal Loss: 0.683 \tVal Frac: 0.7824\n",
      "\tEpoch: 765/1000; 1.43 sec... \tStep: 185895... \tLoss: 0.451... \tVal Loss: 0.687 \tVal Frac: 0.7607\n",
      "\tEpoch: 770/1000; 1.27 sec... \tStep: 187110... \tLoss: 0.352... \tVal Loss: 0.641 \tVal Frac: 0.7890\n",
      "\tEpoch: 775/1000; 1.36 sec... \tStep: 188325... \tLoss: 0.733... \tVal Loss: 0.642 \tVal Frac: 0.7971\n",
      "\tEpoch: 780/1000; 1.33 sec... \tStep: 189540... \tLoss: 0.396... \tVal Loss: 0.627 \tVal Frac: 0.7971\n",
      "\tEpoch: 785/1000; 1.40 sec... \tStep: 190755... \tLoss: 0.553... \tVal Loss: 0.626 \tVal Frac: 0.8040\n",
      "\tEpoch: 790/1000; 1.37 sec... \tStep: 191970... \tLoss: 0.944... \tVal Loss: 0.630 \tVal Frac: 0.8070\n",
      "\tEpoch: 795/1000; 1.19 sec... \tStep: 193185... \tLoss: 1.047... \tVal Loss: 0.716 \tVal Frac: 0.7721\n",
      "\tEpoch: 800/1000; 1.43 sec... \tStep: 194400... \tLoss: 0.772... \tVal Loss: 0.649 \tVal Frac: 0.8029\n",
      "\tEpoch: 805/1000; 1.39 sec... \tStep: 195615... \tLoss: 0.716... \tVal Loss: 0.638 \tVal Frac: 0.7945\n",
      "\tEpoch: 810/1000; 1.39 sec... \tStep: 196830... \tLoss: 1.091... \tVal Loss: 0.665 \tVal Frac: 0.7846\n",
      "\tEpoch: 815/1000; 1.37 sec... \tStep: 198045... \tLoss: 1.064... \tVal Loss: 0.661 \tVal Frac: 0.7805\n",
      "\tEpoch: 820/1000; 1.28 sec... \tStep: 199260... \tLoss: 0.655... \tVal Loss: 0.671 \tVal Frac: 0.7739\n",
      "\tEpoch: 825/1000; 1.39 sec... \tStep: 200475... \tLoss: 0.777... \tVal Loss: 0.689 \tVal Frac: 0.7636\n",
      "\tEpoch: 830/1000; 1.36 sec... \tStep: 201690... \tLoss: 0.804... \tVal Loss: 0.665 \tVal Frac: 0.7522\n",
      "\tEpoch: 835/1000; 1.26 sec... \tStep: 202905... \tLoss: 0.904... \tVal Loss: 0.677 \tVal Frac: 0.7599\n",
      "\tEpoch: 840/1000; 1.44 sec... \tStep: 204120... \tLoss: 0.416... \tVal Loss: 0.621 \tVal Frac: 0.7967\n",
      "\tEpoch: 845/1000; 1.43 sec... \tStep: 205335... \tLoss: 0.609... \tVal Loss: 0.674 \tVal Frac: 0.7790\n",
      "\tEpoch: 850/1000; 1.37 sec... \tStep: 206550... \tLoss: 0.996... \tVal Loss: 0.625 \tVal Frac: 0.7952\n",
      "\tEpoch: 855/1000; 1.52 sec... \tStep: 207765... \tLoss: 0.671... \tVal Loss: 0.628 \tVal Frac: 0.7934\n",
      "\tEpoch: 860/1000; 1.32 sec... \tStep: 208980... \tLoss: 1.117... \tVal Loss: 0.644 \tVal Frac: 0.7864\n",
      "\tEpoch: 865/1000; 1.36 sec... \tStep: 210195... \tLoss: 0.651... \tVal Loss: 0.648 \tVal Frac: 0.7809\n",
      "\tEpoch: 870/1000; 1.44 sec... \tStep: 211410... \tLoss: 0.912... \tVal Loss: 0.635 \tVal Frac: 0.7849\n",
      "\tEpoch: 875/1000; 1.37 sec... \tStep: 212625... \tLoss: 0.803... \tVal Loss: 0.672 \tVal Frac: 0.7838\n",
      "\tEpoch: 880/1000; 1.37 sec... \tStep: 213840... \tLoss: 0.904... \tVal Loss: 0.646 \tVal Frac: 0.7945\n",
      "\tEpoch: 885/1000; 1.40 sec... \tStep: 215055... \tLoss: 0.843... \tVal Loss: 0.630 \tVal Frac: 0.7912\n",
      "\tEpoch: 890/1000; 1.37 sec... \tStep: 216270... \tLoss: 0.707... \tVal Loss: 0.616 \tVal Frac: 0.7985\n",
      "\tEpoch: 895/1000; 1.44 sec... \tStep: 217485... \tLoss: 0.920... \tVal Loss: 0.624 \tVal Frac: 0.7739\n",
      "\tEpoch: 900/1000; 1.27 sec... \tStep: 218700... \tLoss: 0.748... \tVal Loss: 0.628 \tVal Frac: 0.7864\n",
      "\tEpoch: 905/1000; 1.29 sec... \tStep: 219915... \tLoss: 1.194... \tVal Loss: 0.635 \tVal Frac: 0.7853\n",
      "\tEpoch: 910/1000; 1.30 sec... \tStep: 221130... \tLoss: 0.388... \tVal Loss: 0.636 \tVal Frac: 0.7831\n",
      "\tEpoch: 915/1000; 1.39 sec... \tStep: 222345... \tLoss: 1.385... \tVal Loss: 0.758 \tVal Frac: 0.7298\n",
      "\tEpoch: 920/1000; 1.43 sec... \tStep: 223560... \tLoss: 0.931... \tVal Loss: 0.666 \tVal Frac: 0.7665\n",
      "\tEpoch: 925/1000; 1.42 sec... \tStep: 224775... \tLoss: 1.081... \tVal Loss: 0.637 \tVal Frac: 0.7724\n",
      "\tEpoch: 930/1000; 1.45 sec... \tStep: 225990... \tLoss: 0.790... \tVal Loss: 0.617 \tVal Frac: 0.7676\n",
      "\tEpoch: 935/1000; 1.27 sec... \tStep: 227205... \tLoss: 0.828... \tVal Loss: 0.620 \tVal Frac: 0.7985\n",
      "\tEpoch: 940/1000; 1.37 sec... \tStep: 228420... \tLoss: 0.822... \tVal Loss: 0.644 \tVal Frac: 0.7805\n",
      "\tEpoch: 945/1000; 1.30 sec... \tStep: 229635... \tLoss: 0.648... \tVal Loss: 0.647 \tVal Frac: 0.7923\n",
      "\tEpoch: 950/1000; 1.28 sec... \tStep: 230850... \tLoss: 0.980... \tVal Loss: 0.687 \tVal Frac: 0.7860\n",
      "\tEpoch: 955/1000; 1.35 sec... \tStep: 232065... \tLoss: 1.051... \tVal Loss: 0.628 \tVal Frac: 0.7937\n",
      "\tEpoch: 960/1000; 1.66 sec... \tStep: 233280... \tLoss: 1.022... \tVal Loss: 0.644 \tVal Frac: 0.8022\n",
      "\tEpoch: 965/1000; 1.56 sec... \tStep: 234495... \tLoss: 0.466... \tVal Loss: 0.622 \tVal Frac: 0.8018\n",
      "\tEpoch: 970/1000; 1.40 sec... \tStep: 235710... \tLoss: 0.880... \tVal Loss: 0.609 \tVal Frac: 0.8004\n",
      "\tValidation loss decreased (0.612 --> 0.609).  Saving model ...\n",
      "\tEpoch: 975/1000; 1.30 sec... \tStep: 236925... \tLoss: 0.886... \tVal Loss: 0.657 \tVal Frac: 0.7930\n",
      "\tEpoch: 980/1000; 1.37 sec... \tStep: 238140... \tLoss: 0.892... \tVal Loss: 0.656 \tVal Frac: 0.7971\n",
      "\tEpoch: 985/1000; 1.39 sec... \tStep: 239355... \tLoss: 0.330... \tVal Loss: 0.636 \tVal Frac: 0.7982\n",
      "\tEpoch: 990/1000; 1.42 sec... \tStep: 240570... \tLoss: 0.984... \tVal Loss: 0.635 \tVal Frac: 0.8011\n",
      "\tEpoch: 995/1000; 1.36 sec... \tStep: 241785... \tLoss: 1.031... \tVal Loss: 0.648 \tVal Frac: 0.7783\n",
      "\tEpoch: 1000/1000; 1.33 sec... \tStep: 243000... \tLoss: 0.966... \tVal Loss: 0.620 \tVal Frac: 0.7886\n",
      "Completed:  hidden_dim :  20 \n",
      "\tloss: 0.609183 \n",
      "\tfrac:0.809926\n",
      "\n",
      "###########\n",
      "Testing with  hidden_dim :  100\n",
      "\tEpoch: 5/1000; 1.52 sec... \tStep: 1215... \tLoss: 0.536... \tVal Loss: 0.668 \tVal Frac: 0.7879\n",
      "\tValidation loss decreased (inf --> 0.668).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.41 sec... \tStep: 2430... \tLoss: 0.935... \tVal Loss: 0.703 \tVal Frac: 0.7754\n",
      "\tEpoch: 15/1000; 1.39 sec... \tStep: 3645... \tLoss: 0.884... \tVal Loss: 0.714 \tVal Frac: 0.7809\n",
      "\tEpoch: 20/1000; 1.36 sec... \tStep: 4860... \tLoss: 0.805... \tVal Loss: 0.671 \tVal Frac: 0.7971\n",
      "\tEpoch: 25/1000; 1.45 sec... \tStep: 6075... \tLoss: 0.631... \tVal Loss: 0.628 \tVal Frac: 0.7908\n",
      "\tValidation loss decreased (0.668 --> 0.628).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 30/1000; 1.65 sec... \tStep: 7290... \tLoss: 0.936... \tVal Loss: 0.648 \tVal Frac: 0.7787\n",
      "\tEpoch: 35/1000; 1.64 sec... \tStep: 8505... \tLoss: 1.146... \tVal Loss: 0.638 \tVal Frac: 0.7926\n",
      "\tEpoch: 40/1000; 1.64 sec... \tStep: 9720... \tLoss: 0.511... \tVal Loss: 0.588 \tVal Frac: 0.8074\n",
      "\tValidation loss decreased (0.628 --> 0.588).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.57 sec... \tStep: 10935... \tLoss: 0.630... \tVal Loss: 0.606 \tVal Frac: 0.8085\n",
      "\tEpoch: 50/1000; 1.91 sec... \tStep: 12150... \tLoss: 1.077... \tVal Loss: 0.656 \tVal Frac: 0.7956\n",
      "\tEpoch: 55/1000; 1.61 sec... \tStep: 13365... \tLoss: 0.895... \tVal Loss: 0.633 \tVal Frac: 0.7849\n",
      "\tEpoch: 60/1000; 1.70 sec... \tStep: 14580... \tLoss: 1.003... \tVal Loss: 0.778 \tVal Frac: 0.7893\n",
      "\tEpoch: 65/1000; 1.67 sec... \tStep: 15795... \tLoss: 1.088... \tVal Loss: 0.635 \tVal Frac: 0.8055\n",
      "\tEpoch: 70/1000; 1.43 sec... \tStep: 17010... \tLoss: 0.848... \tVal Loss: 0.603 \tVal Frac: 0.8040\n",
      "\tEpoch: 75/1000; 1.36 sec... \tStep: 18225... \tLoss: 0.632... \tVal Loss: 0.633 \tVal Frac: 0.7908\n",
      "\tEpoch: 80/1000; 1.40 sec... \tStep: 19440... \tLoss: 0.836... \tVal Loss: 0.608 \tVal Frac: 0.8029\n",
      "\tEpoch: 85/1000; 1.45 sec... \tStep: 20655... \tLoss: 0.814... \tVal Loss: 0.610 \tVal Frac: 0.8044\n",
      "\tEpoch: 90/1000; 1.54 sec... \tStep: 21870... \tLoss: 0.920... \tVal Loss: 0.618 \tVal Frac: 0.8074\n",
      "\tEpoch: 95/1000; 1.42 sec... \tStep: 23085... \tLoss: 1.154... \tVal Loss: 0.630 \tVal Frac: 0.7912\n",
      "\tEpoch: 100/1000; 1.44 sec... \tStep: 24300... \tLoss: 0.381... \tVal Loss: 0.585 \tVal Frac: 0.8121\n",
      "\tValidation loss decreased (0.588 --> 0.585).  Saving model ...\n",
      "\tEpoch: 105/1000; 1.39 sec... \tStep: 25515... \tLoss: 1.030... \tVal Loss: 0.665 \tVal Frac: 0.7919\n",
      "\tEpoch: 110/1000; 1.49 sec... \tStep: 26730... \tLoss: 0.912... \tVal Loss: 0.578 \tVal Frac: 0.8169\n",
      "\tValidation loss decreased (0.585 --> 0.578).  Saving model ...\n",
      "\tEpoch: 115/1000; 1.47 sec... \tStep: 27945... \tLoss: 0.650... \tVal Loss: 0.595 \tVal Frac: 0.8055\n",
      "\tEpoch: 120/1000; 1.37 sec... \tStep: 29160... \tLoss: 0.892... \tVal Loss: 0.586 \tVal Frac: 0.8107\n",
      "\tEpoch: 125/1000; 1.40 sec... \tStep: 30375... \tLoss: 0.848... \tVal Loss: 0.607 \tVal Frac: 0.8125\n",
      "\tEpoch: 130/1000; 1.43 sec... \tStep: 31590... \tLoss: 0.449... \tVal Loss: 0.619 \tVal Frac: 0.7971\n",
      "\tEpoch: 135/1000; 1.47 sec... \tStep: 32805... \tLoss: 0.592... \tVal Loss: 0.627 \tVal Frac: 0.8118\n",
      "\tEpoch: 140/1000; 1.46 sec... \tStep: 34020... \tLoss: 1.132... \tVal Loss: 0.653 \tVal Frac: 0.8040\n",
      "\tEpoch: 145/1000; 1.67 sec... \tStep: 35235... \tLoss: 0.577... \tVal Loss: 0.598 \tVal Frac: 0.8107\n",
      "\tEpoch: 150/1000; 1.40 sec... \tStep: 36450... \tLoss: 0.667... \tVal Loss: 0.596 \tVal Frac: 0.8147\n",
      "\tEpoch: 155/1000; 1.54 sec... \tStep: 37665... \tLoss: 0.866... \tVal Loss: 0.600 \tVal Frac: 0.8092\n",
      "\tEpoch: 160/1000; 1.57 sec... \tStep: 38880... \tLoss: 0.689... \tVal Loss: 0.591 \tVal Frac: 0.8051\n",
      "\tEpoch: 165/1000; 1.47 sec... \tStep: 40095... \tLoss: 0.661... \tVal Loss: 0.595 \tVal Frac: 0.8011\n",
      "\tEpoch: 170/1000; 1.39 sec... \tStep: 41310... \tLoss: 0.305... \tVal Loss: 0.608 \tVal Frac: 0.7993\n",
      "\tEpoch: 175/1000; 1.49 sec... \tStep: 42525... \tLoss: 0.408... \tVal Loss: 0.606 \tVal Frac: 0.8103\n",
      "\tEpoch: 180/1000; 1.55 sec... \tStep: 43740... \tLoss: 0.745... \tVal Loss: 0.579 \tVal Frac: 0.8143\n",
      "\tEpoch: 185/1000; 1.42 sec... \tStep: 44955... \tLoss: 0.711... \tVal Loss: 0.585 \tVal Frac: 0.8114\n",
      "\tEpoch: 190/1000; 1.50 sec... \tStep: 46170... \tLoss: 0.696... \tVal Loss: 0.645 \tVal Frac: 0.8011\n",
      "\tEpoch: 195/1000; 1.42 sec... \tStep: 47385... \tLoss: 0.753... \tVal Loss: 0.590 \tVal Frac: 0.8125\n",
      "\tEpoch: 200/1000; 1.45 sec... \tStep: 48600... \tLoss: 0.839... \tVal Loss: 0.590 \tVal Frac: 0.8048\n",
      "\tEpoch: 205/1000; 1.43 sec... \tStep: 49815... \tLoss: 0.647... \tVal Loss: 0.569 \tVal Frac: 0.8125\n",
      "\tValidation loss decreased (0.578 --> 0.569).  Saving model ...\n",
      "\tEpoch: 210/1000; 1.47 sec... \tStep: 51030... \tLoss: 0.663... \tVal Loss: 0.609 \tVal Frac: 0.8026\n",
      "\tEpoch: 215/1000; 1.40 sec... \tStep: 52245... \tLoss: 0.831... \tVal Loss: 0.588 \tVal Frac: 0.8213\n",
      "\tEpoch: 220/1000; 1.34 sec... \tStep: 53460... \tLoss: 0.886... \tVal Loss: 0.602 \tVal Frac: 0.8081\n",
      "\tEpoch: 225/1000; 1.40 sec... \tStep: 54675... \tLoss: 0.658... \tVal Loss: 0.604 \tVal Frac: 0.8026\n",
      "\tEpoch: 230/1000; 1.56 sec... \tStep: 55890... \tLoss: 0.606... \tVal Loss: 0.614 \tVal Frac: 0.7952\n",
      "\tEpoch: 235/1000; 1.33 sec... \tStep: 57105... \tLoss: 0.352... \tVal Loss: 0.580 \tVal Frac: 0.8074\n",
      "\tEpoch: 240/1000; 1.39 sec... \tStep: 58320... \tLoss: 0.715... \tVal Loss: 0.617 \tVal Frac: 0.8033\n",
      "\tEpoch: 245/1000; 1.33 sec... \tStep: 59535... \tLoss: 0.373... \tVal Loss: 0.593 \tVal Frac: 0.8015\n",
      "\tEpoch: 250/1000; 1.57 sec... \tStep: 60750... \tLoss: 0.604... \tVal Loss: 0.598 \tVal Frac: 0.8085\n",
      "\tEpoch: 255/1000; 1.49 sec... \tStep: 61965... \tLoss: 0.668... \tVal Loss: 0.589 \tVal Frac: 0.8092\n",
      "\tEpoch: 260/1000; 1.43 sec... \tStep: 63180... \tLoss: 0.841... \tVal Loss: 0.578 \tVal Frac: 0.8129\n",
      "\tEpoch: 265/1000; 1.52 sec... \tStep: 64395... \tLoss: 0.957... \tVal Loss: 0.607 \tVal Frac: 0.8018\n",
      "\tEpoch: 270/1000; 1.43 sec... \tStep: 65610... \tLoss: 0.825... \tVal Loss: 0.629 \tVal Frac: 0.7871\n",
      "\tEpoch: 275/1000; 1.52 sec... \tStep: 66825... \tLoss: 0.697... \tVal Loss: 0.648 \tVal Frac: 0.7904\n",
      "\tEpoch: 280/1000; 1.42 sec... \tStep: 68040... \tLoss: 0.655... \tVal Loss: 0.579 \tVal Frac: 0.8136\n",
      "\tEpoch: 285/1000; 1.37 sec... \tStep: 69255... \tLoss: 0.843... \tVal Loss: 0.602 \tVal Frac: 0.7956\n",
      "\tEpoch: 290/1000; 1.37 sec... \tStep: 70470... \tLoss: 0.416... \tVal Loss: 0.588 \tVal Frac: 0.8092\n",
      "\tEpoch: 295/1000; 1.74 sec... \tStep: 71685... \tLoss: 0.962... \tVal Loss: 0.589 \tVal Frac: 0.8011\n",
      "\tEpoch: 300/1000; 1.49 sec... \tStep: 72900... \tLoss: 0.855... \tVal Loss: 0.600 \tVal Frac: 0.8015\n",
      "\tEpoch: 305/1000; 1.33 sec... \tStep: 74115... \tLoss: 0.509... \tVal Loss: 0.620 \tVal Frac: 0.8029\n",
      "\tEpoch: 310/1000; 1.36 sec... \tStep: 75330... \tLoss: 0.797... \tVal Loss: 0.603 \tVal Frac: 0.8011\n",
      "\tEpoch: 315/1000; 1.49 sec... \tStep: 76545... \tLoss: 0.653... \tVal Loss: 0.573 \tVal Frac: 0.8088\n",
      "\tEpoch: 320/1000; 1.49 sec... \tStep: 77760... \tLoss: 0.797... \tVal Loss: 0.614 \tVal Frac: 0.7904\n",
      "\tEpoch: 325/1000; 1.46 sec... \tStep: 78975... \tLoss: 0.635... \tVal Loss: 0.590 \tVal Frac: 0.8011\n",
      "\tEpoch: 330/1000; 1.42 sec... \tStep: 80190... \tLoss: 0.657... \tVal Loss: 0.582 \tVal Frac: 0.7982\n",
      "\tEpoch: 335/1000; 1.59 sec... \tStep: 81405... \tLoss: 0.742... \tVal Loss: 0.579 \tVal Frac: 0.8132\n",
      "\tEpoch: 340/1000; 1.36 sec... \tStep: 82620... \tLoss: 0.909... \tVal Loss: 0.576 \tVal Frac: 0.8099\n",
      "\tEpoch: 345/1000; 1.54 sec... \tStep: 83835... \tLoss: 0.602... \tVal Loss: 0.573 \tVal Frac: 0.8151\n",
      "\tEpoch: 350/1000; 1.40 sec... \tStep: 85050... \tLoss: 0.473... \tVal Loss: 0.569 \tVal Frac: 0.8217\n",
      "\tEpoch: 355/1000; 1.34 sec... \tStep: 86265... \tLoss: 0.843... \tVal Loss: 0.576 \tVal Frac: 0.8162\n",
      "\tEpoch: 360/1000; 1.40 sec... \tStep: 87480... \tLoss: 0.329... \tVal Loss: 0.568 \tVal Frac: 0.8184\n",
      "\tValidation loss decreased (0.569 --> 0.568).  Saving model ...\n",
      "\tEpoch: 365/1000; 1.34 sec... \tStep: 88695... \tLoss: 0.754... \tVal Loss: 0.577 \tVal Frac: 0.8158\n",
      "\tEpoch: 370/1000; 1.74 sec... \tStep: 89910... \tLoss: 0.699... \tVal Loss: 0.575 \tVal Frac: 0.8140\n",
      "\tEpoch: 375/1000; 1.48 sec... \tStep: 91125... \tLoss: 1.216... \tVal Loss: 0.607 \tVal Frac: 0.8022\n",
      "\tEpoch: 380/1000; 1.37 sec... \tStep: 92340... \tLoss: 0.420... \tVal Loss: 0.611 \tVal Frac: 0.8022\n",
      "\tEpoch: 385/1000; 1.50 sec... \tStep: 93555... \tLoss: 0.546... \tVal Loss: 0.577 \tVal Frac: 0.8165\n",
      "\tEpoch: 390/1000; 1.45 sec... \tStep: 94770... \tLoss: 0.804... \tVal Loss: 0.586 \tVal Frac: 0.8063\n",
      "\tEpoch: 395/1000; 1.59 sec... \tStep: 95985... \tLoss: 0.760... \tVal Loss: 0.607 \tVal Frac: 0.7897\n",
      "\tEpoch: 400/1000; 1.47 sec... \tStep: 97200... \tLoss: 0.501... \tVal Loss: 0.597 \tVal Frac: 0.7937\n",
      "\tEpoch: 405/1000; 1.46 sec... \tStep: 98415... \tLoss: 0.317... \tVal Loss: 0.601 \tVal Frac: 0.7989\n",
      "\tEpoch: 410/1000; 1.47 sec... \tStep: 99630... \tLoss: 0.608... \tVal Loss: 0.616 \tVal Frac: 0.7949\n",
      "\tEpoch: 415/1000; 1.40 sec... \tStep: 100845... \tLoss: 1.029... \tVal Loss: 0.564 \tVal Frac: 0.8246\n",
      "\tValidation loss decreased (0.568 --> 0.564).  Saving model ...\n",
      "\tEpoch: 420/1000; 1.39 sec... \tStep: 102060... \tLoss: 0.787... \tVal Loss: 0.613 \tVal Frac: 0.8007\n",
      "\tEpoch: 425/1000; 1.56 sec... \tStep: 103275... \tLoss: 1.039... \tVal Loss: 0.589 \tVal Frac: 0.8026\n",
      "\tEpoch: 430/1000; 1.36 sec... \tStep: 104490... \tLoss: 0.704... \tVal Loss: 0.582 \tVal Frac: 0.8129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 435/1000; 1.40 sec... \tStep: 105705... \tLoss: 0.838... \tVal Loss: 0.577 \tVal Frac: 0.8169\n",
      "\tEpoch: 440/1000; 1.55 sec... \tStep: 106920... \tLoss: 0.601... \tVal Loss: 0.586 \tVal Frac: 0.8015\n",
      "\tEpoch: 445/1000; 1.54 sec... \tStep: 108135... \tLoss: 0.906... \tVal Loss: 0.566 \tVal Frac: 0.8206\n",
      "\tEpoch: 450/1000; 1.43 sec... \tStep: 109350... \tLoss: 0.660... \tVal Loss: 0.582 \tVal Frac: 0.8121\n",
      "\tEpoch: 455/1000; 1.60 sec... \tStep: 110565... \tLoss: 0.767... \tVal Loss: 0.574 \tVal Frac: 0.8210\n",
      "\tEpoch: 460/1000; 1.37 sec... \tStep: 111780... \tLoss: 0.928... \tVal Loss: 0.571 \tVal Frac: 0.8224\n",
      "\tEpoch: 465/1000; 1.49 sec... \tStep: 112995... \tLoss: 0.509... \tVal Loss: 0.584 \tVal Frac: 0.8033\n",
      "\tEpoch: 470/1000; 1.33 sec... \tStep: 114210... \tLoss: 0.547... \tVal Loss: 0.608 \tVal Frac: 0.8070\n",
      "\tEpoch: 475/1000; 1.55 sec... \tStep: 115425... \tLoss: 0.706... \tVal Loss: 0.588 \tVal Frac: 0.8114\n",
      "\tEpoch: 480/1000; 1.50 sec... \tStep: 116640... \tLoss: 0.643... \tVal Loss: 0.570 \tVal Frac: 0.8118\n",
      "\tEpoch: 485/1000; 1.37 sec... \tStep: 117855... \tLoss: 0.548... \tVal Loss: 0.578 \tVal Frac: 0.8154\n",
      "\tEpoch: 490/1000; 1.39 sec... \tStep: 119070... \tLoss: 0.781... \tVal Loss: 0.601 \tVal Frac: 0.8070\n",
      "\tEpoch: 495/1000; 1.32 sec... \tStep: 120285... \tLoss: 0.377... \tVal Loss: 0.572 \tVal Frac: 0.8132\n",
      "\tEpoch: 500/1000; 1.39 sec... \tStep: 121500... \tLoss: 0.704... \tVal Loss: 0.583 \tVal Frac: 0.7996\n",
      "\tEpoch: 505/1000; 1.52 sec... \tStep: 122715... \tLoss: 0.476... \tVal Loss: 0.568 \tVal Frac: 0.8176\n",
      "\tEpoch: 510/1000; 1.67 sec... \tStep: 123930... \tLoss: 0.485... \tVal Loss: 0.560 \tVal Frac: 0.8136\n",
      "\tValidation loss decreased (0.564 --> 0.560).  Saving model ...\n",
      "\tEpoch: 515/1000; 1.57 sec... \tStep: 125145... \tLoss: 0.996... \tVal Loss: 0.580 \tVal Frac: 0.8099\n",
      "\tEpoch: 520/1000; 1.58 sec... \tStep: 126360... \tLoss: 0.847... \tVal Loss: 0.569 \tVal Frac: 0.8143\n",
      "\tEpoch: 525/1000; 1.35 sec... \tStep: 127575... \tLoss: 0.602... \tVal Loss: 0.586 \tVal Frac: 0.8118\n",
      "\tEpoch: 530/1000; 1.42 sec... \tStep: 128790... \tLoss: 0.710... \tVal Loss: 0.594 \tVal Frac: 0.8176\n",
      "\tEpoch: 535/1000; 1.34 sec... \tStep: 130005... \tLoss: 0.749... \tVal Loss: 0.587 \tVal Frac: 0.8180\n",
      "\tEpoch: 540/1000; 1.37 sec... \tStep: 131220... \tLoss: 0.351... \tVal Loss: 0.588 \tVal Frac: 0.8077\n",
      "\tEpoch: 545/1000; 1.40 sec... \tStep: 132435... \tLoss: 0.616... \tVal Loss: 0.573 \tVal Frac: 0.8081\n",
      "\tEpoch: 550/1000; 1.40 sec... \tStep: 133650... \tLoss: 0.510... \tVal Loss: 0.594 \tVal Frac: 0.8176\n",
      "\tEpoch: 555/1000; 1.51 sec... \tStep: 134865... \tLoss: 0.662... \tVal Loss: 0.580 \tVal Frac: 0.8051\n",
      "\tEpoch: 560/1000; 1.40 sec... \tStep: 136080... \tLoss: 0.473... \tVal Loss: 0.550 \tVal Frac: 0.8169\n",
      "\tValidation loss decreased (0.560 --> 0.550).  Saving model ...\n",
      "\tEpoch: 565/1000; 1.31 sec... \tStep: 137295... \tLoss: 0.425... \tVal Loss: 0.551 \tVal Frac: 0.8261\n",
      "\tEpoch: 570/1000; 1.42 sec... \tStep: 138510... \tLoss: 0.674... \tVal Loss: 0.572 \tVal Frac: 0.8184\n",
      "\tEpoch: 575/1000; 1.35 sec... \tStep: 139725... \tLoss: 0.626... \tVal Loss: 0.559 \tVal Frac: 0.8147\n",
      "\tEpoch: 580/1000; 1.38 sec... \tStep: 140940... \tLoss: 0.772... \tVal Loss: 0.565 \tVal Frac: 0.8140\n",
      "\tEpoch: 585/1000; 1.48 sec... \tStep: 142155... \tLoss: 0.633... \tVal Loss: 0.579 \tVal Frac: 0.8114\n",
      "\tEpoch: 590/1000; 1.40 sec... \tStep: 143370... \tLoss: 0.518... \tVal Loss: 0.578 \tVal Frac: 0.8085\n",
      "\tEpoch: 595/1000; 1.46 sec... \tStep: 144585... \tLoss: 0.627... \tVal Loss: 0.559 \tVal Frac: 0.8136\n",
      "\tEpoch: 600/1000; 1.32 sec... \tStep: 145800... \tLoss: 0.471... \tVal Loss: 0.598 \tVal Frac: 0.8018\n",
      "\tEpoch: 605/1000; 1.42 sec... \tStep: 147015... \tLoss: 0.720... \tVal Loss: 0.589 \tVal Frac: 0.8026\n",
      "\tEpoch: 610/1000; 1.60 sec... \tStep: 148230... \tLoss: 0.539... \tVal Loss: 0.573 \tVal Frac: 0.8136\n",
      "\tEpoch: 615/1000; 1.40 sec... \tStep: 149445... \tLoss: 0.719... \tVal Loss: 0.586 \tVal Frac: 0.8162\n",
      "\tEpoch: 620/1000; 1.45 sec... \tStep: 150660... \tLoss: 0.970... \tVal Loss: 0.576 \tVal Frac: 0.8129\n",
      "\tEpoch: 625/1000; 1.39 sec... \tStep: 151875... \tLoss: 0.670... \tVal Loss: 0.592 \tVal Frac: 0.8096\n",
      "\tEpoch: 630/1000; 1.40 sec... \tStep: 153090... \tLoss: 0.544... \tVal Loss: 0.616 \tVal Frac: 0.7879\n",
      "\tEpoch: 635/1000; 1.52 sec... \tStep: 154305... \tLoss: 0.615... \tVal Loss: 0.586 \tVal Frac: 0.8059\n",
      "\tEpoch: 640/1000; 1.31 sec... \tStep: 155520... \tLoss: 0.563... \tVal Loss: 0.584 \tVal Frac: 0.8004\n",
      "\tEpoch: 645/1000; 1.50 sec... \tStep: 156735... \tLoss: 0.734... \tVal Loss: 0.571 \tVal Frac: 0.7971\n",
      "\tEpoch: 650/1000; 1.47 sec... \tStep: 157950... \tLoss: 0.176... \tVal Loss: 0.597 \tVal Frac: 0.7934\n",
      "\tEpoch: 655/1000; 1.29 sec... \tStep: 159165... \tLoss: 0.989... \tVal Loss: 0.566 \tVal Frac: 0.8151\n",
      "\tEpoch: 660/1000; 1.42 sec... \tStep: 160380... \tLoss: 0.728... \tVal Loss: 0.594 \tVal Frac: 0.8092\n",
      "\tEpoch: 665/1000; 1.45 sec... \tStep: 161595... \tLoss: 0.944... \tVal Loss: 0.588 \tVal Frac: 0.8018\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 0.894... \tVal Loss: 0.588 \tVal Frac: 0.8063\n",
      "\tEpoch: 675/1000; 1.30 sec... \tStep: 164025... \tLoss: 0.702... \tVal Loss: 0.605 \tVal Frac: 0.8074\n",
      "\tEpoch: 680/1000; 1.33 sec... \tStep: 165240... \tLoss: 0.454... \tVal Loss: 0.576 \tVal Frac: 0.8096\n",
      "\tEpoch: 685/1000; 1.43 sec... \tStep: 166455... \tLoss: 0.602... \tVal Loss: 0.595 \tVal Frac: 0.8048\n",
      "\tEpoch: 690/1000; 1.40 sec... \tStep: 167670... \tLoss: 0.759... \tVal Loss: 0.583 \tVal Frac: 0.8040\n",
      "\tEpoch: 695/1000; 1.39 sec... \tStep: 168885... \tLoss: 1.006... \tVal Loss: 0.592 \tVal Frac: 0.8004\n",
      "\tEpoch: 700/1000; 1.37 sec... \tStep: 170100... \tLoss: 0.768... \tVal Loss: 0.605 \tVal Frac: 0.7993\n",
      "\tEpoch: 705/1000; 1.37 sec... \tStep: 171315... \tLoss: 0.405... \tVal Loss: 0.578 \tVal Frac: 0.8026\n",
      "\tEpoch: 710/1000; 1.54 sec... \tStep: 172530... \tLoss: 0.920... \tVal Loss: 0.591 \tVal Frac: 0.7937\n",
      "\tEpoch: 715/1000; 1.40 sec... \tStep: 173745... \tLoss: 0.617... \tVal Loss: 0.575 \tVal Frac: 0.8088\n",
      "\tEpoch: 720/1000; 1.29 sec... \tStep: 174960... \tLoss: 0.866... \tVal Loss: 0.571 \tVal Frac: 0.8118\n",
      "\tEpoch: 725/1000; 1.41 sec... \tStep: 176175... \tLoss: 0.543... \tVal Loss: 0.577 \tVal Frac: 0.8059\n",
      "\tEpoch: 730/1000; 1.36 sec... \tStep: 177390... \tLoss: 0.946... \tVal Loss: 0.592 \tVal Frac: 0.8040\n",
      "\tEpoch: 735/1000; 1.34 sec... \tStep: 178605... \tLoss: 0.788... \tVal Loss: 0.577 \tVal Frac: 0.8213\n",
      "\tEpoch: 740/1000; 1.47 sec... \tStep: 179820... \tLoss: 0.587... \tVal Loss: 0.597 \tVal Frac: 0.7926\n",
      "\tEpoch: 745/1000; 1.37 sec... \tStep: 181035... \tLoss: 0.941... \tVal Loss: 0.574 \tVal Frac: 0.8143\n",
      "\tEpoch: 750/1000; 1.38 sec... \tStep: 182250... \tLoss: 0.562... \tVal Loss: 0.562 \tVal Frac: 0.8276\n",
      "\tEpoch: 755/1000; 1.29 sec... \tStep: 183465... \tLoss: 0.688... \tVal Loss: 0.565 \tVal Frac: 0.8125\n",
      "\tEpoch: 760/1000; 1.59 sec... \tStep: 184680... \tLoss: 0.548... \tVal Loss: 0.594 \tVal Frac: 0.8033\n",
      "\tEpoch: 765/1000; 1.28 sec... \tStep: 185895... \tLoss: 0.663... \tVal Loss: 0.569 \tVal Frac: 0.8022\n",
      "\tEpoch: 770/1000; 1.40 sec... \tStep: 187110... \tLoss: 0.635... \tVal Loss: 0.575 \tVal Frac: 0.8092\n",
      "\tEpoch: 775/1000; 1.45 sec... \tStep: 188325... \tLoss: 0.632... \tVal Loss: 0.562 \tVal Frac: 0.8099\n",
      "\tEpoch: 780/1000; 1.47 sec... \tStep: 189540... \tLoss: 0.857... \tVal Loss: 0.584 \tVal Frac: 0.8007\n",
      "\tEpoch: 785/1000; 1.40 sec... \tStep: 190755... \tLoss: 0.981... \tVal Loss: 0.572 \tVal Frac: 0.8136\n",
      "\tEpoch: 790/1000; 1.37 sec... \tStep: 191970... \tLoss: 0.715... \tVal Loss: 0.574 \tVal Frac: 0.8162\n",
      "\tEpoch: 795/1000; 1.42 sec... \tStep: 193185... \tLoss: 0.896... \tVal Loss: 0.569 \tVal Frac: 0.8210\n",
      "\tEpoch: 800/1000; 1.47 sec... \tStep: 194400... \tLoss: 0.364... \tVal Loss: 0.568 \tVal Frac: 0.8132\n",
      "\tEpoch: 805/1000; 1.44 sec... \tStep: 195615... \tLoss: 0.830... \tVal Loss: 0.546 \tVal Frac: 0.8279\n",
      "\tValidation loss decreased (0.550 --> 0.546).  Saving model ...\n",
      "\tEpoch: 810/1000; 1.55 sec... \tStep: 196830... \tLoss: 0.853... \tVal Loss: 0.565 \tVal Frac: 0.8129\n",
      "\tEpoch: 815/1000; 1.35 sec... \tStep: 198045... \tLoss: 0.530... \tVal Loss: 0.587 \tVal Frac: 0.8018\n",
      "\tEpoch: 820/1000; 1.43 sec... \tStep: 199260... \tLoss: 0.855... \tVal Loss: 0.618 \tVal Frac: 0.8151\n",
      "\tEpoch: 825/1000; 1.43 sec... \tStep: 200475... \tLoss: 0.623... \tVal Loss: 0.612 \tVal Frac: 0.8026\n",
      "\tEpoch: 830/1000; 1.43 sec... \tStep: 201690... \tLoss: 0.813... \tVal Loss: 0.574 \tVal Frac: 0.8110\n",
      "\tEpoch: 835/1000; 1.47 sec... \tStep: 202905... \tLoss: 0.550... \tVal Loss: 0.573 \tVal Frac: 0.7934\n",
      "\tEpoch: 840/1000; 1.43 sec... \tStep: 204120... \tLoss: 0.948... \tVal Loss: 0.574 \tVal Frac: 0.7989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 845/1000; 1.47 sec... \tStep: 205335... \tLoss: 0.844... \tVal Loss: 0.598 \tVal Frac: 0.7934\n",
      "\tEpoch: 850/1000; 1.47 sec... \tStep: 206550... \tLoss: 0.705... \tVal Loss: 0.588 \tVal Frac: 0.8004\n",
      "\tEpoch: 855/1000; 1.53 sec... \tStep: 207765... \tLoss: 0.764... \tVal Loss: 0.587 \tVal Frac: 0.7971\n",
      "\tEpoch: 860/1000; 1.33 sec... \tStep: 208980... \tLoss: 0.672... \tVal Loss: 0.587 \tVal Frac: 0.7967\n",
      "\tEpoch: 865/1000; 1.46 sec... \tStep: 210195... \tLoss: 0.388... \tVal Loss: 0.585 \tVal Frac: 0.7978\n",
      "\tEpoch: 870/1000; 1.36 sec... \tStep: 211410... \tLoss: 0.898... \tVal Loss: 0.587 \tVal Frac: 0.7996\n",
      "\tEpoch: 875/1000; 1.50 sec... \tStep: 212625... \tLoss: 0.483... \tVal Loss: 0.588 \tVal Frac: 0.8151\n",
      "\tEpoch: 880/1000; 1.52 sec... \tStep: 213840... \tLoss: 0.655... \tVal Loss: 0.599 \tVal Frac: 0.7945\n",
      "\tEpoch: 885/1000; 1.25 sec... \tStep: 215055... \tLoss: 0.558... \tVal Loss: 0.571 \tVal Frac: 0.8217\n",
      "\tEpoch: 890/1000; 1.50 sec... \tStep: 216270... \tLoss: 0.585... \tVal Loss: 0.567 \tVal Frac: 0.8162\n",
      "\tEpoch: 895/1000; 1.42 sec... \tStep: 217485... \tLoss: 0.796... \tVal Loss: 0.583 \tVal Frac: 0.8048\n",
      "\tEpoch: 900/1000; 1.40 sec... \tStep: 218700... \tLoss: 0.836... \tVal Loss: 0.590 \tVal Frac: 0.8015\n",
      "\tEpoch: 905/1000; 1.49 sec... \tStep: 219915... \tLoss: 0.586... \tVal Loss: 0.558 \tVal Frac: 0.8147\n",
      "\tEpoch: 910/1000; 1.49 sec... \tStep: 221130... \tLoss: 0.547... \tVal Loss: 0.560 \tVal Frac: 0.8272\n",
      "\tEpoch: 915/1000; 1.42 sec... \tStep: 222345... \tLoss: 0.505... \tVal Loss: 0.557 \tVal Frac: 0.8191\n",
      "\tEpoch: 920/1000; 1.29 sec... \tStep: 223560... \tLoss: 0.679... \tVal Loss: 0.565 \tVal Frac: 0.8213\n",
      "\tEpoch: 925/1000; 1.50 sec... \tStep: 224775... \tLoss: 0.561... \tVal Loss: 0.602 \tVal Frac: 0.8026\n",
      "\tEpoch: 930/1000; 1.47 sec... \tStep: 225990... \tLoss: 0.882... \tVal Loss: 0.575 \tVal Frac: 0.7824\n",
      "\tEpoch: 935/1000; 1.44 sec... \tStep: 227205... \tLoss: 0.703... \tVal Loss: 0.593 \tVal Frac: 0.8007\n",
      "\tEpoch: 940/1000; 1.40 sec... \tStep: 228420... \tLoss: 0.465... \tVal Loss: 0.564 \tVal Frac: 0.8154\n",
      "\tEpoch: 945/1000; 1.49 sec... \tStep: 229635... \tLoss: 0.298... \tVal Loss: 0.587 \tVal Frac: 0.8048\n",
      "\tEpoch: 950/1000; 1.49 sec... \tStep: 230850... \tLoss: 0.718... \tVal Loss: 0.570 \tVal Frac: 0.8206\n",
      "\tEpoch: 955/1000; 1.40 sec... \tStep: 232065... \tLoss: 0.455... \tVal Loss: 0.572 \tVal Frac: 0.8213\n",
      "\tEpoch: 960/1000; 1.29 sec... \tStep: 233280... \tLoss: 0.447... \tVal Loss: 0.581 \tVal Frac: 0.8103\n",
      "\tEpoch: 965/1000; 1.45 sec... \tStep: 234495... \tLoss: 0.513... \tVal Loss: 0.553 \tVal Frac: 0.8257\n",
      "\tEpoch: 970/1000; 1.37 sec... \tStep: 235710... \tLoss: 0.338... \tVal Loss: 0.566 \tVal Frac: 0.8243\n",
      "\tEpoch: 975/1000; 1.46 sec... \tStep: 236925... \tLoss: 0.579... \tVal Loss: 0.566 \tVal Frac: 0.8162\n",
      "\tEpoch: 980/1000; 1.49 sec... \tStep: 238140... \tLoss: 0.596... \tVal Loss: 0.563 \tVal Frac: 0.8184\n",
      "\tEpoch: 985/1000; 1.42 sec... \tStep: 239355... \tLoss: 0.443... \tVal Loss: 0.554 \tVal Frac: 0.8184\n",
      "\tEpoch: 990/1000; 1.41 sec... \tStep: 240570... \tLoss: 0.382... \tVal Loss: 0.553 \tVal Frac: 0.8239\n",
      "\tEpoch: 995/1000; 1.47 sec... \tStep: 241785... \tLoss: 0.733... \tVal Loss: 0.547 \tVal Frac: 0.8268\n",
      "\tEpoch: 1000/1000; 1.33 sec... \tStep: 243000... \tLoss: 1.078... \tVal Loss: 0.545 \tVal Frac: 0.8246\n",
      "\tValidation loss decreased (0.546 --> 0.545).  Saving model ...\n",
      "Completed:  hidden_dim :  100 \n",
      "\tloss: 0.545355 \n",
      "\tfrac:0.827941\n",
      "\n",
      "###########\n",
      "Testing with  n_layers_LSTM :  2\n",
      "\tEpoch: 5/1000; 1.93 sec... \tStep: 1215... \tLoss: 0.577... \tVal Loss: 0.522 \tVal Frac: 0.8449\n",
      "\tValidation loss decreased (inf --> 0.522).  Saving model ...\n",
      "\tEpoch: 10/1000; 2.12 sec... \tStep: 2430... \tLoss: 0.700... \tVal Loss: 0.486 \tVal Frac: 0.8544\n",
      "\tValidation loss decreased (0.522 --> 0.486).  Saving model ...\n",
      "\tEpoch: 15/1000; 2.07 sec... \tStep: 3645... \tLoss: 0.638... \tVal Loss: 0.471 \tVal Frac: 0.8596\n",
      "\tValidation loss decreased (0.486 --> 0.471).  Saving model ...\n",
      "\tEpoch: 20/1000; 2.02 sec... \tStep: 4860... \tLoss: 0.530... \tVal Loss: 0.462 \tVal Frac: 0.8596\n",
      "\tValidation loss decreased (0.471 --> 0.462).  Saving model ...\n",
      "\tEpoch: 25/1000; 2.00 sec... \tStep: 6075... \tLoss: 0.480... \tVal Loss: 0.415 \tVal Frac: 0.8702\n",
      "\tValidation loss decreased (0.462 --> 0.415).  Saving model ...\n",
      "\tEpoch: 30/1000; 2.00 sec... \tStep: 7290... \tLoss: 0.296... \tVal Loss: 0.434 \tVal Frac: 0.8710\n",
      "\tEpoch: 35/1000; 2.07 sec... \tStep: 8505... \tLoss: 0.273... \tVal Loss: 0.406 \tVal Frac: 0.8754\n",
      "\tValidation loss decreased (0.415 --> 0.406).  Saving model ...\n",
      "\tEpoch: 40/1000; 2.06 sec... \tStep: 9720... \tLoss: 0.208... \tVal Loss: 0.473 \tVal Frac: 0.8706\n",
      "\tEpoch: 45/1000; 2.04 sec... \tStep: 10935... \tLoss: 0.227... \tVal Loss: 0.439 \tVal Frac: 0.8614\n",
      "\tEpoch: 50/1000; 2.13 sec... \tStep: 12150... \tLoss: 0.255... \tVal Loss: 0.433 \tVal Frac: 0.8621\n",
      "\tEpoch: 55/1000; 2.09 sec... \tStep: 13365... \tLoss: 1.118... \tVal Loss: 0.408 \tVal Frac: 0.8779\n",
      "\tEpoch: 60/1000; 2.00 sec... \tStep: 14580... \tLoss: 0.580... \tVal Loss: 0.426 \tVal Frac: 0.8743\n",
      "\tEpoch: 65/1000; 2.03 sec... \tStep: 15795... \tLoss: 0.336... \tVal Loss: 0.420 \tVal Frac: 0.8864\n",
      "\tEpoch: 70/1000; 2.02 sec... \tStep: 17010... \tLoss: 0.209... \tVal Loss: 0.415 \tVal Frac: 0.8761\n",
      "\tEpoch: 75/1000; 2.11 sec... \tStep: 18225... \tLoss: 0.484... \tVal Loss: 0.400 \tVal Frac: 0.8746\n",
      "\tValidation loss decreased (0.406 --> 0.400).  Saving model ...\n",
      "\tEpoch: 80/1000; 2.14 sec... \tStep: 19440... \tLoss: 0.620... \tVal Loss: 0.440 \tVal Frac: 0.8721\n",
      "\tEpoch: 85/1000; 2.06 sec... \tStep: 20655... \tLoss: 0.220... \tVal Loss: 0.415 \tVal Frac: 0.8783\n",
      "\tEpoch: 90/1000; 2.06 sec... \tStep: 21870... \tLoss: 0.395... \tVal Loss: 0.460 \tVal Frac: 0.8680\n",
      "\tEpoch: 95/1000; 1.96 sec... \tStep: 23085... \tLoss: 0.077... \tVal Loss: 0.463 \tVal Frac: 0.8662\n",
      "\tEpoch: 100/1000; 1.95 sec... \tStep: 24300... \tLoss: 0.421... \tVal Loss: 0.406 \tVal Frac: 0.8809\n",
      "\tEpoch: 105/1000; 1.97 sec... \tStep: 25515... \tLoss: 0.180... \tVal Loss: 0.404 \tVal Frac: 0.8779\n",
      "\tEpoch: 110/1000; 2.08 sec... \tStep: 26730... \tLoss: 0.658... \tVal Loss: 0.456 \tVal Frac: 0.8643\n",
      "\tEpoch: 115/1000; 2.10 sec... \tStep: 27945... \tLoss: 0.392... \tVal Loss: 0.447 \tVal Frac: 0.8614\n",
      "\tEpoch: 120/1000; 2.00 sec... \tStep: 29160... \tLoss: 0.386... \tVal Loss: 0.438 \tVal Frac: 0.8662\n",
      "\tEpoch: 125/1000; 2.00 sec... \tStep: 30375... \tLoss: 0.463... \tVal Loss: 0.536 \tVal Frac: 0.8265\n",
      "\tEpoch: 130/1000; 2.05 sec... \tStep: 31590... \tLoss: 0.157... \tVal Loss: 0.529 \tVal Frac: 0.8515\n",
      "\tEpoch: 135/1000; 1.97 sec... \tStep: 32805... \tLoss: 0.423... \tVal Loss: 0.483 \tVal Frac: 0.8548\n",
      "\tEpoch: 140/1000; 2.07 sec... \tStep: 34020... \tLoss: 0.548... \tVal Loss: 0.439 \tVal Frac: 0.8581\n",
      "\tEpoch: 145/1000; 1.98 sec... \tStep: 35235... \tLoss: 0.656... \tVal Loss: 0.567 \tVal Frac: 0.8246\n",
      "\tEpoch: 150/1000; 2.12 sec... \tStep: 36450... \tLoss: 0.888... \tVal Loss: 0.546 \tVal Frac: 0.8287\n",
      "\tEpoch: 155/1000; 2.11 sec... \tStep: 37665... \tLoss: 0.543... \tVal Loss: 0.548 \tVal Frac: 0.8515\n",
      "\tEpoch: 160/1000; 2.08 sec... \tStep: 38880... \tLoss: 0.634... \tVal Loss: 0.530 \tVal Frac: 0.8430\n",
      "\tEpoch: 165/1000; 2.07 sec... \tStep: 40095... \tLoss: 0.403... \tVal Loss: 0.466 \tVal Frac: 0.8618\n",
      "\tEpoch: 170/1000; 2.00 sec... \tStep: 41310... \tLoss: 0.421... \tVal Loss: 0.448 \tVal Frac: 0.8651\n",
      "\tEpoch: 175/1000; 2.04 sec... \tStep: 42525... \tLoss: 0.500... \tVal Loss: 0.462 \tVal Frac: 0.8551\n",
      "\tEpoch: 180/1000; 2.03 sec... \tStep: 43740... \tLoss: 0.375... \tVal Loss: 0.484 \tVal Frac: 0.8526\n",
      "\tEpoch: 185/1000; 1.97 sec... \tStep: 44955... \tLoss: 0.703... \tVal Loss: 0.444 \tVal Frac: 0.8721\n",
      "\tEpoch: 190/1000; 2.08 sec... \tStep: 46170... \tLoss: 0.235... \tVal Loss: 0.406 \tVal Frac: 0.8761\n",
      "\tEpoch: 195/1000; 1.97 sec... \tStep: 47385... \tLoss: 0.318... \tVal Loss: 0.433 \tVal Frac: 0.8739\n",
      "\tEpoch: 200/1000; 2.06 sec... \tStep: 48600... \tLoss: 0.312... \tVal Loss: 0.441 \tVal Frac: 0.8706\n",
      "\tEpoch: 205/1000; 1.99 sec... \tStep: 49815... \tLoss: 0.524... \tVal Loss: 0.441 \tVal Frac: 0.8746\n",
      "\tEpoch: 210/1000; 1.99 sec... \tStep: 51030... \tLoss: 0.602... \tVal Loss: 0.475 \tVal Frac: 0.8721\n",
      "\tEpoch: 215/1000; 2.07 sec... \tStep: 52245... \tLoss: 0.865... \tVal Loss: 0.500 \tVal Frac: 0.8518\n",
      "\tEpoch: 220/1000; 2.06 sec... \tStep: 53460... \tLoss: 0.421... \tVal Loss: 0.559 \tVal Frac: 0.8426\n",
      "\tEpoch: 225/1000; 2.19 sec... \tStep: 54675... \tLoss: 0.240... \tVal Loss: 0.501 \tVal Frac: 0.8507\n",
      "\tEpoch: 230/1000; 1.93 sec... \tStep: 55890... \tLoss: 0.743... \tVal Loss: 0.469 \tVal Frac: 0.8640\n",
      "\tEpoch: 235/1000; 2.05 sec... \tStep: 57105... \tLoss: 0.676... \tVal Loss: 0.442 \tVal Frac: 0.8688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 240/1000; 2.11 sec... \tStep: 58320... \tLoss: 0.322... \tVal Loss: 0.454 \tVal Frac: 0.8732\n",
      "\tEpoch: 245/1000; 2.00 sec... \tStep: 59535... \tLoss: 0.388... \tVal Loss: 0.463 \tVal Frac: 0.8629\n",
      "\tEpoch: 250/1000; 2.12 sec... \tStep: 60750... \tLoss: 0.749... \tVal Loss: 0.499 \tVal Frac: 0.8574\n",
      "\tEpoch: 255/1000; 2.21 sec... \tStep: 61965... \tLoss: 0.437... \tVal Loss: 0.487 \tVal Frac: 0.8581\n",
      "\tEpoch: 260/1000; 1.94 sec... \tStep: 63180... \tLoss: 0.376... \tVal Loss: 0.452 \tVal Frac: 0.8695\n",
      "\tEpoch: 265/1000; 2.05 sec... \tStep: 64395... \tLoss: 0.403... \tVal Loss: 0.439 \tVal Frac: 0.8621\n",
      "\tEpoch: 270/1000; 2.07 sec... \tStep: 65610... \tLoss: 0.819... \tVal Loss: 0.505 \tVal Frac: 0.8382\n",
      "\tEpoch: 275/1000; 1.98 sec... \tStep: 66825... \tLoss: 0.661... \tVal Loss: 0.492 \tVal Frac: 0.8570\n",
      "\tEpoch: 280/1000; 1.98 sec... \tStep: 68040... \tLoss: 0.474... \tVal Loss: 0.504 \tVal Frac: 0.8397\n",
      "\tEpoch: 285/1000; 1.97 sec... \tStep: 69255... \tLoss: 0.491... \tVal Loss: 0.451 \tVal Frac: 0.8629\n",
      "\tEpoch: 290/1000; 2.00 sec... \tStep: 70470... \tLoss: 0.382... \tVal Loss: 0.441 \tVal Frac: 0.8596\n",
      "\tEpoch: 295/1000; 2.15 sec... \tStep: 71685... \tLoss: 0.465... \tVal Loss: 0.457 \tVal Frac: 0.8596\n",
      "\tEpoch: 300/1000; 2.03 sec... \tStep: 72900... \tLoss: 0.542... \tVal Loss: 0.545 \tVal Frac: 0.8338\n",
      "\tEpoch: 305/1000; 2.09 sec... \tStep: 74115... \tLoss: 0.320... \tVal Loss: 0.494 \tVal Frac: 0.8496\n",
      "\tEpoch: 310/1000; 2.06 sec... \tStep: 75330... \tLoss: 0.468... \tVal Loss: 0.572 \tVal Frac: 0.8210\n",
      "\tEpoch: 315/1000; 2.12 sec... \tStep: 76545... \tLoss: 0.497... \tVal Loss: 0.464 \tVal Frac: 0.8544\n",
      "\tEpoch: 320/1000; 2.07 sec... \tStep: 77760... \tLoss: 0.610... \tVal Loss: 0.468 \tVal Frac: 0.8574\n",
      "\tEpoch: 325/1000; 2.11 sec... \tStep: 78975... \tLoss: 0.711... \tVal Loss: 0.567 \tVal Frac: 0.8063\n",
      "\tEpoch: 330/1000; 2.02 sec... \tStep: 80190... \tLoss: 0.670... \tVal Loss: 0.579 \tVal Frac: 0.8151\n",
      "\tEpoch: 335/1000; 2.03 sec... \tStep: 81405... \tLoss: 0.493... \tVal Loss: 0.566 \tVal Frac: 0.8272\n",
      "\tEpoch: 340/1000; 2.09 sec... \tStep: 82620... \tLoss: 0.536... \tVal Loss: 0.518 \tVal Frac: 0.8426\n",
      "\tEpoch: 345/1000; 2.02 sec... \tStep: 83835... \tLoss: 0.394... \tVal Loss: 0.522 \tVal Frac: 0.8397\n",
      "\tEpoch: 350/1000; 2.17 sec... \tStep: 85050... \tLoss: 0.428... \tVal Loss: 0.529 \tVal Frac: 0.8272\n",
      "\tEpoch: 355/1000; 2.05 sec... \tStep: 86265... \tLoss: 1.098... \tVal Loss: 0.636 \tVal Frac: 0.8129\n",
      "\tEpoch: 360/1000; 2.13 sec... \tStep: 87480... \tLoss: 0.590... \tVal Loss: 0.552 \tVal Frac: 0.8265\n",
      "\tEpoch: 365/1000; 2.05 sec... \tStep: 88695... \tLoss: 0.646... \tVal Loss: 0.586 \tVal Frac: 0.8199\n",
      "\tEpoch: 370/1000; 2.15 sec... \tStep: 89910... \tLoss: 0.581... \tVal Loss: 0.641 \tVal Frac: 0.8033\n",
      "\tEpoch: 375/1000; 2.14 sec... \tStep: 91125... \tLoss: 0.735... \tVal Loss: 0.624 \tVal Frac: 0.8118\n",
      "\tEpoch: 380/1000; 2.11 sec... \tStep: 92340... \tLoss: 1.183... \tVal Loss: 0.661 \tVal Frac: 0.7945\n",
      "\tEpoch: 385/1000; 1.97 sec... \tStep: 93555... \tLoss: 0.624... \tVal Loss: 0.646 \tVal Frac: 0.8096\n",
      "\tEpoch: 390/1000; 2.14 sec... \tStep: 94770... \tLoss: 0.793... \tVal Loss: 0.666 \tVal Frac: 0.7956\n",
      "\tEpoch: 395/1000; 2.00 sec... \tStep: 95985... \tLoss: 0.532... \tVal Loss: 0.638 \tVal Frac: 0.8007\n",
      "\tEpoch: 400/1000; 2.02 sec... \tStep: 97200... \tLoss: 0.882... \tVal Loss: 0.635 \tVal Frac: 0.7989\n",
      "\tEpoch: 405/1000; 2.04 sec... \tStep: 98415... \tLoss: 0.850... \tVal Loss: 0.674 \tVal Frac: 0.7879\n",
      "\tEpoch: 410/1000; 2.07 sec... \tStep: 99630... \tLoss: 0.685... \tVal Loss: 0.690 \tVal Frac: 0.7820\n",
      "\tEpoch: 415/1000; 2.21 sec... \tStep: 100845... \tLoss: 1.144... \tVal Loss: 0.644 \tVal Frac: 0.7824\n",
      "\tEpoch: 420/1000; 2.02 sec... \tStep: 102060... \tLoss: 1.600... \tVal Loss: 0.685 \tVal Frac: 0.7757\n",
      "\tEpoch: 425/1000; 2.27 sec... \tStep: 103275... \tLoss: 0.910... \tVal Loss: 0.667 \tVal Frac: 0.7827\n",
      "\tEpoch: 430/1000; 2.10 sec... \tStep: 104490... \tLoss: 0.568... \tVal Loss: 0.692 \tVal Frac: 0.7846\n",
      "\tEpoch: 435/1000; 2.07 sec... \tStep: 105705... \tLoss: 0.690... \tVal Loss: 0.667 \tVal Frac: 0.7871\n",
      "\tEpoch: 440/1000; 1.99 sec... \tStep: 106920... \tLoss: 0.980... \tVal Loss: 0.666 \tVal Frac: 0.7960\n",
      "\tEpoch: 445/1000; 2.06 sec... \tStep: 108135... \tLoss: 0.807... \tVal Loss: 0.784 \tVal Frac: 0.7533\n",
      "\tEpoch: 450/1000; 1.95 sec... \tStep: 109350... \tLoss: 0.804... \tVal Loss: 0.776 \tVal Frac: 0.7419\n",
      "\tEpoch: 455/1000; 2.11 sec... \tStep: 110565... \tLoss: 0.699... \tVal Loss: 0.827 \tVal Frac: 0.7327\n",
      "\tEpoch: 460/1000; 2.12 sec... \tStep: 111780... \tLoss: 1.136... \tVal Loss: 0.767 \tVal Frac: 0.7482\n",
      "\tEpoch: 465/1000; 2.08 sec... \tStep: 112995... \tLoss: 1.023... \tVal Loss: 0.709 \tVal Frac: 0.7577\n",
      "\tEpoch: 470/1000; 2.04 sec... \tStep: 114210... \tLoss: 0.910... \tVal Loss: 0.689 \tVal Frac: 0.7669\n",
      "\tEpoch: 475/1000; 2.09 sec... \tStep: 115425... \tLoss: 0.770... \tVal Loss: 0.761 \tVal Frac: 0.7603\n",
      "\tEpoch: 480/1000; 2.15 sec... \tStep: 116640... \tLoss: 0.828... \tVal Loss: 0.924 \tVal Frac: 0.7140\n",
      "\tEpoch: 485/1000; 2.04 sec... \tStep: 117855... \tLoss: 1.135... \tVal Loss: 0.821 \tVal Frac: 0.7386\n",
      "\tEpoch: 490/1000; 2.02 sec... \tStep: 119070... \tLoss: 0.770... \tVal Loss: 0.790 \tVal Frac: 0.7574\n",
      "\tEpoch: 495/1000; 2.12 sec... \tStep: 120285... \tLoss: 0.735... \tVal Loss: 0.747 \tVal Frac: 0.7493\n",
      "\tEpoch: 500/1000; 1.95 sec... \tStep: 121500... \tLoss: 1.089... \tVal Loss: 0.995 \tVal Frac: 0.7007\n",
      "\tEpoch: 505/1000; 2.14 sec... \tStep: 122715... \tLoss: 1.286... \tVal Loss: 0.974 \tVal Frac: 0.6879\n",
      "\tEpoch: 510/1000; 2.24 sec... \tStep: 123930... \tLoss: 0.818... \tVal Loss: 0.844 \tVal Frac: 0.7349\n",
      "\tEpoch: 515/1000; 2.21 sec... \tStep: 125145... \tLoss: 0.846... \tVal Loss: 0.806 \tVal Frac: 0.7463\n",
      "\tEpoch: 520/1000; 2.12 sec... \tStep: 126360... \tLoss: 0.913... \tVal Loss: 0.811 \tVal Frac: 0.7301\n",
      "\tEpoch: 525/1000; 2.09 sec... \tStep: 127575... \tLoss: 0.551... \tVal Loss: 0.808 \tVal Frac: 0.7643\n",
      "\tEpoch: 530/1000; 2.14 sec... \tStep: 128790... \tLoss: 0.865... \tVal Loss: 0.780 \tVal Frac: 0.7654\n",
      "\tEpoch: 535/1000; 2.09 sec... \tStep: 130005... \tLoss: 0.588... \tVal Loss: 0.787 \tVal Frac: 0.7397\n",
      "\tEpoch: 540/1000; 2.07 sec... \tStep: 131220... \tLoss: 1.116... \tVal Loss: 0.923 \tVal Frac: 0.7140\n",
      "\tEpoch: 545/1000; 2.00 sec... \tStep: 132435... \tLoss: 0.637... \tVal Loss: 0.998 \tVal Frac: 0.6728\n",
      "\tEpoch: 550/1000; 2.22 sec... \tStep: 133650... \tLoss: 0.977... \tVal Loss: 0.968 \tVal Frac: 0.6982\n",
      "\tEpoch: 555/1000; 2.09 sec... \tStep: 134865... \tLoss: 1.056... \tVal Loss: 0.914 \tVal Frac: 0.7254\n",
      "\tEpoch: 560/1000; 2.07 sec... \tStep: 136080... \tLoss: 0.729... \tVal Loss: 0.894 \tVal Frac: 0.7125\n",
      "\tEpoch: 565/1000; 2.11 sec... \tStep: 137295... \tLoss: 1.281... \tVal Loss: 0.856 \tVal Frac: 0.7191\n",
      "\tEpoch: 570/1000; 2.17 sec... \tStep: 138510... \tLoss: 0.831... \tVal Loss: 0.777 \tVal Frac: 0.7412\n",
      "\tEpoch: 575/1000; 2.09 sec... \tStep: 139725... \tLoss: 1.621... \tVal Loss: 1.078 \tVal Frac: 0.6180\n",
      "\tEpoch: 580/1000; 2.15 sec... \tStep: 140940... \tLoss: 0.772... \tVal Loss: 0.925 \tVal Frac: 0.7037\n",
      "\tEpoch: 585/1000; 2.05 sec... \tStep: 142155... \tLoss: 1.079... \tVal Loss: 0.891 \tVal Frac: 0.7254\n",
      "\tEpoch: 590/1000; 2.12 sec... \tStep: 143370... \tLoss: 0.916... \tVal Loss: 0.864 \tVal Frac: 0.7331\n",
      "\tEpoch: 595/1000; 2.05 sec... \tStep: 144585... \tLoss: 0.667... \tVal Loss: 0.926 \tVal Frac: 0.6695\n",
      "\tEpoch: 600/1000; 2.06 sec... \tStep: 145800... \tLoss: 0.855... \tVal Loss: 0.899 \tVal Frac: 0.7250\n",
      "\tEpoch: 605/1000; 2.16 sec... \tStep: 147015... \tLoss: 1.125... \tVal Loss: 0.848 \tVal Frac: 0.7441\n",
      "\tEpoch: 610/1000; 2.35 sec... \tStep: 148230... \tLoss: 1.229... \tVal Loss: 0.772 \tVal Frac: 0.7522\n",
      "\tEpoch: 615/1000; 2.09 sec... \tStep: 149445... \tLoss: 1.295... \tVal Loss: 0.898 \tVal Frac: 0.7066\n",
      "\tEpoch: 620/1000; 2.06 sec... \tStep: 150660... \tLoss: 1.259... \tVal Loss: 0.807 \tVal Frac: 0.7342\n",
      "\tEpoch: 625/1000; 2.32 sec... \tStep: 151875... \tLoss: 1.114... \tVal Loss: 0.853 \tVal Frac: 0.7312\n",
      "\tEpoch: 630/1000; 2.09 sec... \tStep: 153090... \tLoss: 1.082... \tVal Loss: 0.961 \tVal Frac: 0.7011\n",
      "\tEpoch: 635/1000; 2.14 sec... \tStep: 154305... \tLoss: 1.283... \tVal Loss: 0.905 \tVal Frac: 0.7309\n",
      "\tEpoch: 640/1000; 2.17 sec... \tStep: 155520... \tLoss: 0.984... \tVal Loss: 0.906 \tVal Frac: 0.7375\n",
      "\tEpoch: 645/1000; 2.24 sec... \tStep: 156735... \tLoss: 1.100... \tVal Loss: 1.057 \tVal Frac: 0.6651\n",
      "\tEpoch: 650/1000; 2.04 sec... \tStep: 157950... \tLoss: 1.119... \tVal Loss: 0.854 \tVal Frac: 0.7371\n",
      "\tEpoch: 655/1000; 2.07 sec... \tStep: 159165... \tLoss: 0.707... \tVal Loss: 0.861 \tVal Frac: 0.7316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 660/1000; 2.24 sec... \tStep: 160380... \tLoss: 1.236... \tVal Loss: 0.930 \tVal Frac: 0.7107\n",
      "\tEpoch: 665/1000; 2.11 sec... \tStep: 161595... \tLoss: 0.881... \tVal Loss: 0.957 \tVal Frac: 0.6960\n",
      "\tEpoch: 670/1000; 1.97 sec... \tStep: 162810... \tLoss: 1.266... \tVal Loss: 0.982 \tVal Frac: 0.7202\n",
      "\tEpoch: 675/1000; 2.06 sec... \tStep: 164025... \tLoss: 1.088... \tVal Loss: 0.918 \tVal Frac: 0.7419\n",
      "\tEpoch: 680/1000; 1.99 sec... \tStep: 165240... \tLoss: 0.795... \tVal Loss: 0.955 \tVal Frac: 0.7158\n",
      "\tEpoch: 685/1000; 2.05 sec... \tStep: 166455... \tLoss: 0.686... \tVal Loss: 0.894 \tVal Frac: 0.7257\n",
      "\tEpoch: 690/1000; 2.09 sec... \tStep: 167670... \tLoss: 1.287... \tVal Loss: 1.011 \tVal Frac: 0.7346\n",
      "\tEpoch: 695/1000; 2.03 sec... \tStep: 168885... \tLoss: 1.220... \tVal Loss: 1.025 \tVal Frac: 0.7000\n",
      "\tEpoch: 700/1000; 2.11 sec... \tStep: 170100... \tLoss: 1.061... \tVal Loss: 1.046 \tVal Frac: 0.6360\n",
      "\tEpoch: 705/1000; 2.09 sec... \tStep: 171315... \tLoss: 1.094... \tVal Loss: 0.993 \tVal Frac: 0.6673\n",
      "\tEpoch: 710/1000; 2.42 sec... \tStep: 172530... \tLoss: 0.865... \tVal Loss: 0.898 \tVal Frac: 0.7228\n",
      "\tEpoch: 715/1000; 2.05 sec... \tStep: 173745... \tLoss: 0.902... \tVal Loss: 0.893 \tVal Frac: 0.7309\n",
      "\tEpoch: 720/1000; 2.11 sec... \tStep: 174960... \tLoss: 0.980... \tVal Loss: 0.971 \tVal Frac: 0.7074\n",
      "\tEpoch: 725/1000; 2.07 sec... \tStep: 176175... \tLoss: 0.848... \tVal Loss: 1.021 \tVal Frac: 0.7254\n",
      "\tEpoch: 730/1000; 2.19 sec... \tStep: 177390... \tLoss: 0.766... \tVal Loss: 0.942 \tVal Frac: 0.7375\n",
      "\tEpoch: 735/1000; 2.02 sec... \tStep: 178605... \tLoss: 1.034... \tVal Loss: 0.913 \tVal Frac: 0.7312\n",
      "\tEpoch: 740/1000; 2.05 sec... \tStep: 179820... \tLoss: 0.916... \tVal Loss: 0.980 \tVal Frac: 0.6974\n",
      "\tEpoch: 745/1000; 2.17 sec... \tStep: 181035... \tLoss: 0.784... \tVal Loss: 0.892 \tVal Frac: 0.7430\n",
      "\tEpoch: 750/1000; 1.97 sec... \tStep: 182250... \tLoss: 1.012... \tVal Loss: 0.892 \tVal Frac: 0.7213\n",
      "\tEpoch: 755/1000; 2.11 sec... \tStep: 183465... \tLoss: 0.993... \tVal Loss: 0.876 \tVal Frac: 0.7393\n",
      "\tEpoch: 760/1000; 2.09 sec... \tStep: 184680... \tLoss: 0.707... \tVal Loss: 0.994 \tVal Frac: 0.7114\n",
      "\tEpoch: 765/1000; 2.05 sec... \tStep: 185895... \tLoss: 1.434... \tVal Loss: 0.922 \tVal Frac: 0.7364\n",
      "\tEpoch: 770/1000; 2.15 sec... \tStep: 187110... \tLoss: 0.841... \tVal Loss: 0.859 \tVal Frac: 0.7438\n",
      "\tEpoch: 775/1000; 2.01 sec... \tStep: 188325... \tLoss: 0.750... \tVal Loss: 0.933 \tVal Frac: 0.7268\n",
      "\tEpoch: 780/1000; 2.34 sec... \tStep: 189540... \tLoss: 0.868... \tVal Loss: 0.990 \tVal Frac: 0.7107\n",
      "\tEpoch: 785/1000; 2.16 sec... \tStep: 190755... \tLoss: 1.316... \tVal Loss: 0.848 \tVal Frac: 0.7386\n",
      "\tEpoch: 790/1000; 2.09 sec... \tStep: 191970... \tLoss: 1.242... \tVal Loss: 0.836 \tVal Frac: 0.7434\n",
      "\tEpoch: 795/1000; 1.97 sec... \tStep: 193185... \tLoss: 1.218... \tVal Loss: 0.840 \tVal Frac: 0.7408\n",
      "\tEpoch: 800/1000; 2.01 sec... \tStep: 194400... \tLoss: 1.145... \tVal Loss: 0.895 \tVal Frac: 0.7379\n",
      "\tEpoch: 805/1000; 2.09 sec... \tStep: 195615... \tLoss: 1.175... \tVal Loss: 0.838 \tVal Frac: 0.7335\n",
      "\tEpoch: 810/1000; 2.07 sec... \tStep: 196830... \tLoss: 1.071... \tVal Loss: 0.837 \tVal Frac: 0.7610\n",
      "\tEpoch: 815/1000; 2.06 sec... \tStep: 198045... \tLoss: 1.075... \tVal Loss: 0.881 \tVal Frac: 0.7415\n",
      "\tEpoch: 820/1000; 2.07 sec... \tStep: 199260... \tLoss: 1.089... \tVal Loss: 0.863 \tVal Frac: 0.7482\n",
      "\tEpoch: 825/1000; 1.99 sec... \tStep: 200475... \tLoss: 0.755... \tVal Loss: 0.814 \tVal Frac: 0.7555\n",
      "\tEpoch: 830/1000; 2.55 sec... \tStep: 201690... \tLoss: 0.699... \tVal Loss: 0.842 \tVal Frac: 0.7555\n",
      "\tEpoch: 835/1000; 2.19 sec... \tStep: 202905... \tLoss: 1.135... \tVal Loss: 0.797 \tVal Frac: 0.7482\n",
      "\tEpoch: 840/1000; 2.68 sec... \tStep: 204120... \tLoss: 0.758... \tVal Loss: 0.824 \tVal Frac: 0.7610\n",
      "\tEpoch: 845/1000; 2.04 sec... \tStep: 205335... \tLoss: 0.888... \tVal Loss: 0.883 \tVal Frac: 0.7585\n",
      "\tEpoch: 850/1000; 2.12 sec... \tStep: 206550... \tLoss: 0.765... \tVal Loss: 0.864 \tVal Frac: 0.7471\n",
      "\tEpoch: 855/1000; 2.12 sec... \tStep: 207765... \tLoss: 0.700... \tVal Loss: 0.795 \tVal Frac: 0.7544\n",
      "\tEpoch: 860/1000; 2.07 sec... \tStep: 208980... \tLoss: 1.156... \tVal Loss: 0.849 \tVal Frac: 0.7327\n",
      "\tEpoch: 865/1000; 2.06 sec... \tStep: 210195... \tLoss: 1.058... \tVal Loss: 0.742 \tVal Frac: 0.7607\n",
      "\tEpoch: 870/1000; 2.11 sec... \tStep: 211410... \tLoss: 0.787... \tVal Loss: 0.740 \tVal Frac: 0.7680\n",
      "\tEpoch: 875/1000; 2.02 sec... \tStep: 212625... \tLoss: 1.215... \tVal Loss: 0.773 \tVal Frac: 0.7566\n",
      "\tEpoch: 880/1000; 2.00 sec... \tStep: 213840... \tLoss: 0.741... \tVal Loss: 0.724 \tVal Frac: 0.7695\n",
      "\tEpoch: 885/1000; 2.04 sec... \tStep: 215055... \tLoss: 1.452... \tVal Loss: 0.739 \tVal Frac: 0.7555\n",
      "\tEpoch: 890/1000; 2.14 sec... \tStep: 216270... \tLoss: 0.855... \tVal Loss: 0.743 \tVal Frac: 0.7471\n",
      "\tEpoch: 895/1000; 2.04 sec... \tStep: 217485... \tLoss: 0.773... \tVal Loss: 0.815 \tVal Frac: 0.7676\n",
      "\tEpoch: 900/1000; 2.11 sec... \tStep: 218700... \tLoss: 0.801... \tVal Loss: 0.695 \tVal Frac: 0.7739\n",
      "\tEpoch: 905/1000; 1.97 sec... \tStep: 219915... \tLoss: 0.831... \tVal Loss: 0.695 \tVal Frac: 0.7654\n",
      "\tEpoch: 910/1000; 1.96 sec... \tStep: 221130... \tLoss: 1.351... \tVal Loss: 0.696 \tVal Frac: 0.7574\n",
      "\tEpoch: 915/1000; 2.07 sec... \tStep: 222345... \tLoss: 1.206... \tVal Loss: 0.816 \tVal Frac: 0.7184\n",
      "\tEpoch: 920/1000; 2.00 sec... \tStep: 223560... \tLoss: 1.235... \tVal Loss: 0.788 \tVal Frac: 0.7346\n",
      "\tEpoch: 925/1000; 2.27 sec... \tStep: 224775... \tLoss: 1.304... \tVal Loss: 0.833 \tVal Frac: 0.7169\n",
      "\tEpoch: 930/1000; 2.14 sec... \tStep: 225990... \tLoss: 0.829... \tVal Loss: 0.845 \tVal Frac: 0.7140\n",
      "\tEpoch: 935/1000; 2.67 sec... \tStep: 227205... \tLoss: 0.994... \tVal Loss: 0.796 \tVal Frac: 0.7224\n",
      "\tEpoch: 940/1000; 2.12 sec... \tStep: 228420... \tLoss: 0.884... \tVal Loss: 0.815 \tVal Frac: 0.7088\n",
      "\tEpoch: 945/1000; 2.02 sec... \tStep: 229635... \tLoss: 1.283... \tVal Loss: 0.765 \tVal Frac: 0.7460\n",
      "\tEpoch: 950/1000; 2.11 sec... \tStep: 230850... \tLoss: 0.642... \tVal Loss: 0.783 \tVal Frac: 0.7423\n",
      "\tEpoch: 955/1000; 2.11 sec... \tStep: 232065... \tLoss: 0.883... \tVal Loss: 0.788 \tVal Frac: 0.7382\n",
      "\tEpoch: 960/1000; 2.15 sec... \tStep: 233280... \tLoss: 1.247... \tVal Loss: 0.730 \tVal Frac: 0.7493\n",
      "\tEpoch: 965/1000; 2.11 sec... \tStep: 234495... \tLoss: 1.129... \tVal Loss: 0.735 \tVal Frac: 0.7522\n",
      "\tEpoch: 970/1000; 2.09 sec... \tStep: 235710... \tLoss: 0.707... \tVal Loss: 0.740 \tVal Frac: 0.7364\n",
      "\tEpoch: 975/1000; 2.15 sec... \tStep: 236925... \tLoss: 1.164... \tVal Loss: 0.726 \tVal Frac: 0.7379\n",
      "\tEpoch: 980/1000; 2.03 sec... \tStep: 238140... \tLoss: 0.872... \tVal Loss: 0.740 \tVal Frac: 0.7485\n",
      "\tEpoch: 985/1000; 2.05 sec... \tStep: 239355... \tLoss: 1.016... \tVal Loss: 0.853 \tVal Frac: 0.7191\n",
      "\tEpoch: 990/1000; 2.06 sec... \tStep: 240570... \tLoss: 1.004... \tVal Loss: 0.873 \tVal Frac: 0.7217\n",
      "\tEpoch: 995/1000; 2.09 sec... \tStep: 241785... \tLoss: 0.896... \tVal Loss: 0.832 \tVal Frac: 0.7257\n",
      "\tEpoch: 1000/1000; 2.04 sec... \tStep: 243000... \tLoss: 0.764... \tVal Loss: 0.768 \tVal Frac: 0.7500\n",
      "Completed:  n_layers_LSTM :  2 \n",
      "\tloss: 0.399848 \n",
      "\tfrac:0.886397\n",
      "\n",
      "###########\n",
      "Testing with  n_layers_LSTM :  3\n",
      "\tEpoch: 5/1000; 3.25 sec... \tStep: 1215... \tLoss: 0.513... \tVal Loss: 0.567 \tVal Frac: 0.8309\n",
      "\tValidation loss decreased (inf --> 0.567).  Saving model ...\n",
      "\tEpoch: 10/1000; 3.19 sec... \tStep: 2430... \tLoss: 0.347... \tVal Loss: 0.453 \tVal Frac: 0.8610\n",
      "\tValidation loss decreased (0.567 --> 0.453).  Saving model ...\n",
      "\tEpoch: 15/1000; 3.19 sec... \tStep: 3645... \tLoss: 0.477... \tVal Loss: 0.452 \tVal Frac: 0.8706\n",
      "\tValidation loss decreased (0.453 --> 0.452).  Saving model ...\n",
      "\tEpoch: 20/1000; 3.18 sec... \tStep: 4860... \tLoss: 0.324... \tVal Loss: 0.403 \tVal Frac: 0.8842\n",
      "\tValidation loss decreased (0.452 --> 0.403).  Saving model ...\n",
      "\tEpoch: 25/1000; 3.12 sec... \tStep: 6075... \tLoss: 0.230... \tVal Loss: 0.513 \tVal Frac: 0.8537\n",
      "\tEpoch: 30/1000; 3.19 sec... \tStep: 7290... \tLoss: 0.362... \tVal Loss: 0.450 \tVal Frac: 0.8728\n",
      "\tEpoch: 35/1000; 3.12 sec... \tStep: 8505... \tLoss: 0.363... \tVal Loss: 0.412 \tVal Frac: 0.8765\n",
      "\tEpoch: 40/1000; 3.43 sec... \tStep: 9720... \tLoss: 0.242... \tVal Loss: 0.399 \tVal Frac: 0.8846\n",
      "\tValidation loss decreased (0.403 --> 0.399).  Saving model ...\n",
      "\tEpoch: 45/1000; 3.24 sec... \tStep: 10935... \tLoss: 0.911... \tVal Loss: 0.452 \tVal Frac: 0.8765\n",
      "\tEpoch: 50/1000; 3.22 sec... \tStep: 12150... \tLoss: 0.550... \tVal Loss: 0.464 \tVal Frac: 0.8739\n",
      "\tEpoch: 55/1000; 3.28 sec... \tStep: 13365... \tLoss: 0.310... \tVal Loss: 0.409 \tVal Frac: 0.8879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 60/1000; 3.19 sec... \tStep: 14580... \tLoss: 0.432... \tVal Loss: 0.424 \tVal Frac: 0.8765\n",
      "\tEpoch: 65/1000; 3.16 sec... \tStep: 15795... \tLoss: 0.209... \tVal Loss: 0.394 \tVal Frac: 0.8901\n",
      "\tValidation loss decreased (0.399 --> 0.394).  Saving model ...\n",
      "\tEpoch: 70/1000; 3.11 sec... \tStep: 17010... \tLoss: 0.203... \tVal Loss: 0.401 \tVal Frac: 0.8820\n",
      "\tEpoch: 75/1000; 3.18 sec... \tStep: 18225... \tLoss: 0.601... \tVal Loss: 0.391 \tVal Frac: 0.8919\n",
      "\tValidation loss decreased (0.394 --> 0.391).  Saving model ...\n",
      "\tEpoch: 80/1000; 3.17 sec... \tStep: 19440... \tLoss: 0.687... \tVal Loss: 0.418 \tVal Frac: 0.8875\n",
      "\tEpoch: 85/1000; 3.19 sec... \tStep: 20655... \tLoss: 0.425... \tVal Loss: 0.431 \tVal Frac: 0.8893\n",
      "\tEpoch: 90/1000; 3.27 sec... \tStep: 21870... \tLoss: 0.384... \tVal Loss: 0.439 \tVal Frac: 0.8868\n",
      "\tEpoch: 95/1000; 3.39 sec... \tStep: 23085... \tLoss: 0.322... \tVal Loss: 0.411 \tVal Frac: 0.8860\n",
      "\tEpoch: 100/1000; 3.32 sec... \tStep: 24300... \tLoss: 0.433... \tVal Loss: 0.440 \tVal Frac: 0.8816\n",
      "\tEpoch: 105/1000; 3.18 sec... \tStep: 25515... \tLoss: 0.279... \tVal Loss: 0.450 \tVal Frac: 0.8816\n",
      "\tEpoch: 110/1000; 3.18 sec... \tStep: 26730... \tLoss: 0.416... \tVal Loss: 0.445 \tVal Frac: 0.8812\n",
      "\tEpoch: 115/1000; 3.26 sec... \tStep: 27945... \tLoss: 0.597... \tVal Loss: 0.403 \tVal Frac: 0.8949\n",
      "\tEpoch: 120/1000; 3.23 sec... \tStep: 29160... \tLoss: 0.923... \tVal Loss: 0.405 \tVal Frac: 0.8846\n",
      "\tEpoch: 125/1000; 3.14 sec... \tStep: 30375... \tLoss: 0.360... \tVal Loss: 0.408 \tVal Frac: 0.8904\n",
      "\tEpoch: 130/1000; 3.16 sec... \tStep: 31590... \tLoss: 0.218... \tVal Loss: 0.438 \tVal Frac: 0.8846\n",
      "\tEpoch: 135/1000; 3.21 sec... \tStep: 32805... \tLoss: 0.362... \tVal Loss: 0.418 \tVal Frac: 0.8915\n",
      "\tEpoch: 140/1000; 3.21 sec... \tStep: 34020... \tLoss: 0.301... \tVal Loss: 0.421 \tVal Frac: 0.8912\n",
      "\tEpoch: 145/1000; 3.16 sec... \tStep: 35235... \tLoss: 0.341... \tVal Loss: 0.434 \tVal Frac: 0.8857\n",
      "\tEpoch: 150/1000; 3.58 sec... \tStep: 36450... \tLoss: 0.579... \tVal Loss: 0.453 \tVal Frac: 0.8732\n",
      "\tEpoch: 155/1000; 3.26 sec... \tStep: 37665... \tLoss: 0.236... \tVal Loss: 0.412 \tVal Frac: 0.8938\n",
      "\tEpoch: 160/1000; 3.19 sec... \tStep: 38880... \tLoss: 0.216... \tVal Loss: 0.416 \tVal Frac: 0.8864\n",
      "\tEpoch: 165/1000; 3.22 sec... \tStep: 40095... \tLoss: 0.577... \tVal Loss: 0.449 \tVal Frac: 0.8754\n",
      "\tEpoch: 170/1000; 3.14 sec... \tStep: 41310... \tLoss: 0.339... \tVal Loss: 0.426 \tVal Frac: 0.8868\n",
      "\tEpoch: 175/1000; 3.19 sec... \tStep: 42525... \tLoss: 0.223... \tVal Loss: 0.471 \tVal Frac: 0.8776\n",
      "\tEpoch: 180/1000; 3.24 sec... \tStep: 43740... \tLoss: 0.017... \tVal Loss: 0.464 \tVal Frac: 0.8779\n",
      "\tEpoch: 185/1000; 3.15 sec... \tStep: 44955... \tLoss: 0.643... \tVal Loss: 0.461 \tVal Frac: 0.8739\n",
      "\tEpoch: 190/1000; 3.21 sec... \tStep: 46170... \tLoss: 0.225... \tVal Loss: 0.458 \tVal Frac: 0.8816\n",
      "\tEpoch: 195/1000; 3.15 sec... \tStep: 47385... \tLoss: 0.254... \tVal Loss: 0.471 \tVal Frac: 0.8665\n",
      "\tEpoch: 200/1000; 3.16 sec... \tStep: 48600... \tLoss: 0.345... \tVal Loss: 0.413 \tVal Frac: 0.8893\n",
      "\tEpoch: 205/1000; 3.32 sec... \tStep: 49815... \tLoss: 0.487... \tVal Loss: 0.410 \tVal Frac: 0.8930\n",
      "\tEpoch: 210/1000; 3.34 sec... \tStep: 51030... \tLoss: 0.351... \tVal Loss: 0.417 \tVal Frac: 0.8875\n",
      "\tEpoch: 215/1000; 3.18 sec... \tStep: 52245... \tLoss: 0.471... \tVal Loss: 0.428 \tVal Frac: 0.8860\n",
      "\tEpoch: 220/1000; 3.19 sec... \tStep: 53460... \tLoss: 0.274... \tVal Loss: 0.397 \tVal Frac: 0.8879\n",
      "\tEpoch: 225/1000; 3.15 sec... \tStep: 54675... \tLoss: 0.643... \tVal Loss: 0.425 \tVal Frac: 0.8787\n",
      "\tEpoch: 230/1000; 3.23 sec... \tStep: 55890... \tLoss: 0.485... \tVal Loss: 0.429 \tVal Frac: 0.8853\n",
      "\tEpoch: 235/1000; 3.18 sec... \tStep: 57105... \tLoss: 0.583... \tVal Loss: 0.425 \tVal Frac: 0.8930\n",
      "\tEpoch: 240/1000; 3.17 sec... \tStep: 58320... \tLoss: 0.468... \tVal Loss: 0.412 \tVal Frac: 0.8893\n",
      "\tEpoch: 245/1000; 3.22 sec... \tStep: 59535... \tLoss: 0.307... \tVal Loss: 0.443 \tVal Frac: 0.8849\n",
      "\tEpoch: 250/1000; 3.27 sec... \tStep: 60750... \tLoss: 0.295... \tVal Loss: 0.472 \tVal Frac: 0.8732\n",
      "\tEpoch: 255/1000; 3.18 sec... \tStep: 61965... \tLoss: 0.784... \tVal Loss: 0.464 \tVal Frac: 0.8721\n",
      "\tEpoch: 260/1000; 3.15 sec... \tStep: 63180... \tLoss: 0.713... \tVal Loss: 0.430 \tVal Frac: 0.8706\n",
      "\tEpoch: 265/1000; 3.76 sec... \tStep: 64395... \tLoss: 0.843... \tVal Loss: 0.497 \tVal Frac: 0.8537\n",
      "\tEpoch: 270/1000; 3.21 sec... \tStep: 65610... \tLoss: 0.359... \tVal Loss: 0.525 \tVal Frac: 0.8496\n",
      "\tEpoch: 275/1000; 3.25 sec... \tStep: 66825... \tLoss: 0.611... \tVal Loss: 0.466 \tVal Frac: 0.8640\n",
      "\tEpoch: 280/1000; 3.17 sec... \tStep: 68040... \tLoss: 0.650... \tVal Loss: 0.655 \tVal Frac: 0.8077\n",
      "\tEpoch: 285/1000; 3.24 sec... \tStep: 69255... \tLoss: 0.408... \tVal Loss: 0.728 \tVal Frac: 0.7857\n",
      "\tEpoch: 290/1000; 3.29 sec... \tStep: 70470... \tLoss: 0.941... \tVal Loss: 0.659 \tVal Frac: 0.8184\n",
      "\tEpoch: 295/1000; 3.22 sec... \tStep: 71685... \tLoss: 0.714... \tVal Loss: 0.544 \tVal Frac: 0.8496\n",
      "\tEpoch: 300/1000; 3.18 sec... \tStep: 72900... \tLoss: 0.474... \tVal Loss: 0.537 \tVal Frac: 0.8467\n",
      "\tEpoch: 305/1000; 3.32 sec... \tStep: 74115... \tLoss: 0.876... \tVal Loss: 0.474 \tVal Frac: 0.8596\n",
      "\tEpoch: 310/1000; 3.26 sec... \tStep: 75330... \tLoss: 0.490... \tVal Loss: 0.522 \tVal Frac: 0.8390\n",
      "\tEpoch: 315/1000; 3.36 sec... \tStep: 76545... \tLoss: 0.463... \tVal Loss: 0.542 \tVal Frac: 0.8338\n",
      "\tEpoch: 320/1000; 3.32 sec... \tStep: 77760... \tLoss: 0.763... \tVal Loss: 0.595 \tVal Frac: 0.8107\n",
      "\tEpoch: 325/1000; 3.26 sec... \tStep: 78975... \tLoss: 0.678... \tVal Loss: 0.621 \tVal Frac: 0.8063\n",
      "\tEpoch: 330/1000; 3.34 sec... \tStep: 80190... \tLoss: 0.703... \tVal Loss: 0.627 \tVal Frac: 0.8048\n",
      "\tEpoch: 335/1000; 3.46 sec... \tStep: 81405... \tLoss: 0.499... \tVal Loss: 0.571 \tVal Frac: 0.8246\n",
      "\tEpoch: 340/1000; 3.91 sec... \tStep: 82620... \tLoss: 0.479... \tVal Loss: 0.591 \tVal Frac: 0.8077\n",
      "\tEpoch: 345/1000; 3.25 sec... \tStep: 83835... \tLoss: 0.476... \tVal Loss: 0.575 \tVal Frac: 0.8176\n",
      "\tEpoch: 350/1000; 3.34 sec... \tStep: 85050... \tLoss: 0.708... \tVal Loss: 0.704 \tVal Frac: 0.7849\n",
      "\tEpoch: 355/1000; 3.28 sec... \tStep: 86265... \tLoss: 0.816... \tVal Loss: 0.726 \tVal Frac: 0.7618\n",
      "\tEpoch: 360/1000; 3.27 sec... \tStep: 87480... \tLoss: 0.876... \tVal Loss: 0.802 \tVal Frac: 0.7562\n",
      "\tEpoch: 365/1000; 3.19 sec... \tStep: 88695... \tLoss: 0.724... \tVal Loss: 0.774 \tVal Frac: 0.7739\n",
      "\tEpoch: 370/1000; 3.19 sec... \tStep: 89910... \tLoss: 1.459... \tVal Loss: 0.965 \tVal Frac: 0.7478\n",
      "\tEpoch: 375/1000; 3.18 sec... \tStep: 91125... \tLoss: 0.770... \tVal Loss: 0.677 \tVal Frac: 0.8037\n",
      "\tEpoch: 380/1000; 3.28 sec... \tStep: 92340... \tLoss: 0.499... \tVal Loss: 1.044 \tVal Frac: 0.7125\n",
      "\tEpoch: 385/1000; 3.23 sec... \tStep: 93555... \tLoss: 0.819... \tVal Loss: 0.796 \tVal Frac: 0.7761\n",
      "\tEpoch: 390/1000; 3.25 sec... \tStep: 94770... \tLoss: 1.196... \tVal Loss: 0.849 \tVal Frac: 0.7713\n",
      "\tEpoch: 395/1000; 3.15 sec... \tStep: 95985... \tLoss: 1.001... \tVal Loss: 0.839 \tVal Frac: 0.7471\n",
      "\tEpoch: 400/1000; 3.27 sec... \tStep: 97200... \tLoss: 0.803... \tVal Loss: 0.744 \tVal Frac: 0.7537\n",
      "\tEpoch: 405/1000; 3.25 sec... \tStep: 98415... \tLoss: 0.927... \tVal Loss: 0.865 \tVal Frac: 0.7338\n",
      "\tEpoch: 410/1000; 3.27 sec... \tStep: 99630... \tLoss: 1.249... \tVal Loss: 1.134 \tVal Frac: 0.6456\n",
      "\tEpoch: 415/1000; 3.22 sec... \tStep: 100845... \tLoss: 1.142... \tVal Loss: 0.872 \tVal Frac: 0.7342\n",
      "\tEpoch: 420/1000; 3.24 sec... \tStep: 102060... \tLoss: 1.034... \tVal Loss: 1.053 \tVal Frac: 0.6368\n",
      "\tEpoch: 425/1000; 3.21 sec... \tStep: 103275... \tLoss: 1.073... \tVal Loss: 1.062 \tVal Frac: 0.7151\n",
      "\tEpoch: 430/1000; 3.32 sec... \tStep: 104490... \tLoss: 1.008... \tVal Loss: 0.937 \tVal Frac: 0.6820\n",
      "\tEpoch: 435/1000; 3.31 sec... \tStep: 105705... \tLoss: 0.713... \tVal Loss: 1.154 \tVal Frac: 0.5680\n",
      "\tEpoch: 440/1000; 3.15 sec... \tStep: 106920... \tLoss: 0.778... \tVal Loss: 0.728 \tVal Frac: 0.7213\n",
      "\tEpoch: 445/1000; 3.29 sec... \tStep: 108135... \tLoss: 1.067... \tVal Loss: 0.768 \tVal Frac: 0.7654\n",
      "\tEpoch: 450/1000; 3.26 sec... \tStep: 109350... \tLoss: 0.821... \tVal Loss: 0.799 \tVal Frac: 0.7662\n",
      "\tEpoch: 455/1000; 3.36 sec... \tStep: 110565... \tLoss: 1.300... \tVal Loss: 0.700 \tVal Frac: 0.7897\n",
      "\tEpoch: 460/1000; 3.31 sec... \tStep: 111780... \tLoss: 0.583... \tVal Loss: 0.667 \tVal Frac: 0.8096\n",
      "\tEpoch: 465/1000; 3.38 sec... \tStep: 112995... \tLoss: 0.573... \tVal Loss: 0.662 \tVal Frac: 0.8033\n",
      "\tEpoch: 470/1000; 3.24 sec... \tStep: 114210... \tLoss: 0.570... \tVal Loss: 0.717 \tVal Frac: 0.7809\n",
      "\tEpoch: 475/1000; 3.24 sec... \tStep: 115425... \tLoss: 0.945... \tVal Loss: 0.814 \tVal Frac: 0.7838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 480/1000; 3.41 sec... \tStep: 116640... \tLoss: 0.632... \tVal Loss: 0.645 \tVal Frac: 0.8129\n",
      "\tEpoch: 485/1000; 3.29 sec... \tStep: 117855... \tLoss: 0.398... \tVal Loss: 0.671 \tVal Frac: 0.7996\n",
      "\tEpoch: 490/1000; 3.28 sec... \tStep: 119070... \tLoss: 0.767... \tVal Loss: 0.642 \tVal Frac: 0.8136\n",
      "\tEpoch: 495/1000; 3.27 sec... \tStep: 120285... \tLoss: 0.714... \tVal Loss: 0.823 \tVal Frac: 0.7511\n",
      "\tEpoch: 500/1000; 3.29 sec... \tStep: 121500... \tLoss: 0.855... \tVal Loss: 0.974 \tVal Frac: 0.7228\n",
      "\tEpoch: 505/1000; 3.29 sec... \tStep: 122715... \tLoss: 0.886... \tVal Loss: 0.785 \tVal Frac: 0.7471\n",
      "\tEpoch: 510/1000; 3.34 sec... \tStep: 123930... \tLoss: 0.723... \tVal Loss: 0.781 \tVal Frac: 0.7625\n",
      "\tEpoch: 515/1000; 3.28 sec... \tStep: 125145... \tLoss: 0.858... \tVal Loss: 0.685 \tVal Frac: 0.7846\n",
      "\tEpoch: 520/1000; 3.41 sec... \tStep: 126360... \tLoss: 0.909... \tVal Loss: 0.737 \tVal Frac: 0.7676\n",
      "\tEpoch: 525/1000; 3.19 sec... \tStep: 127575... \tLoss: 0.798... \tVal Loss: 0.697 \tVal Frac: 0.7831\n",
      "\tEpoch: 530/1000; 3.25 sec... \tStep: 128790... \tLoss: 0.523... \tVal Loss: 0.741 \tVal Frac: 0.7559\n",
      "\tEpoch: 535/1000; 3.28 sec... \tStep: 130005... \tLoss: 1.025... \tVal Loss: 0.774 \tVal Frac: 0.7507\n",
      "\tEpoch: 540/1000; 3.26 sec... \tStep: 131220... \tLoss: 0.752... \tVal Loss: 0.677 \tVal Frac: 0.7857\n",
      "\tEpoch: 545/1000; 3.25 sec... \tStep: 132435... \tLoss: 0.965... \tVal Loss: 0.678 \tVal Frac: 0.7805\n",
      "\tEpoch: 550/1000; 3.19 sec... \tStep: 133650... \tLoss: 0.623... \tVal Loss: 0.802 \tVal Frac: 0.7680\n",
      "\tEpoch: 555/1000; 3.21 sec... \tStep: 134865... \tLoss: 0.595... \tVal Loss: 0.626 \tVal Frac: 0.8040\n",
      "\tEpoch: 560/1000; 3.31 sec... \tStep: 136080... \tLoss: 0.959... \tVal Loss: 0.762 \tVal Frac: 0.7831\n",
      "\tEpoch: 565/1000; 3.29 sec... \tStep: 137295... \tLoss: 0.645... \tVal Loss: 0.805 \tVal Frac: 0.7625\n",
      "\tEpoch: 570/1000; 3.18 sec... \tStep: 138510... \tLoss: 0.647... \tVal Loss: 0.698 \tVal Frac: 0.7735\n",
      "\tEpoch: 575/1000; 3.19 sec... \tStep: 139725... \tLoss: 0.840... \tVal Loss: 0.768 \tVal Frac: 0.7717\n",
      "\tEpoch: 580/1000; 3.22 sec... \tStep: 140940... \tLoss: 0.484... \tVal Loss: 0.810 \tVal Frac: 0.7526\n",
      "\tEpoch: 585/1000; 3.28 sec... \tStep: 142155... \tLoss: 0.608... \tVal Loss: 0.791 \tVal Frac: 0.7798\n",
      "\tEpoch: 590/1000; 3.15 sec... \tStep: 143370... \tLoss: 0.639... \tVal Loss: 0.827 \tVal Frac: 0.7452\n",
      "\tEpoch: 595/1000; 3.18 sec... \tStep: 144585... \tLoss: 0.557... \tVal Loss: 0.829 \tVal Frac: 0.7559\n",
      "\tEpoch: 600/1000; 3.21 sec... \tStep: 145800... \tLoss: 1.006... \tVal Loss: 0.722 \tVal Frac: 0.7643\n",
      "\tEpoch: 605/1000; 3.15 sec... \tStep: 147015... \tLoss: 0.700... \tVal Loss: 0.648 \tVal Frac: 0.7923\n",
      "\tEpoch: 610/1000; 3.13 sec... \tStep: 148230... \tLoss: 0.507... \tVal Loss: 0.845 \tVal Frac: 0.7625\n",
      "\tEpoch: 615/1000; 3.14 sec... \tStep: 149445... \tLoss: 0.361... \tVal Loss: 0.747 \tVal Frac: 0.7610\n",
      "\tEpoch: 620/1000; 3.12 sec... \tStep: 150660... \tLoss: 0.426... \tVal Loss: 0.702 \tVal Frac: 0.7879\n",
      "\tEpoch: 625/1000; 3.13 sec... \tStep: 151875... \tLoss: 0.468... \tVal Loss: 0.707 \tVal Frac: 0.7853\n",
      "\tEpoch: 630/1000; 3.66 sec... \tStep: 153090... \tLoss: 0.646... \tVal Loss: 0.752 \tVal Frac: 0.7680\n",
      "\tEpoch: 635/1000; 3.69 sec... \tStep: 154305... \tLoss: 0.538... \tVal Loss: 0.602 \tVal Frac: 0.8044\n",
      "\tEpoch: 640/1000; 3.57 sec... \tStep: 155520... \tLoss: 0.592... \tVal Loss: 0.650 \tVal Frac: 0.8015\n",
      "\tEpoch: 645/1000; 3.50 sec... \tStep: 156735... \tLoss: 0.751... \tVal Loss: 0.660 \tVal Frac: 0.8066\n",
      "\tEpoch: 650/1000; 3.59 sec... \tStep: 157950... \tLoss: 0.933... \tVal Loss: 0.661 \tVal Frac: 0.8055\n",
      "\tEpoch: 655/1000; 3.49 sec... \tStep: 159165... \tLoss: 0.722... \tVal Loss: 0.564 \tVal Frac: 0.8320\n",
      "\tEpoch: 660/1000; 3.47 sec... \tStep: 160380... \tLoss: 0.714... \tVal Loss: 0.562 \tVal Frac: 0.8316\n",
      "\tEpoch: 665/1000; 3.51 sec... \tStep: 161595... \tLoss: 0.590... \tVal Loss: 0.698 \tVal Frac: 0.7960\n",
      "\tEpoch: 670/1000; 3.39 sec... \tStep: 162810... \tLoss: 0.484... \tVal Loss: 0.608 \tVal Frac: 0.8110\n",
      "\tEpoch: 675/1000; 3.61 sec... \tStep: 164025... \tLoss: 0.621... \tVal Loss: 0.617 \tVal Frac: 0.8026\n",
      "\tEpoch: 680/1000; 3.53 sec... \tStep: 165240... \tLoss: 0.696... \tVal Loss: 0.553 \tVal Frac: 0.8246\n",
      "\tEpoch: 685/1000; 3.46 sec... \tStep: 166455... \tLoss: 0.418... \tVal Loss: 0.588 \tVal Frac: 0.8088\n",
      "\tEpoch: 690/1000; 3.70 sec... \tStep: 167670... \tLoss: 0.344... \tVal Loss: 0.603 \tVal Frac: 0.8044\n",
      "\tEpoch: 695/1000; 3.66 sec... \tStep: 168885... \tLoss: 0.621... \tVal Loss: 0.663 \tVal Frac: 0.7978\n",
      "\tEpoch: 700/1000; 3.56 sec... \tStep: 170100... \tLoss: 0.535... \tVal Loss: 0.702 \tVal Frac: 0.7540\n",
      "\tEpoch: 705/1000; 3.53 sec... \tStep: 171315... \tLoss: 0.767... \tVal Loss: 0.646 \tVal Frac: 0.7735\n",
      "\tEpoch: 710/1000; 3.49 sec... \tStep: 172530... \tLoss: 0.726... \tVal Loss: 0.601 \tVal Frac: 0.7967\n",
      "\tEpoch: 715/1000; 3.33 sec... \tStep: 173745... \tLoss: 0.460... \tVal Loss: 0.596 \tVal Frac: 0.8187\n",
      "\tEpoch: 720/1000; 3.53 sec... \tStep: 174960... \tLoss: 0.279... \tVal Loss: 0.661 \tVal Frac: 0.7901\n",
      "\tEpoch: 725/1000; 3.56 sec... \tStep: 176175... \tLoss: 0.929... \tVal Loss: 0.627 \tVal Frac: 0.8048\n",
      "\tEpoch: 730/1000; 3.71 sec... \tStep: 177390... \tLoss: 0.985... \tVal Loss: 0.718 \tVal Frac: 0.7809\n",
      "\tEpoch: 735/1000; 3.51 sec... \tStep: 178605... \tLoss: 0.445... \tVal Loss: 0.744 \tVal Frac: 0.7765\n",
      "\tEpoch: 740/1000; 3.43 sec... \tStep: 179820... \tLoss: 0.647... \tVal Loss: 0.675 \tVal Frac: 0.8040\n",
      "\tEpoch: 745/1000; 3.56 sec... \tStep: 181035... \tLoss: 0.836... \tVal Loss: 0.663 \tVal Frac: 0.8004\n",
      "\tEpoch: 750/1000; 3.63 sec... \tStep: 182250... \tLoss: 0.915... \tVal Loss: 0.677 \tVal Frac: 0.7805\n",
      "\tEpoch: 755/1000; 3.53 sec... \tStep: 183465... \tLoss: 1.038... \tVal Loss: 0.751 \tVal Frac: 0.8040\n",
      "\tEpoch: 760/1000; 3.66 sec... \tStep: 184680... \tLoss: 0.322... \tVal Loss: 0.695 \tVal Frac: 0.7886\n",
      "\tEpoch: 765/1000; 3.58 sec... \tStep: 185895... \tLoss: 0.625... \tVal Loss: 0.844 \tVal Frac: 0.7305\n",
      "\tEpoch: 770/1000; 3.53 sec... \tStep: 187110... \tLoss: 0.384... \tVal Loss: 0.979 \tVal Frac: 0.7195\n",
      "\tEpoch: 775/1000; 3.61 sec... \tStep: 188325... \tLoss: 0.343... \tVal Loss: 0.701 \tVal Frac: 0.7952\n",
      "\tEpoch: 780/1000; 3.59 sec... \tStep: 189540... \tLoss: 0.745... \tVal Loss: 0.622 \tVal Frac: 0.8099\n",
      "\tEpoch: 785/1000; 3.54 sec... \tStep: 190755... \tLoss: 0.559... \tVal Loss: 0.794 \tVal Frac: 0.7787\n",
      "\tEpoch: 790/1000; 3.60 sec... \tStep: 191970... \tLoss: 0.573... \tVal Loss: 0.673 \tVal Frac: 0.7949\n",
      "\tEpoch: 795/1000; 3.67 sec... \tStep: 193185... \tLoss: 0.519... \tVal Loss: 0.761 \tVal Frac: 0.7585\n",
      "\tEpoch: 800/1000; 3.58 sec... \tStep: 194400... \tLoss: 0.462... \tVal Loss: 0.867 \tVal Frac: 0.7158\n",
      "\tEpoch: 805/1000; 3.63 sec... \tStep: 195615... \tLoss: 0.500... \tVal Loss: 0.800 \tVal Frac: 0.7210\n",
      "\tEpoch: 810/1000; 3.83 sec... \tStep: 196830... \tLoss: 1.401... \tVal Loss: 0.884 \tVal Frac: 0.7184\n",
      "\tEpoch: 815/1000; 3.78 sec... \tStep: 198045... \tLoss: 0.662... \tVal Loss: 0.871 \tVal Frac: 0.7188\n",
      "\tEpoch: 820/1000; 3.51 sec... \tStep: 199260... \tLoss: 0.589... \tVal Loss: 0.727 \tVal Frac: 0.7629\n",
      "\tEpoch: 825/1000; 3.72 sec... \tStep: 200475... \tLoss: 0.690... \tVal Loss: 0.772 \tVal Frac: 0.7588\n",
      "\tEpoch: 830/1000; 3.24 sec... \tStep: 201690... \tLoss: 1.239... \tVal Loss: 0.689 \tVal Frac: 0.7643\n",
      "\tEpoch: 835/1000; 3.29 sec... \tStep: 202905... \tLoss: 1.209... \tVal Loss: 0.770 \tVal Frac: 0.7482\n",
      "\tEpoch: 840/1000; 3.19 sec... \tStep: 204120... \tLoss: 0.745... \tVal Loss: 0.766 \tVal Frac: 0.7658\n",
      "\tEpoch: 845/1000; 3.59 sec... \tStep: 205335... \tLoss: 1.161... \tVal Loss: 0.811 \tVal Frac: 0.7423\n",
      "\tEpoch: 850/1000; 3.23 sec... \tStep: 206550... \tLoss: 0.805... \tVal Loss: 0.690 \tVal Frac: 0.8011\n",
      "\tEpoch: 855/1000; 3.21 sec... \tStep: 207765... \tLoss: 0.920... \tVal Loss: 0.897 \tVal Frac: 0.7268\n",
      "\tEpoch: 860/1000; 3.18 sec... \tStep: 208980... \tLoss: 0.860... \tVal Loss: 0.944 \tVal Frac: 0.7228\n",
      "\tEpoch: 865/1000; 3.17 sec... \tStep: 210195... \tLoss: 0.847... \tVal Loss: 0.859 \tVal Frac: 0.7438\n",
      "\tEpoch: 870/1000; 3.13 sec... \tStep: 211410... \tLoss: 0.687... \tVal Loss: 0.885 \tVal Frac: 0.7070\n",
      "\tEpoch: 875/1000; 3.25 sec... \tStep: 212625... \tLoss: 0.481... \tVal Loss: 0.848 \tVal Frac: 0.7015\n",
      "\tEpoch: 880/1000; 3.26 sec... \tStep: 213840... \tLoss: 0.689... \tVal Loss: 0.939 \tVal Frac: 0.7110\n",
      "\tEpoch: 885/1000; 3.13 sec... \tStep: 215055... \tLoss: 1.121... \tVal Loss: 0.913 \tVal Frac: 0.7283\n",
      "\tEpoch: 890/1000; 3.13 sec... \tStep: 216270... \tLoss: 0.876... \tVal Loss: 0.879 \tVal Frac: 0.7276\n",
      "\tEpoch: 895/1000; 3.17 sec... \tStep: 217485... \tLoss: 0.567... \tVal Loss: 0.871 \tVal Frac: 0.7371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 900/1000; 3.23 sec... \tStep: 218700... \tLoss: 0.980... \tVal Loss: 0.838 \tVal Frac: 0.7566\n",
      "\tEpoch: 905/1000; 3.27 sec... \tStep: 219915... \tLoss: 1.038... \tVal Loss: 0.927 \tVal Frac: 0.7110\n",
      "\tEpoch: 910/1000; 3.28 sec... \tStep: 221130... \tLoss: 1.080... \tVal Loss: 1.501 \tVal Frac: 0.4511\n",
      "\tEpoch: 915/1000; 3.23 sec... \tStep: 222345... \tLoss: 1.139... \tVal Loss: 1.071 \tVal Frac: 0.6504\n",
      "\tEpoch: 920/1000; 3.14 sec... \tStep: 223560... \tLoss: 1.282... \tVal Loss: 1.059 \tVal Frac: 0.6621\n",
      "\tEpoch: 925/1000; 3.19 sec... \tStep: 224775... \tLoss: 0.900... \tVal Loss: 1.000 \tVal Frac: 0.6871\n",
      "\tEpoch: 930/1000; 3.30 sec... \tStep: 225990... \tLoss: 0.961... \tVal Loss: 1.296 \tVal Frac: 0.6235\n",
      "\tEpoch: 935/1000; 3.16 sec... \tStep: 227205... \tLoss: 0.934... \tVal Loss: 1.048 \tVal Frac: 0.6974\n",
      "\tEpoch: 940/1000; 3.21 sec... \tStep: 228420... \tLoss: 1.305... \tVal Loss: 1.203 \tVal Frac: 0.6912\n",
      "\tEpoch: 945/1000; 3.29 sec... \tStep: 229635... \tLoss: 1.083... \tVal Loss: 0.883 \tVal Frac: 0.7441\n",
      "\tEpoch: 950/1000; 3.21 sec... \tStep: 230850... \tLoss: 0.880... \tVal Loss: 0.777 \tVal Frac: 0.7776\n",
      "\tEpoch: 955/1000; 3.28 sec... \tStep: 232065... \tLoss: 0.769... \tVal Loss: 1.051 \tVal Frac: 0.7129\n",
      "\tEpoch: 960/1000; 3.26 sec... \tStep: 233280... \tLoss: 0.540... \tVal Loss: 0.767 \tVal Frac: 0.7801\n",
      "\tEpoch: 965/1000; 3.19 sec... \tStep: 234495... \tLoss: 1.231... \tVal Loss: 0.939 \tVal Frac: 0.7026\n",
      "\tEpoch: 970/1000; 3.19 sec... \tStep: 235710... \tLoss: 1.216... \tVal Loss: 0.995 \tVal Frac: 0.6996\n",
      "\tEpoch: 975/1000; 3.31 sec... \tStep: 236925... \tLoss: 0.919... \tVal Loss: 1.025 \tVal Frac: 0.6809\n",
      "\tEpoch: 980/1000; 3.26 sec... \tStep: 238140... \tLoss: 1.386... \tVal Loss: 1.222 \tVal Frac: 0.6018\n",
      "\tEpoch: 985/1000; 3.21 sec... \tStep: 239355... \tLoss: 1.023... \tVal Loss: 0.897 \tVal Frac: 0.7228\n",
      "\tEpoch: 990/1000; 3.17 sec... \tStep: 240570... \tLoss: 0.832... \tVal Loss: 0.960 \tVal Frac: 0.7301\n",
      "\tEpoch: 995/1000; 3.23 sec... \tStep: 241785... \tLoss: 1.429... \tVal Loss: 0.953 \tVal Frac: 0.7213\n",
      "\tEpoch: 1000/1000; 3.33 sec... \tStep: 243000... \tLoss: 1.327... \tVal Loss: 0.992 \tVal Frac: 0.6827\n",
      "Completed:  n_layers_LSTM :  3 \n",
      "\tloss: 0.391459 \n",
      "\tfrac:0.894853\n",
      "\n",
      "###########\n",
      "Testing with  n_layers_LSTM :  5\n",
      "\tEpoch: 5/1000; 5.75 sec... \tStep: 1215... \tLoss: 0.924... \tVal Loss: 0.614 \tVal Frac: 0.8276\n",
      "\tValidation loss decreased (inf --> 0.614).  Saving model ...\n",
      "\tEpoch: 10/1000; 5.75 sec... \tStep: 2430... \tLoss: 0.398... \tVal Loss: 0.489 \tVal Frac: 0.8430\n",
      "\tValidation loss decreased (0.614 --> 0.489).  Saving model ...\n",
      "\tEpoch: 15/1000; 5.73 sec... \tStep: 3645... \tLoss: 0.470... \tVal Loss: 0.467 \tVal Frac: 0.8632\n",
      "\tValidation loss decreased (0.489 --> 0.467).  Saving model ...\n",
      "\tEpoch: 20/1000; 5.71 sec... \tStep: 4860... \tLoss: 0.839... \tVal Loss: 0.440 \tVal Frac: 0.8676\n",
      "\tValidation loss decreased (0.467 --> 0.440).  Saving model ...\n",
      "\tEpoch: 25/1000; 5.70 sec... \tStep: 6075... \tLoss: 0.678... \tVal Loss: 0.415 \tVal Frac: 0.8717\n",
      "\tValidation loss decreased (0.440 --> 0.415).  Saving model ...\n",
      "\tEpoch: 30/1000; 5.66 sec... \tStep: 7290... \tLoss: 0.325... \tVal Loss: 0.430 \tVal Frac: 0.8761\n",
      "\tEpoch: 35/1000; 5.71 sec... \tStep: 8505... \tLoss: 0.687... \tVal Loss: 0.397 \tVal Frac: 0.8787\n",
      "\tValidation loss decreased (0.415 --> 0.397).  Saving model ...\n",
      "\tEpoch: 40/1000; 5.73 sec... \tStep: 9720... \tLoss: 0.708... \tVal Loss: 0.415 \tVal Frac: 0.8820\n",
      "\tEpoch: 45/1000; 5.70 sec... \tStep: 10935... \tLoss: 0.334... \tVal Loss: 0.396 \tVal Frac: 0.8798\n",
      "\tValidation loss decreased (0.397 --> 0.396).  Saving model ...\n",
      "\tEpoch: 50/1000; 5.80 sec... \tStep: 12150... \tLoss: 0.347... \tVal Loss: 0.398 \tVal Frac: 0.8842\n",
      "\tEpoch: 55/1000; 5.67 sec... \tStep: 13365... \tLoss: 0.264... \tVal Loss: 0.449 \tVal Frac: 0.8765\n",
      "\tEpoch: 60/1000; 5.78 sec... \tStep: 14580... \tLoss: 0.562... \tVal Loss: 0.401 \tVal Frac: 0.8827\n",
      "\tEpoch: 65/1000; 5.76 sec... \tStep: 15795... \tLoss: 0.467... \tVal Loss: 0.429 \tVal Frac: 0.8805\n",
      "\tEpoch: 70/1000; 5.78 sec... \tStep: 17010... \tLoss: 0.348... \tVal Loss: 0.405 \tVal Frac: 0.8835\n",
      "\tEpoch: 75/1000; 5.71 sec... \tStep: 18225... \tLoss: 0.489... \tVal Loss: 0.446 \tVal Frac: 0.8761\n",
      "\tEpoch: 80/1000; 5.70 sec... \tStep: 19440... \tLoss: 0.451... \tVal Loss: 0.467 \tVal Frac: 0.8820\n",
      "\tEpoch: 85/1000; 5.76 sec... \tStep: 20655... \tLoss: 0.204... \tVal Loss: 0.446 \tVal Frac: 0.8831\n",
      "\tEpoch: 90/1000; 5.71 sec... \tStep: 21870... \tLoss: 0.471... \tVal Loss: 0.422 \tVal Frac: 0.8772\n",
      "\tEpoch: 95/1000; 5.78 sec... \tStep: 23085... \tLoss: 0.545... \tVal Loss: 0.432 \tVal Frac: 0.8886\n",
      "\tEpoch: 100/1000; 5.63 sec... \tStep: 24300... \tLoss: 0.702... \tVal Loss: 0.410 \tVal Frac: 0.8846\n",
      "\tEpoch: 105/1000; 5.71 sec... \tStep: 25515... \tLoss: 0.474... \tVal Loss: 0.423 \tVal Frac: 0.8750\n",
      "\tEpoch: 110/1000; 5.71 sec... \tStep: 26730... \tLoss: 0.078... \tVal Loss: 0.433 \tVal Frac: 0.8743\n",
      "\tEpoch: 115/1000; 5.71 sec... \tStep: 27945... \tLoss: 0.321... \tVal Loss: 0.396 \tVal Frac: 0.8835\n",
      "\tValidation loss decreased (0.396 --> 0.396).  Saving model ...\n",
      "\tEpoch: 120/1000; 5.65 sec... \tStep: 29160... \tLoss: 0.291... \tVal Loss: 0.403 \tVal Frac: 0.8849\n",
      "\tEpoch: 125/1000; 5.71 sec... \tStep: 30375... \tLoss: 0.378... \tVal Loss: 0.457 \tVal Frac: 0.8765\n",
      "\tEpoch: 130/1000; 5.71 sec... \tStep: 31590... \tLoss: 0.303... \tVal Loss: 0.419 \tVal Frac: 0.8860\n",
      "\tEpoch: 135/1000; 5.67 sec... \tStep: 32805... \tLoss: 0.748... \tVal Loss: 0.430 \tVal Frac: 0.8926\n",
      "\tEpoch: 140/1000; 5.75 sec... \tStep: 34020... \tLoss: 0.340... \tVal Loss: 0.435 \tVal Frac: 0.8820\n",
      "\tEpoch: 145/1000; 5.71 sec... \tStep: 35235... \tLoss: 0.179... \tVal Loss: 0.378 \tVal Frac: 0.8934\n",
      "\tValidation loss decreased (0.396 --> 0.378).  Saving model ...\n",
      "\tEpoch: 150/1000; 5.80 sec... \tStep: 36450... \tLoss: 0.449... \tVal Loss: 0.443 \tVal Frac: 0.8860\n",
      "\tEpoch: 155/1000; 5.66 sec... \tStep: 37665... \tLoss: 0.376... \tVal Loss: 0.397 \tVal Frac: 0.8879\n",
      "\tEpoch: 160/1000; 5.77 sec... \tStep: 38880... \tLoss: 0.360... \tVal Loss: 0.399 \tVal Frac: 0.8908\n",
      "\tEpoch: 165/1000; 5.74 sec... \tStep: 40095... \tLoss: 0.301... \tVal Loss: 0.489 \tVal Frac: 0.8688\n",
      "\tEpoch: 170/1000; 5.95 sec... \tStep: 41310... \tLoss: 0.332... \tVal Loss: 0.428 \tVal Frac: 0.8871\n",
      "\tEpoch: 175/1000; 5.73 sec... \tStep: 42525... \tLoss: 0.383... \tVal Loss: 0.445 \tVal Frac: 0.8875\n",
      "\tEpoch: 180/1000; 5.76 sec... \tStep: 43740... \tLoss: 0.462... \tVal Loss: 0.418 \tVal Frac: 0.8842\n",
      "\tEpoch: 185/1000; 5.71 sec... \tStep: 44955... \tLoss: 0.395... \tVal Loss: 0.389 \tVal Frac: 0.8908\n",
      "\tEpoch: 190/1000; 5.75 sec... \tStep: 46170... \tLoss: 0.728... \tVal Loss: 0.471 \tVal Frac: 0.8713\n",
      "\tEpoch: 195/1000; 5.85 sec... \tStep: 47385... \tLoss: 0.317... \tVal Loss: 0.408 \tVal Frac: 0.8934\n",
      "\tEpoch: 200/1000; 5.71 sec... \tStep: 48600... \tLoss: 0.342... \tVal Loss: 0.395 \tVal Frac: 0.8890\n",
      "\tEpoch: 205/1000; 5.76 sec... \tStep: 49815... \tLoss: 0.465... \tVal Loss: 0.431 \tVal Frac: 0.8827\n",
      "\tEpoch: 210/1000; 5.74 sec... \tStep: 51030... \tLoss: 0.292... \tVal Loss: 0.431 \tVal Frac: 0.8838\n",
      "\tEpoch: 215/1000; 5.64 sec... \tStep: 52245... \tLoss: 0.395... \tVal Loss: 0.423 \tVal Frac: 0.8864\n",
      "\tEpoch: 220/1000; 5.70 sec... \tStep: 53460... \tLoss: 0.386... \tVal Loss: 0.417 \tVal Frac: 0.8820\n",
      "\tEpoch: 225/1000; 5.87 sec... \tStep: 54675... \tLoss: 0.229... \tVal Loss: 0.400 \tVal Frac: 0.8886\n",
      "\tEpoch: 230/1000; 5.71 sec... \tStep: 55890... \tLoss: 0.414... \tVal Loss: 0.399 \tVal Frac: 0.8882\n",
      "\tEpoch: 235/1000; 5.68 sec... \tStep: 57105... \tLoss: 0.341... \tVal Loss: 0.464 \tVal Frac: 0.8757\n",
      "\tEpoch: 240/1000; 5.70 sec... \tStep: 58320... \tLoss: 0.425... \tVal Loss: 0.440 \tVal Frac: 0.8794\n",
      "\tEpoch: 245/1000; 5.68 sec... \tStep: 59535... \tLoss: 0.306... \tVal Loss: 0.422 \tVal Frac: 0.8860\n",
      "\tEpoch: 250/1000; 5.80 sec... \tStep: 60750... \tLoss: 0.361... \tVal Loss: 0.394 \tVal Frac: 0.8886\n",
      "\tEpoch: 255/1000; 5.65 sec... \tStep: 61965... \tLoss: 0.377... \tVal Loss: 0.415 \tVal Frac: 0.8908\n",
      "\tEpoch: 260/1000; 5.68 sec... \tStep: 63180... \tLoss: 0.165... \tVal Loss: 0.413 \tVal Frac: 0.8871\n",
      "\tEpoch: 265/1000; 5.66 sec... \tStep: 64395... \tLoss: 0.242... \tVal Loss: 0.411 \tVal Frac: 0.8893\n",
      "\tEpoch: 270/1000; 5.63 sec... \tStep: 65610... \tLoss: 0.413... \tVal Loss: 0.424 \tVal Frac: 0.8838\n",
      "\tEpoch: 275/1000; 5.69 sec... \tStep: 66825... \tLoss: 0.260... \tVal Loss: 0.397 \tVal Frac: 0.8886\n",
      "\tEpoch: 280/1000; 5.75 sec... \tStep: 68040... \tLoss: 0.382... \tVal Loss: 0.405 \tVal Frac: 0.8846\n",
      "\tEpoch: 285/1000; 5.78 sec... \tStep: 69255... \tLoss: 0.235... \tVal Loss: 0.392 \tVal Frac: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 290/1000; 5.71 sec... \tStep: 70470... \tLoss: 0.190... \tVal Loss: 0.401 \tVal Frac: 0.8897\n",
      "\tEpoch: 295/1000; 5.73 sec... \tStep: 71685... \tLoss: 0.052... \tVal Loss: 0.416 \tVal Frac: 0.8904\n",
      "\tEpoch: 300/1000; 5.77 sec... \tStep: 72900... \tLoss: 0.369... \tVal Loss: 0.463 \tVal Frac: 0.8860\n",
      "\tEpoch: 305/1000; 5.85 sec... \tStep: 74115... \tLoss: 0.393... \tVal Loss: 0.422 \tVal Frac: 0.8897\n",
      "\tEpoch: 310/1000; 5.80 sec... \tStep: 75330... \tLoss: 0.486... \tVal Loss: 0.431 \tVal Frac: 0.8864\n",
      "\tEpoch: 315/1000; 5.70 sec... \tStep: 76545... \tLoss: 0.234... \tVal Loss: 0.390 \tVal Frac: 0.8919\n",
      "\tEpoch: 320/1000; 5.65 sec... \tStep: 77760... \tLoss: 0.141... \tVal Loss: 0.443 \tVal Frac: 0.8879\n",
      "\tEpoch: 325/1000; 5.75 sec... \tStep: 78975... \tLoss: 0.367... \tVal Loss: 0.461 \tVal Frac: 0.8824\n",
      "\tEpoch: 330/1000; 5.73 sec... \tStep: 80190... \tLoss: 0.287... \tVal Loss: 0.438 \tVal Frac: 0.8879\n",
      "\tEpoch: 335/1000; 5.85 sec... \tStep: 81405... \tLoss: 0.479... \tVal Loss: 0.473 \tVal Frac: 0.8857\n",
      "\tEpoch: 340/1000; 5.72 sec... \tStep: 82620... \tLoss: 0.307... \tVal Loss: 0.439 \tVal Frac: 0.8860\n",
      "\tEpoch: 345/1000; 5.75 sec... \tStep: 83835... \tLoss: 0.424... \tVal Loss: 0.407 \tVal Frac: 0.8897\n",
      "\tEpoch: 350/1000; 5.78 sec... \tStep: 85050... \tLoss: 0.470... \tVal Loss: 0.419 \tVal Frac: 0.8853\n",
      "\tEpoch: 355/1000; 5.70 sec... \tStep: 86265... \tLoss: 0.327... \tVal Loss: 0.444 \tVal Frac: 0.8908\n",
      "\tEpoch: 360/1000; 5.92 sec... \tStep: 87480... \tLoss: 0.437... \tVal Loss: 0.407 \tVal Frac: 0.8893\n",
      "\tEpoch: 365/1000; 5.96 sec... \tStep: 88695... \tLoss: 0.442... \tVal Loss: 0.424 \tVal Frac: 0.8915\n",
      "\tEpoch: 370/1000; 5.87 sec... \tStep: 89910... \tLoss: 0.367... \tVal Loss: 0.418 \tVal Frac: 0.8915\n",
      "\tEpoch: 375/1000; 5.99 sec... \tStep: 91125... \tLoss: 0.325... \tVal Loss: 0.424 \tVal Frac: 0.8960\n",
      "\tEpoch: 380/1000; 5.78 sec... \tStep: 92340... \tLoss: 0.150... \tVal Loss: 0.414 \tVal Frac: 0.8952\n",
      "\tEpoch: 385/1000; 5.88 sec... \tStep: 93555... \tLoss: 0.240... \tVal Loss: 0.425 \tVal Frac: 0.8904\n",
      "\tEpoch: 390/1000; 5.78 sec... \tStep: 94770... \tLoss: 0.152... \tVal Loss: 0.456 \tVal Frac: 0.8794\n",
      "\tEpoch: 395/1000; 5.71 sec... \tStep: 95985... \tLoss: 0.540... \tVal Loss: 0.418 \tVal Frac: 0.8901\n",
      "\tEpoch: 400/1000; 5.75 sec... \tStep: 97200... \tLoss: 0.142... \tVal Loss: 0.413 \tVal Frac: 0.8938\n",
      "\tEpoch: 405/1000; 5.71 sec... \tStep: 98415... \tLoss: 0.502... \tVal Loss: 0.420 \tVal Frac: 0.8926\n",
      "\tEpoch: 410/1000; 5.77 sec... \tStep: 99630... \tLoss: 0.139... \tVal Loss: 0.449 \tVal Frac: 0.8779\n",
      "\tEpoch: 415/1000; 5.70 sec... \tStep: 100845... \tLoss: 0.529... \tVal Loss: 0.426 \tVal Frac: 0.8908\n",
      "\tEpoch: 420/1000; 5.70 sec... \tStep: 102060... \tLoss: 0.177... \tVal Loss: 0.402 \tVal Frac: 0.8879\n",
      "\tEpoch: 425/1000; 5.75 sec... \tStep: 103275... \tLoss: 0.208... \tVal Loss: 0.405 \tVal Frac: 0.8875\n",
      "\tEpoch: 430/1000; 5.68 sec... \tStep: 104490... \tLoss: 0.598... \tVal Loss: 0.404 \tVal Frac: 0.8886\n",
      "\tEpoch: 435/1000; 5.61 sec... \tStep: 105705... \tLoss: 0.368... \tVal Loss: 0.425 \tVal Frac: 0.8820\n",
      "\tEpoch: 440/1000; 5.70 sec... \tStep: 106920... \tLoss: 0.238... \tVal Loss: 0.398 \tVal Frac: 0.8849\n",
      "\tEpoch: 445/1000; 5.67 sec... \tStep: 108135... \tLoss: 0.491... \tVal Loss: 0.407 \tVal Frac: 0.8835\n",
      "\tEpoch: 450/1000; 5.74 sec... \tStep: 109350... \tLoss: 0.381... \tVal Loss: 0.416 \tVal Frac: 0.8849\n",
      "\tEpoch: 455/1000; 5.83 sec... \tStep: 110565... \tLoss: 0.169... \tVal Loss: 0.397 \tVal Frac: 0.8868\n",
      "\tEpoch: 460/1000; 5.70 sec... \tStep: 111780... \tLoss: 0.303... \tVal Loss: 0.469 \tVal Frac: 0.8809\n",
      "\tEpoch: 465/1000; 5.75 sec... \tStep: 112995... \tLoss: 0.601... \tVal Loss: 0.444 \tVal Frac: 0.8846\n",
      "\tEpoch: 470/1000; 5.75 sec... \tStep: 114210... \tLoss: 0.374... \tVal Loss: 0.428 \tVal Frac: 0.8835\n",
      "\tEpoch: 475/1000; 5.70 sec... \tStep: 115425... \tLoss: 0.541... \tVal Loss: 0.433 \tVal Frac: 0.8853\n",
      "\tEpoch: 480/1000; 5.75 sec... \tStep: 116640... \tLoss: 0.466... \tVal Loss: 0.460 \tVal Frac: 0.8860\n",
      "\tEpoch: 485/1000; 5.75 sec... \tStep: 117855... \tLoss: 0.156... \tVal Loss: 0.433 \tVal Frac: 0.8842\n",
      "\tEpoch: 490/1000; 5.75 sec... \tStep: 119070... \tLoss: 0.361... \tVal Loss: 0.434 \tVal Frac: 0.8901\n",
      "\tEpoch: 495/1000; 5.75 sec... \tStep: 120285... \tLoss: 0.156... \tVal Loss: 0.414 \tVal Frac: 0.8812\n",
      "\tEpoch: 500/1000; 5.75 sec... \tStep: 121500... \tLoss: 0.229... \tVal Loss: 0.425 \tVal Frac: 0.8835\n",
      "\tEpoch: 505/1000; 5.78 sec... \tStep: 122715... \tLoss: 0.295... \tVal Loss: 0.441 \tVal Frac: 0.8805\n",
      "\tEpoch: 510/1000; 5.68 sec... \tStep: 123930... \tLoss: 0.288... \tVal Loss: 0.432 \tVal Frac: 0.8857\n",
      "\tEpoch: 515/1000; 5.65 sec... \tStep: 125145... \tLoss: 0.368... \tVal Loss: 0.429 \tVal Frac: 0.8853\n",
      "\tEpoch: 520/1000; 5.73 sec... \tStep: 126360... \tLoss: 0.719... \tVal Loss: 0.422 \tVal Frac: 0.8912\n",
      "\tEpoch: 525/1000; 5.70 sec... \tStep: 127575... \tLoss: 0.727... \tVal Loss: 0.419 \tVal Frac: 0.8827\n",
      "\tEpoch: 530/1000; 5.77 sec... \tStep: 128790... \tLoss: 0.136... \tVal Loss: 0.479 \tVal Frac: 0.8746\n",
      "\tEpoch: 535/1000; 5.70 sec... \tStep: 130005... \tLoss: 0.284... \tVal Loss: 0.435 \tVal Frac: 0.8864\n",
      "\tEpoch: 540/1000; 5.65 sec... \tStep: 131220... \tLoss: 0.553... \tVal Loss: 0.443 \tVal Frac: 0.8849\n",
      "\tEpoch: 545/1000; 5.78 sec... \tStep: 132435... \tLoss: 0.403... \tVal Loss: 0.421 \tVal Frac: 0.8864\n",
      "\tEpoch: 550/1000; 5.76 sec... \tStep: 133650... \tLoss: 0.424... \tVal Loss: 0.411 \tVal Frac: 0.8890\n",
      "\tEpoch: 555/1000; 5.71 sec... \tStep: 134865... \tLoss: 0.460... \tVal Loss: 0.454 \tVal Frac: 0.8768\n",
      "\tEpoch: 560/1000; 5.91 sec... \tStep: 136080... \tLoss: 0.181... \tVal Loss: 0.414 \tVal Frac: 0.8827\n",
      "\tEpoch: 565/1000; 5.86 sec... \tStep: 137295... \tLoss: 0.219... \tVal Loss: 0.397 \tVal Frac: 0.8838\n",
      "\tEpoch: 570/1000; 5.73 sec... \tStep: 138510... \tLoss: 0.266... \tVal Loss: 0.425 \tVal Frac: 0.8849\n",
      "\tEpoch: 575/1000; 5.70 sec... \tStep: 139725... \tLoss: 0.269... \tVal Loss: 0.407 \tVal Frac: 0.8827\n",
      "\tEpoch: 580/1000; 5.71 sec... \tStep: 140940... \tLoss: 0.651... \tVal Loss: 0.415 \tVal Frac: 0.8820\n",
      "\tEpoch: 585/1000; 5.70 sec... \tStep: 142155... \tLoss: 0.205... \tVal Loss: 0.396 \tVal Frac: 0.8897\n",
      "\tEpoch: 590/1000; 5.78 sec... \tStep: 143370... \tLoss: 0.218... \tVal Loss: 0.452 \tVal Frac: 0.8787\n",
      "\tEpoch: 595/1000; 5.73 sec... \tStep: 144585... \tLoss: 0.670... \tVal Loss: 0.450 \tVal Frac: 0.8871\n",
      "\tEpoch: 600/1000; 5.68 sec... \tStep: 145800... \tLoss: 0.247... \tVal Loss: 0.398 \tVal Frac: 0.8882\n",
      "\tEpoch: 605/1000; 5.77 sec... \tStep: 147015... \tLoss: 0.319... \tVal Loss: 0.407 \tVal Frac: 0.8904\n",
      "\tEpoch: 610/1000; 5.78 sec... \tStep: 148230... \tLoss: 0.102... \tVal Loss: 0.402 \tVal Frac: 0.8824\n",
      "\tEpoch: 615/1000; 5.78 sec... \tStep: 149445... \tLoss: 0.322... \tVal Loss: 0.399 \tVal Frac: 0.8857\n",
      "\tEpoch: 620/1000; 5.77 sec... \tStep: 150660... \tLoss: 0.303... \tVal Loss: 0.426 \tVal Frac: 0.8897\n",
      "\tEpoch: 625/1000; 5.74 sec... \tStep: 151875... \tLoss: 0.435... \tVal Loss: 0.394 \tVal Frac: 0.8849\n",
      "\tEpoch: 630/1000; 5.71 sec... \tStep: 153090... \tLoss: 0.336... \tVal Loss: 0.434 \tVal Frac: 0.8765\n",
      "\tEpoch: 635/1000; 5.71 sec... \tStep: 154305... \tLoss: 0.330... \tVal Loss: 0.434 \tVal Frac: 0.8801\n",
      "\tEpoch: 640/1000; 5.70 sec... \tStep: 155520... \tLoss: 0.291... \tVal Loss: 0.413 \tVal Frac: 0.8820\n",
      "\tEpoch: 645/1000; 5.77 sec... \tStep: 156735... \tLoss: 0.813... \tVal Loss: 0.447 \tVal Frac: 0.8801\n",
      "\tEpoch: 650/1000; 5.65 sec... \tStep: 157950... \tLoss: 0.441... \tVal Loss: 0.459 \tVal Frac: 0.8824\n",
      "\tEpoch: 655/1000; 5.71 sec... \tStep: 159165... \tLoss: 0.528... \tVal Loss: 0.459 \tVal Frac: 0.8790\n",
      "\tEpoch: 660/1000; 5.70 sec... \tStep: 160380... \tLoss: 0.393... \tVal Loss: 0.456 \tVal Frac: 0.8798\n",
      "\tEpoch: 665/1000; 5.68 sec... \tStep: 161595... \tLoss: 0.507... \tVal Loss: 0.458 \tVal Frac: 0.8787\n",
      "\tEpoch: 670/1000; 5.77 sec... \tStep: 162810... \tLoss: 0.363... \tVal Loss: 0.451 \tVal Frac: 0.8790\n",
      "\tEpoch: 675/1000; 5.77 sec... \tStep: 164025... \tLoss: 0.045... \tVal Loss: 0.451 \tVal Frac: 0.8831\n",
      "\tEpoch: 680/1000; 5.77 sec... \tStep: 165240... \tLoss: 0.350... \tVal Loss: 0.425 \tVal Frac: 0.8835\n",
      "\tEpoch: 685/1000; 5.79 sec... \tStep: 166455... \tLoss: 0.276... \tVal Loss: 0.426 \tVal Frac: 0.8842\n",
      "\tEpoch: 690/1000; 5.65 sec... \tStep: 167670... \tLoss: 0.535... \tVal Loss: 0.455 \tVal Frac: 0.8831\n",
      "\tEpoch: 695/1000; 5.68 sec... \tStep: 168885... \tLoss: 0.519... \tVal Loss: 0.428 \tVal Frac: 0.8846\n",
      "\tEpoch: 700/1000; 5.77 sec... \tStep: 170100... \tLoss: 0.775... \tVal Loss: 0.421 \tVal Frac: 0.8868\n",
      "\tEpoch: 705/1000; 5.71 sec... \tStep: 171315... \tLoss: 0.451... \tVal Loss: 0.412 \tVal Frac: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 710/1000; 5.83 sec... \tStep: 172530... \tLoss: 0.157... \tVal Loss: 0.443 \tVal Frac: 0.8787\n",
      "\tEpoch: 715/1000; 5.68 sec... \tStep: 173745... \tLoss: 0.203... \tVal Loss: 0.467 \tVal Frac: 0.8743\n",
      "\tEpoch: 720/1000; 5.71 sec... \tStep: 174960... \tLoss: 0.218... \tVal Loss: 0.472 \tVal Frac: 0.8735\n",
      "\tEpoch: 725/1000; 5.81 sec... \tStep: 176175... \tLoss: 0.295... \tVal Loss: 0.439 \tVal Frac: 0.8779\n",
      "\tEpoch: 730/1000; 5.90 sec... \tStep: 177390... \tLoss: 0.520... \tVal Loss: 0.438 \tVal Frac: 0.8735\n",
      "\tEpoch: 735/1000; 5.71 sec... \tStep: 178605... \tLoss: 0.419... \tVal Loss: 0.489 \tVal Frac: 0.8702\n",
      "\tEpoch: 740/1000; 5.75 sec... \tStep: 179820... \tLoss: 0.747... \tVal Loss: 0.452 \tVal Frac: 0.8787\n",
      "\tEpoch: 745/1000; 5.73 sec... \tStep: 181035... \tLoss: 0.354... \tVal Loss: 0.454 \tVal Frac: 0.8746\n",
      "\tEpoch: 750/1000; 6.53 sec... \tStep: 182250... \tLoss: 0.117... \tVal Loss: 0.445 \tVal Frac: 0.8765\n",
      "\tEpoch: 755/1000; 5.70 sec... \tStep: 183465... \tLoss: 0.607... \tVal Loss: 0.431 \tVal Frac: 0.8812\n",
      "\tEpoch: 760/1000; 5.79 sec... \tStep: 184680... \tLoss: 0.480... \tVal Loss: 0.467 \tVal Frac: 0.8772\n",
      "\tEpoch: 765/1000; 5.83 sec... \tStep: 185895... \tLoss: 0.629... \tVal Loss: 0.454 \tVal Frac: 0.8809\n",
      "\tEpoch: 770/1000; 5.79 sec... \tStep: 187110... \tLoss: 0.387... \tVal Loss: 0.500 \tVal Frac: 0.8713\n",
      "\tEpoch: 775/1000; 5.83 sec... \tStep: 188325... \tLoss: 0.587... \tVal Loss: 0.493 \tVal Frac: 0.8713\n",
      "\tEpoch: 780/1000; 5.78 sec... \tStep: 189540... \tLoss: 0.453... \tVal Loss: 0.491 \tVal Frac: 0.8621\n",
      "\tEpoch: 785/1000; 5.86 sec... \tStep: 190755... \tLoss: 0.554... \tVal Loss: 0.479 \tVal Frac: 0.8643\n",
      "\tEpoch: 790/1000; 5.83 sec... \tStep: 191970... \tLoss: 0.507... \tVal Loss: 0.560 \tVal Frac: 0.8397\n",
      "\tEpoch: 795/1000; 5.86 sec... \tStep: 193185... \tLoss: 0.409... \tVal Loss: 0.565 \tVal Frac: 0.8187\n",
      "\tEpoch: 800/1000; 5.82 sec... \tStep: 194400... \tLoss: 0.556... \tVal Loss: 0.539 \tVal Frac: 0.8382\n",
      "\tEpoch: 805/1000; 5.84 sec... \tStep: 195615... \tLoss: 0.510... \tVal Loss: 0.523 \tVal Frac: 0.8500\n",
      "\tEpoch: 810/1000; 5.85 sec... \tStep: 196830... \tLoss: 0.629... \tVal Loss: 0.518 \tVal Frac: 0.8430\n",
      "\tEpoch: 815/1000; 5.87 sec... \tStep: 198045... \tLoss: 0.657... \tVal Loss: 0.560 \tVal Frac: 0.8386\n",
      "\tEpoch: 820/1000; 5.75 sec... \tStep: 199260... \tLoss: 0.521... \tVal Loss: 0.632 \tVal Frac: 0.8015\n",
      "\tEpoch: 825/1000; 5.81 sec... \tStep: 200475... \tLoss: 0.823... \tVal Loss: 0.636 \tVal Frac: 0.8125\n",
      "\tEpoch: 830/1000; 5.88 sec... \tStep: 201690... \tLoss: 0.500... \tVal Loss: 0.646 \tVal Frac: 0.7985\n",
      "\tEpoch: 835/1000; 5.88 sec... \tStep: 202905... \tLoss: 0.643... \tVal Loss: 0.765 \tVal Frac: 0.7471\n",
      "\tEpoch: 840/1000; 5.79 sec... \tStep: 204120... \tLoss: 0.749... \tVal Loss: 0.692 \tVal Frac: 0.7941\n",
      "\tEpoch: 845/1000; 5.81 sec... \tStep: 205335... \tLoss: 0.623... \tVal Loss: 0.680 \tVal Frac: 0.7765\n",
      "\tEpoch: 850/1000; 5.70 sec... \tStep: 206550... \tLoss: 0.788... \tVal Loss: 1.174 \tVal Frac: 0.5735\n",
      "\tEpoch: 855/1000; 5.70 sec... \tStep: 207765... \tLoss: 0.907... \tVal Loss: 0.691 \tVal Frac: 0.7710\n",
      "\tEpoch: 860/1000; 5.85 sec... \tStep: 208980... \tLoss: 0.896... \tVal Loss: 0.690 \tVal Frac: 0.7599\n",
      "\tEpoch: 865/1000; 5.83 sec... \tStep: 210195... \tLoss: 0.658... \tVal Loss: 0.716 \tVal Frac: 0.7460\n",
      "\tEpoch: 870/1000; 5.81 sec... \tStep: 211410... \tLoss: 0.808... \tVal Loss: 0.712 \tVal Frac: 0.7651\n",
      "\tEpoch: 875/1000; 6.01 sec... \tStep: 212625... \tLoss: 0.781... \tVal Loss: 0.711 \tVal Frac: 0.7790\n",
      "\tEpoch: 880/1000; 5.85 sec... \tStep: 213840... \tLoss: 1.047... \tVal Loss: 0.702 \tVal Frac: 0.7790\n",
      "\tEpoch: 885/1000; 5.85 sec... \tStep: 215055... \tLoss: 0.709... \tVal Loss: 0.855 \tVal Frac: 0.7246\n",
      "\tEpoch: 890/1000; 5.82 sec... \tStep: 216270... \tLoss: 0.891... \tVal Loss: 0.767 \tVal Frac: 0.7504\n",
      "\tEpoch: 895/1000; 5.80 sec... \tStep: 217485... \tLoss: 1.355... \tVal Loss: 1.458 \tVal Frac: 0.3206\n",
      "\tEpoch: 900/1000; 5.80 sec... \tStep: 218700... \tLoss: 1.622... \tVal Loss: 1.295 \tVal Frac: 0.5801\n",
      "\tEpoch: 905/1000; 5.77 sec... \tStep: 219915... \tLoss: 1.287... \tVal Loss: 1.219 \tVal Frac: 0.6496\n",
      "\tEpoch: 910/1000; 5.83 sec... \tStep: 221130... \tLoss: 1.352... \tVal Loss: 0.954 \tVal Frac: 0.7022\n",
      "\tEpoch: 915/1000; 5.85 sec... \tStep: 222345... \tLoss: 0.788... \tVal Loss: 1.117 \tVal Frac: 0.6349\n",
      "\tEpoch: 920/1000; 5.92 sec... \tStep: 223560... \tLoss: 1.102... \tVal Loss: 0.823 \tVal Frac: 0.7298\n",
      "\tEpoch: 925/1000; 5.83 sec... \tStep: 224775... \tLoss: 1.533... \tVal Loss: 0.785 \tVal Frac: 0.7316\n",
      "\tEpoch: 930/1000; 5.90 sec... \tStep: 225990... \tLoss: 1.621... \tVal Loss: 0.870 \tVal Frac: 0.7324\n",
      "\tEpoch: 935/1000; 5.92 sec... \tStep: 227205... \tLoss: 0.729... \tVal Loss: 1.166 \tVal Frac: 0.6415\n",
      "\tEpoch: 940/1000; 5.83 sec... \tStep: 228420... \tLoss: 0.731... \tVal Loss: 1.090 \tVal Frac: 0.6607\n",
      "\tEpoch: 945/1000; 5.85 sec... \tStep: 229635... \tLoss: 1.098... \tVal Loss: 1.319 \tVal Frac: 0.5515\n",
      "\tEpoch: 950/1000; 5.87 sec... \tStep: 230850... \tLoss: 1.285... \tVal Loss: 1.387 \tVal Frac: 0.4974\n",
      "\tEpoch: 955/1000; 5.85 sec... \tStep: 232065... \tLoss: 1.279... \tVal Loss: 1.436 \tVal Frac: 0.4691\n",
      "\tEpoch: 960/1000; 5.77 sec... \tStep: 233280... \tLoss: 1.248... \tVal Loss: 1.506 \tVal Frac: 0.3941\n",
      "\tEpoch: 965/1000; 5.79 sec... \tStep: 234495... \tLoss: 1.023... \tVal Loss: 1.478 \tVal Frac: 0.4287\n",
      "\tEpoch: 970/1000; 5.95 sec... \tStep: 235710... \tLoss: 1.014... \tVal Loss: 1.449 \tVal Frac: 0.4724\n",
      "\tEpoch: 975/1000; 5.79 sec... \tStep: 236925... \tLoss: 1.299... \tVal Loss: 1.492 \tVal Frac: 0.4114\n",
      "\tEpoch: 980/1000; 5.83 sec... \tStep: 238140... \tLoss: 1.303... \tVal Loss: 1.478 \tVal Frac: 0.4515\n",
      "\tEpoch: 985/1000; 5.88 sec... \tStep: 239355... \tLoss: 1.261... \tVal Loss: 1.421 \tVal Frac: 0.4673\n",
      "\tEpoch: 990/1000; 5.87 sec... \tStep: 240570... \tLoss: 1.518... \tVal Loss: 1.392 \tVal Frac: 0.5140\n",
      "\tEpoch: 995/1000; 5.93 sec... \tStep: 241785... \tLoss: 1.238... \tVal Loss: 1.340 \tVal Frac: 0.5379\n",
      "\tEpoch: 1000/1000; 5.80 sec... \tStep: 243000... \tLoss: 1.158... \tVal Loss: 1.421 \tVal Frac: 0.4893\n",
      "Completed:  n_layers_LSTM :  5 \n",
      "\tloss: 0.378481 \n",
      "\tfrac:0.895956\n",
      "\n",
      "###########\n",
      "Testing with  lr :  0.001\n",
      "\tEpoch: 5/1000; 1.40 sec... \tStep: 1215... \tLoss: 0.854... \tVal Loss: 0.783 \tVal Frac: 0.7511\n",
      "\tValidation loss decreased (inf --> 0.783).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.40 sec... \tStep: 2430... \tLoss: 0.985... \tVal Loss: 0.804 \tVal Frac: 0.7846\n",
      "\tEpoch: 15/1000; 1.39 sec... \tStep: 3645... \tLoss: 1.013... \tVal Loss: 0.771 \tVal Frac: 0.7596\n",
      "\tValidation loss decreased (0.783 --> 0.771).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.33 sec... \tStep: 4860... \tLoss: 1.376... \tVal Loss: 1.028 \tVal Frac: 0.7562\n",
      "\tEpoch: 25/1000; 1.50 sec... \tStep: 6075... \tLoss: 0.699... \tVal Loss: 0.686 \tVal Frac: 0.7728\n",
      "\tValidation loss decreased (0.771 --> 0.686).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.37 sec... \tStep: 7290... \tLoss: 0.895... \tVal Loss: 0.719 \tVal Frac: 0.7629\n",
      "\tEpoch: 35/1000; 1.36 sec... \tStep: 8505... \tLoss: 0.725... \tVal Loss: 0.664 \tVal Frac: 0.7695\n",
      "\tValidation loss decreased (0.686 --> 0.664).  Saving model ...\n",
      "\tEpoch: 40/1000; 1.41 sec... \tStep: 9720... \tLoss: 0.855... \tVal Loss: 0.722 \tVal Frac: 0.7533\n",
      "\tEpoch: 45/1000; 1.27 sec... \tStep: 10935... \tLoss: 1.009... \tVal Loss: 0.673 \tVal Frac: 0.7746\n",
      "\tEpoch: 50/1000; 1.48 sec... \tStep: 12150... \tLoss: 0.939... \tVal Loss: 0.709 \tVal Frac: 0.7375\n",
      "\tEpoch: 55/1000; 1.54 sec... \tStep: 13365... \tLoss: 0.962... \tVal Loss: 0.691 \tVal Frac: 0.7526\n",
      "\tEpoch: 60/1000; 1.40 sec... \tStep: 14580... \tLoss: 0.655... \tVal Loss: 0.670 \tVal Frac: 0.7794\n",
      "\tEpoch: 65/1000; 1.48 sec... \tStep: 15795... \tLoss: 0.927... \tVal Loss: 0.635 \tVal Frac: 0.7893\n",
      "\tValidation loss decreased (0.664 --> 0.635).  Saving model ...\n",
      "\tEpoch: 70/1000; 1.51 sec... \tStep: 17010... \tLoss: 0.845... \tVal Loss: 0.657 \tVal Frac: 0.7743\n",
      "\tEpoch: 75/1000; 1.42 sec... \tStep: 18225... \tLoss: 1.129... \tVal Loss: 0.643 \tVal Frac: 0.7801\n",
      "\tEpoch: 80/1000; 1.46 sec... \tStep: 19440... \tLoss: 0.541... \tVal Loss: 0.643 \tVal Frac: 0.7963\n",
      "\tEpoch: 85/1000; 1.38 sec... \tStep: 20655... \tLoss: 1.019... \tVal Loss: 0.649 \tVal Frac: 0.7915\n",
      "\tEpoch: 90/1000; 1.49 sec... \tStep: 21870... \tLoss: 0.755... \tVal Loss: 0.613 \tVal Frac: 0.7996\n",
      "\tValidation loss decreased (0.635 --> 0.613).  Saving model ...\n",
      "\tEpoch: 95/1000; 1.44 sec... \tStep: 23085... \tLoss: 0.806... \tVal Loss: 0.633 \tVal Frac: 0.8037\n",
      "\tEpoch: 100/1000; 1.43 sec... \tStep: 24300... \tLoss: 0.790... \tVal Loss: 0.627 \tVal Frac: 0.8070\n",
      "\tEpoch: 105/1000; 1.39 sec... \tStep: 25515... \tLoss: 0.603... \tVal Loss: 0.639 \tVal Frac: 0.7963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 110/1000; 1.55 sec... \tStep: 26730... \tLoss: 1.476... \tVal Loss: 0.820 \tVal Frac: 0.7640\n",
      "\tEpoch: 115/1000; 1.30 sec... \tStep: 27945... \tLoss: 0.913... \tVal Loss: 0.682 \tVal Frac: 0.8004\n",
      "\tEpoch: 120/1000; 1.35 sec... \tStep: 29160... \tLoss: 0.934... \tVal Loss: 0.642 \tVal Frac: 0.8044\n",
      "\tEpoch: 125/1000; 1.44 sec... \tStep: 30375... \tLoss: 1.265... \tVal Loss: 0.683 \tVal Frac: 0.7978\n",
      "\tEpoch: 130/1000; 1.33 sec... \tStep: 31590... \tLoss: 0.758... \tVal Loss: 0.666 \tVal Frac: 0.7890\n",
      "\tEpoch: 135/1000; 1.44 sec... \tStep: 32805... \tLoss: 0.733... \tVal Loss: 0.633 \tVal Frac: 0.7952\n",
      "\tEpoch: 140/1000; 1.49 sec... \tStep: 34020... \tLoss: 0.883... \tVal Loss: 0.655 \tVal Frac: 0.7790\n",
      "\tEpoch: 145/1000; 1.43 sec... \tStep: 35235... \tLoss: 0.549... \tVal Loss: 0.618 \tVal Frac: 0.7952\n",
      "\tEpoch: 150/1000; 1.37 sec... \tStep: 36450... \tLoss: 0.860... \tVal Loss: 0.694 \tVal Frac: 0.7750\n",
      "\tEpoch: 155/1000; 1.50 sec... \tStep: 37665... \tLoss: 0.782... \tVal Loss: 0.662 \tVal Frac: 0.7820\n",
      "\tEpoch: 160/1000; 1.40 sec... \tStep: 38880... \tLoss: 0.859... \tVal Loss: 0.701 \tVal Frac: 0.7875\n",
      "\tEpoch: 165/1000; 1.45 sec... \tStep: 40095... \tLoss: 0.880... \tVal Loss: 0.642 \tVal Frac: 0.7607\n",
      "\tEpoch: 170/1000; 1.49 sec... \tStep: 41310... \tLoss: 1.035... \tVal Loss: 0.653 \tVal Frac: 0.7765\n",
      "\tEpoch: 175/1000; 1.50 sec... \tStep: 42525... \tLoss: 0.792... \tVal Loss: 0.620 \tVal Frac: 0.7812\n",
      "\tEpoch: 180/1000; 1.39 sec... \tStep: 43740... \tLoss: 0.488... \tVal Loss: 0.635 \tVal Frac: 0.7923\n",
      "\tEpoch: 185/1000; 1.39 sec... \tStep: 44955... \tLoss: 0.890... \tVal Loss: 0.698 \tVal Frac: 0.7827\n",
      "\tEpoch: 190/1000; 1.37 sec... \tStep: 46170... \tLoss: 0.639... \tVal Loss: 0.639 \tVal Frac: 0.7732\n",
      "\tEpoch: 195/1000; 1.39 sec... \tStep: 47385... \tLoss: 0.635... \tVal Loss: 0.641 \tVal Frac: 0.7691\n",
      "\tEpoch: 200/1000; 1.39 sec... \tStep: 48600... \tLoss: 0.854... \tVal Loss: 0.643 \tVal Frac: 0.7688\n",
      "\tEpoch: 205/1000; 1.46 sec... \tStep: 49815... \tLoss: 0.872... \tVal Loss: 0.610 \tVal Frac: 0.7857\n",
      "\tValidation loss decreased (0.613 --> 0.610).  Saving model ...\n",
      "\tEpoch: 210/1000; 1.37 sec... \tStep: 51030... \tLoss: 0.698... \tVal Loss: 0.640 \tVal Frac: 0.7886\n",
      "\tEpoch: 215/1000; 1.45 sec... \tStep: 52245... \tLoss: 1.353... \tVal Loss: 0.664 \tVal Frac: 0.7691\n",
      "\tEpoch: 220/1000; 1.56 sec... \tStep: 53460... \tLoss: 0.836... \tVal Loss: 0.704 \tVal Frac: 0.7732\n",
      "\tEpoch: 225/1000; 1.47 sec... \tStep: 54675... \tLoss: 0.907... \tVal Loss: 0.648 \tVal Frac: 0.7739\n",
      "\tEpoch: 230/1000; 1.35 sec... \tStep: 55890... \tLoss: 0.942... \tVal Loss: 0.623 \tVal Frac: 0.7805\n",
      "\tEpoch: 235/1000; 1.39 sec... \tStep: 57105... \tLoss: 0.911... \tVal Loss: 0.623 \tVal Frac: 0.7945\n",
      "\tEpoch: 240/1000; 1.29 sec... \tStep: 58320... \tLoss: 1.033... \tVal Loss: 0.618 \tVal Frac: 0.7963\n",
      "\tEpoch: 245/1000; 1.37 sec... \tStep: 59535... \tLoss: 1.011... \tVal Loss: 0.640 \tVal Frac: 0.7728\n",
      "\tEpoch: 250/1000; 1.46 sec... \tStep: 60750... \tLoss: 0.927... \tVal Loss: 0.664 \tVal Frac: 0.7629\n",
      "\tEpoch: 255/1000; 1.33 sec... \tStep: 61965... \tLoss: 0.910... \tVal Loss: 0.624 \tVal Frac: 0.7779\n",
      "\tEpoch: 260/1000; 1.45 sec... \tStep: 63180... \tLoss: 0.856... \tVal Loss: 0.672 \tVal Frac: 0.7544\n",
      "\tEpoch: 265/1000; 1.37 sec... \tStep: 64395... \tLoss: 0.857... \tVal Loss: 0.657 \tVal Frac: 0.7684\n",
      "\tEpoch: 270/1000; 1.30 sec... \tStep: 65610... \tLoss: 0.770... \tVal Loss: 0.619 \tVal Frac: 0.7794\n",
      "\tEpoch: 275/1000; 1.35 sec... \tStep: 66825... \tLoss: 0.893... \tVal Loss: 0.623 \tVal Frac: 0.7857\n",
      "\tEpoch: 280/1000; 1.41 sec... \tStep: 68040... \tLoss: 1.063... \tVal Loss: 0.710 \tVal Frac: 0.7743\n",
      "\tEpoch: 285/1000; 1.46 sec... \tStep: 69255... \tLoss: 0.710... \tVal Loss: 0.711 \tVal Frac: 0.7537\n",
      "\tEpoch: 290/1000; 1.41 sec... \tStep: 70470... \tLoss: 0.852... \tVal Loss: 0.659 \tVal Frac: 0.7783\n",
      "\tEpoch: 295/1000; 1.45 sec... \tStep: 71685... \tLoss: 0.643... \tVal Loss: 0.632 \tVal Frac: 0.7798\n",
      "\tEpoch: 300/1000; 1.42 sec... \tStep: 72900... \tLoss: 0.825... \tVal Loss: 0.691 \tVal Frac: 0.7607\n",
      "\tEpoch: 305/1000; 1.36 sec... \tStep: 74115... \tLoss: 1.096... \tVal Loss: 0.655 \tVal Frac: 0.7827\n",
      "\tEpoch: 310/1000; 1.43 sec... \tStep: 75330... \tLoss: 0.621... \tVal Loss: 0.646 \tVal Frac: 0.7827\n",
      "\tEpoch: 315/1000; 1.52 sec... \tStep: 76545... \tLoss: 1.097... \tVal Loss: 0.823 \tVal Frac: 0.7287\n",
      "\tEpoch: 320/1000; 1.46 sec... \tStep: 77760... \tLoss: 0.545... \tVal Loss: 0.701 \tVal Frac: 0.7555\n",
      "\tEpoch: 325/1000; 1.40 sec... \tStep: 78975... \tLoss: 1.265... \tVal Loss: 0.652 \tVal Frac: 0.7805\n",
      "\tEpoch: 330/1000; 1.37 sec... \tStep: 80190... \tLoss: 0.770... \tVal Loss: 0.733 \tVal Frac: 0.7610\n",
      "\tEpoch: 335/1000; 1.43 sec... \tStep: 81405... \tLoss: 0.684... \tVal Loss: 0.641 \tVal Frac: 0.7790\n",
      "\tEpoch: 340/1000; 1.43 sec... \tStep: 82620... \tLoss: 0.691... \tVal Loss: 0.665 \tVal Frac: 0.7684\n",
      "\tEpoch: 345/1000; 1.39 sec... \tStep: 83835... \tLoss: 0.988... \tVal Loss: 0.614 \tVal Frac: 0.7835\n",
      "\tEpoch: 350/1000; 1.35 sec... \tStep: 85050... \tLoss: 0.399... \tVal Loss: 0.653 \tVal Frac: 0.7728\n",
      "\tEpoch: 355/1000; 1.33 sec... \tStep: 86265... \tLoss: 1.043... \tVal Loss: 0.664 \tVal Frac: 0.7739\n",
      "\tEpoch: 360/1000; 1.47 sec... \tStep: 87480... \tLoss: 1.024... \tVal Loss: 0.661 \tVal Frac: 0.7875\n",
      "\tEpoch: 365/1000; 1.43 sec... \tStep: 88695... \tLoss: 0.821... \tVal Loss: 0.620 \tVal Frac: 0.8000\n",
      "\tEpoch: 370/1000; 1.37 sec... \tStep: 89910... \tLoss: 0.754... \tVal Loss: 0.621 \tVal Frac: 0.8110\n",
      "\tEpoch: 375/1000; 1.29 sec... \tStep: 91125... \tLoss: 1.168... \tVal Loss: 0.702 \tVal Frac: 0.7882\n",
      "\tEpoch: 380/1000; 1.43 sec... \tStep: 92340... \tLoss: 1.172... \tVal Loss: 0.724 \tVal Frac: 0.7614\n",
      "\tEpoch: 385/1000; 1.49 sec... \tStep: 93555... \tLoss: 0.835... \tVal Loss: 0.626 \tVal Frac: 0.7993\n",
      "\tEpoch: 390/1000; 1.39 sec... \tStep: 94770... \tLoss: 0.844... \tVal Loss: 0.699 \tVal Frac: 0.7904\n",
      "\tEpoch: 395/1000; 1.36 sec... \tStep: 95985... \tLoss: 0.621... \tVal Loss: 0.616 \tVal Frac: 0.8118\n",
      "\tEpoch: 400/1000; 1.33 sec... \tStep: 97200... \tLoss: 0.741... \tVal Loss: 0.608 \tVal Frac: 0.8048\n",
      "\tValidation loss decreased (0.610 --> 0.608).  Saving model ...\n",
      "\tEpoch: 405/1000; 1.47 sec... \tStep: 98415... \tLoss: 0.560... \tVal Loss: 0.610 \tVal Frac: 0.7993\n",
      "\tEpoch: 410/1000; 1.43 sec... \tStep: 99630... \tLoss: 0.731... \tVal Loss: 0.614 \tVal Frac: 0.8066\n",
      "\tEpoch: 415/1000; 1.48 sec... \tStep: 100845... \tLoss: 0.610... \tVal Loss: 0.594 \tVal Frac: 0.8037\n",
      "\tValidation loss decreased (0.608 --> 0.594).  Saving model ...\n",
      "\tEpoch: 420/1000; 1.35 sec... \tStep: 102060... \tLoss: 1.164... \tVal Loss: 0.589 \tVal Frac: 0.8169\n",
      "\tValidation loss decreased (0.594 --> 0.589).  Saving model ...\n",
      "\tEpoch: 425/1000; 1.40 sec... \tStep: 103275... \tLoss: 0.635... \tVal Loss: 0.585 \tVal Frac: 0.8143\n",
      "\tValidation loss decreased (0.589 --> 0.585).  Saving model ...\n",
      "\tEpoch: 430/1000; 1.30 sec... \tStep: 104490... \tLoss: 1.066... \tVal Loss: 0.600 \tVal Frac: 0.7993\n",
      "\tEpoch: 435/1000; 1.33 sec... \tStep: 105705... \tLoss: 0.845... \tVal Loss: 0.613 \tVal Frac: 0.7882\n",
      "\tEpoch: 440/1000; 1.39 sec... \tStep: 106920... \tLoss: 0.732... \tVal Loss: 0.586 \tVal Frac: 0.8066\n",
      "\tEpoch: 445/1000; 1.33 sec... \tStep: 108135... \tLoss: 0.723... \tVal Loss: 0.570 \tVal Frac: 0.8180\n",
      "\tValidation loss decreased (0.585 --> 0.570).  Saving model ...\n",
      "\tEpoch: 450/1000; 1.43 sec... \tStep: 109350... \tLoss: 0.575... \tVal Loss: 0.595 \tVal Frac: 0.8018\n",
      "\tEpoch: 455/1000; 1.39 sec... \tStep: 110565... \tLoss: 0.908... \tVal Loss: 0.580 \tVal Frac: 0.8085\n",
      "\tEpoch: 460/1000; 1.40 sec... \tStep: 111780... \tLoss: 1.023... \tVal Loss: 0.613 \tVal Frac: 0.8040\n",
      "\tEpoch: 465/1000; 1.39 sec... \tStep: 112995... \tLoss: 0.827... \tVal Loss: 0.592 \tVal Frac: 0.8070\n",
      "\tEpoch: 470/1000; 1.49 sec... \tStep: 114210... \tLoss: 0.475... \tVal Loss: 0.623 \tVal Frac: 0.7989\n",
      "\tEpoch: 475/1000; 1.47 sec... \tStep: 115425... \tLoss: 0.586... \tVal Loss: 0.606 \tVal Frac: 0.8055\n",
      "\tEpoch: 480/1000; 1.43 sec... \tStep: 116640... \tLoss: 0.654... \tVal Loss: 0.709 \tVal Frac: 0.7809\n",
      "\tEpoch: 485/1000; 1.52 sec... \tStep: 117855... \tLoss: 0.517... \tVal Loss: 0.619 \tVal Frac: 0.8015\n",
      "\tEpoch: 490/1000; 1.56 sec... \tStep: 119070... \tLoss: 0.532... \tVal Loss: 0.618 \tVal Frac: 0.7937\n",
      "\tEpoch: 495/1000; 1.50 sec... \tStep: 120285... \tLoss: 1.030... \tVal Loss: 0.647 \tVal Frac: 0.7842\n",
      "\tEpoch: 500/1000; 1.35 sec... \tStep: 121500... \tLoss: 0.674... \tVal Loss: 0.617 \tVal Frac: 0.8040\n",
      "\tEpoch: 505/1000; 1.43 sec... \tStep: 122715... \tLoss: 0.751... \tVal Loss: 0.598 \tVal Frac: 0.8044\n",
      "\tEpoch: 510/1000; 1.45 sec... \tStep: 123930... \tLoss: 1.053... \tVal Loss: 0.595 \tVal Frac: 0.8051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 515/1000; 1.46 sec... \tStep: 125145... \tLoss: 0.408... \tVal Loss: 0.597 \tVal Frac: 0.8092\n",
      "\tEpoch: 520/1000; 1.36 sec... \tStep: 126360... \tLoss: 0.942... \tVal Loss: 0.585 \tVal Frac: 0.8143\n",
      "\tEpoch: 525/1000; 1.37 sec... \tStep: 127575... \tLoss: 0.406... \tVal Loss: 0.564 \tVal Frac: 0.8162\n",
      "\tValidation loss decreased (0.570 --> 0.564).  Saving model ...\n",
      "\tEpoch: 530/1000; 1.40 sec... \tStep: 128790... \tLoss: 0.711... \tVal Loss: 0.551 \tVal Frac: 0.8224\n",
      "\tValidation loss decreased (0.564 --> 0.551).  Saving model ...\n",
      "\tEpoch: 535/1000; 1.54 sec... \tStep: 130005... \tLoss: 0.910... \tVal Loss: 0.568 \tVal Frac: 0.8184\n",
      "\tEpoch: 540/1000; 1.30 sec... \tStep: 131220... \tLoss: 0.681... \tVal Loss: 0.612 \tVal Frac: 0.8191\n",
      "\tEpoch: 545/1000; 1.50 sec... \tStep: 132435... \tLoss: 0.941... \tVal Loss: 0.631 \tVal Frac: 0.7993\n",
      "\tEpoch: 550/1000; 1.38 sec... \tStep: 133650... \tLoss: 0.906... \tVal Loss: 0.566 \tVal Frac: 0.8140\n",
      "\tEpoch: 555/1000; 1.46 sec... \tStep: 134865... \tLoss: 0.899... \tVal Loss: 0.576 \tVal Frac: 0.8184\n",
      "\tEpoch: 560/1000; 1.42 sec... \tStep: 136080... \tLoss: 0.860... \tVal Loss: 0.599 \tVal Frac: 0.8125\n",
      "\tEpoch: 565/1000; 1.40 sec... \tStep: 137295... \tLoss: 0.787... \tVal Loss: 0.563 \tVal Frac: 0.8243\n",
      "\tEpoch: 570/1000; 1.38 sec... \tStep: 138510... \tLoss: 1.059... \tVal Loss: 0.573 \tVal Frac: 0.8191\n",
      "\tEpoch: 575/1000; 1.35 sec... \tStep: 139725... \tLoss: 0.649... \tVal Loss: 0.615 \tVal Frac: 0.7952\n",
      "\tEpoch: 580/1000; 1.49 sec... \tStep: 140940... \tLoss: 0.569... \tVal Loss: 0.614 \tVal Frac: 0.8118\n",
      "\tEpoch: 585/1000; 1.29 sec... \tStep: 142155... \tLoss: 0.709... \tVal Loss: 0.580 \tVal Frac: 0.8121\n",
      "\tEpoch: 590/1000; 1.47 sec... \tStep: 143370... \tLoss: 0.909... \tVal Loss: 0.568 \tVal Frac: 0.8169\n",
      "\tEpoch: 595/1000; 1.39 sec... \tStep: 144585... \tLoss: 0.814... \tVal Loss: 0.599 \tVal Frac: 0.8092\n",
      "\tEpoch: 600/1000; 1.32 sec... \tStep: 145800... \tLoss: 1.525... \tVal Loss: 0.555 \tVal Frac: 0.8301\n",
      "\tEpoch: 605/1000; 1.37 sec... \tStep: 147015... \tLoss: 0.851... \tVal Loss: 0.541 \tVal Frac: 0.8254\n",
      "\tValidation loss decreased (0.551 --> 0.541).  Saving model ...\n",
      "\tEpoch: 610/1000; 1.36 sec... \tStep: 148230... \tLoss: 1.166... \tVal Loss: 0.548 \tVal Frac: 0.8298\n",
      "\tEpoch: 615/1000; 1.39 sec... \tStep: 149445... \tLoss: 0.741... \tVal Loss: 0.549 \tVal Frac: 0.8301\n",
      "\tEpoch: 620/1000; 1.43 sec... \tStep: 150660... \tLoss: 0.580... \tVal Loss: 0.511 \tVal Frac: 0.8360\n",
      "\tValidation loss decreased (0.541 --> 0.511).  Saving model ...\n",
      "\tEpoch: 625/1000; 1.44 sec... \tStep: 151875... \tLoss: 0.761... \tVal Loss: 0.625 \tVal Frac: 0.8074\n",
      "\tEpoch: 630/1000; 1.40 sec... \tStep: 153090... \tLoss: 1.112... \tVal Loss: 0.579 \tVal Frac: 0.8232\n",
      "\tEpoch: 635/1000; 1.25 sec... \tStep: 154305... \tLoss: 0.711... \tVal Loss: 0.562 \tVal Frac: 0.8187\n",
      "\tEpoch: 640/1000; 1.32 sec... \tStep: 155520... \tLoss: 0.530... \tVal Loss: 0.518 \tVal Frac: 0.8353\n",
      "\tEpoch: 645/1000; 1.29 sec... \tStep: 156735... \tLoss: 0.651... \tVal Loss: 0.514 \tVal Frac: 0.8331\n",
      "\tEpoch: 650/1000; 1.35 sec... \tStep: 157950... \tLoss: 0.347... \tVal Loss: 0.518 \tVal Frac: 0.8320\n",
      "\tEpoch: 655/1000; 1.49 sec... \tStep: 159165... \tLoss: 0.610... \tVal Loss: 0.497 \tVal Frac: 0.8371\n",
      "\tValidation loss decreased (0.511 --> 0.497).  Saving model ...\n",
      "\tEpoch: 660/1000; 1.46 sec... \tStep: 160380... \tLoss: 0.620... \tVal Loss: 0.540 \tVal Frac: 0.8176\n",
      "\tEpoch: 665/1000; 1.47 sec... \tStep: 161595... \tLoss: 0.704... \tVal Loss: 0.557 \tVal Frac: 0.8364\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 0.645... \tVal Loss: 0.533 \tVal Frac: 0.8279\n",
      "\tEpoch: 675/1000; 1.47 sec... \tStep: 164025... \tLoss: 0.596... \tVal Loss: 0.573 \tVal Frac: 0.8169\n",
      "\tEpoch: 680/1000; 1.35 sec... \tStep: 165240... \tLoss: 0.715... \tVal Loss: 0.520 \tVal Frac: 0.8309\n",
      "\tEpoch: 685/1000; 1.38 sec... \tStep: 166455... \tLoss: 0.378... \tVal Loss: 0.530 \tVal Frac: 0.8327\n",
      "\tEpoch: 690/1000; 1.48 sec... \tStep: 167670... \tLoss: 0.691... \tVal Loss: 0.523 \tVal Frac: 0.8327\n",
      "\tEpoch: 695/1000; 1.40 sec... \tStep: 168885... \tLoss: 0.584... \tVal Loss: 0.529 \tVal Frac: 0.8224\n",
      "\tEpoch: 700/1000; 1.49 sec... \tStep: 170100... \tLoss: 0.855... \tVal Loss: 0.547 \tVal Frac: 0.8272\n",
      "\tEpoch: 705/1000; 1.45 sec... \tStep: 171315... \tLoss: 0.692... \tVal Loss: 0.524 \tVal Frac: 0.8313\n",
      "\tEpoch: 710/1000; 1.44 sec... \tStep: 172530... \tLoss: 0.830... \tVal Loss: 0.521 \tVal Frac: 0.8298\n",
      "\tEpoch: 715/1000; 1.47 sec... \tStep: 173745... \tLoss: 0.587... \tVal Loss: 0.519 \tVal Frac: 0.8301\n",
      "\tEpoch: 720/1000; 1.40 sec... \tStep: 174960... \tLoss: 0.604... \tVal Loss: 0.510 \tVal Frac: 0.8301\n",
      "\tEpoch: 725/1000; 1.56 sec... \tStep: 176175... \tLoss: 0.557... \tVal Loss: 0.599 \tVal Frac: 0.8158\n",
      "\tEpoch: 730/1000; 1.44 sec... \tStep: 177390... \tLoss: 0.980... \tVal Loss: 0.517 \tVal Frac: 0.8301\n",
      "\tEpoch: 735/1000; 1.37 sec... \tStep: 178605... \tLoss: 0.566... \tVal Loss: 0.545 \tVal Frac: 0.8254\n",
      "\tEpoch: 740/1000; 1.43 sec... \tStep: 179820... \tLoss: 0.819... \tVal Loss: 0.506 \tVal Frac: 0.8349\n",
      "\tEpoch: 745/1000; 1.37 sec... \tStep: 181035... \tLoss: 0.791... \tVal Loss: 0.498 \tVal Frac: 0.8415\n",
      "\tEpoch: 750/1000; 1.40 sec... \tStep: 182250... \tLoss: 0.675... \tVal Loss: 0.515 \tVal Frac: 0.8357\n",
      "\tEpoch: 755/1000; 1.39 sec... \tStep: 183465... \tLoss: 0.730... \tVal Loss: 0.517 \tVal Frac: 0.8371\n",
      "\tEpoch: 760/1000; 1.42 sec... \tStep: 184680... \tLoss: 0.715... \tVal Loss: 0.534 \tVal Frac: 0.8239\n",
      "\tEpoch: 765/1000; 1.38 sec... \tStep: 185895... \tLoss: 0.373... \tVal Loss: 0.529 \tVal Frac: 0.8268\n",
      "\tEpoch: 770/1000; 1.32 sec... \tStep: 187110... \tLoss: 0.821... \tVal Loss: 0.534 \tVal Frac: 0.8287\n",
      "\tEpoch: 775/1000; 1.42 sec... \tStep: 188325... \tLoss: 0.568... \tVal Loss: 0.514 \tVal Frac: 0.8320\n",
      "\tEpoch: 780/1000; 1.37 sec... \tStep: 189540... \tLoss: 0.582... \tVal Loss: 0.525 \tVal Frac: 0.8309\n",
      "\tEpoch: 785/1000; 1.44 sec... \tStep: 190755... \tLoss: 0.568... \tVal Loss: 0.546 \tVal Frac: 0.8279\n",
      "\tEpoch: 790/1000; 1.38 sec... \tStep: 191970... \tLoss: 0.660... \tVal Loss: 0.519 \tVal Frac: 0.8279\n",
      "\tEpoch: 795/1000; 1.56 sec... \tStep: 193185... \tLoss: 0.788... \tVal Loss: 0.566 \tVal Frac: 0.8265\n",
      "\tEpoch: 800/1000; 1.50 sec... \tStep: 194400... \tLoss: 0.558... \tVal Loss: 0.550 \tVal Frac: 0.8305\n",
      "\tEpoch: 805/1000; 1.46 sec... \tStep: 195615... \tLoss: 0.410... \tVal Loss: 0.512 \tVal Frac: 0.8379\n",
      "\tEpoch: 810/1000; 1.35 sec... \tStep: 196830... \tLoss: 0.353... \tVal Loss: 0.497 \tVal Frac: 0.8471\n",
      "\tValidation loss decreased (0.497 --> 0.497).  Saving model ...\n",
      "\tEpoch: 815/1000; 1.35 sec... \tStep: 198045... \tLoss: 0.432... \tVal Loss: 0.528 \tVal Frac: 0.8199\n",
      "\tEpoch: 820/1000; 1.32 sec... \tStep: 199260... \tLoss: 0.708... \tVal Loss: 0.565 \tVal Frac: 0.8158\n",
      "\tEpoch: 825/1000; 1.47 sec... \tStep: 200475... \tLoss: 1.027... \tVal Loss: 0.548 \tVal Frac: 0.8187\n",
      "\tEpoch: 830/1000; 1.44 sec... \tStep: 201690... \tLoss: 0.829... \tVal Loss: 0.519 \tVal Frac: 0.8283\n",
      "\tEpoch: 835/1000; 1.35 sec... \tStep: 202905... \tLoss: 0.414... \tVal Loss: 0.533 \tVal Frac: 0.8375\n",
      "\tEpoch: 840/1000; 1.50 sec... \tStep: 204120... \tLoss: 0.582... \tVal Loss: 0.532 \tVal Frac: 0.8357\n",
      "\tEpoch: 845/1000; 1.36 sec... \tStep: 205335... \tLoss: 0.506... \tVal Loss: 0.527 \tVal Frac: 0.8382\n",
      "\tEpoch: 850/1000; 1.37 sec... \tStep: 206550... \tLoss: 0.494... \tVal Loss: 0.572 \tVal Frac: 0.8191\n",
      "\tEpoch: 855/1000; 1.50 sec... \tStep: 207765... \tLoss: 0.621... \tVal Loss: 0.543 \tVal Frac: 0.8232\n",
      "\tEpoch: 860/1000; 1.42 sec... \tStep: 208980... \tLoss: 1.064... \tVal Loss: 0.540 \tVal Frac: 0.8265\n",
      "\tEpoch: 865/1000; 1.32 sec... \tStep: 210195... \tLoss: 0.448... \tVal Loss: 0.554 \tVal Frac: 0.8250\n",
      "\tEpoch: 870/1000; 1.36 sec... \tStep: 211410... \tLoss: 0.530... \tVal Loss: 0.540 \tVal Frac: 0.8228\n",
      "\tEpoch: 875/1000; 1.48 sec... \tStep: 212625... \tLoss: 0.589... \tVal Loss: 0.543 \tVal Frac: 0.8224\n",
      "\tEpoch: 880/1000; 1.38 sec... \tStep: 213840... \tLoss: 0.691... \tVal Loss: 0.526 \tVal Frac: 0.8265\n",
      "\tEpoch: 885/1000; 1.46 sec... \tStep: 215055... \tLoss: 0.714... \tVal Loss: 0.569 \tVal Frac: 0.8173\n",
      "\tEpoch: 890/1000; 1.57 sec... \tStep: 216270... \tLoss: 0.792... \tVal Loss: 0.568 \tVal Frac: 0.8283\n",
      "\tEpoch: 895/1000; 1.42 sec... \tStep: 217485... \tLoss: 0.402... \tVal Loss: 0.588 \tVal Frac: 0.8221\n",
      "\tEpoch: 900/1000; 1.47 sec... \tStep: 218700... \tLoss: 0.943... \tVal Loss: 0.583 \tVal Frac: 0.8110\n",
      "\tEpoch: 905/1000; 1.40 sec... \tStep: 219915... \tLoss: 0.460... \tVal Loss: 0.537 \tVal Frac: 0.8232\n",
      "\tEpoch: 910/1000; 1.32 sec... \tStep: 221130... \tLoss: 0.487... \tVal Loss: 0.529 \tVal Frac: 0.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 915/1000; 1.29 sec... \tStep: 222345... \tLoss: 0.450... \tVal Loss: 0.519 \tVal Frac: 0.8327\n",
      "\tEpoch: 920/1000; 1.34 sec... \tStep: 223560... \tLoss: 0.504... \tVal Loss: 0.547 \tVal Frac: 0.8298\n",
      "\tEpoch: 925/1000; 1.44 sec... \tStep: 224775... \tLoss: 0.976... \tVal Loss: 0.538 \tVal Frac: 0.8342\n",
      "\tEpoch: 930/1000; 1.38 sec... \tStep: 225990... \tLoss: 0.283... \tVal Loss: 0.511 \tVal Frac: 0.8287\n",
      "\tEpoch: 935/1000; 1.45 sec... \tStep: 227205... \tLoss: 0.538... \tVal Loss: 0.562 \tVal Frac: 0.8195\n",
      "\tEpoch: 940/1000; 1.44 sec... \tStep: 228420... \tLoss: 0.508... \tVal Loss: 0.547 \tVal Frac: 0.8232\n",
      "\tEpoch: 945/1000; 1.35 sec... \tStep: 229635... \tLoss: 0.361... \tVal Loss: 0.543 \tVal Frac: 0.8287\n",
      "\tEpoch: 950/1000; 1.34 sec... \tStep: 230850... \tLoss: 0.821... \tVal Loss: 0.512 \tVal Frac: 0.8298\n",
      "\tEpoch: 955/1000; 1.34 sec... \tStep: 232065... \tLoss: 0.500... \tVal Loss: 0.533 \tVal Frac: 0.8243\n",
      "\tEpoch: 960/1000; 1.50 sec... \tStep: 233280... \tLoss: 0.741... \tVal Loss: 0.523 \tVal Frac: 0.8276\n",
      "\tEpoch: 965/1000; 1.27 sec... \tStep: 234495... \tLoss: 0.660... \tVal Loss: 0.512 \tVal Frac: 0.8224\n",
      "\tEpoch: 970/1000; 1.40 sec... \tStep: 235710... \tLoss: 0.638... \tVal Loss: 0.521 \tVal Frac: 0.8294\n",
      "\tEpoch: 975/1000; 1.46 sec... \tStep: 236925... \tLoss: 0.523... \tVal Loss: 0.527 \tVal Frac: 0.8287\n",
      "\tEpoch: 980/1000; 1.40 sec... \tStep: 238140... \tLoss: 0.900... \tVal Loss: 0.528 \tVal Frac: 0.8265\n",
      "\tEpoch: 985/1000; 1.39 sec... \tStep: 239355... \tLoss: 0.864... \tVal Loss: 0.539 \tVal Frac: 0.8165\n",
      "\tEpoch: 990/1000; 1.42 sec... \tStep: 240570... \tLoss: 0.592... \tVal Loss: 0.494 \tVal Frac: 0.8357\n",
      "\tValidation loss decreased (0.497 --> 0.494).  Saving model ...\n",
      "\tEpoch: 995/1000; 1.39 sec... \tStep: 241785... \tLoss: 0.485... \tVal Loss: 0.495 \tVal Frac: 0.8331\n",
      "\tEpoch: 1000/1000; 1.47 sec... \tStep: 243000... \tLoss: 0.702... \tVal Loss: 0.518 \tVal Frac: 0.8290\n",
      "Completed:  lr :  0.001 \n",
      "\tloss: 0.493958 \n",
      "\tfrac:0.847059\n",
      "\n",
      "###########\n",
      "Testing with  lr :  0.01\n",
      "\tEpoch: 5/1000; 1.32 sec... \tStep: 1215... \tLoss: 1.069... \tVal Loss: 0.818 \tVal Frac: 0.7629\n",
      "\tValidation loss decreased (inf --> 0.818).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.45 sec... \tStep: 2430... \tLoss: 0.943... \tVal Loss: 0.765 \tVal Frac: 0.7353\n",
      "\tValidation loss decreased (0.818 --> 0.765).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.42 sec... \tStep: 3645... \tLoss: 0.909... \tVal Loss: 0.724 \tVal Frac: 0.7607\n",
      "\tValidation loss decreased (0.765 --> 0.724).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.39 sec... \tStep: 4860... \tLoss: 1.015... \tVal Loss: 0.764 \tVal Frac: 0.7710\n",
      "\tEpoch: 25/1000; 1.40 sec... \tStep: 6075... \tLoss: 0.530... \tVal Loss: 0.644 \tVal Frac: 0.7886\n",
      "\tValidation loss decreased (0.724 --> 0.644).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.44 sec... \tStep: 7290... \tLoss: 0.740... \tVal Loss: 0.658 \tVal Frac: 0.7996\n",
      "\tEpoch: 35/1000; 1.75 sec... \tStep: 8505... \tLoss: 0.470... \tVal Loss: 0.667 \tVal Frac: 0.7985\n",
      "\tEpoch: 40/1000; 1.42 sec... \tStep: 9720... \tLoss: 0.808... \tVal Loss: 0.736 \tVal Frac: 0.7533\n",
      "\tEpoch: 45/1000; 1.40 sec... \tStep: 10935... \tLoss: 1.045... \tVal Loss: 0.749 \tVal Frac: 0.7555\n",
      "\tEpoch: 50/1000; 1.37 sec... \tStep: 12150... \tLoss: 1.020... \tVal Loss: 0.694 \tVal Frac: 0.7643\n",
      "\tEpoch: 55/1000; 1.44 sec... \tStep: 13365... \tLoss: 0.893... \tVal Loss: 0.633 \tVal Frac: 0.7798\n",
      "\tValidation loss decreased (0.644 --> 0.633).  Saving model ...\n",
      "\tEpoch: 60/1000; 1.37 sec... \tStep: 14580... \tLoss: 0.735... \tVal Loss: 0.662 \tVal Frac: 0.7783\n",
      "\tEpoch: 65/1000; 1.39 sec... \tStep: 15795... \tLoss: 0.819... \tVal Loss: 0.667 \tVal Frac: 0.7673\n",
      "\tEpoch: 70/1000; 1.42 sec... \tStep: 17010... \tLoss: 0.720... \tVal Loss: 0.639 \tVal Frac: 0.7993\n",
      "\tEpoch: 75/1000; 1.40 sec... \tStep: 18225... \tLoss: 1.163... \tVal Loss: 0.641 \tVal Frac: 0.7967\n",
      "\tEpoch: 80/1000; 1.35 sec... \tStep: 19440... \tLoss: 0.735... \tVal Loss: 0.659 \tVal Frac: 0.7706\n",
      "\tEpoch: 85/1000; 1.40 sec... \tStep: 20655... \tLoss: 0.924... \tVal Loss: 0.683 \tVal Frac: 0.7658\n",
      "\tEpoch: 90/1000; 1.35 sec... \tStep: 21870... \tLoss: 0.855... \tVal Loss: 0.659 \tVal Frac: 0.7908\n",
      "\tEpoch: 95/1000; 1.44 sec... \tStep: 23085... \tLoss: 0.822... \tVal Loss: 0.608 \tVal Frac: 0.8044\n",
      "\tValidation loss decreased (0.633 --> 0.608).  Saving model ...\n",
      "\tEpoch: 100/1000; 1.30 sec... \tStep: 24300... \tLoss: 0.680... \tVal Loss: 0.628 \tVal Frac: 0.7710\n",
      "\tEpoch: 105/1000; 1.26 sec... \tStep: 25515... \tLoss: 0.641... \tVal Loss: 0.635 \tVal Frac: 0.7746\n",
      "\tEpoch: 110/1000; 1.40 sec... \tStep: 26730... \tLoss: 0.919... \tVal Loss: 0.589 \tVal Frac: 0.7960\n",
      "\tValidation loss decreased (0.608 --> 0.589).  Saving model ...\n",
      "\tEpoch: 115/1000; 1.40 sec... \tStep: 27945... \tLoss: 0.727... \tVal Loss: 0.603 \tVal Frac: 0.8055\n",
      "\tEpoch: 120/1000; 1.37 sec... \tStep: 29160... \tLoss: 1.003... \tVal Loss: 0.635 \tVal Frac: 0.7974\n",
      "\tEpoch: 125/1000; 1.34 sec... \tStep: 30375... \tLoss: 1.296... \tVal Loss: 0.618 \tVal Frac: 0.8000\n",
      "\tEpoch: 130/1000; 1.47 sec... \tStep: 31590... \tLoss: 0.751... \tVal Loss: 0.635 \tVal Frac: 0.8048\n",
      "\tEpoch: 135/1000; 1.39 sec... \tStep: 32805... \tLoss: 0.747... \tVal Loss: 0.603 \tVal Frac: 0.8125\n",
      "\tEpoch: 140/1000; 1.47 sec... \tStep: 34020... \tLoss: 1.145... \tVal Loss: 0.619 \tVal Frac: 0.7974\n",
      "\tEpoch: 145/1000; 1.42 sec... \tStep: 35235... \tLoss: 0.457... \tVal Loss: 0.625 \tVal Frac: 0.7842\n",
      "\tEpoch: 150/1000; 1.34 sec... \tStep: 36450... \tLoss: 1.201... \tVal Loss: 0.664 \tVal Frac: 0.7798\n",
      "\tEpoch: 155/1000; 1.40 sec... \tStep: 37665... \tLoss: 0.948... \tVal Loss: 0.591 \tVal Frac: 0.8077\n",
      "\tEpoch: 160/1000; 1.41 sec... \tStep: 38880... \tLoss: 0.545... \tVal Loss: 0.619 \tVal Frac: 0.7952\n",
      "\tEpoch: 165/1000; 1.37 sec... \tStep: 40095... \tLoss: 0.878... \tVal Loss: 0.616 \tVal Frac: 0.7945\n",
      "\tEpoch: 170/1000; 1.46 sec... \tStep: 41310... \tLoss: 0.944... \tVal Loss: 0.602 \tVal Frac: 0.8000\n",
      "\tEpoch: 175/1000; 1.37 sec... \tStep: 42525... \tLoss: 0.927... \tVal Loss: 0.633 \tVal Frac: 0.7919\n",
      "\tEpoch: 180/1000; 1.42 sec... \tStep: 43740... \tLoss: 0.553... \tVal Loss: 0.619 \tVal Frac: 0.7934\n",
      "\tEpoch: 185/1000; 1.40 sec... \tStep: 44955... \tLoss: 0.725... \tVal Loss: 0.615 \tVal Frac: 0.7919\n",
      "\tEpoch: 190/1000; 1.37 sec... \tStep: 46170... \tLoss: 0.457... \tVal Loss: 0.603 \tVal Frac: 0.7996\n",
      "\tEpoch: 195/1000; 1.44 sec... \tStep: 47385... \tLoss: 0.669... \tVal Loss: 0.608 \tVal Frac: 0.7882\n",
      "\tEpoch: 200/1000; 1.30 sec... \tStep: 48600... \tLoss: 0.750... \tVal Loss: 0.609 \tVal Frac: 0.7971\n",
      "\tEpoch: 205/1000; 1.53 sec... \tStep: 49815... \tLoss: 0.716... \tVal Loss: 0.629 \tVal Frac: 0.7754\n",
      "\tEpoch: 210/1000; 1.40 sec... \tStep: 51030... \tLoss: 0.754... \tVal Loss: 0.623 \tVal Frac: 0.7846\n",
      "\tEpoch: 215/1000; 1.36 sec... \tStep: 52245... \tLoss: 1.170... \tVal Loss: 0.635 \tVal Frac: 0.7886\n",
      "\tEpoch: 220/1000; 1.54 sec... \tStep: 53460... \tLoss: 0.572... \tVal Loss: 0.609 \tVal Frac: 0.7816\n",
      "\tEpoch: 225/1000; 1.39 sec... \tStep: 54675... \tLoss: 0.877... \tVal Loss: 0.618 \tVal Frac: 0.8004\n",
      "\tEpoch: 230/1000; 1.40 sec... \tStep: 55890... \tLoss: 1.319... \tVal Loss: 0.610 \tVal Frac: 0.8187\n",
      "\tEpoch: 235/1000; 1.38 sec... \tStep: 57105... \tLoss: 1.090... \tVal Loss: 0.602 \tVal Frac: 0.7989\n",
      "\tEpoch: 240/1000; 1.42 sec... \tStep: 58320... \tLoss: 0.661... \tVal Loss: 0.620 \tVal Frac: 0.7993\n",
      "\tEpoch: 245/1000; 1.27 sec... \tStep: 59535... \tLoss: 0.843... \tVal Loss: 0.623 \tVal Frac: 0.7993\n",
      "\tEpoch: 250/1000; 1.44 sec... \tStep: 60750... \tLoss: 0.666... \tVal Loss: 0.602 \tVal Frac: 0.8184\n",
      "\tEpoch: 255/1000; 1.44 sec... \tStep: 61965... \tLoss: 0.753... \tVal Loss: 0.575 \tVal Frac: 0.8232\n",
      "\tValidation loss decreased (0.589 --> 0.575).  Saving model ...\n",
      "\tEpoch: 260/1000; 1.52 sec... \tStep: 63180... \tLoss: 0.713... \tVal Loss: 0.596 \tVal Frac: 0.7985\n",
      "\tEpoch: 265/1000; 1.35 sec... \tStep: 64395... \tLoss: 0.735... \tVal Loss: 0.602 \tVal Frac: 0.7952\n",
      "\tEpoch: 270/1000; 1.42 sec... \tStep: 65610... \tLoss: 1.013... \tVal Loss: 0.594 \tVal Frac: 0.8063\n",
      "\tEpoch: 275/1000; 1.27 sec... \tStep: 66825... \tLoss: 0.930... \tVal Loss: 0.606 \tVal Frac: 0.8051\n",
      "\tEpoch: 280/1000; 1.34 sec... \tStep: 68040... \tLoss: 0.972... \tVal Loss: 0.612 \tVal Frac: 0.7949\n",
      "\tEpoch: 285/1000; 1.42 sec... \tStep: 69255... \tLoss: 0.815... \tVal Loss: 0.666 \tVal Frac: 0.7574\n",
      "\tEpoch: 290/1000; 1.40 sec... \tStep: 70470... \tLoss: 0.655... \tVal Loss: 0.603 \tVal Frac: 0.8004\n",
      "\tEpoch: 295/1000; 1.27 sec... \tStep: 71685... \tLoss: 0.660... \tVal Loss: 0.596 \tVal Frac: 0.8000\n",
      "\tEpoch: 300/1000; 1.47 sec... \tStep: 72900... \tLoss: 0.577... \tVal Loss: 0.596 \tVal Frac: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 305/1000; 1.32 sec... \tStep: 74115... \tLoss: 0.837... \tVal Loss: 0.595 \tVal Frac: 0.7982\n",
      "\tEpoch: 310/1000; 1.37 sec... \tStep: 75330... \tLoss: 0.587... \tVal Loss: 0.603 \tVal Frac: 0.8000\n",
      "\tEpoch: 315/1000; 1.49 sec... \tStep: 76545... \tLoss: 0.890... \tVal Loss: 0.613 \tVal Frac: 0.7761\n",
      "\tEpoch: 320/1000; 1.42 sec... \tStep: 77760... \tLoss: 0.484... \tVal Loss: 0.592 \tVal Frac: 0.8107\n",
      "\tEpoch: 325/1000; 1.32 sec... \tStep: 78975... \tLoss: 1.005... \tVal Loss: 0.601 \tVal Frac: 0.8004\n",
      "\tEpoch: 330/1000; 1.37 sec... \tStep: 80190... \tLoss: 0.627... \tVal Loss: 0.611 \tVal Frac: 0.7952\n",
      "\tEpoch: 335/1000; 1.50 sec... \tStep: 81405... \tLoss: 0.516... \tVal Loss: 0.621 \tVal Frac: 0.7945\n",
      "\tEpoch: 340/1000; 1.42 sec... \tStep: 82620... \tLoss: 0.708... \tVal Loss: 0.600 \tVal Frac: 0.7897\n",
      "\tEpoch: 345/1000; 1.47 sec... \tStep: 83835... \tLoss: 0.837... \tVal Loss: 0.612 \tVal Frac: 0.8026\n",
      "\tEpoch: 350/1000; 1.26 sec... \tStep: 85050... \tLoss: 0.389... \tVal Loss: 0.612 \tVal Frac: 0.7783\n",
      "\tEpoch: 355/1000; 1.35 sec... \tStep: 86265... \tLoss: 0.565... \tVal Loss: 0.621 \tVal Frac: 0.7890\n",
      "\tEpoch: 360/1000; 1.39 sec... \tStep: 87480... \tLoss: 1.148... \tVal Loss: 0.611 \tVal Frac: 0.7971\n",
      "\tEpoch: 365/1000; 1.26 sec... \tStep: 88695... \tLoss: 0.818... \tVal Loss: 0.596 \tVal Frac: 0.7978\n",
      "\tEpoch: 370/1000; 1.54 sec... \tStep: 89910... \tLoss: 0.484... \tVal Loss: 0.589 \tVal Frac: 0.7960\n",
      "\tEpoch: 375/1000; 1.39 sec... \tStep: 91125... \tLoss: 1.088... \tVal Loss: 0.571 \tVal Frac: 0.8118\n",
      "\tValidation loss decreased (0.575 --> 0.571).  Saving model ...\n",
      "\tEpoch: 380/1000; 1.35 sec... \tStep: 92340... \tLoss: 0.613... \tVal Loss: 0.582 \tVal Frac: 0.7949\n",
      "\tEpoch: 385/1000; 1.45 sec... \tStep: 93555... \tLoss: 0.695... \tVal Loss: 0.578 \tVal Frac: 0.8151\n",
      "\tEpoch: 390/1000; 1.30 sec... \tStep: 94770... \tLoss: 0.657... \tVal Loss: 0.579 \tVal Frac: 0.7919\n",
      "\tEpoch: 395/1000; 1.30 sec... \tStep: 95985... \tLoss: 0.535... \tVal Loss: 0.613 \tVal Frac: 0.7967\n",
      "\tEpoch: 400/1000; 1.30 sec... \tStep: 97200... \tLoss: 0.858... \tVal Loss: 0.606 \tVal Frac: 0.7956\n",
      "\tEpoch: 405/1000; 1.32 sec... \tStep: 98415... \tLoss: 0.676... \tVal Loss: 0.587 \tVal Frac: 0.8044\n",
      "\tEpoch: 410/1000; 1.34 sec... \tStep: 99630... \tLoss: 0.573... \tVal Loss: 0.604 \tVal Frac: 0.7875\n",
      "\tEpoch: 415/1000; 1.37 sec... \tStep: 100845... \tLoss: 0.476... \tVal Loss: 0.598 \tVal Frac: 0.7952\n",
      "\tEpoch: 420/1000; 1.27 sec... \tStep: 102060... \tLoss: 0.931... \tVal Loss: 0.592 \tVal Frac: 0.7996\n",
      "\tEpoch: 425/1000; 1.49 sec... \tStep: 103275... \tLoss: 0.434... \tVal Loss: 0.589 \tVal Frac: 0.8099\n",
      "\tEpoch: 430/1000; 1.40 sec... \tStep: 104490... \tLoss: 0.935... \tVal Loss: 0.601 \tVal Frac: 0.7967\n",
      "\tEpoch: 435/1000; 1.38 sec... \tStep: 105705... \tLoss: 0.910... \tVal Loss: 0.606 \tVal Frac: 0.7993\n",
      "\tEpoch: 440/1000; 1.44 sec... \tStep: 106920... \tLoss: 1.074... \tVal Loss: 0.602 \tVal Frac: 0.7974\n",
      "\tEpoch: 445/1000; 1.44 sec... \tStep: 108135... \tLoss: 0.615... \tVal Loss: 0.596 \tVal Frac: 0.8187\n",
      "\tEpoch: 450/1000; 1.39 sec... \tStep: 109350... \tLoss: 0.715... \tVal Loss: 0.601 \tVal Frac: 0.8173\n",
      "\tEpoch: 455/1000; 1.42 sec... \tStep: 110565... \tLoss: 0.826... \tVal Loss: 0.603 \tVal Frac: 0.8114\n",
      "\tEpoch: 460/1000; 1.45 sec... \tStep: 111780... \tLoss: 0.898... \tVal Loss: 0.596 \tVal Frac: 0.8118\n",
      "\tEpoch: 465/1000; 1.30 sec... \tStep: 112995... \tLoss: 0.827... \tVal Loss: 0.603 \tVal Frac: 0.8037\n",
      "\tEpoch: 470/1000; 1.44 sec... \tStep: 114210... \tLoss: 0.409... \tVal Loss: 0.578 \tVal Frac: 0.8081\n",
      "\tEpoch: 475/1000; 1.39 sec... \tStep: 115425... \tLoss: 0.451... \tVal Loss: 0.588 \tVal Frac: 0.8165\n",
      "\tEpoch: 480/1000; 1.39 sec... \tStep: 116640... \tLoss: 0.543... \tVal Loss: 0.641 \tVal Frac: 0.7812\n",
      "\tEpoch: 485/1000; 1.44 sec... \tStep: 117855... \tLoss: 0.566... \tVal Loss: 0.620 \tVal Frac: 0.7978\n",
      "\tEpoch: 490/1000; 1.37 sec... \tStep: 119070... \tLoss: 0.565... \tVal Loss: 0.592 \tVal Frac: 0.8040\n",
      "\tEpoch: 495/1000; 1.34 sec... \tStep: 120285... \tLoss: 0.854... \tVal Loss: 0.616 \tVal Frac: 0.8103\n",
      "\tEpoch: 500/1000; 1.47 sec... \tStep: 121500... \tLoss: 0.629... \tVal Loss: 0.597 \tVal Frac: 0.8154\n",
      "\tEpoch: 505/1000; 1.34 sec... \tStep: 122715... \tLoss: 0.908... \tVal Loss: 0.607 \tVal Frac: 0.8040\n",
      "\tEpoch: 510/1000; 1.42 sec... \tStep: 123930... \tLoss: 0.763... \tVal Loss: 0.622 \tVal Frac: 0.7886\n",
      "\tEpoch: 515/1000; 1.30 sec... \tStep: 125145... \tLoss: 0.612... \tVal Loss: 0.608 \tVal Frac: 0.8110\n",
      "\tEpoch: 520/1000; 1.35 sec... \tStep: 126360... \tLoss: 0.920... \tVal Loss: 0.614 \tVal Frac: 0.7971\n",
      "\tEpoch: 525/1000; 1.39 sec... \tStep: 127575... \tLoss: 0.616... \tVal Loss: 0.607 \tVal Frac: 0.8077\n",
      "\tEpoch: 530/1000; 1.44 sec... \tStep: 128790... \tLoss: 1.039... \tVal Loss: 0.600 \tVal Frac: 0.8044\n",
      "\tEpoch: 535/1000; 1.47 sec... \tStep: 130005... \tLoss: 0.714... \tVal Loss: 0.606 \tVal Frac: 0.7985\n",
      "\tEpoch: 540/1000; 1.50 sec... \tStep: 131220... \tLoss: 0.583... \tVal Loss: 0.605 \tVal Frac: 0.8088\n",
      "\tEpoch: 545/1000; 1.39 sec... \tStep: 132435... \tLoss: 0.789... \tVal Loss: 0.607 \tVal Frac: 0.7886\n",
      "\tEpoch: 550/1000; 1.40 sec... \tStep: 133650... \tLoss: 0.890... \tVal Loss: 0.604 \tVal Frac: 0.8077\n",
      "\tEpoch: 555/1000; 1.37 sec... \tStep: 134865... \tLoss: 0.923... \tVal Loss: 0.606 \tVal Frac: 0.7857\n",
      "\tEpoch: 560/1000; 1.42 sec... \tStep: 136080... \tLoss: 0.583... \tVal Loss: 0.586 \tVal Frac: 0.8081\n",
      "\tEpoch: 565/1000; 1.39 sec... \tStep: 137295... \tLoss: 0.761... \tVal Loss: 0.600 \tVal Frac: 0.8125\n",
      "\tEpoch: 570/1000; 1.39 sec... \tStep: 138510... \tLoss: 0.924... \tVal Loss: 0.576 \tVal Frac: 0.8224\n",
      "\tEpoch: 575/1000; 1.40 sec... \tStep: 139725... \tLoss: 0.691... \tVal Loss: 0.634 \tVal Frac: 0.7754\n",
      "\tEpoch: 580/1000; 1.45 sec... \tStep: 140940... \tLoss: 0.479... \tVal Loss: 0.624 \tVal Frac: 0.8007\n",
      "\tEpoch: 585/1000; 1.44 sec... \tStep: 142155... \tLoss: 0.697... \tVal Loss: 0.582 \tVal Frac: 0.7982\n",
      "\tEpoch: 590/1000; 1.38 sec... \tStep: 143370... \tLoss: 0.707... \tVal Loss: 0.641 \tVal Frac: 0.7809\n",
      "\tEpoch: 595/1000; 1.47 sec... \tStep: 144585... \tLoss: 0.542... \tVal Loss: 0.595 \tVal Frac: 0.7912\n",
      "\tEpoch: 600/1000; 1.40 sec... \tStep: 145800... \tLoss: 1.082... \tVal Loss: 0.602 \tVal Frac: 0.7934\n",
      "\tEpoch: 605/1000; 1.34 sec... \tStep: 147015... \tLoss: 0.846... \tVal Loss: 0.605 \tVal Frac: 0.7904\n",
      "\tEpoch: 610/1000; 1.37 sec... \tStep: 148230... \tLoss: 1.110... \tVal Loss: 0.650 \tVal Frac: 0.7768\n",
      "\tEpoch: 615/1000; 1.32 sec... \tStep: 149445... \tLoss: 0.671... \tVal Loss: 0.611 \tVal Frac: 0.7860\n",
      "\tEpoch: 620/1000; 1.52 sec... \tStep: 150660... \tLoss: 0.504... \tVal Loss: 0.591 \tVal Frac: 0.8070\n",
      "\tEpoch: 625/1000; 1.35 sec... \tStep: 151875... \tLoss: 0.653... \tVal Loss: 0.587 \tVal Frac: 0.8147\n",
      "\tEpoch: 630/1000; 1.24 sec... \tStep: 153090... \tLoss: 1.094... \tVal Loss: 0.624 \tVal Frac: 0.7963\n",
      "\tEpoch: 635/1000; 1.42 sec... \tStep: 154305... \tLoss: 0.729... \tVal Loss: 0.596 \tVal Frac: 0.7934\n",
      "\tEpoch: 640/1000; 1.40 sec... \tStep: 155520... \tLoss: 0.448... \tVal Loss: 0.589 \tVal Frac: 0.8011\n",
      "\tEpoch: 645/1000; 1.47 sec... \tStep: 156735... \tLoss: 0.948... \tVal Loss: 0.629 \tVal Frac: 0.7746\n",
      "\tEpoch: 650/1000; 1.48 sec... \tStep: 157950... \tLoss: 0.355... \tVal Loss: 0.628 \tVal Frac: 0.7735\n",
      "\tEpoch: 655/1000; 1.39 sec... \tStep: 159165... \tLoss: 0.655... \tVal Loss: 0.604 \tVal Frac: 0.7956\n",
      "\tEpoch: 660/1000; 1.46 sec... \tStep: 160380... \tLoss: 0.912... \tVal Loss: 0.612 \tVal Frac: 0.8059\n",
      "\tEpoch: 665/1000; 1.47 sec... \tStep: 161595... \tLoss: 0.616... \tVal Loss: 0.600 \tVal Frac: 0.8000\n",
      "\tEpoch: 670/1000; 1.46 sec... \tStep: 162810... \tLoss: 0.647... \tVal Loss: 0.623 \tVal Frac: 0.8051\n",
      "\tEpoch: 675/1000; 1.44 sec... \tStep: 164025... \tLoss: 0.423... \tVal Loss: 0.639 \tVal Frac: 0.7743\n",
      "\tEpoch: 680/1000; 1.44 sec... \tStep: 165240... \tLoss: 0.698... \tVal Loss: 0.591 \tVal Frac: 0.8059\n",
      "\tEpoch: 685/1000; 1.37 sec... \tStep: 166455... \tLoss: 0.434... \tVal Loss: 0.609 \tVal Frac: 0.8044\n",
      "\tEpoch: 690/1000; 1.38 sec... \tStep: 167670... \tLoss: 0.586... \tVal Loss: 0.604 \tVal Frac: 0.8066\n",
      "\tEpoch: 695/1000; 1.44 sec... \tStep: 168885... \tLoss: 0.738... \tVal Loss: 0.624 \tVal Frac: 0.7982\n",
      "\tEpoch: 700/1000; 1.40 sec... \tStep: 170100... \tLoss: 0.888... \tVal Loss: 0.599 \tVal Frac: 0.8051\n",
      "\tEpoch: 705/1000; 1.37 sec... \tStep: 171315... \tLoss: 0.984... \tVal Loss: 0.595 \tVal Frac: 0.8022\n",
      "\tEpoch: 710/1000; 1.32 sec... \tStep: 172530... \tLoss: 1.008... \tVal Loss: 0.622 \tVal Frac: 0.7812\n",
      "\tEpoch: 715/1000; 1.38 sec... \tStep: 173745... \tLoss: 0.625... \tVal Loss: 0.600 \tVal Frac: 0.7886\n",
      "\tEpoch: 720/1000; 1.47 sec... \tStep: 174960... \tLoss: 0.814... \tVal Loss: 0.601 \tVal Frac: 0.7846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 725/1000; 1.47 sec... \tStep: 176175... \tLoss: 0.675... \tVal Loss: 0.583 \tVal Frac: 0.8110\n",
      "\tEpoch: 730/1000; 1.42 sec... \tStep: 177390... \tLoss: 1.114... \tVal Loss: 0.608 \tVal Frac: 0.7835\n",
      "\tEpoch: 735/1000; 1.42 sec... \tStep: 178605... \tLoss: 0.600... \tVal Loss: 0.591 \tVal Frac: 0.7985\n",
      "\tEpoch: 740/1000; 1.43 sec... \tStep: 179820... \tLoss: 0.823... \tVal Loss: 0.597 \tVal Frac: 0.8044\n",
      "\tEpoch: 745/1000; 1.37 sec... \tStep: 181035... \tLoss: 0.815... \tVal Loss: 0.600 \tVal Frac: 0.8007\n",
      "\tEpoch: 750/1000; 1.52 sec... \tStep: 182250... \tLoss: 0.991... \tVal Loss: 0.599 \tVal Frac: 0.8055\n",
      "\tEpoch: 755/1000; 1.36 sec... \tStep: 183465... \tLoss: 1.003... \tVal Loss: 0.614 \tVal Frac: 0.8011\n",
      "\tEpoch: 760/1000; 1.49 sec... \tStep: 184680... \tLoss: 0.624... \tVal Loss: 0.597 \tVal Frac: 0.7989\n",
      "\tEpoch: 765/1000; 1.40 sec... \tStep: 185895... \tLoss: 0.517... \tVal Loss: 0.610 \tVal Frac: 0.7941\n",
      "\tEpoch: 770/1000; 1.34 sec... \tStep: 187110... \tLoss: 0.582... \tVal Loss: 0.597 \tVal Frac: 0.8077\n",
      "\tEpoch: 775/1000; 1.39 sec... \tStep: 188325... \tLoss: 0.878... \tVal Loss: 0.609 \tVal Frac: 0.8007\n",
      "\tEpoch: 780/1000; 1.36 sec... \tStep: 189540... \tLoss: 0.735... \tVal Loss: 0.590 \tVal Frac: 0.7937\n",
      "\tEpoch: 785/1000; 1.39 sec... \tStep: 190755... \tLoss: 0.763... \tVal Loss: 0.571 \tVal Frac: 0.8136\n",
      "\tValidation loss decreased (0.571 --> 0.571).  Saving model ...\n",
      "\tEpoch: 790/1000; 1.35 sec... \tStep: 191970... \tLoss: 0.741... \tVal Loss: 0.600 \tVal Frac: 0.8085\n",
      "\tEpoch: 795/1000; 1.40 sec... \tStep: 193185... \tLoss: 0.733... \tVal Loss: 0.571 \tVal Frac: 0.8162\n",
      "\tEpoch: 800/1000; 1.32 sec... \tStep: 194400... \tLoss: 0.807... \tVal Loss: 0.615 \tVal Frac: 0.8026\n",
      "\tEpoch: 805/1000; 1.35 sec... \tStep: 195615... \tLoss: 0.530... \tVal Loss: 0.628 \tVal Frac: 0.8000\n",
      "\tEpoch: 810/1000; 1.33 sec... \tStep: 196830... \tLoss: 0.670... \tVal Loss: 0.607 \tVal Frac: 0.7926\n",
      "\tEpoch: 815/1000; 1.38 sec... \tStep: 198045... \tLoss: 0.566... \tVal Loss: 0.620 \tVal Frac: 0.7956\n",
      "\tEpoch: 820/1000; 1.36 sec... \tStep: 199260... \tLoss: 0.598... \tVal Loss: 0.589 \tVal Frac: 0.8051\n",
      "\tEpoch: 825/1000; 1.47 sec... \tStep: 200475... \tLoss: 0.945... \tVal Loss: 0.570 \tVal Frac: 0.8257\n",
      "\tValidation loss decreased (0.571 --> 0.570).  Saving model ...\n",
      "\tEpoch: 830/1000; 1.52 sec... \tStep: 201690... \tLoss: 0.955... \tVal Loss: 0.609 \tVal Frac: 0.7993\n",
      "\tEpoch: 835/1000; 1.44 sec... \tStep: 202905... \tLoss: 0.465... \tVal Loss: 0.588 \tVal Frac: 0.8129\n",
      "\tEpoch: 840/1000; 1.38 sec... \tStep: 204120... \tLoss: 0.704... \tVal Loss: 0.608 \tVal Frac: 0.8074\n",
      "\tEpoch: 845/1000; 1.43 sec... \tStep: 205335... \tLoss: 0.709... \tVal Loss: 0.620 \tVal Frac: 0.8037\n",
      "\tEpoch: 850/1000; 1.43 sec... \tStep: 206550... \tLoss: 0.668... \tVal Loss: 0.627 \tVal Frac: 0.7923\n",
      "\tEpoch: 855/1000; 1.38 sec... \tStep: 207765... \tLoss: 0.587... \tVal Loss: 0.587 \tVal Frac: 0.7941\n",
      "\tEpoch: 860/1000; 1.30 sec... \tStep: 208980... \tLoss: 1.215... \tVal Loss: 0.594 \tVal Frac: 0.8085\n",
      "\tEpoch: 865/1000; 1.42 sec... \tStep: 210195... \tLoss: 0.688... \tVal Loss: 0.607 \tVal Frac: 0.8136\n",
      "\tEpoch: 870/1000; 1.54 sec... \tStep: 211410... \tLoss: 0.553... \tVal Loss: 0.600 \tVal Frac: 0.7945\n",
      "\tEpoch: 875/1000; 1.44 sec... \tStep: 212625... \tLoss: 0.645... \tVal Loss: 0.597 \tVal Frac: 0.8063\n",
      "\tEpoch: 880/1000; 1.40 sec... \tStep: 213840... \tLoss: 0.598... \tVal Loss: 0.587 \tVal Frac: 0.8022\n",
      "\tEpoch: 885/1000; 1.45 sec... \tStep: 215055... \tLoss: 0.576... \tVal Loss: 0.610 \tVal Frac: 0.7945\n",
      "\tEpoch: 890/1000; 1.44 sec... \tStep: 216270... \tLoss: 0.681... \tVal Loss: 0.574 \tVal Frac: 0.8187\n",
      "\tEpoch: 895/1000; 1.44 sec... \tStep: 217485... \tLoss: 0.425... \tVal Loss: 0.598 \tVal Frac: 0.7982\n",
      "\tEpoch: 900/1000; 1.40 sec... \tStep: 218700... \tLoss: 0.821... \tVal Loss: 0.592 \tVal Frac: 0.7993\n",
      "\tEpoch: 905/1000; 1.40 sec... \tStep: 219915... \tLoss: 0.436... \tVal Loss: 0.580 \tVal Frac: 0.8077\n",
      "\tEpoch: 910/1000; 1.37 sec... \tStep: 221130... \tLoss: 0.699... \tVal Loss: 0.591 \tVal Frac: 0.8110\n",
      "\tEpoch: 915/1000; 1.44 sec... \tStep: 222345... \tLoss: 0.520... \tVal Loss: 0.586 \tVal Frac: 0.8070\n",
      "\tEpoch: 920/1000; 1.34 sec... \tStep: 223560... \tLoss: 0.551... \tVal Loss: 0.602 \tVal Frac: 0.8059\n",
      "\tEpoch: 925/1000; 1.40 sec... \tStep: 224775... \tLoss: 0.975... \tVal Loss: 0.593 \tVal Frac: 0.8029\n",
      "\tEpoch: 930/1000; 1.29 sec... \tStep: 225990... \tLoss: 0.454... \tVal Loss: 0.586 \tVal Frac: 0.8162\n",
      "\tEpoch: 935/1000; 1.37 sec... \tStep: 227205... \tLoss: 0.657... \tVal Loss: 0.605 \tVal Frac: 0.7919\n",
      "\tEpoch: 940/1000; 1.34 sec... \tStep: 228420... \tLoss: 0.642... \tVal Loss: 0.588 \tVal Frac: 0.8018\n",
      "\tEpoch: 945/1000; 1.29 sec... \tStep: 229635... \tLoss: 0.565... \tVal Loss: 0.590 \tVal Frac: 0.7989\n",
      "\tEpoch: 950/1000; 1.50 sec... \tStep: 230850... \tLoss: 0.864... \tVal Loss: 0.601 \tVal Frac: 0.7886\n",
      "\tEpoch: 955/1000; 1.47 sec... \tStep: 232065... \tLoss: 0.794... \tVal Loss: 0.618 \tVal Frac: 0.7801\n",
      "\tEpoch: 960/1000; 1.44 sec... \tStep: 233280... \tLoss: 0.891... \tVal Loss: 0.607 \tVal Frac: 0.7824\n",
      "\tEpoch: 965/1000; 1.30 sec... \tStep: 234495... \tLoss: 0.835... \tVal Loss: 0.596 \tVal Frac: 0.7937\n",
      "\tEpoch: 970/1000; 1.43 sec... \tStep: 235710... \tLoss: 0.645... \tVal Loss: 0.601 \tVal Frac: 0.8051\n",
      "\tEpoch: 975/1000; 1.40 sec... \tStep: 236925... \tLoss: 0.563... \tVal Loss: 0.592 \tVal Frac: 0.8029\n",
      "\tEpoch: 980/1000; 1.50 sec... \tStep: 238140... \tLoss: 1.034... \tVal Loss: 0.602 \tVal Frac: 0.7993\n",
      "\tEpoch: 985/1000; 1.40 sec... \tStep: 239355... \tLoss: 0.993... \tVal Loss: 0.607 \tVal Frac: 0.7949\n",
      "\tEpoch: 990/1000; 1.39 sec... \tStep: 240570... \tLoss: 0.792... \tVal Loss: 0.596 \tVal Frac: 0.7967\n",
      "\tEpoch: 995/1000; 1.44 sec... \tStep: 241785... \tLoss: 0.623... \tVal Loss: 0.597 \tVal Frac: 0.7835\n",
      "\tEpoch: 1000/1000; 1.47 sec... \tStep: 243000... \tLoss: 0.605... \tVal Loss: 0.621 \tVal Frac: 0.7691\n",
      "Completed:  lr :  0.01 \n",
      "\tloss: 0.569573 \n",
      "\tfrac:0.825735\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  1\n",
      "\tEpoch: 5/1000; 0.97 sec... \tStep: 1215... \tLoss: 0.775... \tVal Loss: 0.442 \tVal Frac: 0.8798\n",
      "\tValidation loss decreased (inf --> 0.442).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.03 sec... \tStep: 2430... \tLoss: 0.475... \tVal Loss: 0.404 \tVal Frac: 0.8809\n",
      "\tValidation loss decreased (0.442 --> 0.404).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.13 sec... \tStep: 3645... \tLoss: 0.322... \tVal Loss: 0.338 \tVal Frac: 0.8926\n",
      "\tValidation loss decreased (0.404 --> 0.338).  Saving model ...\n",
      "\tEpoch: 20/1000; 0.97 sec... \tStep: 4860... \tLoss: 0.251... \tVal Loss: 0.391 \tVal Frac: 0.8871\n",
      "\tEpoch: 25/1000; 0.98 sec... \tStep: 6075... \tLoss: 0.250... \tVal Loss: 0.335 \tVal Frac: 0.8949\n",
      "\tValidation loss decreased (0.338 --> 0.335).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.07 sec... \tStep: 7290... \tLoss: 0.267... \tVal Loss: 0.330 \tVal Frac: 0.9074\n",
      "\tValidation loss decreased (0.335 --> 0.330).  Saving model ...\n",
      "\tEpoch: 35/1000; 1.06 sec... \tStep: 8505... \tLoss: 0.497... \tVal Loss: 0.344 \tVal Frac: 0.8978\n",
      "\tEpoch: 40/1000; 1.02 sec... \tStep: 9720... \tLoss: 0.609... \tVal Loss: 0.325 \tVal Frac: 0.9000\n",
      "\tValidation loss decreased (0.330 --> 0.325).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.10 sec... \tStep: 10935... \tLoss: 0.447... \tVal Loss: 0.333 \tVal Frac: 0.9007\n",
      "\tEpoch: 50/1000; 1.10 sec... \tStep: 12150... \tLoss: 0.922... \tVal Loss: 0.336 \tVal Frac: 0.8974\n",
      "\tEpoch: 55/1000; 1.06 sec... \tStep: 13365... \tLoss: 0.452... \tVal Loss: 0.334 \tVal Frac: 0.8930\n",
      "\tEpoch: 60/1000; 1.00 sec... \tStep: 14580... \tLoss: 0.505... \tVal Loss: 0.295 \tVal Frac: 0.9051\n",
      "\tValidation loss decreased (0.325 --> 0.295).  Saving model ...\n",
      "\tEpoch: 65/1000; 1.00 sec... \tStep: 15795... \tLoss: 0.308... \tVal Loss: 0.313 \tVal Frac: 0.9048\n",
      "\tEpoch: 70/1000; 1.05 sec... \tStep: 17010... \tLoss: 0.214... \tVal Loss: 0.314 \tVal Frac: 0.9044\n",
      "\tEpoch: 75/1000; 0.99 sec... \tStep: 18225... \tLoss: 0.470... \tVal Loss: 0.302 \tVal Frac: 0.9118\n",
      "\tEpoch: 80/1000; 1.05 sec... \tStep: 19440... \tLoss: 0.502... \tVal Loss: 0.301 \tVal Frac: 0.9096\n",
      "\tEpoch: 85/1000; 1.04 sec... \tStep: 20655... \tLoss: 0.499... \tVal Loss: 0.309 \tVal Frac: 0.9059\n",
      "\tEpoch: 90/1000; 1.15 sec... \tStep: 21870... \tLoss: 0.359... \tVal Loss: 0.291 \tVal Frac: 0.9077\n",
      "\tValidation loss decreased (0.295 --> 0.291).  Saving model ...\n",
      "\tEpoch: 95/1000; 1.07 sec... \tStep: 23085... \tLoss: 0.392... \tVal Loss: 0.310 \tVal Frac: 0.9062\n",
      "\tEpoch: 100/1000; 1.00 sec... \tStep: 24300... \tLoss: 0.215... \tVal Loss: 0.303 \tVal Frac: 0.9099\n",
      "\tEpoch: 105/1000; 1.13 sec... \tStep: 25515... \tLoss: 0.189... \tVal Loss: 0.302 \tVal Frac: 0.9099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 110/1000; 1.04 sec... \tStep: 26730... \tLoss: 0.382... \tVal Loss: 0.290 \tVal Frac: 0.9066\n",
      "\tValidation loss decreased (0.291 --> 0.290).  Saving model ...\n",
      "\tEpoch: 115/1000; 0.93 sec... \tStep: 27945... \tLoss: 0.397... \tVal Loss: 0.295 \tVal Frac: 0.9033\n",
      "\tEpoch: 120/1000; 1.07 sec... \tStep: 29160... \tLoss: 0.195... \tVal Loss: 0.298 \tVal Frac: 0.9081\n",
      "\tEpoch: 125/1000; 1.05 sec... \tStep: 30375... \tLoss: 0.503... \tVal Loss: 0.304 \tVal Frac: 0.9018\n",
      "\tEpoch: 130/1000; 0.99 sec... \tStep: 31590... \tLoss: 0.111... \tVal Loss: 0.302 \tVal Frac: 0.9040\n",
      "\tEpoch: 135/1000; 1.02 sec... \tStep: 32805... \tLoss: 0.413... \tVal Loss: 0.293 \tVal Frac: 0.9048\n",
      "\tEpoch: 140/1000; 0.98 sec... \tStep: 34020... \tLoss: 0.633... \tVal Loss: 0.307 \tVal Frac: 0.9004\n",
      "\tEpoch: 145/1000; 0.97 sec... \tStep: 35235... \tLoss: 0.139... \tVal Loss: 0.303 \tVal Frac: 0.9096\n",
      "\tEpoch: 150/1000; 1.20 sec... \tStep: 36450... \tLoss: 0.148... \tVal Loss: 0.301 \tVal Frac: 0.9077\n",
      "\tEpoch: 155/1000; 1.00 sec... \tStep: 37665... \tLoss: 0.309... \tVal Loss: 0.302 \tVal Frac: 0.9022\n",
      "\tEpoch: 160/1000; 0.99 sec... \tStep: 38880... \tLoss: 0.325... \tVal Loss: 0.306 \tVal Frac: 0.9048\n",
      "\tEpoch: 165/1000; 1.00 sec... \tStep: 40095... \tLoss: 0.625... \tVal Loss: 0.336 \tVal Frac: 0.8938\n",
      "\tEpoch: 170/1000; 0.99 sec... \tStep: 41310... \tLoss: 0.419... \tVal Loss: 0.313 \tVal Frac: 0.8996\n",
      "\tEpoch: 175/1000; 1.12 sec... \tStep: 42525... \tLoss: 0.444... \tVal Loss: 0.313 \tVal Frac: 0.9048\n",
      "\tEpoch: 180/1000; 0.98 sec... \tStep: 43740... \tLoss: 0.316... \tVal Loss: 0.308 \tVal Frac: 0.9022\n",
      "\tEpoch: 185/1000; 1.01 sec... \tStep: 44955... \tLoss: 0.254... \tVal Loss: 0.307 \tVal Frac: 0.9004\n",
      "\tEpoch: 190/1000; 1.10 sec... \tStep: 46170... \tLoss: 0.558... \tVal Loss: 0.306 \tVal Frac: 0.9011\n",
      "\tEpoch: 195/1000; 0.99 sec... \tStep: 47385... \tLoss: 0.355... \tVal Loss: 0.317 \tVal Frac: 0.9018\n",
      "\tEpoch: 200/1000; 0.99 sec... \tStep: 48600... \tLoss: 0.393... \tVal Loss: 0.316 \tVal Frac: 0.9029\n",
      "\tEpoch: 205/1000; 1.06 sec... \tStep: 49815... \tLoss: 0.359... \tVal Loss: 0.313 \tVal Frac: 0.9066\n",
      "\tEpoch: 210/1000; 1.02 sec... \tStep: 51030... \tLoss: 0.581... \tVal Loss: 0.334 \tVal Frac: 0.8945\n",
      "\tEpoch: 215/1000; 1.10 sec... \tStep: 52245... \tLoss: 0.656... \tVal Loss: 0.312 \tVal Frac: 0.9066\n",
      "\tEpoch: 220/1000; 1.09 sec... \tStep: 53460... \tLoss: 0.527... \tVal Loss: 0.362 \tVal Frac: 0.8919\n",
      "\tEpoch: 225/1000; 1.06 sec... \tStep: 54675... \tLoss: 0.174... \tVal Loss: 0.308 \tVal Frac: 0.9040\n",
      "\tEpoch: 230/1000; 1.09 sec... \tStep: 55890... \tLoss: 0.501... \tVal Loss: 0.329 \tVal Frac: 0.8904\n",
      "\tEpoch: 235/1000; 0.99 sec... \tStep: 57105... \tLoss: 0.344... \tVal Loss: 0.318 \tVal Frac: 0.8930\n",
      "\tEpoch: 240/1000; 1.13 sec... \tStep: 58320... \tLoss: 0.476... \tVal Loss: 0.310 \tVal Frac: 0.8904\n",
      "\tEpoch: 245/1000; 0.97 sec... \tStep: 59535... \tLoss: 0.109... \tVal Loss: 0.295 \tVal Frac: 0.9066\n",
      "\tEpoch: 250/1000; 0.99 sec... \tStep: 60750... \tLoss: 0.161... \tVal Loss: 0.300 \tVal Frac: 0.9015\n",
      "\tEpoch: 255/1000; 1.06 sec... \tStep: 61965... \tLoss: 0.255... \tVal Loss: 0.314 \tVal Frac: 0.8985\n",
      "\tEpoch: 260/1000; 1.10 sec... \tStep: 63180... \tLoss: 0.229... \tVal Loss: 0.293 \tVal Frac: 0.9051\n",
      "\tEpoch: 265/1000; 0.94 sec... \tStep: 64395... \tLoss: 0.454... \tVal Loss: 0.296 \tVal Frac: 0.9103\n",
      "\tEpoch: 270/1000; 1.03 sec... \tStep: 65610... \tLoss: 0.301... \tVal Loss: 0.278 \tVal Frac: 0.9151\n",
      "\tValidation loss decreased (0.290 --> 0.278).  Saving model ...\n",
      "\tEpoch: 275/1000; 1.09 sec... \tStep: 66825... \tLoss: 0.386... \tVal Loss: 0.286 \tVal Frac: 0.9129\n",
      "\tEpoch: 280/1000; 1.00 sec... \tStep: 68040... \tLoss: 0.177... \tVal Loss: 0.279 \tVal Frac: 0.9118\n",
      "\tEpoch: 285/1000; 0.92 sec... \tStep: 69255... \tLoss: 0.164... \tVal Loss: 0.297 \tVal Frac: 0.9096\n",
      "\tEpoch: 290/1000; 1.04 sec... \tStep: 70470... \tLoss: 0.598... \tVal Loss: 0.276 \tVal Frac: 0.9121\n",
      "\tValidation loss decreased (0.278 --> 0.276).  Saving model ...\n",
      "\tEpoch: 295/1000; 0.99 sec... \tStep: 71685... \tLoss: 0.237... \tVal Loss: 0.288 \tVal Frac: 0.9132\n",
      "\tEpoch: 300/1000; 1.10 sec... \tStep: 72900... \tLoss: 0.549... \tVal Loss: 0.298 \tVal Frac: 0.9062\n",
      "\tEpoch: 305/1000; 0.99 sec... \tStep: 74115... \tLoss: 0.427... \tVal Loss: 0.283 \tVal Frac: 0.9092\n",
      "\tEpoch: 310/1000; 1.15 sec... \tStep: 75330... \tLoss: 0.405... \tVal Loss: 0.284 \tVal Frac: 0.9099\n",
      "\tEpoch: 315/1000; 1.03 sec... \tStep: 76545... \tLoss: 0.178... \tVal Loss: 0.291 \tVal Frac: 0.9070\n",
      "\tEpoch: 320/1000; 1.09 sec... \tStep: 77760... \tLoss: 0.358... \tVal Loss: 0.288 \tVal Frac: 0.9033\n",
      "\tEpoch: 325/1000; 0.99 sec... \tStep: 78975... \tLoss: 0.597... \tVal Loss: 0.297 \tVal Frac: 0.9099\n",
      "\tEpoch: 330/1000; 0.97 sec... \tStep: 80190... \tLoss: 0.642... \tVal Loss: 0.298 \tVal Frac: 0.9066\n",
      "\tEpoch: 335/1000; 1.12 sec... \tStep: 81405... \tLoss: 0.272... \tVal Loss: 0.292 \tVal Frac: 0.9110\n",
      "\tEpoch: 340/1000; 1.03 sec... \tStep: 82620... \tLoss: 0.392... \tVal Loss: 0.302 \tVal Frac: 0.9018\n",
      "\tEpoch: 345/1000; 1.00 sec... \tStep: 83835... \tLoss: 0.255... \tVal Loss: 0.292 \tVal Frac: 0.9040\n",
      "\tEpoch: 350/1000; 1.03 sec... \tStep: 85050... \tLoss: 0.187... \tVal Loss: 0.291 \tVal Frac: 0.9077\n",
      "\tEpoch: 355/1000; 0.97 sec... \tStep: 86265... \tLoss: 0.221... \tVal Loss: 0.286 \tVal Frac: 0.9110\n",
      "\tEpoch: 360/1000; 1.07 sec... \tStep: 87480... \tLoss: 0.271... \tVal Loss: 0.297 \tVal Frac: 0.9103\n",
      "\tEpoch: 365/1000; 1.07 sec... \tStep: 88695... \tLoss: 0.369... \tVal Loss: 0.290 \tVal Frac: 0.9143\n",
      "\tEpoch: 370/1000; 1.13 sec... \tStep: 89910... \tLoss: 0.137... \tVal Loss: 0.284 \tVal Frac: 0.9088\n",
      "\tEpoch: 375/1000; 1.02 sec... \tStep: 91125... \tLoss: 0.550... \tVal Loss: 0.274 \tVal Frac: 0.9114\n",
      "\tValidation loss decreased (0.276 --> 0.274).  Saving model ...\n",
      "\tEpoch: 380/1000; 1.09 sec... \tStep: 92340... \tLoss: 0.490... \tVal Loss: 0.276 \tVal Frac: 0.9114\n",
      "\tEpoch: 385/1000; 1.09 sec... \tStep: 93555... \tLoss: 0.308... \tVal Loss: 0.284 \tVal Frac: 0.9136\n",
      "\tEpoch: 390/1000; 1.02 sec... \tStep: 94770... \tLoss: 0.228... \tVal Loss: 0.283 \tVal Frac: 0.9099\n",
      "\tEpoch: 395/1000; 1.06 sec... \tStep: 95985... \tLoss: 0.455... \tVal Loss: 0.289 \tVal Frac: 0.9107\n",
      "\tEpoch: 400/1000; 1.13 sec... \tStep: 97200... \tLoss: 0.649... \tVal Loss: 0.291 \tVal Frac: 0.9096\n",
      "\tEpoch: 405/1000; 0.90 sec... \tStep: 98415... \tLoss: 0.244... \tVal Loss: 0.295 \tVal Frac: 0.9092\n",
      "\tEpoch: 410/1000; 1.09 sec... \tStep: 99630... \tLoss: 0.569... \tVal Loss: 0.296 \tVal Frac: 0.9140\n",
      "\tEpoch: 415/1000; 1.07 sec... \tStep: 100845... \tLoss: 0.140... \tVal Loss: 0.280 \tVal Frac: 0.9132\n",
      "\tEpoch: 420/1000; 0.97 sec... \tStep: 102060... \tLoss: 0.572... \tVal Loss: 0.283 \tVal Frac: 0.9154\n",
      "\tEpoch: 425/1000; 0.99 sec... \tStep: 103275... \tLoss: 0.297... \tVal Loss: 0.294 \tVal Frac: 0.9103\n",
      "\tEpoch: 430/1000; 1.02 sec... \tStep: 104490... \tLoss: 0.384... \tVal Loss: 0.289 \tVal Frac: 0.9085\n",
      "\tEpoch: 435/1000; 1.09 sec... \tStep: 105705... \tLoss: 0.439... \tVal Loss: 0.280 \tVal Frac: 0.9110\n",
      "\tEpoch: 440/1000; 1.05 sec... \tStep: 106920... \tLoss: 0.522... \tVal Loss: 0.299 \tVal Frac: 0.9074\n",
      "\tEpoch: 445/1000; 1.02 sec... \tStep: 108135... \tLoss: 0.335... \tVal Loss: 0.294 \tVal Frac: 0.9033\n",
      "\tEpoch: 450/1000; 1.00 sec... \tStep: 109350... \tLoss: 0.311... \tVal Loss: 0.307 \tVal Frac: 0.9029\n",
      "\tEpoch: 455/1000; 0.92 sec... \tStep: 110565... \tLoss: 0.654... \tVal Loss: 0.301 \tVal Frac: 0.9007\n",
      "\tEpoch: 460/1000; 1.06 sec... \tStep: 111780... \tLoss: 0.728... \tVal Loss: 0.295 \tVal Frac: 0.9037\n",
      "\tEpoch: 465/1000; 0.94 sec... \tStep: 112995... \tLoss: 0.349... \tVal Loss: 0.288 \tVal Frac: 0.9077\n",
      "\tEpoch: 470/1000; 1.03 sec... \tStep: 114210... \tLoss: 0.234... \tVal Loss: 0.291 \tVal Frac: 0.9055\n",
      "\tEpoch: 475/1000; 0.97 sec... \tStep: 115425... \tLoss: 0.134... \tVal Loss: 0.288 \tVal Frac: 0.9085\n",
      "\tEpoch: 480/1000; 1.03 sec... \tStep: 116640... \tLoss: 0.487... \tVal Loss: 0.280 \tVal Frac: 0.9051\n",
      "\tEpoch: 485/1000; 1.07 sec... \tStep: 117855... \tLoss: 0.472... \tVal Loss: 0.294 \tVal Frac: 0.9011\n",
      "\tEpoch: 490/1000; 1.06 sec... \tStep: 119070... \tLoss: 0.572... \tVal Loss: 0.297 \tVal Frac: 0.9037\n",
      "\tEpoch: 495/1000; 1.10 sec... \tStep: 120285... \tLoss: 0.398... \tVal Loss: 0.300 \tVal Frac: 0.9048\n",
      "\tEpoch: 500/1000; 1.02 sec... \tStep: 121500... \tLoss: 0.474... \tVal Loss: 0.303 \tVal Frac: 0.8989\n",
      "\tEpoch: 505/1000; 1.06 sec... \tStep: 122715... \tLoss: 0.394... \tVal Loss: 0.298 \tVal Frac: 0.9018\n",
      "\tEpoch: 510/1000; 0.98 sec... \tStep: 123930... \tLoss: 0.290... \tVal Loss: 0.279 \tVal Frac: 0.9051\n",
      "\tEpoch: 515/1000; 0.94 sec... \tStep: 125145... \tLoss: 0.441... \tVal Loss: 0.291 \tVal Frac: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 520/1000; 1.03 sec... \tStep: 126360... \tLoss: 0.420... \tVal Loss: 0.312 \tVal Frac: 0.9007\n",
      "\tEpoch: 525/1000; 1.05 sec... \tStep: 127575... \tLoss: 0.345... \tVal Loss: 0.301 \tVal Frac: 0.9055\n",
      "\tEpoch: 530/1000; 0.97 sec... \tStep: 128790... \tLoss: 0.258... \tVal Loss: 0.299 \tVal Frac: 0.9026\n",
      "\tEpoch: 535/1000; 0.99 sec... \tStep: 130005... \tLoss: 0.131... \tVal Loss: 0.282 \tVal Frac: 0.9048\n",
      "\tEpoch: 540/1000; 1.02 sec... \tStep: 131220... \tLoss: 0.273... \tVal Loss: 0.266 \tVal Frac: 0.9110\n",
      "\tValidation loss decreased (0.274 --> 0.266).  Saving model ...\n",
      "\tEpoch: 545/1000; 1.07 sec... \tStep: 132435... \tLoss: 0.585... \tVal Loss: 0.272 \tVal Frac: 0.9040\n",
      "\tEpoch: 550/1000; 0.94 sec... \tStep: 133650... \tLoss: 0.119... \tVal Loss: 0.272 \tVal Frac: 0.9066\n",
      "\tEpoch: 555/1000; 1.07 sec... \tStep: 134865... \tLoss: 0.261... \tVal Loss: 0.267 \tVal Frac: 0.9118\n",
      "\tEpoch: 560/1000; 0.99 sec... \tStep: 136080... \tLoss: 0.286... \tVal Loss: 0.253 \tVal Frac: 0.9169\n",
      "\tValidation loss decreased (0.266 --> 0.253).  Saving model ...\n",
      "\tEpoch: 565/1000; 0.96 sec... \tStep: 137295... \tLoss: 0.249... \tVal Loss: 0.260 \tVal Frac: 0.9176\n",
      "\tEpoch: 570/1000; 1.09 sec... \tStep: 138510... \tLoss: 0.196... \tVal Loss: 0.257 \tVal Frac: 0.9121\n",
      "\tEpoch: 575/1000; 0.95 sec... \tStep: 139725... \tLoss: 0.431... \tVal Loss: 0.245 \tVal Frac: 0.9180\n",
      "\tValidation loss decreased (0.253 --> 0.245).  Saving model ...\n",
      "\tEpoch: 580/1000; 1.10 sec... \tStep: 140940... \tLoss: 0.258... \tVal Loss: 0.243 \tVal Frac: 0.9232\n",
      "\tValidation loss decreased (0.245 --> 0.243).  Saving model ...\n",
      "\tEpoch: 585/1000; 1.04 sec... \tStep: 142155... \tLoss: 0.228... \tVal Loss: 0.251 \tVal Frac: 0.9158\n",
      "\tEpoch: 590/1000; 0.96 sec... \tStep: 143370... \tLoss: 0.461... \tVal Loss: 0.233 \tVal Frac: 0.9235\n",
      "\tValidation loss decreased (0.243 --> 0.233).  Saving model ...\n",
      "\tEpoch: 595/1000; 1.07 sec... \tStep: 144585... \tLoss: 0.401... \tVal Loss: 0.250 \tVal Frac: 0.9147\n",
      "\tEpoch: 600/1000; 0.92 sec... \tStep: 145800... \tLoss: 0.221... \tVal Loss: 0.273 \tVal Frac: 0.9074\n",
      "\tEpoch: 605/1000; 1.06 sec... \tStep: 147015... \tLoss: 0.096... \tVal Loss: 0.261 \tVal Frac: 0.9114\n",
      "\tEpoch: 610/1000; 1.04 sec... \tStep: 148230... \tLoss: 0.161... \tVal Loss: 0.261 \tVal Frac: 0.9191\n",
      "\tEpoch: 615/1000; 1.02 sec... \tStep: 149445... \tLoss: 0.407... \tVal Loss: 0.268 \tVal Frac: 0.9129\n",
      "\tEpoch: 620/1000; 1.00 sec... \tStep: 150660... \tLoss: 0.368... \tVal Loss: 0.272 \tVal Frac: 0.9118\n",
      "\tEpoch: 625/1000; 1.03 sec... \tStep: 151875... \tLoss: 0.469... \tVal Loss: 0.283 \tVal Frac: 0.9129\n",
      "\tEpoch: 630/1000; 1.00 sec... \tStep: 153090... \tLoss: 0.473... \tVal Loss: 0.279 \tVal Frac: 0.9162\n",
      "\tEpoch: 635/1000; 1.02 sec... \tStep: 154305... \tLoss: 0.325... \tVal Loss: 0.292 \tVal Frac: 0.9085\n",
      "\tEpoch: 640/1000; 1.03 sec... \tStep: 155520... \tLoss: 0.142... \tVal Loss: 0.286 \tVal Frac: 0.9066\n",
      "\tEpoch: 645/1000; 1.00 sec... \tStep: 156735... \tLoss: 0.408... \tVal Loss: 0.280 \tVal Frac: 0.9085\n",
      "\tEpoch: 650/1000; 1.01 sec... \tStep: 157950... \tLoss: 0.486... \tVal Loss: 0.272 \tVal Frac: 0.9136\n",
      "\tEpoch: 655/1000; 1.07 sec... \tStep: 159165... \tLoss: 0.227... \tVal Loss: 0.273 \tVal Frac: 0.9118\n",
      "\tEpoch: 660/1000; 0.99 sec... \tStep: 160380... \tLoss: 0.189... \tVal Loss: 0.286 \tVal Frac: 0.9118\n",
      "\tEpoch: 665/1000; 1.06 sec... \tStep: 161595... \tLoss: 0.311... \tVal Loss: 0.292 \tVal Frac: 0.9099\n",
      "\tEpoch: 670/1000; 0.99 sec... \tStep: 162810... \tLoss: 0.253... \tVal Loss: 0.297 \tVal Frac: 0.9114\n",
      "\tEpoch: 675/1000; 1.03 sec... \tStep: 164025... \tLoss: 0.468... \tVal Loss: 0.284 \tVal Frac: 0.9136\n",
      "\tEpoch: 680/1000; 1.03 sec... \tStep: 165240... \tLoss: 0.264... \tVal Loss: 0.290 \tVal Frac: 0.9099\n",
      "\tEpoch: 685/1000; 1.00 sec... \tStep: 166455... \tLoss: 0.206... \tVal Loss: 0.291 \tVal Frac: 0.9107\n",
      "\tEpoch: 690/1000; 0.95 sec... \tStep: 167670... \tLoss: 0.348... \tVal Loss: 0.297 \tVal Frac: 0.9081\n",
      "\tEpoch: 695/1000; 1.02 sec... \tStep: 168885... \tLoss: 0.209... \tVal Loss: 0.285 \tVal Frac: 0.9077\n",
      "\tEpoch: 700/1000; 1.10 sec... \tStep: 170100... \tLoss: 0.451... \tVal Loss: 0.274 \tVal Frac: 0.9121\n",
      "\tEpoch: 705/1000; 1.02 sec... \tStep: 171315... \tLoss: 0.463... \tVal Loss: 0.276 \tVal Frac: 0.9158\n",
      "\tEpoch: 710/1000; 1.01 sec... \tStep: 172530... \tLoss: 0.129... \tVal Loss: 0.274 \tVal Frac: 0.9103\n",
      "\tEpoch: 715/1000; 1.09 sec... \tStep: 173745... \tLoss: 0.163... \tVal Loss: 0.282 \tVal Frac: 0.9103\n",
      "\tEpoch: 720/1000; 1.03 sec... \tStep: 174960... \tLoss: 0.093... \tVal Loss: 0.285 \tVal Frac: 0.9118\n",
      "\tEpoch: 725/1000; 1.03 sec... \tStep: 176175... \tLoss: 0.278... \tVal Loss: 0.288 \tVal Frac: 0.9066\n",
      "\tEpoch: 730/1000; 1.02 sec... \tStep: 177390... \tLoss: 0.148... \tVal Loss: 0.285 \tVal Frac: 0.9074\n",
      "\tEpoch: 735/1000; 0.98 sec... \tStep: 178605... \tLoss: 0.329... \tVal Loss: 0.288 \tVal Frac: 0.9077\n",
      "\tEpoch: 740/1000; 1.17 sec... \tStep: 179820... \tLoss: 0.187... \tVal Loss: 0.276 \tVal Frac: 0.9118\n",
      "\tEpoch: 745/1000; 0.97 sec... \tStep: 181035... \tLoss: 0.393... \tVal Loss: 0.282 \tVal Frac: 0.9158\n",
      "\tEpoch: 750/1000; 0.99 sec... \tStep: 182250... \tLoss: 0.449... \tVal Loss: 0.296 \tVal Frac: 0.9085\n",
      "\tEpoch: 755/1000; 1.02 sec... \tStep: 183465... \tLoss: 0.450... \tVal Loss: 0.284 \tVal Frac: 0.9077\n",
      "\tEpoch: 760/1000; 0.98 sec... \tStep: 184680... \tLoss: 0.301... \tVal Loss: 0.288 \tVal Frac: 0.9107\n",
      "\tEpoch: 765/1000; 1.06 sec... \tStep: 185895... \tLoss: 0.242... \tVal Loss: 0.289 \tVal Frac: 0.9114\n",
      "\tEpoch: 770/1000; 1.02 sec... \tStep: 187110... \tLoss: 0.260... \tVal Loss: 0.294 \tVal Frac: 0.9059\n",
      "\tEpoch: 775/1000; 1.07 sec... \tStep: 188325... \tLoss: 0.082... \tVal Loss: 0.295 \tVal Frac: 0.9066\n",
      "\tEpoch: 780/1000; 1.06 sec... \tStep: 189540... \tLoss: 0.230... \tVal Loss: 0.297 \tVal Frac: 0.9077\n",
      "\tEpoch: 785/1000; 1.02 sec... \tStep: 190755... \tLoss: 0.564... \tVal Loss: 0.293 \tVal Frac: 0.9077\n",
      "\tEpoch: 790/1000; 0.97 sec... \tStep: 191970... \tLoss: 0.164... \tVal Loss: 0.292 \tVal Frac: 0.9110\n",
      "\tEpoch: 795/1000; 1.06 sec... \tStep: 193185... \tLoss: 0.073... \tVal Loss: 0.295 \tVal Frac: 0.9092\n",
      "\tEpoch: 800/1000; 1.07 sec... \tStep: 194400... \tLoss: 0.104... \tVal Loss: 0.281 \tVal Frac: 0.9099\n",
      "\tEpoch: 805/1000; 0.90 sec... \tStep: 195615... \tLoss: 0.289... \tVal Loss: 0.282 \tVal Frac: 0.9114\n",
      "\tEpoch: 810/1000; 0.97 sec... \tStep: 196830... \tLoss: 0.152... \tVal Loss: 0.285 \tVal Frac: 0.9121\n",
      "\tEpoch: 815/1000; 0.94 sec... \tStep: 198045... \tLoss: 0.379... \tVal Loss: 0.288 \tVal Frac: 0.9107\n",
      "\tEpoch: 820/1000; 1.12 sec... \tStep: 199260... \tLoss: 0.161... \tVal Loss: 0.288 \tVal Frac: 0.9074\n",
      "\tEpoch: 825/1000; 1.09 sec... \tStep: 200475... \tLoss: 0.307... \tVal Loss: 0.284 \tVal Frac: 0.9092\n",
      "\tEpoch: 830/1000; 1.00 sec... \tStep: 201690... \tLoss: 0.285... \tVal Loss: 0.281 \tVal Frac: 0.9088\n",
      "\tEpoch: 835/1000; 1.07 sec... \tStep: 202905... \tLoss: 0.069... \tVal Loss: 0.284 \tVal Frac: 0.9121\n",
      "\tEpoch: 840/1000; 1.10 sec... \tStep: 204120... \tLoss: 0.189... \tVal Loss: 0.275 \tVal Frac: 0.9151\n",
      "\tEpoch: 845/1000; 1.00 sec... \tStep: 205335... \tLoss: 0.448... \tVal Loss: 0.276 \tVal Frac: 0.9136\n",
      "\tEpoch: 850/1000; 1.06 sec... \tStep: 206550... \tLoss: 0.173... \tVal Loss: 0.265 \tVal Frac: 0.9136\n",
      "\tEpoch: 855/1000; 1.08 sec... \tStep: 207765... \tLoss: 0.326... \tVal Loss: 0.263 \tVal Frac: 0.9165\n",
      "\tEpoch: 860/1000; 1.03 sec... \tStep: 208980... \tLoss: 0.451... \tVal Loss: 0.254 \tVal Frac: 0.9180\n",
      "\tEpoch: 865/1000; 1.05 sec... \tStep: 210195... \tLoss: 0.237... \tVal Loss: 0.260 \tVal Frac: 0.9206\n",
      "\tEpoch: 870/1000; 1.05 sec... \tStep: 211410... \tLoss: 0.119... \tVal Loss: 0.264 \tVal Frac: 0.9180\n",
      "\tEpoch: 875/1000; 1.15 sec... \tStep: 212625... \tLoss: 0.228... \tVal Loss: 0.279 \tVal Frac: 0.9165\n",
      "\tEpoch: 880/1000; 1.02 sec... \tStep: 213840... \tLoss: 0.320... \tVal Loss: 0.271 \tVal Frac: 0.9254\n",
      "\tEpoch: 885/1000; 0.96 sec... \tStep: 215055... \tLoss: 0.266... \tVal Loss: 0.288 \tVal Frac: 0.9165\n",
      "\tEpoch: 890/1000; 1.07 sec... \tStep: 216270... \tLoss: 0.318... \tVal Loss: 0.292 \tVal Frac: 0.9162\n",
      "\tEpoch: 895/1000; 1.04 sec... \tStep: 217485... \tLoss: 0.424... \tVal Loss: 0.279 \tVal Frac: 0.9147\n",
      "\tEpoch: 900/1000; 0.99 sec... \tStep: 218700... \tLoss: 0.163... \tVal Loss: 0.278 \tVal Frac: 0.9129\n",
      "\tEpoch: 905/1000; 0.97 sec... \tStep: 219915... \tLoss: 0.475... \tVal Loss: 0.288 \tVal Frac: 0.9143\n",
      "\tEpoch: 910/1000; 1.00 sec... \tStep: 221130... \tLoss: 0.426... \tVal Loss: 0.271 \tVal Frac: 0.9180\n",
      "\tEpoch: 915/1000; 1.07 sec... \tStep: 222345... \tLoss: 0.294... \tVal Loss: 0.270 \tVal Frac: 0.9147\n",
      "\tEpoch: 920/1000; 0.99 sec... \tStep: 223560... \tLoss: 0.209... \tVal Loss: 0.265 \tVal Frac: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 925/1000; 1.02 sec... \tStep: 224775... \tLoss: 0.325... \tVal Loss: 0.269 \tVal Frac: 0.9191\n",
      "\tEpoch: 930/1000; 1.03 sec... \tStep: 225990... \tLoss: 0.389... \tVal Loss: 0.281 \tVal Frac: 0.9147\n",
      "\tEpoch: 935/1000; 1.07 sec... \tStep: 227205... \tLoss: 0.281... \tVal Loss: 0.268 \tVal Frac: 0.9143\n",
      "\tEpoch: 940/1000; 1.10 sec... \tStep: 228420... \tLoss: 0.572... \tVal Loss: 0.290 \tVal Frac: 0.9059\n",
      "\tEpoch: 945/1000; 1.13 sec... \tStep: 229635... \tLoss: 0.230... \tVal Loss: 0.284 \tVal Frac: 0.9081\n",
      "\tEpoch: 950/1000; 1.07 sec... \tStep: 230850... \tLoss: 0.175... \tVal Loss: 0.273 \tVal Frac: 0.9096\n",
      "\tEpoch: 955/1000; 0.97 sec... \tStep: 232065... \tLoss: 0.326... \tVal Loss: 0.274 \tVal Frac: 0.9077\n",
      "\tEpoch: 960/1000; 1.00 sec... \tStep: 233280... \tLoss: 0.117... \tVal Loss: 0.279 \tVal Frac: 0.9121\n",
      "\tEpoch: 965/1000; 0.96 sec... \tStep: 234495... \tLoss: 0.136... \tVal Loss: 0.268 \tVal Frac: 0.9154\n",
      "\tEpoch: 970/1000; 1.02 sec... \tStep: 235710... \tLoss: 0.259... \tVal Loss: 0.269 \tVal Frac: 0.9132\n",
      "\tEpoch: 975/1000; 1.13 sec... \tStep: 236925... \tLoss: 0.459... \tVal Loss: 0.274 \tVal Frac: 0.9132\n",
      "\tEpoch: 980/1000; 1.00 sec... \tStep: 238140... \tLoss: 0.560... \tVal Loss: 0.265 \tVal Frac: 0.9191\n",
      "\tEpoch: 985/1000; 1.07 sec... \tStep: 239355... \tLoss: 0.322... \tVal Loss: 0.271 \tVal Frac: 0.9118\n",
      "\tEpoch: 990/1000; 0.99 sec... \tStep: 240570... \tLoss: 0.153... \tVal Loss: 0.256 \tVal Frac: 0.9213\n",
      "\tEpoch: 995/1000; 1.07 sec... \tStep: 241785... \tLoss: 0.245... \tVal Loss: 0.272 \tVal Frac: 0.9151\n",
      "\tEpoch: 1000/1000; 1.10 sec... \tStep: 243000... \tLoss: 0.154... \tVal Loss: 0.268 \tVal Frac: 0.9158\n",
      "Completed:  n_windows :  1 \n",
      "\tloss: 0.232972 \n",
      "\tfrac:0.925368\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  20\n",
      "\tEpoch: 5/1000; 1.13 sec... \tStep: 1215... \tLoss: 1.227... \tVal Loss: 1.298 \tVal Frac: 0.6610\n",
      "\tValidation loss decreased (inf --> 1.298).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.19 sec... \tStep: 2430... \tLoss: 1.618... \tVal Loss: 1.344 \tVal Frac: 0.7335\n",
      "\tEpoch: 15/1000; 1.23 sec... \tStep: 3645... \tLoss: 1.357... \tVal Loss: 1.149 \tVal Frac: 0.7404\n",
      "\tValidation loss decreased (1.298 --> 1.149).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.17 sec... \tStep: 4860... \tLoss: 1.139... \tVal Loss: 1.205 \tVal Frac: 0.7390\n",
      "\tEpoch: 25/1000; 1.12 sec... \tStep: 6075... \tLoss: 1.404... \tVal Loss: 1.231 \tVal Frac: 0.7423\n",
      "\tEpoch: 30/1000; 1.16 sec... \tStep: 7290... \tLoss: 1.185... \tVal Loss: 1.221 \tVal Frac: 0.7379\n",
      "\tEpoch: 35/1000; 1.10 sec... \tStep: 8505... \tLoss: 1.385... \tVal Loss: 1.208 \tVal Frac: 0.7140\n",
      "\tEpoch: 40/1000; 1.27 sec... \tStep: 9720... \tLoss: 1.063... \tVal Loss: 0.991 \tVal Frac: 0.7574\n",
      "\tValidation loss decreased (1.149 --> 0.991).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.07 sec... \tStep: 10935... \tLoss: 1.240... \tVal Loss: 1.277 \tVal Frac: 0.7232\n",
      "\tEpoch: 50/1000; 1.23 sec... \tStep: 12150... \tLoss: 1.772... \tVal Loss: 1.339 \tVal Frac: 0.7026\n",
      "\tEpoch: 55/1000; 1.26 sec... \tStep: 13365... \tLoss: 1.379... \tVal Loss: 1.151 \tVal Frac: 0.7136\n",
      "\tEpoch: 60/1000; 1.14 sec... \tStep: 14580... \tLoss: 1.607... \tVal Loss: 1.185 \tVal Frac: 0.7298\n",
      "\tEpoch: 65/1000; 1.20 sec... \tStep: 15795... \tLoss: 1.340... \tVal Loss: 1.231 \tVal Frac: 0.7250\n",
      "\tEpoch: 70/1000; 1.21 sec... \tStep: 17010... \tLoss: 1.065... \tVal Loss: 1.188 \tVal Frac: 0.4507\n",
      "\tEpoch: 75/1000; 1.20 sec... \tStep: 18225... \tLoss: 1.560... \tVal Loss: 1.186 \tVal Frac: 0.7474\n",
      "\tEpoch: 80/1000; 1.10 sec... \tStep: 19440... \tLoss: 1.116... \tVal Loss: 1.223 \tVal Frac: 0.7088\n",
      "\tEpoch: 85/1000; 1.19 sec... \tStep: 20655... \tLoss: 1.390... \tVal Loss: 1.182 \tVal Frac: 0.7371\n",
      "\tEpoch: 90/1000; 1.05 sec... \tStep: 21870... \tLoss: 0.972... \tVal Loss: 1.071 \tVal Frac: 0.7482\n",
      "\tEpoch: 95/1000; 1.19 sec... \tStep: 23085... \tLoss: 1.413... \tVal Loss: 1.272 \tVal Frac: 0.4658\n",
      "\tEpoch: 100/1000; 1.19 sec... \tStep: 24300... \tLoss: 1.227... \tVal Loss: 1.263 \tVal Frac: 0.4004\n",
      "\tEpoch: 105/1000; 1.19 sec... \tStep: 25515... \tLoss: 1.181... \tVal Loss: 1.257 \tVal Frac: 0.7011\n",
      "\tEpoch: 110/1000; 1.17 sec... \tStep: 26730... \tLoss: 1.454... \tVal Loss: 1.335 \tVal Frac: 0.0673\n",
      "\tEpoch: 115/1000; 1.17 sec... \tStep: 27945... \tLoss: 1.309... \tVal Loss: 1.230 \tVal Frac: 0.0555\n",
      "\tEpoch: 120/1000; 1.10 sec... \tStep: 29160... \tLoss: 1.255... \tVal Loss: 1.069 \tVal Frac: 0.7360\n",
      "\tEpoch: 125/1000; 1.03 sec... \tStep: 30375... \tLoss: 1.469... \tVal Loss: 1.209 \tVal Frac: 0.0651\n",
      "\tEpoch: 130/1000; 1.27 sec... \tStep: 31590... \tLoss: 0.986... \tVal Loss: 1.227 \tVal Frac: 0.4265\n",
      "\tEpoch: 135/1000; 1.17 sec... \tStep: 32805... \tLoss: 1.184... \tVal Loss: 1.196 \tVal Frac: 0.2768\n",
      "\tEpoch: 140/1000; 1.10 sec... \tStep: 34020... \tLoss: 1.446... \tVal Loss: 1.180 \tVal Frac: 0.7195\n",
      "\tEpoch: 145/1000; 1.19 sec... \tStep: 35235... \tLoss: 1.193... \tVal Loss: 1.221 \tVal Frac: 0.7338\n",
      "\tEpoch: 150/1000; 1.32 sec... \tStep: 36450... \tLoss: 1.321... \tVal Loss: 1.218 \tVal Frac: 0.7500\n",
      "\tEpoch: 155/1000; 1.20 sec... \tStep: 37665... \tLoss: 1.151... \tVal Loss: 1.246 \tVal Frac: 0.0739\n",
      "\tEpoch: 160/1000; 1.20 sec... \tStep: 38880... \tLoss: 1.371... \tVal Loss: 1.239 \tVal Frac: 0.4489\n",
      "\tEpoch: 165/1000; 1.19 sec... \tStep: 40095... \tLoss: 1.346... \tVal Loss: 1.221 \tVal Frac: 0.7375\n",
      "\tEpoch: 170/1000; 1.25 sec... \tStep: 41310... \tLoss: 1.135... \tVal Loss: 1.202 \tVal Frac: 0.6860\n",
      "\tEpoch: 175/1000; 1.28 sec... \tStep: 42525... \tLoss: 1.265... \tVal Loss: 1.109 \tVal Frac: 0.7445\n",
      "\tEpoch: 180/1000; 1.16 sec... \tStep: 43740... \tLoss: 1.139... \tVal Loss: 1.159 \tVal Frac: 0.0985\n",
      "\tEpoch: 185/1000; 1.20 sec... \tStep: 44955... \tLoss: 1.381... \tVal Loss: 1.191 \tVal Frac: 0.7301\n",
      "\tEpoch: 190/1000; 1.24 sec... \tStep: 46170... \tLoss: 1.256... \tVal Loss: 1.204 \tVal Frac: 0.7272\n",
      "\tEpoch: 195/1000; 1.20 sec... \tStep: 47385... \tLoss: 1.456... \tVal Loss: 1.241 \tVal Frac: 0.2423\n",
      "\tEpoch: 200/1000; 1.23 sec... \tStep: 48600... \tLoss: 1.205... \tVal Loss: 1.097 \tVal Frac: 0.7232\n",
      "\tEpoch: 205/1000; 1.09 sec... \tStep: 49815... \tLoss: 1.236... \tVal Loss: 1.041 \tVal Frac: 0.7261\n",
      "\tEpoch: 210/1000; 1.19 sec... \tStep: 51030... \tLoss: 1.230... \tVal Loss: 0.954 \tVal Frac: 0.7243\n",
      "\tValidation loss decreased (0.991 --> 0.954).  Saving model ...\n",
      "\tEpoch: 215/1000; 1.30 sec... \tStep: 52245... \tLoss: 1.358... \tVal Loss: 1.182 \tVal Frac: 0.7051\n",
      "\tEpoch: 220/1000; 1.19 sec... \tStep: 53460... \tLoss: 0.879... \tVal Loss: 0.957 \tVal Frac: 0.7246\n",
      "\tEpoch: 225/1000; 1.09 sec... \tStep: 54675... \tLoss: 1.275... \tVal Loss: 1.034 \tVal Frac: 0.7353\n",
      "\tEpoch: 230/1000; 1.20 sec... \tStep: 55890... \tLoss: 1.158... \tVal Loss: 1.197 \tVal Frac: 0.7301\n",
      "\tEpoch: 235/1000; 1.26 sec... \tStep: 57105... \tLoss: 1.025... \tVal Loss: 1.185 \tVal Frac: 0.7298\n",
      "\tEpoch: 240/1000; 1.31 sec... \tStep: 58320... \tLoss: 1.306... \tVal Loss: 1.204 \tVal Frac: 0.7217\n",
      "\tEpoch: 245/1000; 1.22 sec... \tStep: 59535... \tLoss: 1.084... \tVal Loss: 1.167 \tVal Frac: 0.7342\n",
      "\tEpoch: 250/1000; 1.35 sec... \tStep: 60750... \tLoss: 1.322... \tVal Loss: 1.159 \tVal Frac: 0.7276\n",
      "\tEpoch: 255/1000; 1.13 sec... \tStep: 61965... \tLoss: 1.227... \tVal Loss: 1.068 \tVal Frac: 0.7555\n",
      "\tEpoch: 260/1000; 1.17 sec... \tStep: 63180... \tLoss: 1.361... \tVal Loss: 1.161 \tVal Frac: 0.7312\n",
      "\tEpoch: 265/1000; 1.17 sec... \tStep: 64395... \tLoss: 1.092... \tVal Loss: 1.138 \tVal Frac: 0.7401\n",
      "\tEpoch: 270/1000; 1.27 sec... \tStep: 65610... \tLoss: 1.312... \tVal Loss: 1.166 \tVal Frac: 0.7732\n",
      "\tEpoch: 275/1000; 1.18 sec... \tStep: 66825... \tLoss: 0.962... \tVal Loss: 1.047 \tVal Frac: 0.7415\n",
      "\tEpoch: 280/1000; 1.22 sec... \tStep: 68040... \tLoss: 1.133... \tVal Loss: 1.094 \tVal Frac: 0.7199\n",
      "\tEpoch: 285/1000; 1.25 sec... \tStep: 69255... \tLoss: 0.969... \tVal Loss: 1.146 \tVal Frac: 0.7430\n",
      "\tEpoch: 290/1000; 1.29 sec... \tStep: 70470... \tLoss: 1.277... \tVal Loss: 1.037 \tVal Frac: 0.7324\n",
      "\tEpoch: 295/1000; 1.17 sec... \tStep: 71685... \tLoss: 0.958... \tVal Loss: 1.115 \tVal Frac: 0.7449\n",
      "\tEpoch: 300/1000; 1.17 sec... \tStep: 72900... \tLoss: 1.305... \tVal Loss: 1.179 \tVal Frac: 0.0746\n",
      "\tEpoch: 305/1000; 1.20 sec... \tStep: 74115... \tLoss: 1.120... \tVal Loss: 1.205 \tVal Frac: 0.0831\n",
      "\tEpoch: 310/1000; 1.13 sec... \tStep: 75330... \tLoss: 1.033... \tVal Loss: 1.159 \tVal Frac: 0.7202\n",
      "\tEpoch: 315/1000; 1.11 sec... \tStep: 76545... \tLoss: 1.139... \tVal Loss: 1.184 \tVal Frac: 0.0684\n",
      "\tEpoch: 320/1000; 1.25 sec... \tStep: 77760... \tLoss: 1.281... \tVal Loss: 1.193 \tVal Frac: 0.0728\n",
      "\tEpoch: 325/1000; 1.22 sec... \tStep: 78975... \tLoss: 1.107... \tVal Loss: 1.187 \tVal Frac: 0.7290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 330/1000; 1.20 sec... \tStep: 80190... \tLoss: 1.191... \tVal Loss: 1.167 \tVal Frac: 0.7390\n",
      "\tEpoch: 335/1000; 1.13 sec... \tStep: 81405... \tLoss: 1.027... \tVal Loss: 1.185 \tVal Frac: 0.7305\n",
      "\tEpoch: 340/1000; 1.19 sec... \tStep: 82620... \tLoss: 1.409... \tVal Loss: 1.172 \tVal Frac: 0.6331\n",
      "\tEpoch: 345/1000; 1.20 sec... \tStep: 83835... \tLoss: 1.463... \tVal Loss: 1.185 \tVal Frac: 0.7265\n",
      "\tEpoch: 350/1000; 1.08 sec... \tStep: 85050... \tLoss: 1.179... \tVal Loss: 1.159 \tVal Frac: 0.7460\n",
      "\tEpoch: 355/1000; 1.17 sec... \tStep: 86265... \tLoss: 0.961... \tVal Loss: 1.016 \tVal Frac: 0.7390\n",
      "\tEpoch: 360/1000; 1.19 sec... \tStep: 87480... \tLoss: 1.375... \tVal Loss: 1.133 \tVal Frac: 0.7268\n",
      "\tEpoch: 365/1000; 1.08 sec... \tStep: 88695... \tLoss: 1.132... \tVal Loss: 1.177 \tVal Frac: 0.7364\n",
      "\tEpoch: 370/1000; 1.09 sec... \tStep: 89910... \tLoss: 1.644... \tVal Loss: 1.176 \tVal Frac: 0.4132\n",
      "\tEpoch: 375/1000; 1.19 sec... \tStep: 91125... \tLoss: 1.309... \tVal Loss: 1.172 \tVal Frac: 0.7316\n",
      "\tEpoch: 380/1000; 1.20 sec... \tStep: 92340... \tLoss: 1.477... \tVal Loss: 1.178 \tVal Frac: 0.6210\n",
      "\tEpoch: 385/1000; 1.18 sec... \tStep: 93555... \tLoss: 1.570... \tVal Loss: 1.169 \tVal Frac: 0.6228\n",
      "\tEpoch: 390/1000; 1.17 sec... \tStep: 94770... \tLoss: 1.098... \tVal Loss: 1.191 \tVal Frac: 0.0669\n",
      "\tEpoch: 395/1000; 1.17 sec... \tStep: 95985... \tLoss: 1.290... \tVal Loss: 1.179 \tVal Frac: 0.6294\n",
      "\tEpoch: 400/1000; 1.17 sec... \tStep: 97200... \tLoss: 1.354... \tVal Loss: 1.229 \tVal Frac: 0.5963\n",
      "\tEpoch: 405/1000; 1.18 sec... \tStep: 98415... \tLoss: 1.191... \tVal Loss: 1.163 \tVal Frac: 0.4801\n",
      "\tEpoch: 410/1000; 1.20 sec... \tStep: 99630... \tLoss: 1.381... \tVal Loss: 1.159 \tVal Frac: 0.7471\n",
      "\tEpoch: 415/1000; 1.18 sec... \tStep: 100845... \tLoss: 1.303... \tVal Loss: 1.188 \tVal Frac: 0.0754\n",
      "\tEpoch: 420/1000; 1.23 sec... \tStep: 102060... \tLoss: 1.381... \tVal Loss: 1.215 \tVal Frac: 0.6026\n",
      "\tEpoch: 425/1000; 1.23 sec... \tStep: 103275... \tLoss: 1.216... \tVal Loss: 1.216 \tVal Frac: 0.5761\n",
      "\tEpoch: 430/1000; 1.29 sec... \tStep: 104490... \tLoss: 1.255... \tVal Loss: 1.214 \tVal Frac: 0.6574\n",
      "\tEpoch: 435/1000; 1.16 sec... \tStep: 105705... \tLoss: 1.332... \tVal Loss: 1.220 \tVal Frac: 0.6562\n",
      "\tEpoch: 440/1000; 1.32 sec... \tStep: 106920... \tLoss: 1.398... \tVal Loss: 1.270 \tVal Frac: 0.1574\n",
      "\tEpoch: 445/1000; 1.12 sec... \tStep: 108135... \tLoss: 1.261... \tVal Loss: 1.172 \tVal Frac: 0.6765\n",
      "\tEpoch: 450/1000; 1.27 sec... \tStep: 109350... \tLoss: 1.510... \tVal Loss: 1.404 \tVal Frac: 0.1882\n",
      "\tEpoch: 455/1000; 1.13 sec... \tStep: 110565... \tLoss: 1.311... \tVal Loss: 1.182 \tVal Frac: 0.7033\n",
      "\tEpoch: 460/1000; 1.13 sec... \tStep: 111780... \tLoss: 1.100... \tVal Loss: 1.073 \tVal Frac: 0.7276\n",
      "\tEpoch: 465/1000; 1.19 sec... \tStep: 112995... \tLoss: 1.342... \tVal Loss: 1.205 \tVal Frac: 0.6838\n",
      "\tEpoch: 470/1000; 1.20 sec... \tStep: 114210... \tLoss: 1.282... \tVal Loss: 1.215 \tVal Frac: 0.7287\n",
      "\tEpoch: 475/1000; 1.23 sec... \tStep: 115425... \tLoss: 1.196... \tVal Loss: 1.184 \tVal Frac: 0.7143\n",
      "\tEpoch: 480/1000; 1.28 sec... \tStep: 116640... \tLoss: 1.625... \tVal Loss: 1.208 \tVal Frac: 0.7169\n",
      "\tEpoch: 485/1000; 1.12 sec... \tStep: 117855... \tLoss: 1.248... \tVal Loss: 1.176 \tVal Frac: 0.0864\n",
      "\tEpoch: 490/1000; 1.16 sec... \tStep: 119070... \tLoss: 1.372... \tVal Loss: 1.212 \tVal Frac: 0.0743\n",
      "\tEpoch: 495/1000; 1.13 sec... \tStep: 120285... \tLoss: 1.149... \tVal Loss: 1.185 \tVal Frac: 0.7434\n",
      "\tEpoch: 500/1000; 1.14 sec... \tStep: 121500... \tLoss: 1.352... \tVal Loss: 1.192 \tVal Frac: 0.7346\n",
      "\tEpoch: 505/1000; 1.09 sec... \tStep: 122715... \tLoss: 1.403... \tVal Loss: 1.172 \tVal Frac: 0.7393\n",
      "\tEpoch: 510/1000; 1.17 sec... \tStep: 123930... \tLoss: 1.123... \tVal Loss: 1.150 \tVal Frac: 0.6985\n",
      "\tEpoch: 515/1000; 1.04 sec... \tStep: 125145... \tLoss: 1.382... \tVal Loss: 1.154 \tVal Frac: 0.7316\n",
      "\tEpoch: 520/1000; 1.19 sec... \tStep: 126360... \tLoss: 1.331... \tVal Loss: 1.229 \tVal Frac: 0.6548\n",
      "\tEpoch: 525/1000; 1.13 sec... \tStep: 127575... \tLoss: 1.552... \tVal Loss: 1.218 \tVal Frac: 0.7257\n",
      "\tEpoch: 530/1000; 1.05 sec... \tStep: 128790... \tLoss: 0.990... \tVal Loss: 1.183 \tVal Frac: 0.4346\n",
      "\tEpoch: 535/1000; 1.23 sec... \tStep: 130005... \tLoss: 1.148... \tVal Loss: 1.186 \tVal Frac: 0.7162\n",
      "\tEpoch: 540/1000; 1.22 sec... \tStep: 131220... \tLoss: 1.026... \tVal Loss: 1.154 \tVal Frac: 0.6989\n",
      "\tEpoch: 545/1000; 1.20 sec... \tStep: 132435... \tLoss: 1.391... \tVal Loss: 1.172 \tVal Frac: 0.7055\n",
      "\tEpoch: 550/1000; 1.13 sec... \tStep: 133650... \tLoss: 1.223... \tVal Loss: 1.182 \tVal Frac: 0.6919\n",
      "\tEpoch: 555/1000; 1.23 sec... \tStep: 134865... \tLoss: 1.312... \tVal Loss: 1.189 \tVal Frac: 0.0640\n",
      "\tEpoch: 560/1000; 1.16 sec... \tStep: 136080... \tLoss: 1.267... \tVal Loss: 1.174 \tVal Frac: 0.6463\n",
      "\tEpoch: 565/1000; 1.20 sec... \tStep: 137295... \tLoss: 1.294... \tVal Loss: 1.134 \tVal Frac: 0.4588\n",
      "\tEpoch: 570/1000; 1.22 sec... \tStep: 138510... \tLoss: 1.398... \tVal Loss: 1.187 \tVal Frac: 0.7195\n",
      "\tEpoch: 575/1000; 1.17 sec... \tStep: 139725... \tLoss: 1.240... \tVal Loss: 1.162 \tVal Frac: 0.7213\n",
      "\tEpoch: 580/1000; 1.15 sec... \tStep: 140940... \tLoss: 1.571... \tVal Loss: 1.181 \tVal Frac: 0.6162\n",
      "\tEpoch: 585/1000; 1.13 sec... \tStep: 142155... \tLoss: 1.217... \tVal Loss: 1.147 \tVal Frac: 0.7059\n",
      "\tEpoch: 590/1000; 1.19 sec... \tStep: 143370... \tLoss: 1.325... \tVal Loss: 1.162 \tVal Frac: 0.7349\n",
      "\tEpoch: 595/1000; 1.19 sec... \tStep: 144585... \tLoss: 1.249... \tVal Loss: 1.171 \tVal Frac: 0.7485\n",
      "\tEpoch: 600/1000; 1.19 sec... \tStep: 145800... \tLoss: 1.116... \tVal Loss: 1.156 \tVal Frac: 0.0919\n",
      "\tEpoch: 605/1000; 1.09 sec... \tStep: 147015... \tLoss: 1.115... \tVal Loss: 1.157 \tVal Frac: 0.7342\n",
      "\tEpoch: 610/1000; 1.17 sec... \tStep: 148230... \tLoss: 1.270... \tVal Loss: 1.155 \tVal Frac: 0.7438\n",
      "\tEpoch: 615/1000; 1.17 sec... \tStep: 149445... \tLoss: 1.257... \tVal Loss: 1.194 \tVal Frac: 0.6324\n",
      "\tEpoch: 620/1000; 1.12 sec... \tStep: 150660... \tLoss: 1.142... \tVal Loss: 1.117 \tVal Frac: 0.7478\n",
      "\tEpoch: 625/1000; 1.22 sec... \tStep: 151875... \tLoss: 1.309... \tVal Loss: 1.121 \tVal Frac: 0.7540\n",
      "\tEpoch: 630/1000; 1.18 sec... \tStep: 153090... \tLoss: 1.384... \tVal Loss: 1.076 \tVal Frac: 0.7618\n",
      "\tEpoch: 635/1000; 1.11 sec... \tStep: 154305... \tLoss: 1.281... \tVal Loss: 1.184 \tVal Frac: 0.7559\n",
      "\tEpoch: 640/1000; 1.22 sec... \tStep: 155520... \tLoss: 1.041... \tVal Loss: 1.116 \tVal Frac: 0.7643\n",
      "\tEpoch: 645/1000; 1.20 sec... \tStep: 156735... \tLoss: 1.174... \tVal Loss: 1.121 \tVal Frac: 0.7640\n",
      "\tEpoch: 650/1000; 1.19 sec... \tStep: 157950... \tLoss: 1.420... \tVal Loss: 1.126 \tVal Frac: 0.6386\n",
      "\tEpoch: 655/1000; 1.26 sec... \tStep: 159165... \tLoss: 1.020... \tVal Loss: 1.144 \tVal Frac: 0.2371\n",
      "\tEpoch: 660/1000; 1.27 sec... \tStep: 160380... \tLoss: 1.326... \tVal Loss: 1.043 \tVal Frac: 0.7544\n",
      "\tEpoch: 665/1000; 1.25 sec... \tStep: 161595... \tLoss: 1.257... \tVal Loss: 1.238 \tVal Frac: 0.0989\n",
      "\tEpoch: 670/1000; 1.17 sec... \tStep: 162810... \tLoss: 1.048... \tVal Loss: 1.120 \tVal Frac: 0.7371\n",
      "\tEpoch: 675/1000; 1.10 sec... \tStep: 164025... \tLoss: 1.406... \tVal Loss: 1.044 \tVal Frac: 0.7540\n",
      "\tEpoch: 680/1000; 1.17 sec... \tStep: 165240... \tLoss: 1.473... \tVal Loss: 1.040 \tVal Frac: 0.7305\n",
      "\tEpoch: 685/1000; 1.18 sec... \tStep: 166455... \tLoss: 1.127... \tVal Loss: 1.058 \tVal Frac: 0.7478\n",
      "\tEpoch: 690/1000; 1.10 sec... \tStep: 167670... \tLoss: 1.421... \tVal Loss: 1.166 \tVal Frac: 0.3066\n",
      "\tEpoch: 695/1000; 1.23 sec... \tStep: 168885... \tLoss: 1.107... \tVal Loss: 0.993 \tVal Frac: 0.7643\n",
      "\tEpoch: 700/1000; 1.13 sec... \tStep: 170100... \tLoss: 1.121... \tVal Loss: 1.032 \tVal Frac: 0.7732\n",
      "\tEpoch: 705/1000; 1.24 sec... \tStep: 171315... \tLoss: 1.255... \tVal Loss: 1.081 \tVal Frac: 0.7559\n",
      "\tEpoch: 710/1000; 1.15 sec... \tStep: 172530... \tLoss: 1.194... \tVal Loss: 1.038 \tVal Frac: 0.7790\n",
      "\tEpoch: 715/1000; 1.16 sec... \tStep: 173745... \tLoss: 1.154... \tVal Loss: 1.029 \tVal Frac: 0.7544\n",
      "\tEpoch: 720/1000; 1.23 sec... \tStep: 174960... \tLoss: 1.107... \tVal Loss: 1.061 \tVal Frac: 0.7658\n",
      "\tEpoch: 725/1000; 1.24 sec... \tStep: 176175... \tLoss: 1.163... \tVal Loss: 1.082 \tVal Frac: 0.7496\n",
      "\tEpoch: 730/1000; 1.22 sec... \tStep: 177390... \tLoss: 1.199... \tVal Loss: 1.034 \tVal Frac: 0.7423\n",
      "\tEpoch: 735/1000; 1.19 sec... \tStep: 178605... \tLoss: 1.504... \tVal Loss: 1.128 \tVal Frac: 0.5224\n",
      "\tEpoch: 740/1000; 1.23 sec... \tStep: 179820... \tLoss: 1.280... \tVal Loss: 1.051 \tVal Frac: 0.7559\n",
      "\tEpoch: 745/1000; 1.18 sec... \tStep: 181035... \tLoss: 1.635... \tVal Loss: 1.118 \tVal Frac: 0.5224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 750/1000; 1.20 sec... \tStep: 182250... \tLoss: 1.007... \tVal Loss: 1.120 \tVal Frac: 0.5456\n",
      "\tEpoch: 755/1000; 1.16 sec... \tStep: 183465... \tLoss: 1.190... \tVal Loss: 1.116 \tVal Frac: 0.6077\n",
      "\tEpoch: 760/1000; 1.17 sec... \tStep: 184680... \tLoss: 1.110... \tVal Loss: 1.015 \tVal Frac: 0.7279\n",
      "\tEpoch: 765/1000; 1.23 sec... \tStep: 185895... \tLoss: 1.284... \tVal Loss: 1.162 \tVal Frac: 0.1276\n",
      "\tEpoch: 770/1000; 1.09 sec... \tStep: 187110... \tLoss: 1.236... \tVal Loss: 1.140 \tVal Frac: 0.4923\n",
      "\tEpoch: 775/1000; 1.30 sec... \tStep: 188325... \tLoss: 1.067... \tVal Loss: 1.015 \tVal Frac: 0.7493\n",
      "\tEpoch: 780/1000; 1.16 sec... \tStep: 189540... \tLoss: 1.194... \tVal Loss: 1.143 \tVal Frac: 0.5092\n",
      "\tEpoch: 785/1000; 1.27 sec... \tStep: 190755... \tLoss: 1.050... \tVal Loss: 1.062 \tVal Frac: 0.7588\n",
      "\tEpoch: 790/1000; 1.19 sec... \tStep: 191970... \tLoss: 1.077... \tVal Loss: 1.095 \tVal Frac: 0.7779\n",
      "\tEpoch: 795/1000; 1.20 sec... \tStep: 193185... \tLoss: 1.076... \tVal Loss: 1.033 \tVal Frac: 0.7438\n",
      "\tEpoch: 800/1000; 1.09 sec... \tStep: 194400... \tLoss: 0.907... \tVal Loss: 0.976 \tVal Frac: 0.7493\n",
      "\tEpoch: 805/1000; 1.23 sec... \tStep: 195615... \tLoss: 1.366... \tVal Loss: 1.196 \tVal Frac: 0.5978\n",
      "\tEpoch: 810/1000; 1.22 sec... \tStep: 196830... \tLoss: 1.079... \tVal Loss: 1.152 \tVal Frac: 0.7489\n",
      "\tEpoch: 815/1000; 1.22 sec... \tStep: 198045... \tLoss: 1.197... \tVal Loss: 1.156 \tVal Frac: 0.4846\n",
      "\tEpoch: 820/1000; 1.22 sec... \tStep: 199260... \tLoss: 1.159... \tVal Loss: 0.982 \tVal Frac: 0.7713\n",
      "\tEpoch: 825/1000; 1.17 sec... \tStep: 200475... \tLoss: 1.256... \tVal Loss: 1.146 \tVal Frac: 0.7680\n",
      "\tEpoch: 830/1000; 1.09 sec... \tStep: 201690... \tLoss: 1.230... \tVal Loss: 1.147 \tVal Frac: 0.7430\n",
      "\tEpoch: 835/1000; 1.27 sec... \tStep: 202905... \tLoss: 0.974... \tVal Loss: 1.026 \tVal Frac: 0.7415\n",
      "\tEpoch: 840/1000; 1.22 sec... \tStep: 204120... \tLoss: 1.206... \tVal Loss: 1.011 \tVal Frac: 0.7717\n",
      "\tEpoch: 845/1000; 1.16 sec... \tStep: 205335... \tLoss: 1.410... \tVal Loss: 1.128 \tVal Frac: 0.7710\n",
      "\tEpoch: 850/1000; 1.18 sec... \tStep: 206550... \tLoss: 1.197... \tVal Loss: 1.129 \tVal Frac: 0.1173\n",
      "\tEpoch: 855/1000; 1.15 sec... \tStep: 207765... \tLoss: 1.337... \tVal Loss: 1.124 \tVal Frac: 0.0974\n",
      "\tEpoch: 860/1000; 1.16 sec... \tStep: 208980... \tLoss: 1.065... \tVal Loss: 1.104 \tVal Frac: 0.7783\n",
      "\tEpoch: 865/1000; 1.20 sec... \tStep: 210195... \tLoss: 0.985... \tVal Loss: 1.141 \tVal Frac: 0.7625\n",
      "\tEpoch: 870/1000; 1.17 sec... \tStep: 211410... \tLoss: 1.101... \tVal Loss: 1.127 \tVal Frac: 0.7732\n",
      "\tEpoch: 875/1000; 1.20 sec... \tStep: 212625... \tLoss: 1.236... \tVal Loss: 1.154 \tVal Frac: 0.7739\n",
      "\tEpoch: 880/1000; 1.10 sec... \tStep: 213840... \tLoss: 1.084... \tVal Loss: 1.137 \tVal Frac: 0.3077\n",
      "\tEpoch: 885/1000; 1.18 sec... \tStep: 215055... \tLoss: 1.222... \tVal Loss: 1.150 \tVal Frac: 0.6088\n",
      "\tEpoch: 890/1000; 1.22 sec... \tStep: 216270... \tLoss: 1.225... \tVal Loss: 1.133 \tVal Frac: 0.7460\n",
      "\tEpoch: 895/1000; 1.14 sec... \tStep: 217485... \tLoss: 1.296... \tVal Loss: 1.125 \tVal Frac: 0.7485\n",
      "\tEpoch: 900/1000; 1.12 sec... \tStep: 218700... \tLoss: 1.193... \tVal Loss: 1.155 \tVal Frac: 0.5276\n",
      "\tEpoch: 905/1000; 1.10 sec... \tStep: 219915... \tLoss: 1.475... \tVal Loss: 1.119 \tVal Frac: 0.7121\n",
      "\tEpoch: 910/1000; 1.07 sec... \tStep: 221130... \tLoss: 1.096... \tVal Loss: 1.107 \tVal Frac: 0.7548\n",
      "\tEpoch: 915/1000; 1.17 sec... \tStep: 222345... \tLoss: 1.221... \tVal Loss: 1.110 \tVal Frac: 0.6982\n",
      "\tEpoch: 920/1000; 1.15 sec... \tStep: 223560... \tLoss: 1.021... \tVal Loss: 1.122 \tVal Frac: 0.7732\n",
      "\tEpoch: 925/1000; 1.16 sec... \tStep: 224775... \tLoss: 1.170... \tVal Loss: 1.110 \tVal Frac: 0.7669\n",
      "\tEpoch: 930/1000; 1.24 sec... \tStep: 225990... \tLoss: 1.148... \tVal Loss: 1.100 \tVal Frac: 0.7518\n",
      "\tEpoch: 935/1000; 1.19 sec... \tStep: 227205... \tLoss: 1.059... \tVal Loss: 1.096 \tVal Frac: 0.1261\n",
      "\tEpoch: 940/1000; 1.19 sec... \tStep: 228420... \tLoss: 1.414... \tVal Loss: 1.111 \tVal Frac: 0.4059\n",
      "\tEpoch: 945/1000; 1.20 sec... \tStep: 229635... \tLoss: 1.128... \tVal Loss: 1.130 \tVal Frac: 0.1033\n",
      "\tEpoch: 950/1000; 1.22 sec... \tStep: 230850... \tLoss: 1.216... \tVal Loss: 1.114 \tVal Frac: 0.1114\n",
      "\tEpoch: 955/1000; 1.15 sec... \tStep: 232065... \tLoss: 1.196... \tVal Loss: 1.139 \tVal Frac: 0.7728\n",
      "\tEpoch: 960/1000; 1.15 sec... \tStep: 233280... \tLoss: 1.171... \tVal Loss: 1.120 \tVal Frac: 0.7393\n",
      "\tEpoch: 965/1000; 1.20 sec... \tStep: 234495... \tLoss: 0.930... \tVal Loss: 1.159 \tVal Frac: 0.4342\n",
      "\tEpoch: 970/1000; 1.27 sec... \tStep: 235710... \tLoss: 1.073... \tVal Loss: 1.167 \tVal Frac: 0.0989\n",
      "\tEpoch: 975/1000; 1.15 sec... \tStep: 236925... \tLoss: 1.094... \tVal Loss: 1.085 \tVal Frac: 0.7632\n",
      "\tEpoch: 980/1000; 1.17 sec... \tStep: 238140... \tLoss: 1.252... \tVal Loss: 1.118 \tVal Frac: 0.7438\n",
      "\tEpoch: 985/1000; 1.15 sec... \tStep: 239355... \tLoss: 1.223... \tVal Loss: 1.101 \tVal Frac: 0.7651\n",
      "\tEpoch: 990/1000; 1.23 sec... \tStep: 240570... \tLoss: 0.992... \tVal Loss: 1.095 \tVal Frac: 0.7724\n",
      "\tEpoch: 995/1000; 1.17 sec... \tStep: 241785... \tLoss: 1.132... \tVal Loss: 1.118 \tVal Frac: 0.1210\n",
      "\tEpoch: 1000/1000; 1.26 sec... \tStep: 243000... \tLoss: 0.927... \tVal Loss: 1.103 \tVal Frac: 0.6930\n",
      "Completed:  n_windows :  20 \n",
      "\tloss: 0.954318 \n",
      "\tfrac:0.779044\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  50\n",
      "\tEpoch: 5/1000; 1.37 sec... \tStep: 1215... \tLoss: 0.931... \tVal Loss: 0.797 \tVal Frac: 0.7235\n",
      "\tValidation loss decreased (inf --> 0.797).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.37 sec... \tStep: 2430... \tLoss: 0.929... \tVal Loss: 0.705 \tVal Frac: 0.7790\n",
      "\tValidation loss decreased (0.797 --> 0.705).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.27 sec... \tStep: 3645... \tLoss: 0.865... \tVal Loss: 0.762 \tVal Frac: 0.7621\n",
      "\tEpoch: 20/1000; 1.40 sec... \tStep: 4860... \tLoss: 0.880... \tVal Loss: 0.788 \tVal Frac: 0.7798\n",
      "\tEpoch: 25/1000; 1.38 sec... \tStep: 6075... \tLoss: 0.775... \tVal Loss: 0.750 \tVal Frac: 0.7772\n",
      "\tEpoch: 30/1000; 1.44 sec... \tStep: 7290... \tLoss: 1.196... \tVal Loss: 0.746 \tVal Frac: 0.7724\n",
      "\tEpoch: 35/1000; 1.49 sec... \tStep: 8505... \tLoss: 0.673... \tVal Loss: 0.652 \tVal Frac: 0.7651\n",
      "\tValidation loss decreased (0.705 --> 0.652).  Saving model ...\n",
      "\tEpoch: 40/1000; 1.40 sec... \tStep: 9720... \tLoss: 0.685... \tVal Loss: 0.643 \tVal Frac: 0.7945\n",
      "\tValidation loss decreased (0.652 --> 0.643).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.47 sec... \tStep: 10935... \tLoss: 0.784... \tVal Loss: 0.617 \tVal Frac: 0.8000\n",
      "\tValidation loss decreased (0.643 --> 0.617).  Saving model ...\n",
      "\tEpoch: 50/1000; 1.34 sec... \tStep: 12150... \tLoss: 1.282... \tVal Loss: 0.666 \tVal Frac: 0.7824\n",
      "\tEpoch: 55/1000; 1.32 sec... \tStep: 13365... \tLoss: 0.928... \tVal Loss: 0.690 \tVal Frac: 0.7713\n",
      "\tEpoch: 60/1000; 1.36 sec... \tStep: 14580... \tLoss: 0.634... \tVal Loss: 0.657 \tVal Frac: 0.7809\n",
      "\tEpoch: 65/1000; 1.45 sec... \tStep: 15795... \tLoss: 0.939... \tVal Loss: 0.798 \tVal Frac: 0.7555\n",
      "\tEpoch: 70/1000; 1.46 sec... \tStep: 17010... \tLoss: 0.890... \tVal Loss: 0.750 \tVal Frac: 0.7551\n",
      "\tEpoch: 75/1000; 1.42 sec... \tStep: 18225... \tLoss: 1.017... \tVal Loss: 0.631 \tVal Frac: 0.8055\n",
      "\tEpoch: 80/1000; 1.39 sec... \tStep: 19440... \tLoss: 0.635... \tVal Loss: 0.703 \tVal Frac: 0.7864\n",
      "\tEpoch: 85/1000; 1.40 sec... \tStep: 20655... \tLoss: 1.003... \tVal Loss: 0.629 \tVal Frac: 0.7816\n",
      "\tEpoch: 90/1000; 1.37 sec... \tStep: 21870... \tLoss: 0.651... \tVal Loss: 0.619 \tVal Frac: 0.7739\n",
      "\tEpoch: 95/1000; 1.45 sec... \tStep: 23085... \tLoss: 0.769... \tVal Loss: 0.622 \tVal Frac: 0.8055\n",
      "\tEpoch: 100/1000; 1.32 sec... \tStep: 24300... \tLoss: 0.837... \tVal Loss: 0.666 \tVal Frac: 0.7919\n",
      "\tEpoch: 105/1000; 1.44 sec... \tStep: 25515... \tLoss: 0.551... \tVal Loss: 0.624 \tVal Frac: 0.7989\n",
      "\tEpoch: 110/1000; 1.47 sec... \tStep: 26730... \tLoss: 1.140... \tVal Loss: 0.678 \tVal Frac: 0.7897\n",
      "\tEpoch: 115/1000; 1.33 sec... \tStep: 27945... \tLoss: 0.811... \tVal Loss: 0.628 \tVal Frac: 0.8011\n",
      "\tEpoch: 120/1000; 1.35 sec... \tStep: 29160... \tLoss: 0.916... \tVal Loss: 0.659 \tVal Frac: 0.7776\n",
      "\tEpoch: 125/1000; 1.34 sec... \tStep: 30375... \tLoss: 1.253... \tVal Loss: 0.632 \tVal Frac: 0.7996\n",
      "\tEpoch: 130/1000; 1.35 sec... \tStep: 31590... \tLoss: 0.626... \tVal Loss: 0.621 \tVal Frac: 0.7985\n",
      "\tEpoch: 135/1000; 1.50 sec... \tStep: 32805... \tLoss: 0.691... \tVal Loss: 0.629 \tVal Frac: 0.7952\n",
      "\tEpoch: 140/1000; 1.44 sec... \tStep: 34020... \tLoss: 1.030... \tVal Loss: 0.633 \tVal Frac: 0.7960\n",
      "\tEpoch: 145/1000; 1.49 sec... \tStep: 35235... \tLoss: 0.492... \tVal Loss: 0.632 \tVal Frac: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 150/1000; 1.47 sec... \tStep: 36450... \tLoss: 0.975... \tVal Loss: 0.634 \tVal Frac: 0.8059\n",
      "\tEpoch: 155/1000; 1.36 sec... \tStep: 37665... \tLoss: 1.044... \tVal Loss: 0.600 \tVal Frac: 0.8004\n",
      "\tValidation loss decreased (0.617 --> 0.600).  Saving model ...\n",
      "\tEpoch: 160/1000; 1.34 sec... \tStep: 38880... \tLoss: 0.487... \tVal Loss: 0.612 \tVal Frac: 0.8051\n",
      "\tEpoch: 165/1000; 1.42 sec... \tStep: 40095... \tLoss: 0.880... \tVal Loss: 0.630 \tVal Frac: 0.8000\n",
      "\tEpoch: 170/1000; 1.47 sec... \tStep: 41310... \tLoss: 0.912... \tVal Loss: 0.598 \tVal Frac: 0.7864\n",
      "\tValidation loss decreased (0.600 --> 0.598).  Saving model ...\n",
      "\tEpoch: 175/1000; 1.42 sec... \tStep: 42525... \tLoss: 0.869... \tVal Loss: 0.629 \tVal Frac: 0.7901\n",
      "\tEpoch: 180/1000; 1.36 sec... \tStep: 43740... \tLoss: 0.668... \tVal Loss: 0.631 \tVal Frac: 0.7978\n",
      "\tEpoch: 185/1000; 1.36 sec... \tStep: 44955... \tLoss: 0.612... \tVal Loss: 0.630 \tVal Frac: 0.7937\n",
      "\tEpoch: 190/1000; 1.40 sec... \tStep: 46170... \tLoss: 0.541... \tVal Loss: 0.587 \tVal Frac: 0.8162\n",
      "\tValidation loss decreased (0.598 --> 0.587).  Saving model ...\n",
      "\tEpoch: 195/1000; 1.37 sec... \tStep: 47385... \tLoss: 0.785... \tVal Loss: 0.636 \tVal Frac: 0.8029\n",
      "\tEpoch: 200/1000; 1.39 sec... \tStep: 48600... \tLoss: 0.841... \tVal Loss: 0.659 \tVal Frac: 0.7886\n",
      "\tEpoch: 205/1000; 1.37 sec... \tStep: 49815... \tLoss: 0.681... \tVal Loss: 0.605 \tVal Frac: 0.8033\n",
      "\tEpoch: 210/1000; 1.36 sec... \tStep: 51030... \tLoss: 0.724... \tVal Loss: 0.663 \tVal Frac: 0.7831\n",
      "\tEpoch: 215/1000; 1.32 sec... \tStep: 52245... \tLoss: 1.183... \tVal Loss: 0.631 \tVal Frac: 0.7835\n",
      "\tEpoch: 220/1000; 1.60 sec... \tStep: 53460... \tLoss: 0.458... \tVal Loss: 0.633 \tVal Frac: 0.7908\n",
      "\tEpoch: 225/1000; 1.37 sec... \tStep: 54675... \tLoss: 0.962... \tVal Loss: 0.604 \tVal Frac: 0.8055\n",
      "\tEpoch: 230/1000; 1.36 sec... \tStep: 55890... \tLoss: 1.321... \tVal Loss: 0.677 \tVal Frac: 0.7890\n",
      "\tEpoch: 235/1000; 1.45 sec... \tStep: 57105... \tLoss: 0.897... \tVal Loss: 0.613 \tVal Frac: 0.7967\n",
      "\tEpoch: 240/1000; 1.40 sec... \tStep: 58320... \tLoss: 0.817... \tVal Loss: 0.621 \tVal Frac: 0.8022\n",
      "\tEpoch: 245/1000; 1.34 sec... \tStep: 59535... \tLoss: 0.761... \tVal Loss: 0.614 \tVal Frac: 0.8051\n",
      "\tEpoch: 250/1000; 1.32 sec... \tStep: 60750... \tLoss: 0.628... \tVal Loss: 0.598 \tVal Frac: 0.7956\n",
      "\tEpoch: 255/1000; 1.27 sec... \tStep: 61965... \tLoss: 0.807... \tVal Loss: 0.589 \tVal Frac: 0.8055\n",
      "\tEpoch: 260/1000; 1.42 sec... \tStep: 63180... \tLoss: 0.718... \tVal Loss: 0.618 \tVal Frac: 0.7882\n",
      "\tEpoch: 265/1000; 1.30 sec... \tStep: 64395... \tLoss: 0.562... \tVal Loss: 0.610 \tVal Frac: 0.7779\n",
      "\tEpoch: 270/1000; 1.42 sec... \tStep: 65610... \tLoss: 1.130... \tVal Loss: 0.606 \tVal Frac: 0.7982\n",
      "\tEpoch: 275/1000; 1.50 sec... \tStep: 66825... \tLoss: 0.947... \tVal Loss: 0.619 \tVal Frac: 0.7945\n",
      "\tEpoch: 280/1000; 1.44 sec... \tStep: 68040... \tLoss: 1.101... \tVal Loss: 0.619 \tVal Frac: 0.7967\n",
      "\tEpoch: 285/1000; 1.32 sec... \tStep: 69255... \tLoss: 0.634... \tVal Loss: 0.614 \tVal Frac: 0.7960\n",
      "\tEpoch: 290/1000; 1.34 sec... \tStep: 70470... \tLoss: 0.926... \tVal Loss: 0.656 \tVal Frac: 0.7949\n",
      "\tEpoch: 295/1000; 1.40 sec... \tStep: 71685... \tLoss: 0.511... \tVal Loss: 0.621 \tVal Frac: 0.7893\n",
      "\tEpoch: 300/1000; 1.50 sec... \tStep: 72900... \tLoss: 0.676... \tVal Loss: 0.645 \tVal Frac: 0.7776\n",
      "\tEpoch: 305/1000; 1.32 sec... \tStep: 74115... \tLoss: 0.942... \tVal Loss: 0.647 \tVal Frac: 0.7952\n",
      "\tEpoch: 310/1000; 1.49 sec... \tStep: 75330... \tLoss: 0.698... \tVal Loss: 0.655 \tVal Frac: 0.7728\n",
      "\tEpoch: 315/1000; 1.45 sec... \tStep: 76545... \tLoss: 0.918... \tVal Loss: 0.632 \tVal Frac: 0.7860\n",
      "\tEpoch: 320/1000; 1.42 sec... \tStep: 77760... \tLoss: 0.534... \tVal Loss: 0.578 \tVal Frac: 0.8132\n",
      "\tValidation loss decreased (0.587 --> 0.578).  Saving model ...\n",
      "\tEpoch: 325/1000; 1.32 sec... \tStep: 78975... \tLoss: 1.068... \tVal Loss: 0.600 \tVal Frac: 0.8099\n",
      "\tEpoch: 330/1000; 1.47 sec... \tStep: 80190... \tLoss: 0.585... \tVal Loss: 0.621 \tVal Frac: 0.7915\n",
      "\tEpoch: 335/1000; 1.42 sec... \tStep: 81405... \tLoss: 0.558... \tVal Loss: 0.606 \tVal Frac: 0.8044\n",
      "\tEpoch: 340/1000; 1.50 sec... \tStep: 82620... \tLoss: 0.769... \tVal Loss: 0.628 \tVal Frac: 0.7897\n",
      "\tEpoch: 345/1000; 1.45 sec... \tStep: 83835... \tLoss: 0.910... \tVal Loss: 0.603 \tVal Frac: 0.8055\n",
      "\tEpoch: 350/1000; 1.37 sec... \tStep: 85050... \tLoss: 0.253... \tVal Loss: 0.588 \tVal Frac: 0.7919\n",
      "\tEpoch: 355/1000; 1.27 sec... \tStep: 86265... \tLoss: 0.683... \tVal Loss: 0.591 \tVal Frac: 0.8015\n",
      "\tEpoch: 360/1000; 1.46 sec... \tStep: 87480... \tLoss: 1.112... \tVal Loss: 0.581 \tVal Frac: 0.8118\n",
      "\tEpoch: 365/1000; 1.27 sec... \tStep: 88695... \tLoss: 0.865... \tVal Loss: 0.594 \tVal Frac: 0.7963\n",
      "\tEpoch: 370/1000; 1.40 sec... \tStep: 89910... \tLoss: 0.638... \tVal Loss: 0.648 \tVal Frac: 0.7941\n",
      "\tEpoch: 375/1000; 1.39 sec... \tStep: 91125... \tLoss: 1.115... \tVal Loss: 0.591 \tVal Frac: 0.8125\n",
      "\tEpoch: 380/1000; 1.31 sec... \tStep: 92340... \tLoss: 0.891... \tVal Loss: 0.626 \tVal Frac: 0.7871\n",
      "\tEpoch: 385/1000; 1.45 sec... \tStep: 93555... \tLoss: 0.842... \tVal Loss: 0.630 \tVal Frac: 0.8044\n",
      "\tEpoch: 390/1000; 1.37 sec... \tStep: 94770... \tLoss: 0.773... \tVal Loss: 0.615 \tVal Frac: 0.8048\n",
      "\tEpoch: 395/1000; 1.36 sec... \tStep: 95985... \tLoss: 0.494... \tVal Loss: 0.601 \tVal Frac: 0.8092\n",
      "\tEpoch: 400/1000; 1.31 sec... \tStep: 97200... \tLoss: 0.691... \tVal Loss: 0.593 \tVal Frac: 0.8081\n",
      "\tEpoch: 405/1000; 1.46 sec... \tStep: 98415... \tLoss: 0.560... \tVal Loss: 0.579 \tVal Frac: 0.8085\n",
      "\tEpoch: 410/1000; 1.35 sec... \tStep: 99630... \tLoss: 0.567... \tVal Loss: 0.613 \tVal Frac: 0.7835\n",
      "\tEpoch: 415/1000; 1.30 sec... \tStep: 100845... \tLoss: 0.648... \tVal Loss: 0.631 \tVal Frac: 0.7849\n",
      "\tEpoch: 420/1000; 1.49 sec... \tStep: 102060... \tLoss: 0.938... \tVal Loss: 0.603 \tVal Frac: 0.7963\n",
      "\tEpoch: 425/1000; 1.44 sec... \tStep: 103275... \tLoss: 0.665... \tVal Loss: 0.610 \tVal Frac: 0.8011\n",
      "\tEpoch: 430/1000; 1.43 sec... \tStep: 104490... \tLoss: 1.090... \tVal Loss: 0.615 \tVal Frac: 0.8000\n",
      "\tEpoch: 435/1000; 1.36 sec... \tStep: 105705... \tLoss: 0.979... \tVal Loss: 0.617 \tVal Frac: 0.7989\n",
      "\tEpoch: 440/1000; 1.36 sec... \tStep: 106920... \tLoss: 0.896... \tVal Loss: 0.605 \tVal Frac: 0.7930\n",
      "\tEpoch: 445/1000; 1.42 sec... \tStep: 108135... \tLoss: 0.644... \tVal Loss: 0.586 \tVal Frac: 0.7937\n",
      "\tEpoch: 450/1000; 1.47 sec... \tStep: 109350... \tLoss: 0.813... \tVal Loss: 0.625 \tVal Frac: 0.7816\n",
      "\tEpoch: 455/1000; 1.47 sec... \tStep: 110565... \tLoss: 0.765... \tVal Loss: 0.610 \tVal Frac: 0.7956\n",
      "\tEpoch: 460/1000; 1.32 sec... \tStep: 111780... \tLoss: 0.937... \tVal Loss: 0.609 \tVal Frac: 0.7989\n",
      "\tEpoch: 465/1000; 1.44 sec... \tStep: 112995... \tLoss: 0.906... \tVal Loss: 0.607 \tVal Frac: 0.8070\n",
      "\tEpoch: 470/1000; 1.47 sec... \tStep: 114210... \tLoss: 0.370... \tVal Loss: 0.622 \tVal Frac: 0.7934\n",
      "\tEpoch: 475/1000; 1.44 sec... \tStep: 115425... \tLoss: 0.455... \tVal Loss: 0.596 \tVal Frac: 0.8154\n",
      "\tEpoch: 480/1000; 1.41 sec... \tStep: 116640... \tLoss: 0.494... \tVal Loss: 0.598 \tVal Frac: 0.8110\n",
      "\tEpoch: 485/1000; 1.53 sec... \tStep: 117855... \tLoss: 0.678... \tVal Loss: 0.615 \tVal Frac: 0.7963\n",
      "\tEpoch: 490/1000; 1.38 sec... \tStep: 119070... \tLoss: 0.440... \tVal Loss: 0.593 \tVal Frac: 0.7934\n",
      "\tEpoch: 495/1000; 1.30 sec... \tStep: 120285... \tLoss: 1.017... \tVal Loss: 0.617 \tVal Frac: 0.7949\n",
      "\tEpoch: 500/1000; 1.46 sec... \tStep: 121500... \tLoss: 0.649... \tVal Loss: 0.602 \tVal Frac: 0.7827\n",
      "\tEpoch: 505/1000; 1.34 sec... \tStep: 122715... \tLoss: 0.843... \tVal Loss: 0.642 \tVal Frac: 0.7794\n",
      "\tEpoch: 510/1000; 1.45 sec... \tStep: 123930... \tLoss: 0.828... \tVal Loss: 0.622 \tVal Frac: 0.7827\n",
      "\tEpoch: 515/1000; 1.35 sec... \tStep: 125145... \tLoss: 0.607... \tVal Loss: 0.646 \tVal Frac: 0.7746\n",
      "\tEpoch: 520/1000; 1.46 sec... \tStep: 126360... \tLoss: 0.813... \tVal Loss: 0.610 \tVal Frac: 0.7846\n",
      "\tEpoch: 525/1000; 1.27 sec... \tStep: 127575... \tLoss: 0.518... \tVal Loss: 0.586 \tVal Frac: 0.8026\n",
      "\tEpoch: 530/1000; 1.39 sec... \tStep: 128790... \tLoss: 0.850... \tVal Loss: 0.612 \tVal Frac: 0.7934\n",
      "\tEpoch: 535/1000; 1.45 sec... \tStep: 130005... \tLoss: 0.902... \tVal Loss: 0.642 \tVal Frac: 0.7952\n",
      "\tEpoch: 540/1000; 1.34 sec... \tStep: 131220... \tLoss: 0.678... \tVal Loss: 0.607 \tVal Frac: 0.7816\n",
      "\tEpoch: 545/1000; 1.34 sec... \tStep: 132435... \tLoss: 0.875... \tVal Loss: 0.588 \tVal Frac: 0.7993\n",
      "\tEpoch: 550/1000; 1.39 sec... \tStep: 133650... \tLoss: 0.748... \tVal Loss: 0.573 \tVal Frac: 0.8099\n",
      "\tValidation loss decreased (0.578 --> 0.573).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 555/1000; 1.35 sec... \tStep: 134865... \tLoss: 0.948... \tVal Loss: 0.585 \tVal Frac: 0.8114\n",
      "\tEpoch: 560/1000; 1.42 sec... \tStep: 136080... \tLoss: 0.691... \tVal Loss: 0.589 \tVal Frac: 0.7978\n",
      "\tEpoch: 565/1000; 1.32 sec... \tStep: 137295... \tLoss: 0.593... \tVal Loss: 0.584 \tVal Frac: 0.8096\n",
      "\tEpoch: 570/1000; 1.54 sec... \tStep: 138510... \tLoss: 1.006... \tVal Loss: 0.617 \tVal Frac: 0.7926\n",
      "\tEpoch: 575/1000; 1.40 sec... \tStep: 139725... \tLoss: 0.812... \tVal Loss: 0.616 \tVal Frac: 0.7941\n",
      "\tEpoch: 580/1000; 1.44 sec... \tStep: 140940... \tLoss: 0.541... \tVal Loss: 0.624 \tVal Frac: 0.8029\n",
      "\tEpoch: 585/1000; 1.39 sec... \tStep: 142155... \tLoss: 0.642... \tVal Loss: 0.586 \tVal Frac: 0.7949\n",
      "\tEpoch: 590/1000; 1.47 sec... \tStep: 143370... \tLoss: 0.896... \tVal Loss: 0.612 \tVal Frac: 0.7912\n",
      "\tEpoch: 595/1000; 1.36 sec... \tStep: 144585... \tLoss: 0.583... \tVal Loss: 0.586 \tVal Frac: 0.8081\n",
      "\tEpoch: 600/1000; 1.37 sec... \tStep: 145800... \tLoss: 1.224... \tVal Loss: 0.620 \tVal Frac: 0.7993\n",
      "\tEpoch: 605/1000; 1.29 sec... \tStep: 147015... \tLoss: 0.770... \tVal Loss: 0.608 \tVal Frac: 0.7908\n",
      "\tEpoch: 610/1000; 1.37 sec... \tStep: 148230... \tLoss: 0.973... \tVal Loss: 0.611 \tVal Frac: 0.7904\n",
      "\tEpoch: 615/1000; 1.39 sec... \tStep: 149445... \tLoss: 0.737... \tVal Loss: 0.616 \tVal Frac: 0.7904\n",
      "\tEpoch: 620/1000; 1.42 sec... \tStep: 150660... \tLoss: 0.557... \tVal Loss: 0.572 \tVal Frac: 0.7996\n",
      "\tValidation loss decreased (0.573 --> 0.572).  Saving model ...\n",
      "\tEpoch: 625/1000; 1.47 sec... \tStep: 151875... \tLoss: 0.372... \tVal Loss: 0.594 \tVal Frac: 0.8033\n",
      "\tEpoch: 630/1000; 1.35 sec... \tStep: 153090... \tLoss: 0.809... \tVal Loss: 0.608 \tVal Frac: 0.8004\n",
      "\tEpoch: 635/1000; 1.35 sec... \tStep: 154305... \tLoss: 0.664... \tVal Loss: 0.608 \tVal Frac: 0.7732\n",
      "\tEpoch: 640/1000; 1.42 sec... \tStep: 155520... \tLoss: 0.667... \tVal Loss: 0.615 \tVal Frac: 0.7761\n",
      "\tEpoch: 645/1000; 1.42 sec... \tStep: 156735... \tLoss: 0.853... \tVal Loss: 0.625 \tVal Frac: 0.7798\n",
      "\tEpoch: 650/1000; 1.47 sec... \tStep: 157950... \tLoss: 0.334... \tVal Loss: 0.606 \tVal Frac: 0.7930\n",
      "\tEpoch: 655/1000; 1.46 sec... \tStep: 159165... \tLoss: 0.725... \tVal Loss: 0.578 \tVal Frac: 0.8140\n",
      "\tEpoch: 660/1000; 1.46 sec... \tStep: 160380... \tLoss: 0.655... \tVal Loss: 0.590 \tVal Frac: 0.8037\n",
      "\tEpoch: 665/1000; 1.37 sec... \tStep: 161595... \tLoss: 0.779... \tVal Loss: 0.606 \tVal Frac: 0.7930\n",
      "\tEpoch: 670/1000; 1.40 sec... \tStep: 162810... \tLoss: 0.637... \tVal Loss: 0.619 \tVal Frac: 0.7908\n",
      "\tEpoch: 675/1000; 1.44 sec... \tStep: 164025... \tLoss: 0.547... \tVal Loss: 0.597 \tVal Frac: 0.8007\n",
      "\tEpoch: 680/1000; 1.38 sec... \tStep: 165240... \tLoss: 0.765... \tVal Loss: 0.582 \tVal Frac: 0.7989\n",
      "\tEpoch: 685/1000; 1.46 sec... \tStep: 166455... \tLoss: 0.415... \tVal Loss: 0.610 \tVal Frac: 0.7919\n",
      "\tEpoch: 690/1000; 1.46 sec... \tStep: 167670... \tLoss: 0.639... \tVal Loss: 0.611 \tVal Frac: 0.7919\n",
      "\tEpoch: 695/1000; 1.32 sec... \tStep: 168885... \tLoss: 0.760... \tVal Loss: 0.603 \tVal Frac: 0.7963\n",
      "\tEpoch: 700/1000; 1.37 sec... \tStep: 170100... \tLoss: 0.829... \tVal Loss: 0.609 \tVal Frac: 0.7879\n",
      "\tEpoch: 705/1000; 1.34 sec... \tStep: 171315... \tLoss: 0.737... \tVal Loss: 0.592 \tVal Frac: 0.7937\n",
      "\tEpoch: 710/1000; 1.40 sec... \tStep: 172530... \tLoss: 0.841... \tVal Loss: 0.594 \tVal Frac: 0.7794\n",
      "\tEpoch: 715/1000; 1.52 sec... \tStep: 173745... \tLoss: 0.680... \tVal Loss: 0.604 \tVal Frac: 0.7835\n",
      "\tEpoch: 720/1000; 1.40 sec... \tStep: 174960... \tLoss: 0.721... \tVal Loss: 0.589 \tVal Frac: 0.7996\n",
      "\tEpoch: 725/1000; 1.42 sec... \tStep: 176175... \tLoss: 0.719... \tVal Loss: 0.586 \tVal Frac: 0.7945\n",
      "\tEpoch: 730/1000; 1.36 sec... \tStep: 177390... \tLoss: 0.825... \tVal Loss: 0.608 \tVal Frac: 0.7820\n",
      "\tEpoch: 735/1000; 1.54 sec... \tStep: 178605... \tLoss: 0.603... \tVal Loss: 0.609 \tVal Frac: 0.7912\n",
      "\tEpoch: 740/1000; 1.49 sec... \tStep: 179820... \tLoss: 0.776... \tVal Loss: 0.604 \tVal Frac: 0.7831\n",
      "\tEpoch: 745/1000; 1.40 sec... \tStep: 181035... \tLoss: 0.866... \tVal Loss: 0.607 \tVal Frac: 0.7890\n",
      "\tEpoch: 750/1000; 1.47 sec... \tStep: 182250... \tLoss: 0.718... \tVal Loss: 0.581 \tVal Frac: 0.8107\n",
      "\tEpoch: 755/1000; 1.42 sec... \tStep: 183465... \tLoss: 0.859... \tVal Loss: 0.599 \tVal Frac: 0.7926\n",
      "\tEpoch: 760/1000; 1.52 sec... \tStep: 184680... \tLoss: 0.767... \tVal Loss: 0.603 \tVal Frac: 0.7827\n",
      "\tEpoch: 765/1000; 1.37 sec... \tStep: 185895... \tLoss: 0.459... \tVal Loss: 0.611 \tVal Frac: 0.7864\n",
      "\tEpoch: 770/1000; 1.44 sec... \tStep: 187110... \tLoss: 0.576... \tVal Loss: 0.604 \tVal Frac: 0.8015\n",
      "\tEpoch: 775/1000; 1.48 sec... \tStep: 188325... \tLoss: 0.762... \tVal Loss: 0.609 \tVal Frac: 0.7912\n",
      "\tEpoch: 780/1000; 1.40 sec... \tStep: 189540... \tLoss: 0.669... \tVal Loss: 0.580 \tVal Frac: 0.8099\n",
      "\tEpoch: 785/1000; 1.40 sec... \tStep: 190755... \tLoss: 0.742... \tVal Loss: 0.593 \tVal Frac: 0.8070\n",
      "\tEpoch: 790/1000; 1.47 sec... \tStep: 191970... \tLoss: 0.914... \tVal Loss: 0.612 \tVal Frac: 0.7937\n",
      "\tEpoch: 795/1000; 1.49 sec... \tStep: 193185... \tLoss: 0.641... \tVal Loss: 0.586 \tVal Frac: 0.8121\n",
      "\tEpoch: 800/1000; 1.44 sec... \tStep: 194400... \tLoss: 0.715... \tVal Loss: 0.593 \tVal Frac: 0.7985\n",
      "\tEpoch: 805/1000; 1.44 sec... \tStep: 195615... \tLoss: 0.400... \tVal Loss: 0.583 \tVal Frac: 0.8074\n",
      "\tEpoch: 810/1000; 1.45 sec... \tStep: 196830... \tLoss: 0.678... \tVal Loss: 0.597 \tVal Frac: 0.8103\n",
      "\tEpoch: 815/1000; 1.30 sec... \tStep: 198045... \tLoss: 0.524... \tVal Loss: 0.627 \tVal Frac: 0.7875\n",
      "\tEpoch: 820/1000; 1.40 sec... \tStep: 199260... \tLoss: 0.803... \tVal Loss: 0.571 \tVal Frac: 0.8110\n",
      "\tValidation loss decreased (0.572 --> 0.571).  Saving model ...\n",
      "\tEpoch: 825/1000; 1.36 sec... \tStep: 200475... \tLoss: 1.021... \tVal Loss: 0.581 \tVal Frac: 0.7989\n",
      "\tEpoch: 830/1000; 1.48 sec... \tStep: 201690... \tLoss: 0.807... \tVal Loss: 0.583 \tVal Frac: 0.7993\n",
      "\tEpoch: 835/1000; 1.37 sec... \tStep: 202905... \tLoss: 0.545... \tVal Loss: 0.608 \tVal Frac: 0.8007\n",
      "\tEpoch: 840/1000; 1.42 sec... \tStep: 204120... \tLoss: 0.535... \tVal Loss: 0.582 \tVal Frac: 0.8048\n",
      "\tEpoch: 845/1000; 1.44 sec... \tStep: 205335... \tLoss: 0.916... \tVal Loss: 0.584 \tVal Frac: 0.8114\n",
      "\tEpoch: 850/1000; 1.35 sec... \tStep: 206550... \tLoss: 0.651... \tVal Loss: 0.640 \tVal Frac: 0.7971\n",
      "\tEpoch: 855/1000; 1.47 sec... \tStep: 207765... \tLoss: 0.686... \tVal Loss: 0.587 \tVal Frac: 0.8132\n",
      "\tEpoch: 860/1000; 1.52 sec... \tStep: 208980... \tLoss: 1.198... \tVal Loss: 0.600 \tVal Frac: 0.8044\n",
      "\tEpoch: 865/1000; 1.50 sec... \tStep: 210195... \tLoss: 0.440... \tVal Loss: 0.600 \tVal Frac: 0.7982\n",
      "\tEpoch: 870/1000; 1.44 sec... \tStep: 211410... \tLoss: 0.579... \tVal Loss: 0.607 \tVal Frac: 0.7926\n",
      "\tEpoch: 875/1000; 1.36 sec... \tStep: 212625... \tLoss: 0.796... \tVal Loss: 0.614 \tVal Frac: 0.7963\n",
      "\tEpoch: 880/1000; 1.38 sec... \tStep: 213840... \tLoss: 0.538... \tVal Loss: 0.590 \tVal Frac: 0.8026\n",
      "\tEpoch: 885/1000; 1.39 sec... \tStep: 215055... \tLoss: 0.582... \tVal Loss: 0.602 \tVal Frac: 0.7956\n",
      "\tEpoch: 890/1000; 1.42 sec... \tStep: 216270... \tLoss: 0.668... \tVal Loss: 0.591 \tVal Frac: 0.8004\n",
      "\tEpoch: 895/1000; 1.47 sec... \tStep: 217485... \tLoss: 0.499... \tVal Loss: 0.607 \tVal Frac: 0.7812\n",
      "\tEpoch: 900/1000; 1.29 sec... \tStep: 218700... \tLoss: 0.729... \tVal Loss: 0.613 \tVal Frac: 0.7860\n",
      "\tEpoch: 905/1000; 1.35 sec... \tStep: 219915... \tLoss: 0.392... \tVal Loss: 0.577 \tVal Frac: 0.7996\n",
      "\tEpoch: 910/1000; 1.39 sec... \tStep: 221130... \tLoss: 0.640... \tVal Loss: 0.596 \tVal Frac: 0.8132\n",
      "\tEpoch: 915/1000; 1.34 sec... \tStep: 222345... \tLoss: 0.621... \tVal Loss: 0.604 \tVal Frac: 0.7982\n",
      "\tEpoch: 920/1000; 1.27 sec... \tStep: 223560... \tLoss: 0.417... \tVal Loss: 0.577 \tVal Frac: 0.7989\n",
      "\tEpoch: 925/1000; 1.35 sec... \tStep: 224775... \tLoss: 0.991... \tVal Loss: 0.587 \tVal Frac: 0.8029\n",
      "\tEpoch: 930/1000; 1.45 sec... \tStep: 225990... \tLoss: 0.459... \tVal Loss: 0.601 \tVal Frac: 0.7919\n",
      "\tEpoch: 935/1000; 1.32 sec... \tStep: 227205... \tLoss: 0.560... \tVal Loss: 0.596 \tVal Frac: 0.8033\n",
      "\tEpoch: 940/1000; 1.37 sec... \tStep: 228420... \tLoss: 0.651... \tVal Loss: 0.581 \tVal Frac: 0.8048\n",
      "\tEpoch: 945/1000; 1.34 sec... \tStep: 229635... \tLoss: 0.479... \tVal Loss: 0.575 \tVal Frac: 0.8147\n",
      "\tEpoch: 950/1000; 1.34 sec... \tStep: 230850... \tLoss: 0.811... \tVal Loss: 0.578 \tVal Frac: 0.8000\n",
      "\tEpoch: 955/1000; 1.42 sec... \tStep: 232065... \tLoss: 0.640... \tVal Loss: 0.594 \tVal Frac: 0.8048\n",
      "\tEpoch: 960/1000; 1.36 sec... \tStep: 233280... \tLoss: 0.887... \tVal Loss: 0.576 \tVal Frac: 0.8077\n",
      "\tEpoch: 965/1000; 1.37 sec... \tStep: 234495... \tLoss: 0.760... \tVal Loss: 0.573 \tVal Frac: 0.8011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 970/1000; 1.38 sec... \tStep: 235710... \tLoss: 0.672... \tVal Loss: 0.581 \tVal Frac: 0.8114\n",
      "\tEpoch: 975/1000; 1.36 sec... \tStep: 236925... \tLoss: 0.512... \tVal Loss: 0.602 \tVal Frac: 0.8026\n",
      "\tEpoch: 980/1000; 1.34 sec... \tStep: 238140... \tLoss: 0.921... \tVal Loss: 0.582 \tVal Frac: 0.8096\n",
      "\tEpoch: 985/1000; 1.32 sec... \tStep: 239355... \tLoss: 0.928... \tVal Loss: 0.596 \tVal Frac: 0.8096\n",
      "\tEpoch: 990/1000; 1.40 sec... \tStep: 240570... \tLoss: 0.737... \tVal Loss: 0.596 \tVal Frac: 0.8029\n",
      "\tEpoch: 995/1000; 1.39 sec... \tStep: 241785... \tLoss: 0.601... \tVal Loss: 0.585 \tVal Frac: 0.8022\n",
      "\tEpoch: 1000/1000; 1.30 sec... \tStep: 243000... \tLoss: 0.599... \tVal Loss: 0.577 \tVal Frac: 0.8110\n",
      "Completed:  n_windows :  50 \n",
      "\tloss: 0.571452 \n",
      "\tfrac:0.816176\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  100\n",
      "\tEpoch: 5/1000; 1.57 sec... \tStep: 1210... \tLoss: 1.522... \tVal Loss: 1.323 \tVal Frac: 0.0763\n",
      "\tValidation loss decreased (inf --> 1.323).  Saving model ...\n",
      "\tEpoch: 10/1000; 1.78 sec... \tStep: 2420... \tLoss: 1.389... \tVal Loss: 1.289 \tVal Frac: 0.0510\n",
      "\tValidation loss decreased (1.323 --> 1.289).  Saving model ...\n",
      "\tEpoch: 15/1000; 1.67 sec... \tStep: 3630... \tLoss: 1.501... \tVal Loss: 1.280 \tVal Frac: 0.0536\n",
      "\tValidation loss decreased (1.289 --> 1.280).  Saving model ...\n",
      "\tEpoch: 20/1000; 1.70 sec... \tStep: 4840... \tLoss: 1.646... \tVal Loss: 1.316 \tVal Frac: 0.0432\n",
      "\tEpoch: 25/1000; 1.65 sec... \tStep: 6050... \tLoss: 1.312... \tVal Loss: 1.256 \tVal Frac: 0.0625\n",
      "\tValidation loss decreased (1.280 --> 1.256).  Saving model ...\n",
      "\tEpoch: 30/1000; 1.71 sec... \tStep: 7260... \tLoss: 1.445... \tVal Loss: 1.285 \tVal Frac: 0.0711\n",
      "\tEpoch: 35/1000; 1.64 sec... \tStep: 8470... \tLoss: 1.299... \tVal Loss: 1.277 \tVal Frac: 0.0778\n",
      "\tEpoch: 40/1000; 1.60 sec... \tStep: 9680... \tLoss: 1.352... \tVal Loss: 1.196 \tVal Frac: 0.0565\n",
      "\tValidation loss decreased (1.256 --> 1.196).  Saving model ...\n",
      "\tEpoch: 45/1000; 1.64 sec... \tStep: 10890... \tLoss: 1.206... \tVal Loss: 1.197 \tVal Frac: 0.0580\n",
      "\tEpoch: 50/1000; 1.66 sec... \tStep: 12100... \tLoss: 1.304... \tVal Loss: 1.217 \tVal Frac: 0.0837\n",
      "\tEpoch: 55/1000; 1.72 sec... \tStep: 13310... \tLoss: 1.379... \tVal Loss: 1.216 \tVal Frac: 0.0711\n",
      "\tEpoch: 60/1000; 1.76 sec... \tStep: 14520... \tLoss: 1.319... \tVal Loss: 1.256 \tVal Frac: 0.0528\n",
      "\tEpoch: 65/1000; 1.71 sec... \tStep: 15730... \tLoss: 1.431... \tVal Loss: 1.226 \tVal Frac: 0.0547\n",
      "\tEpoch: 70/1000; 1.71 sec... \tStep: 16940... \tLoss: 1.585... \tVal Loss: 1.281 \tVal Frac: 0.0818\n",
      "\tEpoch: 75/1000; 1.74 sec... \tStep: 18150... \tLoss: 1.665... \tVal Loss: 1.224 \tVal Frac: 0.1019\n",
      "\tEpoch: 80/1000; 1.74 sec... \tStep: 19360... \tLoss: 1.519... \tVal Loss: 1.216 \tVal Frac: 0.0673\n",
      "\tEpoch: 85/1000; 1.87 sec... \tStep: 20570... \tLoss: 1.558... \tVal Loss: 1.221 \tVal Frac: 0.0480\n",
      "\tEpoch: 90/1000; 1.83 sec... \tStep: 21780... \tLoss: 1.344... \tVal Loss: 1.241 \tVal Frac: 0.0461\n",
      "\tEpoch: 95/1000; 1.57 sec... \tStep: 22990... \tLoss: 1.514... \tVal Loss: 1.227 \tVal Frac: 0.0528\n",
      "\tEpoch: 100/1000; 1.75 sec... \tStep: 24200... \tLoss: 1.270... \tVal Loss: 1.239 \tVal Frac: 0.0551\n",
      "\tEpoch: 105/1000; 1.67 sec... \tStep: 25410... \tLoss: 1.674... \tVal Loss: 1.201 \tVal Frac: 0.0554\n",
      "\tEpoch: 110/1000; 1.64 sec... \tStep: 26620... \tLoss: 1.526... \tVal Loss: 1.246 \tVal Frac: 0.0435\n",
      "\tEpoch: 115/1000; 1.72 sec... \tStep: 27830... \tLoss: 1.710... \tVal Loss: 1.237 \tVal Frac: 0.0774\n",
      "\tEpoch: 120/1000; 1.74 sec... \tStep: 29040... \tLoss: 1.256... \tVal Loss: 1.197 \tVal Frac: 0.0495\n",
      "\tEpoch: 125/1000; 1.79 sec... \tStep: 30250... \tLoss: 1.240... \tVal Loss: 1.202 \tVal Frac: 0.0528\n",
      "\tEpoch: 130/1000; 1.68 sec... \tStep: 31460... \tLoss: 1.597... \tVal Loss: 1.197 \tVal Frac: 0.0495\n",
      "\tEpoch: 135/1000; 1.79 sec... \tStep: 32670... \tLoss: 1.172... \tVal Loss: 1.213 \tVal Frac: 0.0532\n",
      "\tEpoch: 140/1000; 1.69 sec... \tStep: 33880... \tLoss: 1.147... \tVal Loss: 1.211 \tVal Frac: 0.0461\n",
      "\tEpoch: 145/1000; 1.70 sec... \tStep: 35090... \tLoss: 1.289... \tVal Loss: 1.192 \tVal Frac: 0.0472\n",
      "\tValidation loss decreased (1.196 --> 1.192).  Saving model ...\n",
      "\tEpoch: 150/1000; 1.64 sec... \tStep: 36300... \tLoss: 1.378... \tVal Loss: 1.199 \tVal Frac: 0.0804\n",
      "\tEpoch: 155/1000; 1.81 sec... \tStep: 37510... \tLoss: 1.198... \tVal Loss: 1.197 \tVal Frac: 0.0923\n",
      "\tEpoch: 160/1000; 1.60 sec... \tStep: 38720... \tLoss: 1.506... \tVal Loss: 1.229 \tVal Frac: 0.0900\n",
      "\tEpoch: 165/1000; 1.75 sec... \tStep: 39930... \tLoss: 1.148... \tVal Loss: 1.206 \tVal Frac: 0.0867\n",
      "\tEpoch: 170/1000; 1.82 sec... \tStep: 41140... \tLoss: 1.366... \tVal Loss: 1.213 \tVal Frac: 0.0830\n",
      "\tEpoch: 175/1000; 1.71 sec... \tStep: 42350... \tLoss: 1.294... \tVal Loss: 1.241 \tVal Frac: 0.0621\n",
      "\tEpoch: 180/1000; 1.64 sec... \tStep: 43560... \tLoss: 1.316... \tVal Loss: 1.187 \tVal Frac: 0.0789\n",
      "\tValidation loss decreased (1.192 --> 1.187).  Saving model ...\n",
      "\tEpoch: 185/1000; 1.65 sec... \tStep: 44770... \tLoss: 1.384... \tVal Loss: 1.204 \tVal Frac: 0.0778\n",
      "\tEpoch: 190/1000; 1.77 sec... \tStep: 45980... \tLoss: 1.249... \tVal Loss: 1.173 \tVal Frac: 0.0800\n",
      "\tValidation loss decreased (1.187 --> 1.173).  Saving model ...\n",
      "\tEpoch: 195/1000; 1.59 sec... \tStep: 47190... \tLoss: 1.393... \tVal Loss: 1.199 \tVal Frac: 0.0577\n",
      "\tEpoch: 200/1000; 1.82 sec... \tStep: 48400... \tLoss: 1.445... \tVal Loss: 1.182 \tVal Frac: 0.0990\n",
      "\tEpoch: 205/1000; 1.74 sec... \tStep: 49610... \tLoss: 1.329... \tVal Loss: 1.199 \tVal Frac: 0.0930\n",
      "\tEpoch: 210/1000; 1.74 sec... \tStep: 50820... \tLoss: 1.059... \tVal Loss: 1.211 \tVal Frac: 0.0837\n",
      "\tEpoch: 215/1000; 1.72 sec... \tStep: 52030... \tLoss: 1.335... \tVal Loss: 1.203 \tVal Frac: 0.0569\n",
      "\tEpoch: 220/1000; 1.78 sec... \tStep: 53240... \tLoss: 1.545... \tVal Loss: 1.200 \tVal Frac: 0.0770\n",
      "\tEpoch: 225/1000; 1.89 sec... \tStep: 54450... \tLoss: 1.423... \tVal Loss: 1.196 \tVal Frac: 0.0755\n",
      "\tEpoch: 230/1000; 1.76 sec... \tStep: 55660... \tLoss: 1.519... \tVal Loss: 1.178 \tVal Frac: 0.0554\n",
      "\tEpoch: 235/1000; 1.76 sec... \tStep: 56870... \tLoss: 1.208... \tVal Loss: 1.181 \tVal Frac: 0.0755\n",
      "\tEpoch: 240/1000; 1.62 sec... \tStep: 58080... \tLoss: 1.049... \tVal Loss: 1.159 \tVal Frac: 0.0833\n",
      "\tValidation loss decreased (1.173 --> 1.159).  Saving model ...\n",
      "\tEpoch: 245/1000; 1.59 sec... \tStep: 59290... \tLoss: 1.608... \tVal Loss: 1.210 \tVal Frac: 0.0543\n",
      "\tEpoch: 250/1000; 1.74 sec... \tStep: 60500... \tLoss: 1.426... \tVal Loss: 1.162 \tVal Frac: 0.0874\n",
      "\tEpoch: 255/1000; 1.69 sec... \tStep: 61710... \tLoss: 1.207... \tVal Loss: 1.158 \tVal Frac: 0.0938\n",
      "\tValidation loss decreased (1.159 --> 1.158).  Saving model ...\n",
      "\tEpoch: 260/1000; 1.72 sec... \tStep: 62920... \tLoss: 1.505... \tVal Loss: 1.195 \tVal Frac: 0.0506\n",
      "\tEpoch: 265/1000; 1.72 sec... \tStep: 64130... \tLoss: 1.359... \tVal Loss: 1.184 \tVal Frac: 0.0625\n",
      "\tEpoch: 270/1000; 1.81 sec... \tStep: 65340... \tLoss: 1.029... \tVal Loss: 1.148 \tVal Frac: 0.0688\n",
      "\tValidation loss decreased (1.158 --> 1.148).  Saving model ...\n",
      "\tEpoch: 275/1000; 1.71 sec... \tStep: 66550... \tLoss: 1.183... \tVal Loss: 1.178 \tVal Frac: 0.0618\n",
      "\tEpoch: 280/1000; 1.81 sec... \tStep: 67760... \tLoss: 1.045... \tVal Loss: 1.159 \tVal Frac: 0.0863\n",
      "\tEpoch: 285/1000; 1.64 sec... \tStep: 68970... \tLoss: 1.136... \tVal Loss: 1.196 \tVal Frac: 0.0662\n",
      "\tEpoch: 290/1000; 1.71 sec... \tStep: 70180... \tLoss: 0.995... \tVal Loss: 1.167 \tVal Frac: 0.0543\n",
      "\tEpoch: 295/1000; 1.64 sec... \tStep: 71390... \tLoss: 1.166... \tVal Loss: 1.156 \tVal Frac: 0.0949\n",
      "\tEpoch: 300/1000; 1.82 sec... \tStep: 72600... \tLoss: 1.363... \tVal Loss: 1.152 \tVal Frac: 0.0573\n",
      "\tEpoch: 305/1000; 1.64 sec... \tStep: 73810... \tLoss: 1.173... \tVal Loss: 1.163 \tVal Frac: 0.0878\n",
      "\tEpoch: 310/1000; 1.69 sec... \tStep: 75020... \tLoss: 1.216... \tVal Loss: 1.148 \tVal Frac: 0.0670\n",
      "\tValidation loss decreased (1.148 --> 1.148).  Saving model ...\n",
      "\tEpoch: 315/1000; 1.86 sec... \tStep: 76230... \tLoss: 1.183... \tVal Loss: 1.137 \tVal Frac: 0.0685\n",
      "\tValidation loss decreased (1.148 --> 1.137).  Saving model ...\n",
      "\tEpoch: 320/1000; 1.76 sec... \tStep: 77440... \tLoss: 1.513... \tVal Loss: 1.144 \tVal Frac: 0.0707\n",
      "\tEpoch: 325/1000; 1.74 sec... \tStep: 78650... \tLoss: 1.236... \tVal Loss: 1.165 \tVal Frac: 0.0837\n",
      "\tEpoch: 330/1000; 1.86 sec... \tStep: 79860... \tLoss: 1.439... \tVal Loss: 1.172 \tVal Frac: 0.0707\n",
      "\tEpoch: 335/1000; 1.69 sec... \tStep: 81070... \tLoss: 1.382... \tVal Loss: 1.146 \tVal Frac: 0.0975\n",
      "\tEpoch: 340/1000; 1.71 sec... \tStep: 82280... \tLoss: 1.154... \tVal Loss: 1.177 \tVal Frac: 0.0755\n",
      "\tEpoch: 345/1000; 1.67 sec... \tStep: 83490... \tLoss: 1.282... \tVal Loss: 1.159 \tVal Frac: 0.0822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 350/1000; 1.64 sec... \tStep: 84700... \tLoss: 1.467... \tVal Loss: 1.156 \tVal Frac: 0.0729\n",
      "\tEpoch: 355/1000; 1.71 sec... \tStep: 85910... \tLoss: 1.278... \tVal Loss: 1.146 \tVal Frac: 0.0986\n",
      "\tEpoch: 360/1000; 1.62 sec... \tStep: 87120... \tLoss: 1.255... \tVal Loss: 1.148 \tVal Frac: 0.1097\n",
      "\tEpoch: 365/1000; 1.72 sec... \tStep: 88330... \tLoss: 1.116... \tVal Loss: 1.146 \tVal Frac: 0.1109\n",
      "\tEpoch: 370/1000; 1.58 sec... \tStep: 89540... \tLoss: 1.190... \tVal Loss: 1.142 \tVal Frac: 0.1112\n",
      "\tEpoch: 375/1000; 1.87 sec... \tStep: 90750... \tLoss: 1.292... \tVal Loss: 1.139 \tVal Frac: 0.1008\n",
      "\tEpoch: 380/1000; 1.65 sec... \tStep: 91960... \tLoss: 1.125... \tVal Loss: 1.153 \tVal Frac: 0.1008\n",
      "\tEpoch: 385/1000; 1.59 sec... \tStep: 93170... \tLoss: 1.430... \tVal Loss: 1.149 \tVal Frac: 0.0975\n",
      "\tEpoch: 390/1000; 1.56 sec... \tStep: 94380... \tLoss: 1.444... \tVal Loss: 1.189 \tVal Frac: 0.0696\n",
      "\tEpoch: 395/1000; 1.67 sec... \tStep: 95590... \tLoss: 1.333... \tVal Loss: 1.157 \tVal Frac: 0.0547\n",
      "\tEpoch: 400/1000; 1.76 sec... \tStep: 96800... \tLoss: 1.012... \tVal Loss: 1.159 \tVal Frac: 0.0666\n",
      "\tEpoch: 405/1000; 1.61 sec... \tStep: 98010... \tLoss: 1.548... \tVal Loss: 1.134 \tVal Frac: 0.0766\n",
      "\tValidation loss decreased (1.137 --> 1.134).  Saving model ...\n",
      "\tEpoch: 410/1000; 1.74 sec... \tStep: 99220... \tLoss: 1.207... \tVal Loss: 1.156 \tVal Frac: 0.0666\n",
      "\tEpoch: 415/1000; 1.66 sec... \tStep: 100430... \tLoss: 1.309... \tVal Loss: 1.148 \tVal Frac: 0.0848\n",
      "\tEpoch: 420/1000; 1.74 sec... \tStep: 101640... \tLoss: 1.091... \tVal Loss: 1.154 \tVal Frac: 0.0625\n",
      "\tEpoch: 425/1000; 1.74 sec... \tStep: 102850... \tLoss: 1.189... \tVal Loss: 1.166 \tVal Frac: 0.0725\n",
      "\tEpoch: 430/1000; 1.75 sec... \tStep: 104060... \tLoss: 1.092... \tVal Loss: 1.158 \tVal Frac: 0.0629\n",
      "\tEpoch: 435/1000; 1.74 sec... \tStep: 105270... \tLoss: 1.353... \tVal Loss: 1.135 \tVal Frac: 0.0655\n",
      "\tEpoch: 440/1000; 1.62 sec... \tStep: 106480... \tLoss: 1.306... \tVal Loss: 1.141 \tVal Frac: 0.0588\n",
      "\tEpoch: 445/1000; 1.76 sec... \tStep: 107690... \tLoss: 1.242... \tVal Loss: 1.178 \tVal Frac: 0.0658\n",
      "\tEpoch: 450/1000; 1.79 sec... \tStep: 108900... \tLoss: 1.321... \tVal Loss: 1.124 \tVal Frac: 0.0711\n",
      "\tValidation loss decreased (1.134 --> 1.124).  Saving model ...\n",
      "\tEpoch: 455/1000; 1.65 sec... \tStep: 110110... \tLoss: 1.098... \tVal Loss: 1.125 \tVal Frac: 0.0982\n",
      "\tEpoch: 460/1000; 1.86 sec... \tStep: 111320... \tLoss: 1.329... \tVal Loss: 1.138 \tVal Frac: 0.0759\n",
      "\tEpoch: 465/1000; 1.68 sec... \tStep: 112530... \tLoss: 1.212... \tVal Loss: 1.138 \tVal Frac: 0.0651\n",
      "\tEpoch: 470/1000; 1.75 sec... \tStep: 113740... \tLoss: 1.302... \tVal Loss: 1.147 \tVal Frac: 0.0960\n",
      "\tEpoch: 475/1000; 1.77 sec... \tStep: 114950... \tLoss: 1.495... \tVal Loss: 1.132 \tVal Frac: 0.0815\n",
      "\tEpoch: 480/1000; 1.72 sec... \tStep: 116160... \tLoss: 1.437... \tVal Loss: 1.150 \tVal Frac: 0.0763\n",
      "\tEpoch: 485/1000; 1.77 sec... \tStep: 117370... \tLoss: 1.214... \tVal Loss: 1.149 \tVal Frac: 0.0673\n",
      "\tEpoch: 490/1000; 1.82 sec... \tStep: 118580... \tLoss: 1.263... \tVal Loss: 1.152 \tVal Frac: 0.0733\n",
      "\tEpoch: 495/1000; 1.68 sec... \tStep: 119790... \tLoss: 1.133... \tVal Loss: 1.146 \tVal Frac: 0.0748\n",
      "\tEpoch: 500/1000; 1.72 sec... \tStep: 121000... \tLoss: 1.282... \tVal Loss: 1.160 \tVal Frac: 0.0647\n",
      "\tEpoch: 505/1000; 1.77 sec... \tStep: 122210... \tLoss: 1.177... \tVal Loss: 1.147 \tVal Frac: 0.0577\n",
      "\tEpoch: 510/1000; 1.71 sec... \tStep: 123420... \tLoss: 1.100... \tVal Loss: 1.143 \tVal Frac: 0.0759\n",
      "\tEpoch: 515/1000; 1.77 sec... \tStep: 124630... \tLoss: 1.568... \tVal Loss: 1.145 \tVal Frac: 0.0647\n",
      "\tEpoch: 520/1000; 1.72 sec... \tStep: 125840... \tLoss: 1.024... \tVal Loss: 1.142 \tVal Frac: 0.0737\n",
      "\tEpoch: 525/1000; 1.74 sec... \tStep: 127050... \tLoss: 1.308... \tVal Loss: 1.175 \tVal Frac: 0.0573\n",
      "\tEpoch: 530/1000; 1.67 sec... \tStep: 128260... \tLoss: 1.051... \tVal Loss: 1.125 \tVal Frac: 0.1083\n",
      "\tEpoch: 535/1000; 1.67 sec... \tStep: 129470... \tLoss: 1.272... \tVal Loss: 1.147 \tVal Frac: 0.0744\n",
      "\tEpoch: 540/1000; 1.74 sec... \tStep: 130680... \tLoss: 1.412... \tVal Loss: 1.123 \tVal Frac: 0.0792\n",
      "\tValidation loss decreased (1.124 --> 1.123).  Saving model ...\n",
      "\tEpoch: 545/1000; 1.75 sec... \tStep: 131890... \tLoss: 1.392... \tVal Loss: 1.135 \tVal Frac: 0.0595\n",
      "\tEpoch: 550/1000; 1.72 sec... \tStep: 133100... \tLoss: 1.315... \tVal Loss: 1.132 \tVal Frac: 0.0800\n",
      "\tEpoch: 555/1000; 1.75 sec... \tStep: 134310... \tLoss: 1.319... \tVal Loss: 1.125 \tVal Frac: 0.0696\n",
      "\tEpoch: 560/1000; 1.64 sec... \tStep: 135520... \tLoss: 1.088... \tVal Loss: 1.136 \tVal Frac: 0.0703\n",
      "\tEpoch: 565/1000; 1.64 sec... \tStep: 136730... \tLoss: 1.106... \tVal Loss: 1.137 \tVal Frac: 0.0640\n",
      "\tEpoch: 570/1000; 1.82 sec... \tStep: 137940... \tLoss: 1.353... \tVal Loss: 1.123 \tVal Frac: 0.0751\n",
      "\tEpoch: 575/1000; 1.60 sec... \tStep: 139150... \tLoss: 1.093... \tVal Loss: 1.126 \tVal Frac: 0.0610\n",
      "\tEpoch: 580/1000; 1.77 sec... \tStep: 140360... \tLoss: 1.163... \tVal Loss: 1.148 \tVal Frac: 0.0573\n",
      "\tEpoch: 585/1000; 1.69 sec... \tStep: 141570... \tLoss: 1.255... \tVal Loss: 1.137 \tVal Frac: 0.0800\n",
      "\tEpoch: 590/1000; 1.81 sec... \tStep: 142780... \tLoss: 1.035... \tVal Loss: 1.123 \tVal Frac: 0.0766\n",
      "\tEpoch: 595/1000; 1.87 sec... \tStep: 143990... \tLoss: 1.087... \tVal Loss: 1.130 \tVal Frac: 0.0822\n",
      "\tEpoch: 600/1000; 1.65 sec... \tStep: 145200... \tLoss: 1.034... \tVal Loss: 1.115 \tVal Frac: 0.0774\n",
      "\tValidation loss decreased (1.123 --> 1.115).  Saving model ...\n",
      "\tEpoch: 605/1000; 1.66 sec... \tStep: 146410... \tLoss: 1.143... \tVal Loss: 1.112 \tVal Frac: 0.0956\n",
      "\tValidation loss decreased (1.115 --> 1.112).  Saving model ...\n",
      "\tEpoch: 610/1000; 1.65 sec... \tStep: 147620... \tLoss: 1.319... \tVal Loss: 1.147 \tVal Frac: 0.0759\n",
      "\tEpoch: 615/1000; 1.82 sec... \tStep: 148830... \tLoss: 1.210... \tVal Loss: 1.125 \tVal Frac: 0.0606\n",
      "\tEpoch: 620/1000; 1.74 sec... \tStep: 150040... \tLoss: 1.106... \tVal Loss: 1.128 \tVal Frac: 0.0614\n",
      "\tEpoch: 625/1000; 1.83 sec... \tStep: 151250... \tLoss: 1.403... \tVal Loss: 1.136 \tVal Frac: 0.0930\n",
      "\tEpoch: 630/1000; 1.79 sec... \tStep: 152460... \tLoss: 1.062... \tVal Loss: 1.116 \tVal Frac: 0.1034\n",
      "\tEpoch: 635/1000; 1.74 sec... \tStep: 153670... \tLoss: 1.188... \tVal Loss: 1.119 \tVal Frac: 0.0677\n",
      "\tEpoch: 640/1000; 1.61 sec... \tStep: 154880... \tLoss: 0.995... \tVal Loss: 1.147 \tVal Frac: 0.0655\n",
      "\tEpoch: 645/1000; 1.60 sec... \tStep: 156090... \tLoss: 1.281... \tVal Loss: 1.115 \tVal Frac: 0.0926\n",
      "\tEpoch: 650/1000; 1.54 sec... \tStep: 157300... \tLoss: 1.321... \tVal Loss: 1.135 \tVal Frac: 0.0699\n",
      "\tEpoch: 655/1000; 1.64 sec... \tStep: 158510... \tLoss: 1.262... \tVal Loss: 1.121 \tVal Frac: 0.0692\n",
      "\tEpoch: 660/1000; 1.68 sec... \tStep: 159720... \tLoss: 1.249... \tVal Loss: 1.141 \tVal Frac: 0.0785\n",
      "\tEpoch: 665/1000; 1.77 sec... \tStep: 160930... \tLoss: 1.348... \tVal Loss: 1.170 \tVal Frac: 0.0930\n",
      "\tEpoch: 670/1000; 1.71 sec... \tStep: 162140... \tLoss: 1.351... \tVal Loss: 1.103 \tVal Frac: 0.0990\n",
      "\tValidation loss decreased (1.112 --> 1.103).  Saving model ...\n",
      "\tEpoch: 675/1000; 1.74 sec... \tStep: 163350... \tLoss: 1.121... \tVal Loss: 1.107 \tVal Frac: 0.0952\n",
      "\tEpoch: 680/1000; 1.66 sec... \tStep: 164560... \tLoss: 1.292... \tVal Loss: 1.115 \tVal Frac: 0.1142\n",
      "\tEpoch: 685/1000; 1.69 sec... \tStep: 165770... \tLoss: 1.213... \tVal Loss: 1.126 \tVal Frac: 0.0911\n",
      "\tEpoch: 690/1000; 1.65 sec... \tStep: 166980... \tLoss: 1.177... \tVal Loss: 1.113 \tVal Frac: 0.0759\n",
      "\tEpoch: 695/1000; 1.84 sec... \tStep: 168190... \tLoss: 1.313... \tVal Loss: 1.101 \tVal Frac: 0.0729\n",
      "\tValidation loss decreased (1.103 --> 1.101).  Saving model ...\n",
      "\tEpoch: 700/1000; 1.71 sec... \tStep: 169400... \tLoss: 1.130... \tVal Loss: 1.104 \tVal Frac: 0.0770\n",
      "\tEpoch: 705/1000; 1.67 sec... \tStep: 170610... \tLoss: 1.066... \tVal Loss: 1.114 \tVal Frac: 0.1064\n",
      "\tEpoch: 710/1000; 1.74 sec... \tStep: 171820... \tLoss: 1.054... \tVal Loss: 1.144 \tVal Frac: 0.0889\n",
      "\tEpoch: 715/1000; 1.74 sec... \tStep: 173030... \tLoss: 1.196... \tVal Loss: 1.116 \tVal Frac: 0.0822\n",
      "\tEpoch: 720/1000; 1.79 sec... \tStep: 174240... \tLoss: 1.120... \tVal Loss: 1.120 \tVal Frac: 0.1034\n",
      "\tEpoch: 725/1000; 1.74 sec... \tStep: 175450... \tLoss: 1.129... \tVal Loss: 1.139 \tVal Frac: 0.0800\n",
      "\tEpoch: 730/1000; 1.61 sec... \tStep: 176660... \tLoss: 1.475... \tVal Loss: 1.136 \tVal Frac: 0.0774\n",
      "\tEpoch: 735/1000; 1.71 sec... \tStep: 177870... \tLoss: 1.372... \tVal Loss: 1.142 \tVal Frac: 0.0644\n",
      "\tEpoch: 740/1000; 1.65 sec... \tStep: 179080... \tLoss: 1.042... \tVal Loss: 1.141 \tVal Frac: 0.0670\n",
      "\tEpoch: 745/1000; 1.83 sec... \tStep: 180290... \tLoss: 1.128... \tVal Loss: 1.120 \tVal Frac: 0.0592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 750/1000; 1.67 sec... \tStep: 181500... \tLoss: 1.533... \tVal Loss: 1.120 \tVal Frac: 0.0934\n",
      "\tEpoch: 755/1000; 1.82 sec... \tStep: 182710... \tLoss: 1.132... \tVal Loss: 1.119 \tVal Frac: 0.0856\n",
      "\tEpoch: 760/1000; 1.66 sec... \tStep: 183920... \tLoss: 1.239... \tVal Loss: 1.128 \tVal Frac: 0.0923\n",
      "\tEpoch: 765/1000; 1.63 sec... \tStep: 185130... \tLoss: 1.326... \tVal Loss: 1.129 \tVal Frac: 0.0822\n",
      "\tEpoch: 770/1000; 1.75 sec... \tStep: 186340... \tLoss: 1.149... \tVal Loss: 1.134 \tVal Frac: 0.0725\n",
      "\tEpoch: 775/1000; 1.82 sec... \tStep: 187550... \tLoss: 1.223... \tVal Loss: 1.125 \tVal Frac: 0.0804\n",
      "\tEpoch: 780/1000; 1.64 sec... \tStep: 188760... \tLoss: 0.911... \tVal Loss: 1.123 \tVal Frac: 0.0666\n",
      "\tEpoch: 785/1000; 1.84 sec... \tStep: 189970... \tLoss: 1.231... \tVal Loss: 1.108 \tVal Frac: 0.0722\n",
      "\tEpoch: 790/1000; 1.67 sec... \tStep: 191180... \tLoss: 1.170... \tVal Loss: 1.105 \tVal Frac: 0.0744\n",
      "\tEpoch: 795/1000; 1.69 sec... \tStep: 192390... \tLoss: 1.193... \tVal Loss: 1.148 \tVal Frac: 0.0744\n",
      "\tEpoch: 800/1000; 1.77 sec... \tStep: 193600... \tLoss: 1.092... \tVal Loss: 1.100 \tVal Frac: 0.0900\n",
      "\tValidation loss decreased (1.101 --> 1.100).  Saving model ...\n",
      "\tEpoch: 805/1000; 1.64 sec... \tStep: 194810... \tLoss: 1.275... \tVal Loss: 1.119 \tVal Frac: 0.0673\n",
      "\tEpoch: 810/1000; 1.76 sec... \tStep: 196020... \tLoss: 1.134... \tVal Loss: 1.120 \tVal Frac: 0.0941\n",
      "\tEpoch: 815/1000; 1.72 sec... \tStep: 197230... \tLoss: 1.180... \tVal Loss: 1.116 \tVal Frac: 0.1138\n",
      "\tEpoch: 820/1000; 1.74 sec... \tStep: 198440... \tLoss: 0.925... \tVal Loss: 1.140 \tVal Frac: 0.1190\n",
      "\tEpoch: 825/1000; 1.77 sec... \tStep: 199650... \tLoss: 1.180... \tVal Loss: 1.122 \tVal Frac: 0.0811\n",
      "\tEpoch: 830/1000; 1.64 sec... \tStep: 200860... \tLoss: 1.273... \tVal Loss: 1.107 \tVal Frac: 0.1071\n",
      "\tEpoch: 835/1000; 1.74 sec... \tStep: 202070... \tLoss: 1.173... \tVal Loss: 1.107 \tVal Frac: 0.0964\n",
      "\tEpoch: 840/1000; 1.64 sec... \tStep: 203280... \tLoss: 1.246... \tVal Loss: 1.116 \tVal Frac: 0.0990\n",
      "\tEpoch: 845/1000; 1.66 sec... \tStep: 204490... \tLoss: 1.381... \tVal Loss: 1.096 \tVal Frac: 0.0748\n",
      "\tValidation loss decreased (1.100 --> 1.096).  Saving model ...\n",
      "\tEpoch: 850/1000; 1.81 sec... \tStep: 205700... \tLoss: 1.080... \tVal Loss: 1.152 \tVal Frac: 0.0655\n",
      "\tEpoch: 855/1000; 1.77 sec... \tStep: 206910... \tLoss: 1.341... \tVal Loss: 1.125 \tVal Frac: 0.0770\n",
      "\tEpoch: 860/1000; 1.69 sec... \tStep: 208120... \tLoss: 1.154... \tVal Loss: 1.099 \tVal Frac: 0.0740\n",
      "\tEpoch: 865/1000; 1.86 sec... \tStep: 209330... \tLoss: 1.022... \tVal Loss: 1.116 \tVal Frac: 0.0997\n",
      "\tEpoch: 870/1000; 1.62 sec... \tStep: 210540... \tLoss: 1.043... \tVal Loss: 1.104 \tVal Frac: 0.0844\n",
      "\tEpoch: 875/1000; 1.77 sec... \tStep: 211750... \tLoss: 1.197... \tVal Loss: 1.116 \tVal Frac: 0.0792\n",
      "\tEpoch: 880/1000; 1.79 sec... \tStep: 212960... \tLoss: 1.326... \tVal Loss: 1.136 \tVal Frac: 0.0711\n",
      "\tEpoch: 885/1000; 1.80 sec... \tStep: 214170... \tLoss: 1.411... \tVal Loss: 1.145 \tVal Frac: 0.0763\n",
      "\tEpoch: 890/1000; 1.74 sec... \tStep: 215380... \tLoss: 1.520... \tVal Loss: 1.118 \tVal Frac: 0.0792\n",
      "\tEpoch: 895/1000; 1.77 sec... \tStep: 216590... \tLoss: 1.376... \tVal Loss: 1.095 \tVal Frac: 0.0696\n",
      "\tValidation loss decreased (1.096 --> 1.095).  Saving model ...\n",
      "\tEpoch: 900/1000; 1.78 sec... \tStep: 217800... \tLoss: 1.430... \tVal Loss: 1.117 \tVal Frac: 0.0975\n",
      "\tEpoch: 905/1000; 1.89 sec... \tStep: 219010... \tLoss: 0.999... \tVal Loss: 1.119 \tVal Frac: 0.0833\n",
      "\tEpoch: 910/1000; 1.71 sec... \tStep: 220220... \tLoss: 1.222... \tVal Loss: 1.108 \tVal Frac: 0.1064\n",
      "\tEpoch: 915/1000; 1.60 sec... \tStep: 221430... \tLoss: 0.908... \tVal Loss: 1.109 \tVal Frac: 0.0871\n",
      "\tEpoch: 920/1000; 1.70 sec... \tStep: 222640... \tLoss: 1.580... \tVal Loss: 1.100 \tVal Frac: 0.0826\n",
      "\tEpoch: 925/1000; 1.74 sec... \tStep: 223850... \tLoss: 1.099... \tVal Loss: 1.120 \tVal Frac: 0.0893\n",
      "\tEpoch: 930/1000; 1.61 sec... \tStep: 225060... \tLoss: 1.315... \tVal Loss: 1.108 \tVal Frac: 0.0867\n",
      "\tEpoch: 935/1000; 1.65 sec... \tStep: 226270... \tLoss: 1.212... \tVal Loss: 1.119 \tVal Frac: 0.0785\n",
      "\tEpoch: 940/1000; 1.69 sec... \tStep: 227480... \tLoss: 1.338... \tVal Loss: 1.140 \tVal Frac: 0.1057\n",
      "\tEpoch: 945/1000; 1.71 sec... \tStep: 228690... \tLoss: 1.053... \tVal Loss: 1.121 \tVal Frac: 0.1150\n",
      "\tEpoch: 950/1000; 1.71 sec... \tStep: 229900... \tLoss: 1.241... \tVal Loss: 1.106 \tVal Frac: 0.0744\n",
      "\tEpoch: 955/1000; 1.69 sec... \tStep: 231110... \tLoss: 1.220... \tVal Loss: 1.117 \tVal Frac: 0.0737\n",
      "\tEpoch: 960/1000; 1.66 sec... \tStep: 232320... \tLoss: 1.276... \tVal Loss: 1.100 \tVal Frac: 0.1116\n",
      "\tEpoch: 965/1000; 1.73 sec... \tStep: 233530... \tLoss: 1.265... \tVal Loss: 1.110 \tVal Frac: 0.1068\n",
      "\tEpoch: 970/1000; 1.66 sec... \tStep: 234740... \tLoss: 1.401... \tVal Loss: 1.109 \tVal Frac: 0.0956\n",
      "\tEpoch: 975/1000; 1.54 sec... \tStep: 235950... \tLoss: 1.152... \tVal Loss: 1.108 \tVal Frac: 0.0681\n",
      "\tEpoch: 980/1000; 1.72 sec... \tStep: 237160... \tLoss: 1.114... \tVal Loss: 1.109 \tVal Frac: 0.0688\n",
      "\tEpoch: 985/1000; 1.75 sec... \tStep: 238370... \tLoss: 0.954... \tVal Loss: 1.119 \tVal Frac: 0.0506\n",
      "\tEpoch: 990/1000; 1.63 sec... \tStep: 239580... \tLoss: 1.371... \tVal Loss: 1.115 \tVal Frac: 0.1097\n",
      "\tEpoch: 995/1000; 1.61 sec... \tStep: 240790... \tLoss: 1.342... \tVal Loss: 1.105 \tVal Frac: 0.1157\n",
      "\tEpoch: 1000/1000; 1.67 sec... \tStep: 242000... \tLoss: 1.483... \tVal Loss: 1.136 \tVal Frac: 0.0878\n",
      "Completed:  n_windows :  100 \n",
      "\tloss: 1.094808 \n",
      "\tfrac:0.119048\n",
      "\n",
      "###########\n",
      "Testing with  n_windows :  250\n",
      "\tEpoch: 5/1000; 3.53 sec... \tStep: 1205... \tLoss: 1.237... \tVal Loss: 1.008 \tVal Frac: 0.7433\n",
      "\tValidation loss decreased (inf --> 1.008).  Saving model ...\n",
      "\tEpoch: 10/1000; 3.44 sec... \tStep: 2410... \tLoss: 0.951... \tVal Loss: 0.855 \tVal Frac: 0.7556\n",
      "\tValidation loss decreased (1.008 --> 0.855).  Saving model ...\n",
      "\tEpoch: 15/1000; 3.60 sec... \tStep: 3615... \tLoss: 1.124... \tVal Loss: 0.758 \tVal Frac: 0.7634\n",
      "\tValidation loss decreased (0.855 --> 0.758).  Saving model ...\n",
      "\tEpoch: 20/1000; 3.43 sec... \tStep: 4820... \tLoss: 1.794... \tVal Loss: 0.850 \tVal Frac: 0.7240\n",
      "\tEpoch: 25/1000; 3.47 sec... \tStep: 6025... \tLoss: 1.273... \tVal Loss: 0.733 \tVal Frac: 0.7593\n",
      "\tValidation loss decreased (0.758 --> 0.733).  Saving model ...\n",
      "\tEpoch: 30/1000; 3.41 sec... \tStep: 7230... \tLoss: 0.961... \tVal Loss: 0.728 \tVal Frac: 0.7630\n",
      "\tValidation loss decreased (0.733 --> 0.728).  Saving model ...\n",
      "\tEpoch: 35/1000; 3.40 sec... \tStep: 8435... \tLoss: 0.511... \tVal Loss: 0.746 \tVal Frac: 0.7191\n",
      "\tEpoch: 40/1000; 3.50 sec... \tStep: 9640... \tLoss: 1.034... \tVal Loss: 0.674 \tVal Frac: 0.7641\n",
      "\tValidation loss decreased (0.728 --> 0.674).  Saving model ...\n",
      "\tEpoch: 45/1000; 3.37 sec... \tStep: 10845... \tLoss: 0.928... \tVal Loss: 0.713 \tVal Frac: 0.7690\n",
      "\tEpoch: 50/1000; 3.49 sec... \tStep: 12050... \tLoss: 0.788... \tVal Loss: 0.681 \tVal Frac: 0.7690\n",
      "\tEpoch: 55/1000; 3.43 sec... \tStep: 13255... \tLoss: 0.791... \tVal Loss: 0.709 \tVal Frac: 0.7626\n",
      "\tEpoch: 60/1000; 3.59 sec... \tStep: 14460... \tLoss: 0.938... \tVal Loss: 0.714 \tVal Frac: 0.7656\n",
      "\tEpoch: 65/1000; 3.36 sec... \tStep: 15665... \tLoss: 0.707... \tVal Loss: 0.689 \tVal Frac: 0.7675\n",
      "\tEpoch: 70/1000; 3.43 sec... \tStep: 16870... \tLoss: 0.754... \tVal Loss: 0.695 \tVal Frac: 0.7619\n",
      "\tEpoch: 75/1000; 3.50 sec... \tStep: 18075... \tLoss: 0.731... \tVal Loss: 0.668 \tVal Frac: 0.7742\n",
      "\tValidation loss decreased (0.674 --> 0.668).  Saving model ...\n",
      "\tEpoch: 80/1000; 3.50 sec... \tStep: 19280... \tLoss: 0.884... \tVal Loss: 0.694 \tVal Frac: 0.7872\n",
      "\tEpoch: 85/1000; 3.43 sec... \tStep: 20485... \tLoss: 1.019... \tVal Loss: 0.680 \tVal Frac: 0.7667\n",
      "\tEpoch: 90/1000; 3.41 sec... \tStep: 21690... \tLoss: 0.920... \tVal Loss: 0.696 \tVal Frac: 0.7768\n",
      "\tEpoch: 95/1000; 3.41 sec... \tStep: 22895... \tLoss: 1.015... \tVal Loss: 0.701 \tVal Frac: 0.7556\n",
      "\tEpoch: 100/1000; 3.54 sec... \tStep: 24100... \tLoss: 0.368... \tVal Loss: 0.707 \tVal Frac: 0.7764\n",
      "\tEpoch: 105/1000; 3.41 sec... \tStep: 25305... \tLoss: 0.936... \tVal Loss: 0.723 \tVal Frac: 0.7533\n",
      "\tEpoch: 110/1000; 3.41 sec... \tStep: 26510... \tLoss: 0.566... \tVal Loss: 0.674 \tVal Frac: 0.7824\n",
      "\tEpoch: 115/1000; 3.53 sec... \tStep: 27715... \tLoss: 1.016... \tVal Loss: 0.697 \tVal Frac: 0.7690\n",
      "\tEpoch: 120/1000; 3.35 sec... \tStep: 28920... \tLoss: 0.991... \tVal Loss: 0.694 \tVal Frac: 0.7708\n",
      "\tEpoch: 125/1000; 3.47 sec... \tStep: 30125... \tLoss: 0.945... \tVal Loss: 0.660 \tVal Frac: 0.7768\n",
      "\tValidation loss decreased (0.668 --> 0.660).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 130/1000; 3.44 sec... \tStep: 31330... \tLoss: 0.751... \tVal Loss: 0.715 \tVal Frac: 0.7630\n",
      "\tEpoch: 135/1000; 3.33 sec... \tStep: 32535... \tLoss: 0.852... \tVal Loss: 0.658 \tVal Frac: 0.7909\n",
      "\tValidation loss decreased (0.660 --> 0.658).  Saving model ...\n",
      "\tEpoch: 140/1000; 3.54 sec... \tStep: 33740... \tLoss: 0.952... \tVal Loss: 0.742 \tVal Frac: 0.7597\n",
      "\tEpoch: 145/1000; 3.41 sec... \tStep: 34945... \tLoss: 1.024... \tVal Loss: 0.677 \tVal Frac: 0.7667\n",
      "\tEpoch: 150/1000; 3.39 sec... \tStep: 36150... \tLoss: 0.600... \tVal Loss: 0.750 \tVal Frac: 0.7556\n",
      "\tEpoch: 155/1000; 3.39 sec... \tStep: 37355... \tLoss: 0.658... \tVal Loss: 0.675 \tVal Frac: 0.7775\n",
      "\tEpoch: 160/1000; 3.58 sec... \tStep: 38560... \tLoss: 0.543... \tVal Loss: 0.644 \tVal Frac: 0.7842\n",
      "\tValidation loss decreased (0.658 --> 0.644).  Saving model ...\n",
      "\tEpoch: 165/1000; 3.40 sec... \tStep: 39765... \tLoss: 1.101... \tVal Loss: 0.667 \tVal Frac: 0.7805\n",
      "\tEpoch: 170/1000; 3.41 sec... \tStep: 40970... \tLoss: 0.832... \tVal Loss: 0.669 \tVal Frac: 0.7760\n",
      "\tEpoch: 175/1000; 3.44 sec... \tStep: 42175... \tLoss: 0.989... \tVal Loss: 0.661 \tVal Frac: 0.7920\n",
      "\tEpoch: 180/1000; 3.56 sec... \tStep: 43380... \tLoss: 0.667... \tVal Loss: 0.672 \tVal Frac: 0.7861\n",
      "\tEpoch: 185/1000; 3.48 sec... \tStep: 44585... \tLoss: 0.418... \tVal Loss: 0.664 \tVal Frac: 0.7846\n",
      "\tEpoch: 190/1000; 3.31 sec... \tStep: 45790... \tLoss: 0.952... \tVal Loss: 0.679 \tVal Frac: 0.7671\n",
      "\tEpoch: 195/1000; 3.41 sec... \tStep: 46995... \tLoss: 0.719... \tVal Loss: 0.677 \tVal Frac: 0.7742\n",
      "\tEpoch: 200/1000; 3.42 sec... \tStep: 48200... \tLoss: 0.794... \tVal Loss: 0.663 \tVal Frac: 0.7861\n",
      "\tEpoch: 205/1000; 3.52 sec... \tStep: 49405... \tLoss: 0.777... \tVal Loss: 0.677 \tVal Frac: 0.7753\n",
      "\tEpoch: 210/1000; 3.51 sec... \tStep: 50610... \tLoss: 0.780... \tVal Loss: 0.668 \tVal Frac: 0.7760\n",
      "\tEpoch: 215/1000; 3.68 sec... \tStep: 51815... \tLoss: 0.436... \tVal Loss: 0.672 \tVal Frac: 0.7760\n",
      "\tEpoch: 220/1000; 3.49 sec... \tStep: 53020... \tLoss: 0.786... \tVal Loss: 0.671 \tVal Frac: 0.7816\n",
      "\tEpoch: 225/1000; 3.41 sec... \tStep: 54225... \tLoss: 1.292... \tVal Loss: 0.653 \tVal Frac: 0.7693\n",
      "\tEpoch: 230/1000; 3.47 sec... \tStep: 55430... \tLoss: 0.749... \tVal Loss: 0.678 \tVal Frac: 0.7634\n",
      "\tEpoch: 235/1000; 3.43 sec... \tStep: 56635... \tLoss: 0.435... \tVal Loss: 0.639 \tVal Frac: 0.7887\n",
      "\tValidation loss decreased (0.644 --> 0.639).  Saving model ...\n",
      "\tEpoch: 240/1000; 3.45 sec... \tStep: 57840... \tLoss: 0.952... \tVal Loss: 0.638 \tVal Frac: 0.7954\n",
      "\tValidation loss decreased (0.639 --> 0.638).  Saving model ...\n",
      "\tEpoch: 245/1000; 3.41 sec... \tStep: 59045... \tLoss: 0.694... \tVal Loss: 0.659 \tVal Frac: 0.7879\n",
      "\tEpoch: 250/1000; 3.39 sec... \tStep: 60250... \tLoss: 0.572... \tVal Loss: 0.686 \tVal Frac: 0.7746\n",
      "\tEpoch: 255/1000; 3.39 sec... \tStep: 61455... \tLoss: 0.872... \tVal Loss: 0.661 \tVal Frac: 0.7749\n",
      "\tEpoch: 260/1000; 3.47 sec... \tStep: 62660... \tLoss: 0.810... \tVal Loss: 0.632 \tVal Frac: 0.7879\n",
      "\tValidation loss decreased (0.638 --> 0.632).  Saving model ...\n",
      "\tEpoch: 265/1000; 3.53 sec... \tStep: 63865... \tLoss: 0.561... \tVal Loss: 0.678 \tVal Frac: 0.7656\n",
      "\tEpoch: 270/1000; 3.42 sec... \tStep: 65070... \tLoss: 0.701... \tVal Loss: 0.678 \tVal Frac: 0.7716\n",
      "\tEpoch: 275/1000; 3.41 sec... \tStep: 66275... \tLoss: 0.920... \tVal Loss: 0.700 \tVal Frac: 0.7734\n",
      "\tEpoch: 280/1000; 3.48 sec... \tStep: 67480... \tLoss: 0.703... \tVal Loss: 0.655 \tVal Frac: 0.7887\n",
      "\tEpoch: 285/1000; 3.36 sec... \tStep: 68685... \tLoss: 1.043... \tVal Loss: 0.649 \tVal Frac: 0.7972\n",
      "\tEpoch: 290/1000; 3.42 sec... \tStep: 69890... \tLoss: 0.669... \tVal Loss: 0.652 \tVal Frac: 0.7891\n",
      "\tEpoch: 295/1000; 3.47 sec... \tStep: 71095... \tLoss: 0.566... \tVal Loss: 0.638 \tVal Frac: 0.7984\n",
      "\tEpoch: 300/1000; 3.41 sec... \tStep: 72300... \tLoss: 0.668... \tVal Loss: 0.675 \tVal Frac: 0.7842\n",
      "\tEpoch: 305/1000; 3.51 sec... \tStep: 73505... \tLoss: 0.563... \tVal Loss: 0.672 \tVal Frac: 0.7805\n",
      "\tEpoch: 310/1000; 3.43 sec... \tStep: 74710... \tLoss: 1.102... \tVal Loss: 0.652 \tVal Frac: 0.7861\n",
      "\tEpoch: 315/1000; 3.52 sec... \tStep: 75915... \tLoss: 0.633... \tVal Loss: 0.634 \tVal Frac: 0.7876\n",
      "\tEpoch: 320/1000; 3.47 sec... \tStep: 77120... \tLoss: 0.343... \tVal Loss: 0.635 \tVal Frac: 0.7883\n",
      "\tEpoch: 325/1000; 3.42 sec... \tStep: 78325... \tLoss: 1.163... \tVal Loss: 0.663 \tVal Frac: 0.7861\n",
      "\tEpoch: 330/1000; 3.44 sec... \tStep: 79530... \tLoss: 1.053... \tVal Loss: 0.666 \tVal Frac: 0.7816\n",
      "\tEpoch: 335/1000; 3.59 sec... \tStep: 80735... \tLoss: 1.000... \tVal Loss: 0.662 \tVal Frac: 0.7757\n",
      "\tEpoch: 340/1000; 3.44 sec... \tStep: 81940... \tLoss: 0.656... \tVal Loss: 0.651 \tVal Frac: 0.7809\n",
      "\tEpoch: 345/1000; 3.43 sec... \tStep: 83145... \tLoss: 0.910... \tVal Loss: 0.650 \tVal Frac: 0.7872\n",
      "\tEpoch: 350/1000; 3.42 sec... \tStep: 84350... \tLoss: 0.717... \tVal Loss: 0.654 \tVal Frac: 0.7812\n",
      "\tEpoch: 355/1000; 3.46 sec... \tStep: 85555... \tLoss: 0.830... \tVal Loss: 0.628 \tVal Frac: 0.7946\n",
      "\tValidation loss decreased (0.632 --> 0.628).  Saving model ...\n",
      "\tEpoch: 360/1000; 3.65 sec... \tStep: 86760... \tLoss: 0.998... \tVal Loss: 0.644 \tVal Frac: 0.7820\n",
      "\tEpoch: 365/1000; 3.48 sec... \tStep: 87965... \tLoss: 0.582... \tVal Loss: 0.661 \tVal Frac: 0.7682\n",
      "\tEpoch: 370/1000; 3.36 sec... \tStep: 89170... \tLoss: 1.077... \tVal Loss: 0.655 \tVal Frac: 0.7768\n",
      "\tEpoch: 375/1000; 3.42 sec... \tStep: 90375... \tLoss: 0.860... \tVal Loss: 0.634 \tVal Frac: 0.7801\n",
      "\tEpoch: 380/1000; 3.54 sec... \tStep: 91580... \tLoss: 1.099... \tVal Loss: 0.660 \tVal Frac: 0.7827\n",
      "\tEpoch: 385/1000; 3.41 sec... \tStep: 92785... \tLoss: 0.843... \tVal Loss: 0.653 \tVal Frac: 0.7831\n",
      "\tEpoch: 390/1000; 3.48 sec... \tStep: 93990... \tLoss: 0.356... \tVal Loss: 0.672 \tVal Frac: 0.7772\n",
      "\tEpoch: 395/1000; 3.49 sec... \tStep: 95195... \tLoss: 0.926... \tVal Loss: 0.631 \tVal Frac: 0.7865\n",
      "\tEpoch: 400/1000; 3.51 sec... \tStep: 96400... \tLoss: 0.818... \tVal Loss: 0.662 \tVal Frac: 0.7853\n",
      "\tEpoch: 405/1000; 3.46 sec... \tStep: 97605... \tLoss: 0.679... \tVal Loss: 0.682 \tVal Frac: 0.7738\n",
      "\tEpoch: 410/1000; 3.52 sec... \tStep: 98810... \tLoss: 0.819... \tVal Loss: 0.689 \tVal Frac: 0.7697\n",
      "\tEpoch: 415/1000; 3.64 sec... \tStep: 100015... \tLoss: 0.678... \tVal Loss: 0.655 \tVal Frac: 0.7879\n",
      "\tEpoch: 420/1000; 3.39 sec... \tStep: 101220... \tLoss: 0.694... \tVal Loss: 0.656 \tVal Frac: 0.7835\n",
      "\tEpoch: 425/1000; 3.48 sec... \tStep: 102425... \tLoss: 0.792... \tVal Loss: 0.659 \tVal Frac: 0.7898\n",
      "\tEpoch: 430/1000; 3.46 sec... \tStep: 103630... \tLoss: 0.697... \tVal Loss: 0.688 \tVal Frac: 0.7716\n",
      "\tEpoch: 435/1000; 3.32 sec... \tStep: 104835... \tLoss: 0.721... \tVal Loss: 0.667 \tVal Frac: 0.7805\n",
      "\tEpoch: 440/1000; 3.41 sec... \tStep: 106040... \tLoss: 0.689... \tVal Loss: 0.667 \tVal Frac: 0.7809\n",
      "\tEpoch: 445/1000; 3.56 sec... \tStep: 107245... \tLoss: 1.030... \tVal Loss: 0.649 \tVal Frac: 0.7790\n",
      "\tEpoch: 450/1000; 3.41 sec... \tStep: 108450... \tLoss: 0.791... \tVal Loss: 0.645 \tVal Frac: 0.7879\n",
      "\tEpoch: 455/1000; 3.48 sec... \tStep: 109655... \tLoss: 0.475... \tVal Loss: 0.638 \tVal Frac: 0.7798\n",
      "\tEpoch: 460/1000; 3.54 sec... \tStep: 110860... \tLoss: 0.741... \tVal Loss: 0.650 \tVal Frac: 0.7805\n",
      "\tEpoch: 465/1000; 3.55 sec... \tStep: 112065... \tLoss: 0.913... \tVal Loss: 0.636 \tVal Frac: 0.8028\n",
      "\tEpoch: 470/1000; 3.46 sec... \tStep: 113270... \tLoss: 0.789... \tVal Loss: 0.658 \tVal Frac: 0.7879\n",
      "\tEpoch: 475/1000; 3.48 sec... \tStep: 114475... \tLoss: 0.686... \tVal Loss: 0.621 \tVal Frac: 0.7976\n",
      "\tValidation loss decreased (0.628 --> 0.621).  Saving model ...\n",
      "\tEpoch: 480/1000; 3.46 sec... \tStep: 115680... \tLoss: 0.957... \tVal Loss: 0.650 \tVal Frac: 0.7924\n",
      "\tEpoch: 485/1000; 3.46 sec... \tStep: 116885... \tLoss: 0.930... \tVal Loss: 0.653 \tVal Frac: 0.8010\n",
      "\tEpoch: 490/1000; 3.49 sec... \tStep: 118090... \tLoss: 0.819... \tVal Loss: 0.639 \tVal Frac: 0.7857\n",
      "\tEpoch: 495/1000; 3.34 sec... \tStep: 119295... \tLoss: 0.653... \tVal Loss: 0.647 \tVal Frac: 0.7853\n",
      "\tEpoch: 500/1000; 3.36 sec... \tStep: 120500... \tLoss: 0.605... \tVal Loss: 0.648 \tVal Frac: 0.7876\n",
      "\tEpoch: 505/1000; 3.54 sec... \tStep: 121705... \tLoss: 0.686... \tVal Loss: 0.658 \tVal Frac: 0.7898\n",
      "\tEpoch: 510/1000; 3.49 sec... \tStep: 122910... \tLoss: 0.805... \tVal Loss: 0.665 \tVal Frac: 0.7790\n",
      "\tEpoch: 515/1000; 3.59 sec... \tStep: 124115... \tLoss: 0.490... \tVal Loss: 0.628 \tVal Frac: 0.7924\n",
      "\tEpoch: 520/1000; 3.59 sec... \tStep: 125320... \tLoss: 0.515... \tVal Loss: 0.641 \tVal Frac: 0.7798\n",
      "\tEpoch: 525/1000; 3.46 sec... \tStep: 126525... \tLoss: 0.720... \tVal Loss: 0.633 \tVal Frac: 0.7961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 530/1000; 3.49 sec... \tStep: 127730... \tLoss: 0.714... \tVal Loss: 0.620 \tVal Frac: 0.7928\n",
      "\tValidation loss decreased (0.621 --> 0.620).  Saving model ...\n",
      "\tEpoch: 535/1000; 3.58 sec... \tStep: 128935... \tLoss: 0.652... \tVal Loss: 0.635 \tVal Frac: 0.7917\n",
      "\tEpoch: 540/1000; 3.46 sec... \tStep: 130140... \tLoss: 0.850... \tVal Loss: 0.651 \tVal Frac: 0.7861\n",
      "\tEpoch: 545/1000; 3.45 sec... \tStep: 131345... \tLoss: 0.784... \tVal Loss: 0.632 \tVal Frac: 0.7928\n",
      "\tEpoch: 550/1000; 3.51 sec... \tStep: 132550... \tLoss: 0.850... \tVal Loss: 0.630 \tVal Frac: 0.7827\n",
      "\tEpoch: 555/1000; 3.42 sec... \tStep: 133755... \tLoss: 1.168... \tVal Loss: 0.610 \tVal Frac: 0.8036\n",
      "\tValidation loss decreased (0.620 --> 0.610).  Saving model ...\n",
      "\tEpoch: 560/1000; 3.56 sec... \tStep: 134960... \tLoss: 0.785... \tVal Loss: 0.646 \tVal Frac: 0.7868\n",
      "\tEpoch: 565/1000; 3.55 sec... \tStep: 136165... \tLoss: 0.949... \tVal Loss: 0.640 \tVal Frac: 0.7943\n",
      "\tEpoch: 570/1000; 3.52 sec... \tStep: 137370... \tLoss: 0.850... \tVal Loss: 0.669 \tVal Frac: 0.7798\n",
      "\tEpoch: 575/1000; 3.56 sec... \tStep: 138575... \tLoss: 0.587... \tVal Loss: 0.641 \tVal Frac: 0.7943\n",
      "\tEpoch: 580/1000; 3.49 sec... \tStep: 139780... \tLoss: 0.459... \tVal Loss: 0.675 \tVal Frac: 0.7779\n",
      "\tEpoch: 585/1000; 3.41 sec... \tStep: 140985... \tLoss: 0.880... \tVal Loss: 0.659 \tVal Frac: 0.7760\n",
      "\tEpoch: 590/1000; 3.43 sec... \tStep: 142190... \tLoss: 0.930... \tVal Loss: 0.647 \tVal Frac: 0.7764\n",
      "\tEpoch: 595/1000; 3.54 sec... \tStep: 143395... \tLoss: 0.452... \tVal Loss: 0.639 \tVal Frac: 0.7935\n",
      "\tEpoch: 600/1000; 3.42 sec... \tStep: 144600... \tLoss: 0.719... \tVal Loss: 0.640 \tVal Frac: 0.7913\n",
      "\tEpoch: 605/1000; 3.48 sec... \tStep: 145805... \tLoss: 0.743... \tVal Loss: 0.647 \tVal Frac: 0.7958\n",
      "\tEpoch: 610/1000; 3.42 sec... \tStep: 147010... \tLoss: 0.844... \tVal Loss: 0.650 \tVal Frac: 0.7827\n",
      "\tEpoch: 615/1000; 3.56 sec... \tStep: 148215... \tLoss: 0.625... \tVal Loss: 0.652 \tVal Frac: 0.7842\n",
      "\tEpoch: 620/1000; 3.44 sec... \tStep: 149420... \tLoss: 0.566... \tVal Loss: 0.626 \tVal Frac: 0.7987\n",
      "\tEpoch: 625/1000; 3.48 sec... \tStep: 150625... \tLoss: 0.480... \tVal Loss: 0.648 \tVal Frac: 0.7991\n",
      "\tEpoch: 630/1000; 3.46 sec... \tStep: 151830... \tLoss: 0.668... \tVal Loss: 0.637 \tVal Frac: 0.7898\n",
      "\tEpoch: 635/1000; 3.45 sec... \tStep: 153035... \tLoss: 0.951... \tVal Loss: 0.637 \tVal Frac: 0.7928\n",
      "\tEpoch: 640/1000; 3.37 sec... \tStep: 154240... \tLoss: 0.612... \tVal Loss: 0.643 \tVal Frac: 0.7872\n",
      "\tEpoch: 645/1000; 3.35 sec... \tStep: 155445... \tLoss: 0.913... \tVal Loss: 0.640 \tVal Frac: 0.7894\n",
      "\tEpoch: 650/1000; 3.47 sec... \tStep: 156650... \tLoss: 0.911... \tVal Loss: 0.654 \tVal Frac: 0.7820\n",
      "\tEpoch: 655/1000; 3.41 sec... \tStep: 157855... \tLoss: 1.023... \tVal Loss: 0.632 \tVal Frac: 0.7839\n",
      "\tEpoch: 660/1000; 3.48 sec... \tStep: 159060... \tLoss: 0.632... \tVal Loss: 0.660 \tVal Frac: 0.7887\n",
      "\tEpoch: 665/1000; 3.42 sec... \tStep: 160265... \tLoss: 0.521... \tVal Loss: 0.648 \tVal Frac: 0.7846\n",
      "\tEpoch: 670/1000; 3.46 sec... \tStep: 161470... \tLoss: 0.437... \tVal Loss: 0.631 \tVal Frac: 0.7991\n",
      "\tEpoch: 675/1000; 3.44 sec... \tStep: 162675... \tLoss: 0.922... \tVal Loss: 0.652 \tVal Frac: 0.7872\n",
      "\tEpoch: 680/1000; 3.58 sec... \tStep: 163880... \tLoss: 0.805... \tVal Loss: 0.646 \tVal Frac: 0.7861\n",
      "\tEpoch: 685/1000; 3.44 sec... \tStep: 165085... \tLoss: 0.894... \tVal Loss: 0.642 \tVal Frac: 0.7883\n",
      "\tEpoch: 690/1000; 3.49 sec... \tStep: 166290... \tLoss: 0.821... \tVal Loss: 0.642 \tVal Frac: 0.7768\n",
      "\tEpoch: 695/1000; 3.46 sec... \tStep: 167495... \tLoss: 0.652... \tVal Loss: 0.652 \tVal Frac: 0.7842\n",
      "\tEpoch: 700/1000; 3.51 sec... \tStep: 168700... \tLoss: 0.435... \tVal Loss: 0.650 \tVal Frac: 0.7902\n",
      "\tEpoch: 705/1000; 3.48 sec... \tStep: 169905... \tLoss: 0.499... \tVal Loss: 0.633 \tVal Frac: 0.7909\n",
      "\tEpoch: 710/1000; 3.46 sec... \tStep: 171110... \tLoss: 0.580... \tVal Loss: 0.639 \tVal Frac: 0.7961\n",
      "\tEpoch: 715/1000; 3.56 sec... \tStep: 172315... \tLoss: 0.517... \tVal Loss: 0.650 \tVal Frac: 0.7935\n",
      "\tEpoch: 720/1000; 3.52 sec... \tStep: 173520... \tLoss: 0.590... \tVal Loss: 0.640 \tVal Frac: 0.7932\n",
      "\tEpoch: 725/1000; 3.48 sec... \tStep: 174725... \tLoss: 0.820... \tVal Loss: 0.644 \tVal Frac: 0.7861\n",
      "\tEpoch: 730/1000; 3.40 sec... \tStep: 175930... \tLoss: 0.977... \tVal Loss: 0.643 \tVal Frac: 0.7972\n",
      "\tEpoch: 735/1000; 3.43 sec... \tStep: 177135... \tLoss: 0.809... \tVal Loss: 0.680 \tVal Frac: 0.7920\n",
      "\tEpoch: 740/1000; 3.37 sec... \tStep: 178340... \tLoss: 0.819... \tVal Loss: 0.645 \tVal Frac: 0.8032\n",
      "\tEpoch: 745/1000; 3.41 sec... \tStep: 179545... \tLoss: 0.580... \tVal Loss: 0.640 \tVal Frac: 0.7943\n",
      "\tEpoch: 750/1000; 3.52 sec... \tStep: 180750... \tLoss: 0.643... \tVal Loss: 0.633 \tVal Frac: 0.8025\n",
      "\tEpoch: 755/1000; 3.54 sec... \tStep: 181955... \tLoss: 0.518... \tVal Loss: 0.624 \tVal Frac: 0.8043\n",
      "\tEpoch: 760/1000; 3.44 sec... \tStep: 183160... \tLoss: 0.739... \tVal Loss: 0.634 \tVal Frac: 0.7928\n",
      "\tEpoch: 765/1000; 3.41 sec... \tStep: 184365... \tLoss: 0.704... \tVal Loss: 0.634 \tVal Frac: 0.7876\n",
      "\tEpoch: 770/1000; 3.38 sec... \tStep: 185570... \tLoss: 0.554... \tVal Loss: 0.626 \tVal Frac: 0.7887\n",
      "\tEpoch: 775/1000; 3.46 sec... \tStep: 186775... \tLoss: 0.447... \tVal Loss: 0.634 \tVal Frac: 0.7913\n",
      "\tEpoch: 780/1000; 3.38 sec... \tStep: 187980... \tLoss: 0.612... \tVal Loss: 0.649 \tVal Frac: 0.7853\n",
      "\tEpoch: 785/1000; 3.44 sec... \tStep: 189185... \tLoss: 0.451... \tVal Loss: 0.632 \tVal Frac: 0.7872\n",
      "\tEpoch: 790/1000; 3.49 sec... \tStep: 190390... \tLoss: 0.609... \tVal Loss: 0.622 \tVal Frac: 0.7835\n",
      "\tEpoch: 795/1000; 3.49 sec... \tStep: 191595... \tLoss: 0.661... \tVal Loss: 0.665 \tVal Frac: 0.7656\n",
      "\tEpoch: 800/1000; 3.52 sec... \tStep: 192800... \tLoss: 0.413... \tVal Loss: 0.633 \tVal Frac: 0.7920\n",
      "\tEpoch: 805/1000; 3.42 sec... \tStep: 194005... \tLoss: 0.511... \tVal Loss: 0.636 \tVal Frac: 0.7790\n",
      "\tEpoch: 810/1000; 3.47 sec... \tStep: 195210... \tLoss: 0.259... \tVal Loss: 0.613 \tVal Frac: 0.8006\n",
      "\tEpoch: 815/1000; 3.62 sec... \tStep: 196415... \tLoss: 1.073... \tVal Loss: 0.631 \tVal Frac: 0.7887\n",
      "\tEpoch: 820/1000; 3.45 sec... \tStep: 197620... \tLoss: 0.620... \tVal Loss: 0.633 \tVal Frac: 0.7902\n",
      "\tEpoch: 825/1000; 3.39 sec... \tStep: 198825... \tLoss: 0.801... \tVal Loss: 0.642 \tVal Frac: 0.7906\n",
      "\tEpoch: 830/1000; 3.52 sec... \tStep: 200030... \tLoss: 0.485... \tVal Loss: 0.624 \tVal Frac: 0.7906\n",
      "\tEpoch: 835/1000; 3.49 sec... \tStep: 201235... \tLoss: 0.555... \tVal Loss: 0.627 \tVal Frac: 0.7894\n",
      "\tEpoch: 840/1000; 3.47 sec... \tStep: 202440... \tLoss: 1.131... \tVal Loss: 0.628 \tVal Frac: 0.7909\n",
      "\tEpoch: 845/1000; 3.46 sec... \tStep: 203645... \tLoss: 0.868... \tVal Loss: 0.621 \tVal Frac: 0.7909\n",
      "\tEpoch: 850/1000; 3.46 sec... \tStep: 204850... \tLoss: 0.810... \tVal Loss: 0.637 \tVal Frac: 0.7775\n",
      "\tEpoch: 855/1000; 3.49 sec... \tStep: 206055... \tLoss: 0.510... \tVal Loss: 0.640 \tVal Frac: 0.7835\n",
      "\tEpoch: 860/1000; 3.48 sec... \tStep: 207260... \tLoss: 0.718... \tVal Loss: 0.663 \tVal Frac: 0.7876\n",
      "\tEpoch: 865/1000; 3.52 sec... \tStep: 208465... \tLoss: 0.694... \tVal Loss: 0.645 \tVal Frac: 0.7786\n",
      "\tEpoch: 870/1000; 3.42 sec... \tStep: 209670... \tLoss: 0.761... \tVal Loss: 0.666 \tVal Frac: 0.7742\n",
      "\tEpoch: 875/1000; 3.52 sec... \tStep: 210875... \tLoss: 0.632... \tVal Loss: 0.670 \tVal Frac: 0.7850\n",
      "\tEpoch: 880/1000; 3.49 sec... \tStep: 212080... \tLoss: 0.408... \tVal Loss: 0.637 \tVal Frac: 0.7853\n",
      "\tEpoch: 885/1000; 3.63 sec... \tStep: 213285... \tLoss: 0.596... \tVal Loss: 0.629 \tVal Frac: 0.7972\n",
      "\tEpoch: 890/1000; 3.36 sec... \tStep: 214490... \tLoss: 1.157... \tVal Loss: 0.635 \tVal Frac: 0.7939\n",
      "\tEpoch: 895/1000; 3.44 sec... \tStep: 215695... \tLoss: 0.980... \tVal Loss: 0.692 \tVal Frac: 0.7623\n",
      "\tEpoch: 900/1000; 3.46 sec... \tStep: 216900... \tLoss: 0.696... \tVal Loss: 0.640 \tVal Frac: 0.7850\n",
      "\tEpoch: 905/1000; 3.41 sec... \tStep: 218105... \tLoss: 0.502... \tVal Loss: 0.643 \tVal Frac: 0.7987\n",
      "\tEpoch: 910/1000; 3.57 sec... \tStep: 219310... \tLoss: 0.709... \tVal Loss: 0.666 \tVal Frac: 0.7812\n",
      "\tEpoch: 915/1000; 3.56 sec... \tStep: 220515... \tLoss: 0.822... \tVal Loss: 0.660 \tVal Frac: 0.7842\n",
      "\tEpoch: 920/1000; 3.41 sec... \tStep: 221720... \tLoss: 0.909... \tVal Loss: 0.649 \tVal Frac: 0.7835\n",
      "\tEpoch: 925/1000; 3.39 sec... \tStep: 222925... \tLoss: 0.944... \tVal Loss: 0.617 \tVal Frac: 0.7972\n",
      "\tEpoch: 930/1000; 3.48 sec... \tStep: 224130... \tLoss: 0.787... \tVal Loss: 0.635 \tVal Frac: 0.7917\n",
      "\tEpoch: 935/1000; 3.46 sec... \tStep: 225335... \tLoss: 0.678... \tVal Loss: 0.643 \tVal Frac: 0.7898\n",
      "\tEpoch: 940/1000; 3.39 sec... \tStep: 226540... \tLoss: 0.660... \tVal Loss: 0.636 \tVal Frac: 0.7850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 945/1000; 3.48 sec... \tStep: 227745... \tLoss: 0.893... \tVal Loss: 0.644 \tVal Frac: 0.7790\n",
      "\tEpoch: 950/1000; 3.51 sec... \tStep: 228950... \tLoss: 0.465... \tVal Loss: 0.636 \tVal Frac: 0.7839\n",
      "\tEpoch: 955/1000; 3.43 sec... \tStep: 230155... \tLoss: 0.552... \tVal Loss: 0.636 \tVal Frac: 0.7868\n",
      "\tEpoch: 960/1000; 3.51 sec... \tStep: 231360... \tLoss: 0.961... \tVal Loss: 0.624 \tVal Frac: 0.7965\n",
      "\tEpoch: 965/1000; 3.45 sec... \tStep: 232565... \tLoss: 0.665... \tVal Loss: 0.626 \tVal Frac: 0.7961\n",
      "\tEpoch: 970/1000; 3.35 sec... \tStep: 233770... \tLoss: 0.685... \tVal Loss: 0.649 \tVal Frac: 0.7842\n",
      "\tEpoch: 975/1000; 3.50 sec... \tStep: 234975... \tLoss: 0.466... \tVal Loss: 0.652 \tVal Frac: 0.7865\n",
      "\tEpoch: 980/1000; 3.41 sec... \tStep: 236180... \tLoss: 0.728... \tVal Loss: 0.609 \tVal Frac: 0.8058\n",
      "\tValidation loss decreased (0.610 --> 0.609).  Saving model ...\n",
      "\tEpoch: 985/1000; 3.49 sec... \tStep: 237385... \tLoss: 0.806... \tVal Loss: 0.651 \tVal Frac: 0.7824\n",
      "\tEpoch: 990/1000; 3.41 sec... \tStep: 238590... \tLoss: 0.791... \tVal Loss: 0.646 \tVal Frac: 0.7839\n",
      "\tEpoch: 995/1000; 3.41 sec... \tStep: 239795... \tLoss: 1.003... \tVal Loss: 0.651 \tVal Frac: 0.7961\n",
      "\tEpoch: 1000/1000; 3.43 sec... \tStep: 241000... \tLoss: 0.525... \tVal Loss: 0.614 \tVal Frac: 0.7924\n",
      "Completed:  n_windows :  250 \n",
      "\tloss: 0.609275 \n",
      "\tfrac:0.805804\n"
     ]
    }
   ],
   "source": [
    "for key in sweeps.keys():\n",
    "    for param_val in sweeps[key]:\n",
    "        n.random.seed(2358)\n",
    "        torch.manual_seed(2358)\n",
    "        params = copy.copy(params_original)\n",
    "        params[key] = param_val\n",
    "        \n",
    "        print(\"\\n###########\\nTesting with \", key, \": \", param_val)\n",
    "        \n",
    "        params_result, model = test_model(params, datas, device, int(n.random.random()*1e9))\n",
    "        \n",
    "        results.append((copy.deepcopy(params_result), copy.deepcopy(model)))\n",
    "        \n",
    "        print(\"Completed: \", key, \": \", param_val, \"\\n\\tloss: {:.6f} \\n\\tfrac:{:.6f}\".format(params_result['valid_loss_min'], params_result['valid_frac_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
