{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy.lib.recfunctions as rfn\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, splits= (0.7, 0.2, 0.1), separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = data_len // chunk_size\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = splits[0]*num_chunks\n",
    "    val_chunk_idx_bound = (splits[0]+splits[1])*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    features_val = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    targets_val = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        elif i < val_chunk_idx_bound:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('val')\n",
    "            features_val.append(curr_chunk)\n",
    "            targets_val.append(curr_chunk_t)\n",
    "\n",
    "#     print(len(features_separate))\n",
    "#     print(len(targets_separate))\n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    features_val = n.concatenate(features_val, axis=0)\n",
    "    \n",
    "    targets_val = n.concatenate(targets_val)\n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, features_val, targets_train, targets_test, targets_val\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_val, features_separate,\\\n",
    "                targets_train, targets_test, targets_val, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, architecture):\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.D_in = self.architecture['D_in']\n",
    "        self.D_out = self.architecture['D_out']\n",
    "        self.hidden_dims = self.architecture['hidden_dims']\n",
    "        \n",
    "        self.layers = []\n",
    "        prev_dim = self.architecture['D_in']\n",
    "        for dim in self.architecture['hidden_dims']:\n",
    "            self.layers += [torch.nn.Linear(prev_dim, dim),\n",
    "                           torch.nn.ReLU()]\n",
    "            prev_dim = dim\n",
    "        self.layers += [torch.nn.Linear(prev_dim, self.D_out)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.trackers = {}\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.model.apply(init_weights)\n",
    "   \n",
    "    def start_trackers(self,track, reset_trackers=True):\n",
    "        if reset_trackers: self.trackers = {}\n",
    "        for t in track:\n",
    "            self.trackers[t] = []\n",
    "    \n",
    "    def track(self):\n",
    "        for variable in self.trackers.keys():\n",
    "            self.trackers[variable].append(copy.deepcopy(getattr(self, variable)))\n",
    "        \n",
    "    def learn(self,learning, training_set, test_set, reset_trackers=True, verbose=True):\n",
    "        \n",
    "        #self.init_weights()\n",
    "        \n",
    "        # set up variables\n",
    "        N_batch = learning['N_batch']\n",
    "        N_epochs = learning['N_epochs']\n",
    "        loss_fn = learning['loss_fn']\n",
    "        print_interval = learning['print_interval']\n",
    "        track = learning['track']\n",
    "        learning_rate = learning['learning_rate']\n",
    "        \n",
    "        self.learning = learning\n",
    "        \n",
    "        optimizer = learning['optimizer'](self.model.parameters(), learning_rate)\n",
    "        self.start_trackers(track, reset_trackers)\n",
    "        \n",
    "        # load data\n",
    "        x_train, y_train = training_set\n",
    "        x_test, y_test = test_set\n",
    "\n",
    "        N_training = len(y_train)\n",
    "        N_test = len(y_test)\n",
    "    \n",
    "        self.t = 0\n",
    "        tic = time.time()\n",
    "        end = False\n",
    "        for self.epoch_idx in range(N_epochs):\n",
    "            if verbose: print(\"### EPOCH {:2d} ###\".format(self.epoch_idx))\n",
    "                \n",
    "            # randomize batches\n",
    "            indices = n.random.choice(range(N_training), N_training, False)\n",
    "            num_batches = len(indices) // N_batch + 1\n",
    "\n",
    "            for self.batch_idx in range(num_batches):\n",
    "                # load batch\n",
    "                b_idx = self.batch_idx\n",
    "                x_train_batch = x_train[indices[b_idx*N_batch :(b_idx+1)*N_batch]]\n",
    "                y_train_batch = y_train[indices[b_idx*N_batch : (b_idx+1)*N_batch]]\n",
    "\n",
    "                \n",
    "                # predict, loss and learn\n",
    "                y_train_batch_pred = self.model(x_train_batch)\n",
    "                loss = loss_fn(y_train_batch_pred, y_train_batch)\n",
    "                self.train_loss = loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.t += 1\n",
    "\n",
    "                if self.batch_idx % print_interval == 0:\n",
    "                    \n",
    "#                     y_train_pred = self.model(x_train)\n",
    "#                     self.train_loss = loss_fn(y_train_pred, y_train).item()\n",
    "#                     pred_labels = n.argmax(y_train_pred.detach().numpy(),axis=1)\n",
    "#                     true_labels = y_train.detach().numpy()\n",
    "#                     correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.train_frac_correct = 0# n.mean(correct_preds)\n",
    "                    \n",
    "                    y_test_pred = self.model(x_test)\n",
    "                    self.test_loss = loss_fn(y_test_pred, y_test).item()\n",
    "                    pred_labels = n.argmax(y_test_pred.detach().numpy(),axis=1)\n",
    "                    true_labels = y_test.detach().numpy()\n",
    "                    correct_preds = n.array(pred_labels == true_labels, n.int)\n",
    "                    self.test_frac_correct = n.mean(correct_preds)\n",
    "\n",
    "                    toc = time.time()\n",
    "                    delta = toc - tic\n",
    "                    tic = toc\n",
    "                    print(\"Time: {:4.2f}, Batch {:3d}, Train Loss (for batch): {:4.2f}, Test Loss: {:4.2f}, Test Correct Frac: {:.3f}\".format(\\\n",
    "                                       delta, self.batch_idx, self.train_loss, self.test_loss, self.test_frac_correct))\n",
    "            \n",
    "                    self.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_net_results(net, x_test, y_test):\n",
    "    plt.plot(net.trackers['t'],net.trackers['train_loss'], label='train')\n",
    "    plt.plot(net.trackers['t'],net.trackers['test_loss'], label='test')\n",
    "    plt.title(\"Cross Entropy Loss through training\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,10)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(net.trackers['t'], net.trackers['test_frac_correct'])\n",
    "    plt.title(\"Fraction of correct labels on test set through training\")\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.show()\n",
    "    \n",
    "    max_perf_ind = n.argmax(net.trackers['test_frac_correct'])\n",
    "    min_loss_ind = n.argmin(net.trackers['test_loss'])\n",
    "    max_perf_model = net.trackers['model'][max_perf_ind]\n",
    "    \n",
    "    prediction_test = max_perf_model(x_test)\n",
    "    pred = n.argmax(prediction_test.detach().numpy(),axis=1)\n",
    "    true = y_test.detach().numpy()\n",
    "    confmat = confusion_matrix(true, pred, normalize='true')\n",
    "    f, ax = plt.subplots(figsize=(10,10))\n",
    "    m = ax.matshow(confmat, cmap='Blues', vmin=0,  vmax=1)\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "    ax.set_xticks(list(range(len(categories))))\n",
    "    ax.set_xticklabels(categories, rotation=45)\n",
    "    ax.set_yticks(list(range(len(categories))))\n",
    "    ax.set_yticklabels(categories, rotation=45)\n",
    "    f.colorbar(m)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = '/mnt/d/Ali/Google Drive/SWC/Experimental Neuroscience/social and affective systems/rat_social_beh'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n",
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = list(all_data.keys())\n",
    "datas = []\n",
    "for key in all_data.keys():\n",
    "    all_data[key] = calculate_velocity(all_data[key])\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 0\n",
    "bounds = []\n",
    "for key in all_data:\n",
    "    bounds.append((x, x+all_data[key].shape[0]))\n",
    "    x += all_data[key].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 30270),\n",
       " (30270, 56659),\n",
       " (56659, 92060),\n",
       " (92060, 116675),\n",
       " (116675, 146661),\n",
       " (146661, 178109),\n",
       " (178109, 210617)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_params = [(bounds[i], (0.98,0.01,0.01)) for i in range(0,len(bounds),2)]\n",
    "\n",
    "n_layerss = [3, 5, 8]\n",
    "layer_widths = [100, 300, 500, 1000]\n",
    "\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "N_batchs = [4,16,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft_tr, ft_ts, ft_vl,ft_sp, tg_tr, tg_ts, tg_vl, tg_sp = split_sets(features_all, targets_int, splits=(0.95, 0.02, 0.03),separate_vid_idx=bounds[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Net with  3 layers, 100 wide. Using learning rate of 0.000010 and  4 batches\n",
      "### EPOCH  0 ###\n",
      "Time: 0.06, Batch   0, Train Loss (for batch): 292.78, Test Loss: 298.90, Test Correct Frac: 0.015\n",
      "Time: 0.36, Batch 200, Train Loss (for batch): 0.00, Test Loss: 3.85, Test Correct Frac: 0.604\n",
      "Time: 0.36, Batch 400, Train Loss (for batch): 0.21, Test Loss: 3.44, Test Correct Frac: 0.610\n",
      "Time: 0.37, Batch 600, Train Loss (for batch): 2.64, Test Loss: 3.11, Test Correct Frac: 0.617\n",
      "Time: 0.37, Batch 800, Train Loss (for batch): 1.54, Test Loss: 2.86, Test Correct Frac: 0.622\n",
      "Time: 0.36, Batch 1000, Train Loss (for batch): 3.62, Test Loss: 2.64, Test Correct Frac: 0.625\n",
      "Time: 0.36, Batch 1200, Train Loss (for batch): 1.76, Test Loss: 2.48, Test Correct Frac: 0.628\n",
      "Time: 0.36, Batch 1400, Train Loss (for batch): 0.00, Test Loss: 2.27, Test Correct Frac: 0.635\n",
      "Time: 0.35, Batch 1600, Train Loss (for batch): 0.75, Test Loss: 2.10, Test Correct Frac: 0.645\n",
      "Time: 0.35, Batch 1800, Train Loss (for batch): 0.00, Test Loss: 1.82, Test Correct Frac: 0.653\n",
      "Time: 0.37, Batch 2000, Train Loss (for batch): 0.00, Test Loss: 1.69, Test Correct Frac: 0.657\n",
      "Time: 0.36, Batch 2200, Train Loss (for batch): 4.86, Test Loss: 1.57, Test Correct Frac: 0.660\n",
      "Time: 0.36, Batch 2400, Train Loss (for batch): 0.00, Test Loss: 1.50, Test Correct Frac: 0.666\n",
      "Time: 0.35, Batch 2600, Train Loss (for batch): 1.07, Test Loss: 1.43, Test Correct Frac: 0.672\n",
      "Time: 0.37, Batch 2800, Train Loss (for batch): 0.06, Test Loss: 1.41, Test Correct Frac: 0.673\n",
      "Time: 0.36, Batch 3000, Train Loss (for batch): 0.01, Test Loss: 1.35, Test Correct Frac: 0.675\n",
      "Time: 0.36, Batch 3200, Train Loss (for batch): 0.62, Test Loss: 1.31, Test Correct Frac: 0.677\n",
      "Time: 0.35, Batch 3400, Train Loss (for batch): 0.00, Test Loss: 1.24, Test Correct Frac: 0.689\n",
      "Time: 0.36, Batch 3600, Train Loss (for batch): 0.28, Test Loss: 1.20, Test Correct Frac: 0.693\n",
      "Time: 0.36, Batch 3800, Train Loss (for batch): 0.00, Test Loss: 1.18, Test Correct Frac: 0.692\n",
      "Time: 0.36, Batch 4000, Train Loss (for batch): 1.71, Test Loss: 1.11, Test Correct Frac: 0.705\n",
      "Time: 0.36, Batch 4200, Train Loss (for batch): 0.00, Test Loss: 1.10, Test Correct Frac: 0.708\n",
      "Time: 0.35, Batch 4400, Train Loss (for batch): 1.04, Test Loss: 1.08, Test Correct Frac: 0.709\n",
      "Time: 0.35, Batch 4600, Train Loss (for batch): 0.47, Test Loss: 1.06, Test Correct Frac: 0.710\n",
      "Time: 0.37, Batch 4800, Train Loss (for batch): 0.00, Test Loss: 1.05, Test Correct Frac: 0.713\n",
      "Time: 0.36, Batch 5000, Train Loss (for batch): 0.43, Test Loss: 1.05, Test Correct Frac: 0.710\n",
      "Time: 0.36, Batch 5200, Train Loss (for batch): 0.47, Test Loss: 0.99, Test Correct Frac: 0.718\n",
      "Time: 0.36, Batch 5400, Train Loss (for batch): 0.00, Test Loss: 1.04, Test Correct Frac: 0.717\n",
      "Time: 0.36, Batch 5600, Train Loss (for batch): 0.01, Test Loss: 0.96, Test Correct Frac: 0.721\n",
      "Time: 0.36, Batch 5800, Train Loss (for batch): 1.31, Test Loss: 0.94, Test Correct Frac: 0.724\n",
      "Time: 0.37, Batch 6000, Train Loss (for batch): 0.00, Test Loss: 0.96, Test Correct Frac: 0.723\n",
      "Time: 0.37, Batch 6200, Train Loss (for batch): 0.86, Test Loss: 0.91, Test Correct Frac: 0.728\n",
      "Time: 0.38, Batch 6400, Train Loss (for batch): 0.00, Test Loss: 0.88, Test Correct Frac: 0.731\n",
      "Time: 0.37, Batch 6600, Train Loss (for batch): 0.00, Test Loss: 0.94, Test Correct Frac: 0.731\n",
      "Time: 0.36, Batch 6800, Train Loss (for batch): 0.00, Test Loss: 0.89, Test Correct Frac: 0.738\n",
      "Time: 0.38, Batch 7000, Train Loss (for batch): 2.55, Test Loss: 0.85, Test Correct Frac: 0.740\n",
      "Time: 0.35, Batch 7200, Train Loss (for batch): 0.00, Test Loss: 0.83, Test Correct Frac: 0.740\n",
      "Time: 0.35, Batch 7400, Train Loss (for batch): 1.01, Test Loss: 0.81, Test Correct Frac: 0.744\n",
      "Time: 0.38, Batch 7600, Train Loss (for batch): 0.00, Test Loss: 0.89, Test Correct Frac: 0.740\n",
      "Time: 0.36, Batch 7800, Train Loss (for batch): 0.46, Test Loss: 0.83, Test Correct Frac: 0.749\n",
      "Time: 0.41, Batch 8000, Train Loss (for batch): 0.58, Test Loss: 0.82, Test Correct Frac: 0.745\n",
      "Time: 0.42, Batch 8200, Train Loss (for batch): 0.00, Test Loss: 0.78, Test Correct Frac: 0.747\n",
      "Time: 0.35, Batch 8400, Train Loss (for batch): 0.00, Test Loss: 0.77, Test Correct Frac: 0.746\n",
      "Time: 0.36, Batch 8600, Train Loss (for batch): 0.01, Test Loss: 0.75, Test Correct Frac: 0.751\n",
      "Time: 0.35, Batch 8800, Train Loss (for batch): 0.01, Test Loss: 0.75, Test Correct Frac: 0.742\n",
      "Time: 0.37, Batch 9000, Train Loss (for batch): 0.06, Test Loss: 0.74, Test Correct Frac: 0.746\n",
      "Time: 0.38, Batch 9200, Train Loss (for batch): 0.00, Test Loss: 0.73, Test Correct Frac: 0.751\n",
      "Time: 0.36, Batch 9400, Train Loss (for batch): 0.21, Test Loss: 0.73, Test Correct Frac: 0.748\n",
      "Time: 0.35, Batch 9600, Train Loss (for batch): 23.65, Test Loss: 0.71, Test Correct Frac: 0.754\n",
      "Time: 0.36, Batch 9800, Train Loss (for batch): 1.62, Test Loss: 0.72, Test Correct Frac: 0.752\n",
      "Time: 0.36, Batch 10000, Train Loss (for batch): 1.89, Test Loss: 0.70, Test Correct Frac: 0.756\n",
      "Time: 0.37, Batch 10200, Train Loss (for batch): 0.00, Test Loss: 0.71, Test Correct Frac: 0.754\n",
      "Time: 0.38, Batch 10400, Train Loss (for batch): 0.03, Test Loss: 0.71, Test Correct Frac: 0.754\n",
      "Time: 0.43, Batch 10600, Train Loss (for batch): 0.26, Test Loss: 0.72, Test Correct Frac: 0.758\n",
      "Time: 0.35, Batch 10800, Train Loss (for batch): 0.00, Test Loss: 0.70, Test Correct Frac: 0.761\n",
      "Time: 0.39, Batch 11000, Train Loss (for batch): 0.53, Test Loss: 0.70, Test Correct Frac: 0.765\n",
      "Time: 0.35, Batch 11200, Train Loss (for batch): 0.24, Test Loss: 0.66, Test Correct Frac: 0.773\n",
      "Time: 0.35, Batch 11400, Train Loss (for batch): 0.36, Test Loss: 0.65, Test Correct Frac: 0.778\n",
      "Time: 0.36, Batch 11600, Train Loss (for batch): 0.48, Test Loss: 0.71, Test Correct Frac: 0.768\n",
      "Time: 0.38, Batch 11800, Train Loss (for batch): 0.52, Test Loss: 0.69, Test Correct Frac: 0.765\n",
      "Time: 0.38, Batch 12000, Train Loss (for batch): 0.00, Test Loss: 0.68, Test Correct Frac: 0.759\n",
      "Time: 0.37, Batch 12200, Train Loss (for batch): 0.03, Test Loss: 0.69, Test Correct Frac: 0.771\n",
      "Time: 0.36, Batch 12400, Train Loss (for batch): 0.38, Test Loss: 0.67, Test Correct Frac: 0.772\n",
      "Time: 0.36, Batch 12600, Train Loss (for batch): 0.00, Test Loss: 0.66, Test Correct Frac: 0.766\n",
      "Time: 0.39, Batch 12800, Train Loss (for batch): 0.00, Test Loss: 0.68, Test Correct Frac: 0.762\n",
      "Time: 0.39, Batch 13000, Train Loss (for batch): 0.58, Test Loss: 0.65, Test Correct Frac: 0.769\n",
      "Time: 0.38, Batch 13200, Train Loss (for batch): 0.33, Test Loss: 0.64, Test Correct Frac: 0.776\n",
      "Time: 0.37, Batch 13400, Train Loss (for batch): 0.34, Test Loss: 0.65, Test Correct Frac: 0.775\n",
      "Time: 0.39, Batch 13600, Train Loss (for batch): 0.34, Test Loss: 0.65, Test Correct Frac: 0.776\n",
      "Time: 0.35, Batch 13800, Train Loss (for batch): 0.03, Test Loss: 0.63, Test Correct Frac: 0.778\n",
      "Time: 0.36, Batch 14000, Train Loss (for batch): 0.73, Test Loss: 0.66, Test Correct Frac: 0.773\n",
      "Time: 0.40, Batch 14200, Train Loss (for batch): 0.24, Test Loss: 0.64, Test Correct Frac: 0.777\n",
      "Time: 0.35, Batch 14400, Train Loss (for batch): 0.47, Test Loss: 0.65, Test Correct Frac: 0.775\n",
      "Time: 0.36, Batch 14600, Train Loss (for batch): 0.09, Test Loss: 0.64, Test Correct Frac: 0.777\n",
      "Time: 0.38, Batch 14800, Train Loss (for batch): 0.00, Test Loss: 0.63, Test Correct Frac: 0.780\n",
      "Time: 0.38, Batch 15000, Train Loss (for batch): 0.00, Test Loss: 0.63, Test Correct Frac: 0.777\n",
      "Time: 0.44, Batch 15200, Train Loss (for batch): 0.43, Test Loss: 0.65, Test Correct Frac: 0.770\n",
      "Time: 0.35, Batch 15400, Train Loss (for batch): 0.01, Test Loss: 0.63, Test Correct Frac: 0.766\n",
      "Time: 0.36, Batch 15600, Train Loss (for batch): 0.13, Test Loss: 0.61, Test Correct Frac: 0.781\n",
      "Time: 0.35, Batch 15800, Train Loss (for batch): 0.00, Test Loss: 0.60, Test Correct Frac: 0.784\n",
      "Time: 0.36, Batch 16000, Train Loss (for batch): 0.00, Test Loss: 0.59, Test Correct Frac: 0.786\n",
      "Time: 0.36, Batch 16200, Train Loss (for batch): 0.00, Test Loss: 0.63, Test Correct Frac: 0.786\n",
      "Time: 0.38, Batch 16400, Train Loss (for batch): 0.00, Test Loss: 0.62, Test Correct Frac: 0.783\n",
      "Time: 0.36, Batch 16600, Train Loss (for batch): 0.00, Test Loss: 0.58, Test Correct Frac: 0.791\n",
      "Time: 0.35, Batch 16800, Train Loss (for batch): 0.00, Test Loss: 0.60, Test Correct Frac: 0.786\n",
      "Time: 0.36, Batch 17000, Train Loss (for batch): 0.01, Test Loss: 0.62, Test Correct Frac: 0.786\n",
      "Time: 0.36, Batch 17200, Train Loss (for batch): 0.06, Test Loss: 0.61, Test Correct Frac: 0.788\n",
      "Time: 0.35, Batch 17400, Train Loss (for batch): 0.00, Test Loss: 0.60, Test Correct Frac: 0.791\n",
      "Time: 0.36, Batch 17600, Train Loss (for batch): 0.73, Test Loss: 0.60, Test Correct Frac: 0.790\n",
      "Time: 0.36, Batch 17800, Train Loss (for batch): 0.12, Test Loss: 0.59, Test Correct Frac: 0.788\n",
      "Time: 0.40, Batch 18000, Train Loss (for batch): 0.36, Test Loss: 0.63, Test Correct Frac: 0.779\n",
      "Time: 0.47, Batch 18200, Train Loss (for batch): 0.00, Test Loss: 0.59, Test Correct Frac: 0.784\n",
      "Time: 0.36, Batch 18400, Train Loss (for batch): 0.66, Test Loss: 0.60, Test Correct Frac: 0.779\n",
      "Time: 0.38, Batch 18600, Train Loss (for batch): 0.00, Test Loss: 0.64, Test Correct Frac: 0.778\n",
      "Time: 0.38, Batch 18800, Train Loss (for batch): 0.00, Test Loss: 0.60, Test Correct Frac: 0.788\n",
      "Time: 0.38, Batch 19000, Train Loss (for batch): 0.00, Test Loss: 0.60, Test Correct Frac: 0.783\n",
      "Time: 0.36, Batch 19200, Train Loss (for batch): 0.00, Test Loss: 0.61, Test Correct Frac: 0.783\n",
      "Time: 0.43, Batch 19400, Train Loss (for batch): 0.07, Test Loss: 0.68, Test Correct Frac: 0.777\n",
      "Time: 0.38, Batch 19600, Train Loss (for batch): 0.01, Test Loss: 0.65, Test Correct Frac: 0.782\n",
      "Time: 0.36, Batch 19800, Train Loss (for batch): 0.04, Test Loss: 0.63, Test Correct Frac: 0.789\n",
      "Time: 0.37, Batch 20000, Train Loss (for batch): 0.03, Test Loss: 0.60, Test Correct Frac: 0.799\n",
      "Time: 0.37, Batch 20200, Train Loss (for batch): 0.00, Test Loss: 0.60, Test Correct Frac: 0.788\n",
      "Time: 0.35, Batch 20400, Train Loss (for batch): 0.00, Test Loss: 0.59, Test Correct Frac: 0.789\n",
      "Time: 0.35, Batch 20600, Train Loss (for batch): 0.00, Test Loss: 0.56, Test Correct Frac: 0.803\n",
      "Time: 0.39, Batch 20800, Train Loss (for batch): 0.12, Test Loss: 0.56, Test Correct Frac: 0.803\n",
      "Time: 0.38, Batch 21000, Train Loss (for batch): 0.01, Test Loss: 0.58, Test Correct Frac: 0.791\n",
      "Time: 0.37, Batch 21200, Train Loss (for batch): 0.00, Test Loss: 0.57, Test Correct Frac: 0.796\n",
      "Time: 0.35, Batch 21400, Train Loss (for batch): 1.03, Test Loss: 0.58, Test Correct Frac: 0.797\n",
      "Time: 0.35, Batch 21600, Train Loss (for batch): 0.00, Test Loss: 0.58, Test Correct Frac: 0.797\n",
      "Time: 0.36, Batch 21800, Train Loss (for batch): 0.01, Test Loss: 0.56, Test Correct Frac: 0.802\n",
      "Time: 0.35, Batch 22000, Train Loss (for batch): 0.00, Test Loss: 0.57, Test Correct Frac: 0.803\n",
      "Time: 0.35, Batch 22200, Train Loss (for batch): 0.00, Test Loss: 0.58, Test Correct Frac: 0.804\n",
      "Time: 0.35, Batch 22400, Train Loss (for batch): 0.05, Test Loss: 0.57, Test Correct Frac: 0.801\n",
      "Time: 0.38, Batch 22600, Train Loss (for batch): 0.00, Test Loss: 0.56, Test Correct Frac: 0.809\n",
      "Time: 0.38, Batch 22800, Train Loss (for batch): 25.14, Test Loss: 0.58, Test Correct Frac: 0.796\n",
      "Time: 0.38, Batch 23000, Train Loss (for batch): 0.00, Test Loss: 0.59, Test Correct Frac: 0.793\n",
      "Time: 0.36, Batch 23200, Train Loss (for batch): 0.06, Test Loss: 0.57, Test Correct Frac: 0.803\n",
      "Time: 0.35, Batch 23400, Train Loss (for batch): 0.00, Test Loss: 0.56, Test Correct Frac: 0.806\n"
     ]
    }
   ],
   "source": [
    "default_architecture = {\n",
    "    'D_in' : features_all.shape[1],\n",
    "    'D_out' : N_categories,\n",
    "    'hidden_dims' : [300, 300, 300],\n",
    "    'act_func' : torch.nn.ReLU,\n",
    "}\n",
    "\n",
    "default_learning = {\n",
    "    'N_batch' : 16,\n",
    "    'N_epochs' : 3,\n",
    "    'optimizer' : torch.optim.Adam,\n",
    "    'loss_fn' : torch.nn.CrossEntropyLoss(),\n",
    "    'print_interval' : 200,  \n",
    "    'learning_rate' : 1e-4,\n",
    "    'track' : ['test_loss', 'batch_idx', 'epoch_idx','t', 'train_loss','test_frac_correct','train_frac_correct','model']\n",
    "}\n",
    "\n",
    "result_nets = []\n",
    "\n",
    "for N_batch in N_batchs:\n",
    "    result_nets.append([])\n",
    "    for learning_rate in learning_rates:\n",
    "        result_nets.append([])\n",
    "        for n_layers in n_layerss:\n",
    "            result_nets.append([])\n",
    "            for layer_width in layer_widths:\n",
    "                descriptor = \"Net with {:2d} layers, {:3d} wide. Using learning rate of {:.6f} and {:2d} batches\".format(n_layers, layer_width, learning_rate, N_batch)\n",
    "                architecture_n = copy.deepcopy(default_architecture)\n",
    "                learning_n = copy.deepcopy(default_learning)\n",
    "                architecture_n['hidden_dims'] = [layer_width] * n_layers\n",
    "                learning_n['N_batch'] = N_batch\n",
    "                learning_n['learning_rate'] = learning_rate\n",
    "\n",
    "                n.random.seed(0)\n",
    "                torch.manual_seed(0)\n",
    "\n",
    "                # split features and targets (ft/tg) into training, test and validation sets\n",
    "                ft_tr, ft_ts, ft_vl,ft_sp, tg_tr, tg_ts, tg_vl, tg_sp = split_sets(features_all, targets_int, splits=(0.95, 0.02, 0.03),separate_vid_idx=(0,30270))\n",
    "\n",
    "\n",
    "                ft_tr = torch.tensor(ft_tr).float()\n",
    "                ft_ts = torch.tensor(ft_ts).float()\n",
    "                ft_vl = torch.tensor(ft_vl).float()\n",
    "                ft_sp = torch.tensor(ft_sp).float()\n",
    "                tg_tr = torch.tensor(tg_tr).long()\n",
    "                tg_ts = torch.tensor(tg_ts).long()\n",
    "                tg_vl = torch.tensor(tg_vl).long()\n",
    "                tg_sp = torch.tensor(tg_sp).long()\n",
    "\n",
    "                net = MLP(architecture_n)\n",
    "                print(\"\\n\\n\",descriptor)\n",
    "                net.learn(learning_n, (ft_tr,tg_tr), (ft_sp, tg_sp))\n",
    "                print(\"\\n\\n\",descriptor)\n",
    "                plot_net_results(net, ft_sp, tg_sp)\n",
    "                result_nets[-1][-1][-1].append(copy.deepcopy(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gp]",
   "language": "python",
   "name": "conda-env-gp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
