{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy.lib.recfunctions as rfn\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "#### Load data files\n",
    "`data_root` should contain the root directory of the folder downloaded from Dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root, dlc_dir, ann_dir, verbose=False):\n",
    "    \n",
    "    dlc_path = os.path.join(data_root, dlc_dir)\n",
    "    ann_path = os.path.join(data_root, ann_dir)\n",
    "    all_data = {}\n",
    "    if verbose: print(\"Loading files: \")\n",
    "    for f_name in os.listdir(dlc_path):\n",
    "        if f_name[-3:] != 'npy':\n",
    "            continue\n",
    "\n",
    "        dlc_file=os.path.join(dlc_path, f_name)\n",
    "        ann_file=os.path.join(ann_path, 'Annotated_' + f_name)\n",
    "        if verbose: print(\"\\t\" + f_name + \"\\n\\tAnnotated_\" + f_name)\n",
    "        data_dlc = n.load(dlc_file)\n",
    "        data_ann = n.load(ann_file)\n",
    "        labels = data_dlc[0]\n",
    "        dtype = [('t', n.int), ('ann', 'U30')]\n",
    "        i = 0\n",
    "        for label in data_dlc[0]:\n",
    "            i += 1\n",
    "            coord = 'x' if i % 2 == 0 else 'y'\n",
    "            dtype += [(label + '_' + coord , n.float32 )]\n",
    "\n",
    "        data_concat = n.concatenate((data_ann, data_dlc[1:]),axis=1)\n",
    "        data = n.array(n.zeros(data_concat.shape[0]), dtype = dtype)\n",
    "        for i in range(data_concat.shape[1]):\n",
    "            data[dtype[i][0]] = data_concat[:, i]\n",
    "        all_data[f_name[:-4]] = data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return n.sum(a * b, axis=-1)\n",
    "\n",
    "def mag(a):\n",
    "    return n.sqrt(n.sum(a*a, axis=-1))\n",
    "\n",
    "def get_angle(a, b):\n",
    "    cosab = dot(a, b) / (mag(a) * mag(b)) # cosine of angle between vectors\n",
    "    angle = n.arccos(cosab) # what you currently have (absolute angle)\n",
    "\n",
    "    b_t = b[:,[1,0]] * [1, -1] # perpendicular of b\n",
    "\n",
    "    is_cc = dot(a, b_t) < 0\n",
    "\n",
    "    # invert the angles for counter-clockwise rotations\n",
    "    angle[is_cc] = 2*n.pi - angle[is_cc]\n",
    "    return 360 - n.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(trial):\n",
    "    names = []; dtypes = []; datas = []\n",
    "    velocities_calculated = []\n",
    "    for label in trial.dtype.names:\n",
    "        if label[-2:] in ['_x', '_y']:\n",
    "            names.append(label+'_vel')  \n",
    "            dtypes += [n.float]\n",
    "            datas += [n.zeros(trial.shape[0])]\n",
    "            velocities_calculated.append(label)\n",
    "    trial = rfn.append_fields(trial, names, datas, dtypes)\n",
    "    trial = n.array(trial, trial.dtype)\n",
    "    for label in velocities_calculated:\n",
    "        vel = n.gradient(trial[label])\n",
    "        trial[label + '_vel'] = vel\n",
    "    return trial\n",
    "def normalize_trial(trial, feature_labels, nan = -10000, only_rat1 = False):\n",
    "    ref_x = trial[feature_labels[1]].copy()\n",
    "    ref_y = trial[feature_labels[0]].copy()\n",
    "    for i,label in enumerate(feature_labels):\n",
    "        if label[-1] == 'y':\n",
    "    #         print('y-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_y\n",
    "    #         print('y-post:', n.nanmax(features[:,i]))\n",
    "        elif label[-1] == 'x':\n",
    "    #         print('x-pre:',n.nanmax(features[:,i]))\n",
    "            trial[label] -= ref_x\n",
    "    #         print('x-post:', n.nanmax(features[:,i]))\n",
    "\n",
    "    mouse_1_pos_labels = []\n",
    "    mouse_2_pos_labels = []\n",
    "    mouse_1_vel_labels = []\n",
    "    mouse_2_vel_labels = []\n",
    "    for label in feature_labels:\n",
    "        if label[-3:] == 'vel':\n",
    "            if label[-7] == '1':\n",
    "                mouse_1_vel_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_vel_labels.append(label)\n",
    "        else:\n",
    "            if label[-3] == '1':\n",
    "                mouse_1_pos_labels.append(label)\n",
    "            else:\n",
    "                mouse_2_pos_labels.append(label)\n",
    "\n",
    "\n",
    "    mouse_1_pos = n.zeros((len(mouse_1_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_pos_labels): mouse_1_pos[i]=trial[l]\n",
    "    mouse_2_pos = n.zeros((len(mouse_2_pos_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_pos_labels): mouse_2_pos[i]=trial[l]\n",
    "    mouse_1_vel = n.zeros((len(mouse_1_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_1_vel_labels): mouse_1_vel[i]=trial[l]\n",
    "    mouse_2_vel = n.zeros((len(mouse_2_vel_labels),len(trial)))\n",
    "    for i,l in enumerate(mouse_2_vel_labels): mouse_2_vel[i]=trial[l]\n",
    "    # TODO how to normalize??\n",
    "    if not only_rat1:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_2_pos, mouse_1_vel, mouse_2_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_2_pos_labels, mouse_1_vel_labels, mouse_2_vel_labels])\n",
    "    else:\n",
    "        trial_data = n.concatenate([mouse_1_pos, mouse_1_vel])\n",
    "        trial_labels = n.concatenate([mouse_1_pos_labels, mouse_1_vel_labels])\n",
    "    if nan is not None:\n",
    "        trial_data = n.nan_to_num(trial_data, nan=nan)\n",
    "    \n",
    "    return trial_data, trial_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate train, test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features_all,targets_all, chunk_size=500, splits= (0.7, 0.2, 0.1), separate_vid_idx = None):\n",
    "    data_len = features_all.shape[0]\n",
    "    num_chunks = data_len // chunk_size\n",
    "    chunk_list = n.random.choice(range(num_chunks), size=num_chunks, replace=False)\n",
    "\n",
    "    test_chunk_idx_bound = splits[0]*num_chunks\n",
    "    val_chunk_idx_bound = (splits[0]+splits[1])*num_chunks\n",
    "\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    features_val = []\n",
    "    targets_train = []\n",
    "    targets_test = []\n",
    "    targets_val = []\n",
    "    \n",
    "    if separate_vid_idx is not None:\n",
    "        targets_separate = []\n",
    "        features_separate = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        curr_chunk_idx = chunk_list[i]*chunk_size\n",
    "        curr_chunk = features_all[curr_chunk_idx:curr_chunk_idx+chunk_size,:]\n",
    "        curr_chunk_t = targets_all[curr_chunk_idx:curr_chunk_idx+chunk_size]\n",
    "#         print(curr_chunk_idx)\n",
    "        if separate_vid_idx is not None and curr_chunk_idx+chunk_size > separate_vid_idx[0] and curr_chunk_idx < separate_vid_idx[1]:\n",
    "#                 print(curr_chunk_idx, separate_vid_idx[0])\n",
    "#                 print(curr_chunk_idx+chunk_size, separate_vid_idx[1])\n",
    "                targets_separate.append(curr_chunk_t)\n",
    "                features_separate.append(curr_chunk)\n",
    "        elif i < test_chunk_idx_bound:\n",
    "#             print(\"train!!\")\n",
    "            features_train.append(curr_chunk)\n",
    "            targets_train.append(curr_chunk_t)\n",
    "        elif i < val_chunk_idx_bound:\n",
    "#             print('test')\n",
    "            features_test.append(curr_chunk)\n",
    "            targets_test.append(curr_chunk_t)\n",
    "        else:\n",
    "#             print('val')\n",
    "            features_val.append(curr_chunk)\n",
    "            targets_val.append(curr_chunk_t)\n",
    "\n",
    "#     print(len(features_separate))\n",
    "#     print(len(targets_separate))\n",
    "    features_train = n.concatenate(features_train, axis=0)\n",
    "    features_test = n.concatenate(features_test, axis=0)\n",
    "    features_val = n.concatenate(features_val, axis=0)\n",
    "    \n",
    "    targets_val = n.concatenate(targets_val)\n",
    "    targets_test = n.concatenate(targets_test)\n",
    "    targets_train = n.concatenate(targets_train)\n",
    "    \n",
    "    if separate_vid_idx is None:\n",
    "        return features_train, features_test, features_val, targets_train, targets_test, targets_val\n",
    "    else:\n",
    "        features_separate = n.concatenate(features_separate, axis=0)\n",
    "        targets_separate = n.concatenate(targets_separate)\n",
    "        return features_train, features_test, features_val, features_separate,\\\n",
    "                targets_train, targets_test, targets_val, targets_separate\n",
    "\n",
    "def str_to_int(targets, mapping = None):\n",
    "    categories = n.unique(targets)\n",
    "    N_categories = len(categories)\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "        i = 0\n",
    "        for c in categories:\n",
    "            mapping[c] = i\n",
    "            i += 1\n",
    "    targets_int = n.array([mapping[s] for s in targets], dtype=int)\n",
    "    \n",
    "    return targets_int, mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / n.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return (n.arccos(n.clip(n.dot(v1_u, v2_u), -1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a structured array\n",
    "data_root = 'C:/Users/Neuropixel/AH-EN'\n",
    "dlc_dir = 'postprocessedXYCoordinates'\n",
    "ann_dir = 'manualannotations'\n",
    "all_data = load_data(data_root, dlc_dir, ann_dir)\n",
    "\n",
    "# Choose which position labels we care about\n",
    "feature_labels = all_data['Female1'].dtype.names[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate velocity and preprocess/scale/normalize data\n",
    "trial_keys = ['Female1']#list(all_data.keys())\n",
    "datas = []\n",
    "# for key in all_data.keys():\n",
    "#     all_data[key] = calculate_velocity(all_data[key])\n",
    "for key in trial_keys:\n",
    "    datas.append(normalize_trial(all_data[key], feature_labels, None, True)[0])\n",
    "features_all = n.concatenate(datas, axis=1).T\n",
    "\n",
    "# Format category labels\n",
    "targets_all = n.concatenate([all_data[key]['ann'] for key in trial_keys]).T\n",
    "targets_int, target_map = str_to_int(targets_all)\n",
    "categories = target_map.keys()\n",
    "N_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4866346588828399"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_to_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.34558105, 51.51176453])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.69473821698646"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7763568394002505e-15"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_points = n.zeros((features_all.shape[0], int(features_all.shape[1]/2), 2))\n",
    "# t = 1000\n",
    "angles = []\n",
    "for t in range(features_all.shape[0]):\n",
    "    points = features_all[t]\n",
    "    # x, y format\n",
    "    anchor_point = n.array([points[1], points[0]])\n",
    "    second_point = n.array([points[3], points[2]])\n",
    "    anchor_vector = second_point - anchor_point\n",
    "    angle_to_rotate = -angle_between(anchor_vector, n.array((1,0)))\n",
    "    if n.all(anchor_vector[1] < 0): angle_to_rotate *= -1\n",
    "    angles.append(angle_to_rotate)\n",
    "#     if angle_to_rotate < 0: angle_to_rotate += 2*n.pi\n",
    "    # given vector (x1, y1), rotating it by A around origin gives:\n",
    "    # x2 = cosA x1 - sinA y1\n",
    "    # y2 = sinA x1 + cosA y1\n",
    "    num_points = int(len(points)/2)\n",
    "    new_points = n.zeros((num_points,2))\n",
    "    for points_idx in range(1,num_points):\n",
    "        second_point = n.array([points[points_idx*2+1], points[points_idx*2]])\n",
    "        vector = second_point - anchor_point\n",
    "        new_x = n.cos(angle_to_rotate) * vector[0] - n.sin(angle_to_rotate) * vector[1]\n",
    "        new_y = n.sin(angle_to_rotate) * vector[0] + n.cos(angle_to_rotate) * vector[1]\n",
    "\n",
    "        ego_points[t][points_idx] = [new_x, new_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 10000\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(ego_points[t,i,0], ego_points[t,i,1])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-200,200)\n",
    "plt.xlim(-200,200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_points):\n",
    "    plt.scatter(features_all[t][i*2+1], features_all[t][i*2])\n",
    "\n",
    "s = n.nanmean(features_all[t])\n",
    "plt.ylim(-200,200)\n",
    "plt.xlim(-200,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\videos\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\data\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\results\"\n",
      "Created \"C:\\Users\\Neuropixel\\AH-EN\\swc-en-social\\vame\\single-mouse-f1-Nov20-2020\\model\"\n",
      "1  videos from the directory C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1 were added to the project.\n",
      "Copying the videos \n",
      "\n",
      "A VAME project has been created. \n",
      "\n",
      "Next use vame.create_trainset(config) to split your data into a train and test set. \n",
      "Afterwards you can use vame.rnn_model() to train the model on your data.\n"
     ]
    }
   ],
   "source": [
    "config = vame.init_new_project(project='single-mouse-f1', videos = ['C:/Users/Neuropixel/AH-EN/swc-en-social/vame/Female1'], \n",
    "                      working_directory='C:/Users/Neuropixel/AH-EN/swc-en-social/vame', videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\single-mouse-f1-Nov20-2020\\\\config.yaml'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30270, 10, 2)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(, 'w+') as f:\n",
    "f = \"C:\\\\Users\\\\Neuropixel\\\\AH-EN\\\\swc-en-social\\\\vame\\\\single-mouse-f1-Nov20-2020\\\\data\\\\Female1\\\\Female1-PE-seq.npy\"\n",
    "n.save(f, ego_points.reshape(-1, 20).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset.\n",
      "Lenght of train data: 24216\n",
      "Lenght of test data: 6054\n"
     ]
    }
   ],
   "source": [
    "trainset = vame.create_trainset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RNN model!\n",
      "Using CUDA\n",
      "GPU active: True\n",
      "GPU used: Quadro P620\n",
      "Latent Dimensions: 30, Beta: 1, lr: 0.0005\n",
      "Compute mean and std for temporal dataset.\n",
      "Initialize train data. Datapoints 24216\n",
      "Initialize test data. Datapoints 6054\n",
      "Epoch: 1\n",
      "Train: \n",
      "Epoch: 1.  loss: 240607.7500\n",
      "Average Train loss: 45083.7238, MSE-Loss: 28401.3596, MSE-Future-Loss 16682.3641, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 2242.4663, MSE-Loss: 2242.4663, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 2\n",
      "Train: \n",
      "Epoch: 2.  loss: 20272.3301\n",
      "Average Train loss: 19168.1105, MSE-Loss: 11154.4096, MSE-Future-Loss 8013.7009, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 1694.9520, MSE-Loss: 1694.9520, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 3\n",
      "Train: \n",
      "Epoch: 3.  loss: 18419.5059\n",
      "Average Train loss: 15205.0624, MSE-Loss: 8588.5394, MSE-Future-Loss 6616.5230, KL-Loss: 0.0000,  Kmeans-Loss: 0.0000, weigt: 0.0000\n",
      "Test: \n",
      "Average Test loss: 1333.4067, MSE-Loss: 1333.4067, KL-Loss: 0.0000, Kmeans-Loss: 0.0000\n",
      "lr: 0.0005\n",
      "Epoch: 4\n",
      "Train: \n",
      "Epoch: 4.  loss: 13919.4775\n",
      "Average Train loss: 12936.9534, MSE-Loss: 7025.1373, MSE-Future-Loss 5903.0632, KL-Loss: 5.2824,  Kmeans-Loss: 3.4705, weigt: 0.5000\n",
      "Test: \n",
      "Average Test loss: 1123.5923, MSE-Loss: 1115.6178, KL-Loss: 5.6659, Kmeans-Loss: 2.3086\n",
      "lr: 0.0005\n",
      "Epoch: 5\n",
      "Train: \n",
      "Epoch: 5.  loss: 10919.7246\n",
      "Average Train loss: 11917.4542, MSE-Loss: 6187.9039, MSE-Future-Loss 5716.5309, KL-Loss: 8.2965,  Kmeans-Loss: 4.7229, weigt: 0.6250\n",
      "Test: \n",
      "Average Test loss: 1021.9699, MSE-Loss: 1010.2224, KL-Loss: 8.5156, Kmeans-Loss: 3.2320\n",
      "lr: 0.0005\n",
      "Epoch: 6\n",
      "Train: \n",
      "Epoch: 6.  loss: 9640.6240\n",
      "Average Train loss: 10968.6813, MSE-Loss: 5458.8559, MSE-Future-Loss 5491.9125, KL-Loss: 11.8305,  Kmeans-Loss: 6.0825, weigt: 0.7500\n",
      "Test: \n",
      "Average Test loss: 900.1999, MSE-Loss: 884.1543, KL-Loss: 11.8048, Kmeans-Loss: 4.2408\n",
      "lr: 0.0005\n",
      "Epoch: 7\n",
      "Train: \n",
      "Epoch: 7.  loss: 11003.0742\n",
      "Average Train loss: 10238.0558, MSE-Loss: 4965.2872, MSE-Future-Loss 5249.6406, KL-Loss: 15.5939,  Kmeans-Loss: 7.5341, weigt: 0.8750\n",
      "Test: \n",
      "Average Test loss: 780.9250, MSE-Loss: 760.1892, KL-Loss: 15.3693, Kmeans-Loss: 5.3665\n",
      "lr: 0.0005\n",
      "Epoch: 8\n",
      "Train: \n",
      "Epoch: 8.  loss: 9435.9287\n",
      "Average Train loss: 9638.3636, MSE-Loss: 4465.4086, MSE-Future-Loss 5144.0141, KL-Loss: 19.8470,  Kmeans-Loss: 9.0938, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 749.2237, MSE-Loss: 723.0848, KL-Loss: 19.5198, Kmeans-Loss: 6.6191\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 9\n",
      "Train: \n",
      "Epoch: 9.  loss: 9329.5068\n",
      "Average Train loss: 9220.4676, MSE-Loss: 4228.6595, MSE-Future-Loss 4960.5878, KL-Loss: 21.6768,  Kmeans-Loss: 9.5435, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 718.6952, MSE-Loss: 690.5878, KL-Loss: 21.0847, Kmeans-Loss: 7.0227\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 10\n",
      "Train: \n",
      "Epoch: 10.  loss: 8982.4746\n",
      "Average Train loss: 8804.3826, MSE-Loss: 3926.5726, MSE-Future-Loss 4844.7238, KL-Loss: 23.1654,  Kmeans-Loss: 9.9208, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 673.3208, MSE-Loss: 644.0805, KL-Loss: 21.9621, Kmeans-Loss: 7.2782\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 11\n",
      "Train: \n",
      "Epoch: 11.  loss: 7759.4263\n",
      "Average Train loss: 8511.2123, MSE-Loss: 3674.9094, MSE-Future-Loss 4800.9995, KL-Loss: 24.9922,  Kmeans-Loss: 10.3112, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 650.2838, MSE-Loss: 618.5625, KL-Loss: 24.0034, Kmeans-Loss: 7.7180\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 12\n",
      "Train: \n",
      "Epoch: 12.  loss: 8600.6006\n",
      "Average Train loss: 8135.1934, MSE-Loss: 3424.2547, MSE-Future-Loss 4673.6046, KL-Loss: 26.6779,  Kmeans-Loss: 10.6562, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 655.2769, MSE-Loss: 621.2802, KL-Loss: 25.9483, Kmeans-Loss: 8.0484\n",
      "lr: 0.0005\n",
      "Epoch: 13\n",
      "Train: \n",
      "Epoch: 13.  loss: 8133.7847\n",
      "Average Train loss: 7993.7957, MSE-Loss: 3430.0685, MSE-Future-Loss 4524.5875, KL-Loss: 28.1773,  Kmeans-Loss: 10.9625, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 593.8340, MSE-Loss: 558.8780, KL-Loss: 26.7100, Kmeans-Loss: 8.2460\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 14\n",
      "Train: \n",
      "Epoch: 14.  loss: 7877.2700\n",
      "Average Train loss: 7640.3998, MSE-Loss: 3222.5716, MSE-Future-Loss 4376.9366, KL-Loss: 29.6706,  Kmeans-Loss: 11.2210, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 600.3789, MSE-Loss: 563.1553, KL-Loss: 28.5947, Kmeans-Loss: 8.6289\n",
      "lr: 0.0005\n",
      "Epoch: 15\n",
      "Train: \n",
      "Epoch: 15.  loss: 7824.0034\n",
      "Average Train loss: 7481.8780, MSE-Loss: 3150.1004, MSE-Future-Loss 4289.5145, KL-Loss: 30.7846,  Kmeans-Loss: 11.4785, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 613.5966, MSE-Loss: 576.1397, KL-Loss: 28.7273, Kmeans-Loss: 8.7296\n",
      "lr: 0.0005\n",
      "Epoch: 16\n",
      "Train: \n",
      "Epoch: 16.  loss: 7273.0571\n",
      "Average Train loss: 7261.6879, MSE-Loss: 3015.4448, MSE-Future-Loss 4202.7746, KL-Loss: 31.7736,  Kmeans-Loss: 11.6948, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 550.9567, MSE-Loss: 512.4247, KL-Loss: 29.6357, Kmeans-Loss: 8.8962\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 17\n",
      "Train: \n",
      "Epoch: 17.  loss: 7818.1855\n",
      "Average Train loss: 7048.9870, MSE-Loss: 2983.6234, MSE-Future-Loss 4020.6059, KL-Loss: 32.8221,  Kmeans-Loss: 11.9357, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 543.5142, MSE-Loss: 503.6278, KL-Loss: 30.6699, Kmeans-Loss: 9.2165\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 18\n",
      "Train: \n",
      "Epoch: 18.  loss: 7738.3813\n",
      "Average Train loss: 6972.2946, MSE-Loss: 2890.0522, MSE-Future-Loss 4036.4555, KL-Loss: 33.6614,  Kmeans-Loss: 12.1256, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 527.6534, MSE-Loss: 486.8173, KL-Loss: 31.4909, Kmeans-Loss: 9.3453\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 19\n",
      "Train: \n",
      "Epoch: 19.  loss: 6318.7573\n",
      "Average Train loss: 6758.6990, MSE-Loss: 2871.6210, MSE-Future-Loss 3840.1587, KL-Loss: 34.6043,  Kmeans-Loss: 12.3151, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 525.2751, MSE-Loss: 484.3605, KL-Loss: 31.4984, Kmeans-Loss: 9.4162\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 20\n",
      "Train: \n",
      "Epoch: 20.  loss: 6355.7056\n",
      "Average Train loss: 6531.5352, MSE-Loss: 2773.3821, MSE-Future-Loss 3710.2607, KL-Loss: 35.3723,  Kmeans-Loss: 12.5201, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 498.2609, MSE-Loss: 455.8102, KL-Loss: 32.8338, Kmeans-Loss: 9.6170\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 21\n",
      "Train: \n",
      "Epoch: 21.  loss: 5504.0864\n",
      "Average Train loss: 6496.3106, MSE-Loss: 2725.8641, MSE-Future-Loss 3721.3990, KL-Loss: 36.3179,  Kmeans-Loss: 12.7296, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 501.4061, MSE-Loss: 458.5295, KL-Loss: 33.0692, Kmeans-Loss: 9.8074\n",
      "lr: 0.0005\n",
      "Epoch: 22\n",
      "Train: \n",
      "Epoch: 22.  loss: 5667.3809\n",
      "Average Train loss: 6334.3668, MSE-Loss: 2664.6662, MSE-Future-Loss 3619.9314, KL-Loss: 36.8766,  Kmeans-Loss: 12.8926, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 511.7521, MSE-Loss: 467.7684, KL-Loss: 34.0427, Kmeans-Loss: 9.9410\n",
      "lr: 0.0005\n",
      "Epoch: 23\n",
      "Train: \n",
      "Epoch: 23.  loss: 6566.8071\n",
      "Average Train loss: 6150.0054, MSE-Loss: 2668.9892, MSE-Future-Loss 3430.2355, KL-Loss: 37.7039,  Kmeans-Loss: 13.0768, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 490.2865, MSE-Loss: 445.5562, KL-Loss: 34.6186, Kmeans-Loss: 10.1117\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 24\n",
      "Train: \n",
      "Epoch: 24.  loss: 6254.4185\n",
      "Average Train loss: 6083.4652, MSE-Loss: 2596.6489, MSE-Future-Loss 3434.9869, KL-Loss: 38.5716,  Kmeans-Loss: 13.2578, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 494.4017, MSE-Loss: 448.8386, KL-Loss: 35.2676, Kmeans-Loss: 10.2954\n",
      "lr: 0.0005\n",
      "Epoch: 25\n",
      "Train: \n",
      "Epoch: 25.  loss: 7023.8755\n",
      "Average Train loss: 5895.8900, MSE-Loss: 2552.3771, MSE-Future-Loss 3290.8827, KL-Loss: 39.2075,  Kmeans-Loss: 13.4226, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 487.7902, MSE-Loss: 441.2724, KL-Loss: 36.0496, Kmeans-Loss: 10.4682\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 26\n",
      "Train: \n",
      "Epoch: 26.  loss: 5982.5605\n",
      "Average Train loss: 5843.4120, MSE-Loss: 2552.7461, MSE-Future-Loss 3237.1974, KL-Loss: 39.8602,  Kmeans-Loss: 13.6083, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 483.8322, MSE-Loss: 437.7875, KL-Loss: 35.5436, Kmeans-Loss: 10.5012\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 27\n",
      "Train: \n",
      "Epoch: 27.  loss: 5668.4062\n",
      "Average Train loss: 5635.1895, MSE-Loss: 2461.9891, MSE-Future-Loss 3119.3356, KL-Loss: 40.1503,  Kmeans-Loss: 13.7144, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 483.0468, MSE-Loss: 436.7704, KL-Loss: 35.6639, Kmeans-Loss: 10.6124\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 28\n",
      "Train: \n",
      "Epoch: 28.  loss: 5212.9102\n",
      "Average Train loss: 5449.7989, MSE-Loss: 2379.2307, MSE-Future-Loss 3016.1108, KL-Loss: 40.6042,  Kmeans-Loss: 13.8533, weigt: 1.0000\n",
      "Test: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 480.6234, MSE-Loss: 433.5298, KL-Loss: 36.3212, Kmeans-Loss: 10.7724\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 29\n",
      "Train: \n",
      "Epoch: 29.  loss: 5331.4399\n",
      "Average Train loss: 5378.4647, MSE-Loss: 2384.6454, MSE-Future-Loss 2938.5734, KL-Loss: 41.2276,  Kmeans-Loss: 14.0183, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 489.5567, MSE-Loss: 441.0815, KL-Loss: 37.4461, Kmeans-Loss: 11.0291\n",
      "lr: 0.0005\n",
      "Epoch: 30\n",
      "Train: \n",
      "Epoch: 30.  loss: 5077.9932\n",
      "Average Train loss: 5236.8491, MSE-Loss: 2301.8510, MSE-Future-Loss 2879.0088, KL-Loss: 41.8048,  Kmeans-Loss: 14.1845, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 491.8839, MSE-Loss: 443.3529, KL-Loss: 37.4714, Kmeans-Loss: 11.0595\n",
      "lr: 0.0005\n",
      "Epoch: 31\n",
      "Train: \n",
      "Epoch: 31.  loss: 5059.1431\n",
      "Average Train loss: 5132.6219, MSE-Loss: 2278.4973, MSE-Future-Loss 2797.8666, KL-Loss: 41.9854,  Kmeans-Loss: 14.2725, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 463.1592, MSE-Loss: 413.7404, KL-Loss: 38.1983, Kmeans-Loss: 11.2205\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 32\n",
      "Train: \n",
      "Epoch: 32.  loss: 4729.0615\n",
      "Average Train loss: 5021.4497, MSE-Loss: 2227.1582, MSE-Future-Loss 2737.2906, KL-Loss: 42.5958,  Kmeans-Loss: 14.4051, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 485.1872, MSE-Loss: 435.1253, KL-Loss: 38.7909, Kmeans-Loss: 11.2710\n",
      "lr: 0.0005\n",
      "Epoch: 33\n",
      "Train: \n",
      "Epoch: 33.  loss: 4928.7554\n",
      "Average Train loss: 5015.8640, MSE-Loss: 2280.9925, MSE-Future-Loss 2677.3435, KL-Loss: 42.9776,  Kmeans-Loss: 14.5503, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 461.4305, MSE-Loss: 411.1105, KL-Loss: 38.7987, Kmeans-Loss: 11.5213\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 34\n",
      "Train: \n",
      "Epoch: 34.  loss: 4861.5220\n",
      "Average Train loss: 4775.7723, MSE-Loss: 2152.8257, MSE-Future-Loss 2564.9242, KL-Loss: 43.3529,  Kmeans-Loss: 14.6694, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 449.0610, MSE-Loss: 397.5497, KL-Loss: 39.8448, Kmeans-Loss: 11.6665\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 35\n",
      "Train: \n",
      "Epoch: 35.  loss: 5372.5996\n",
      "Average Train loss: 4658.4705, MSE-Loss: 2136.1952, MSE-Future-Loss 2463.4390, KL-Loss: 43.9950,  Kmeans-Loss: 14.8411, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 450.4447, MSE-Loss: 398.4970, KL-Loss: 40.1397, Kmeans-Loss: 11.8081\n",
      "lr: 0.0005\n",
      "Epoch: 36\n",
      "Train: \n",
      "Epoch: 36.  loss: 4228.3354\n",
      "Average Train loss: 4604.6672, MSE-Loss: 2095.3140, MSE-Future-Loss 2450.0180, KL-Loss: 44.3938,  Kmeans-Loss: 14.9414, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 435.4710, MSE-Loss: 383.0261, KL-Loss: 40.6260, Kmeans-Loss: 11.8189\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 37\n",
      "Train: \n",
      "Epoch: 37.  loss: 4331.4609\n",
      "Average Train loss: 4531.6889, MSE-Loss: 2097.1164, MSE-Future-Loss 2374.5797, KL-Loss: 44.9247,  Kmeans-Loss: 15.0681, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 456.3710, MSE-Loss: 403.1995, KL-Loss: 41.2029, Kmeans-Loss: 11.9686\n",
      "lr: 0.0005\n",
      "Epoch: 38\n",
      "Train: \n",
      "Epoch: 38.  loss: 4466.4429\n",
      "Average Train loss: 4507.1228, MSE-Loss: 2126.0309, MSE-Future-Loss 2320.9412, KL-Loss: 45.0120,  Kmeans-Loss: 15.1387, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 454.1327, MSE-Loss: 401.5116, KL-Loss: 40.6530, Kmeans-Loss: 11.9681\n",
      "lr: 0.0005\n",
      "Epoch: 39\n",
      "Train: \n",
      "Epoch: 39.  loss: 3810.5166\n",
      "Average Train loss: 4375.9104, MSE-Loss: 2046.2234, MSE-Future-Loss 2269.3795, KL-Loss: 45.1165,  Kmeans-Loss: 15.1911, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 446.5925, MSE-Loss: 392.4058, KL-Loss: 41.9886, Kmeans-Loss: 12.1981\n",
      "lr: 0.0005\n",
      "Epoch: 40\n",
      "Train: \n",
      "Epoch: 40.  loss: 4656.3945\n",
      "Average Train loss: 4259.3993, MSE-Loss: 2022.5968, MSE-Future-Loss 2175.3878, KL-Loss: 46.0263,  Kmeans-Loss: 15.3884, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 452.7581, MSE-Loss: 398.8756, KL-Loss: 41.6722, Kmeans-Loss: 12.2103\n",
      "lr: 0.0005\n",
      "Epoch: 41\n",
      "Train: \n",
      "Epoch: 41.  loss: 4109.0015\n",
      "Average Train loss: 4331.2780, MSE-Loss: 2049.9188, MSE-Future-Loss 2219.6509, KL-Loss: 46.2427,  Kmeans-Loss: 15.4657, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 486.4922, MSE-Loss: 431.1438, KL-Loss: 42.9496, Kmeans-Loss: 12.3988\n",
      "lr: 0.0005\n",
      "Epoch: 42\n",
      "Train: \n",
      "Epoch: 42.  loss: 4248.7114\n",
      "Average Train loss: 4179.1275, MSE-Loss: 2014.7118, MSE-Future-Loss 2102.1873, KL-Loss: 46.6661,  Kmeans-Loss: 15.5624, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 438.7648, MSE-Loss: 383.6255, KL-Loss: 42.7063, Kmeans-Loss: 12.4330\n",
      "lr: 0.0005\n",
      "Epoch: 43\n",
      "Train: \n",
      "Epoch: 43.  loss: 3625.3643\n",
      "Average Train loss: 4091.0649, MSE-Loss: 1996.2163, MSE-Future-Loss 2032.2554, KL-Loss: 46.9593,  Kmeans-Loss: 15.6340, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 431.9088, MSE-Loss: 376.3618, KL-Loss: 43.0596, Kmeans-Loss: 12.4873\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 44\n",
      "Train: \n",
      "Epoch: 44.  loss: 3709.6697\n",
      "Average Train loss: 3941.3862, MSE-Loss: 1909.1291, MSE-Future-Loss 1969.2563, KL-Loss: 47.2721,  Kmeans-Loss: 15.7288, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 431.1518, MSE-Loss: 375.1119, KL-Loss: 43.4385, Kmeans-Loss: 12.6013\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 45\n",
      "Train: \n",
      "Epoch: 45.  loss: 4770.7021\n",
      "Average Train loss: 3924.6574, MSE-Loss: 1921.9973, MSE-Future-Loss 1939.2884, KL-Loss: 47.5773,  Kmeans-Loss: 15.7945, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 444.0299, MSE-Loss: 387.6129, KL-Loss: 43.7882, Kmeans-Loss: 12.6288\n",
      "lr: 0.0005\n",
      "Epoch: 46\n",
      "Train: \n",
      "Epoch: 46.  loss: 3696.7146\n",
      "Average Train loss: 3940.1694, MSE-Loss: 1923.0460, MSE-Future-Loss 1953.0914, KL-Loss: 48.1288,  Kmeans-Loss: 15.9032, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 436.3710, MSE-Loss: 379.4260, KL-Loss: 44.1684, Kmeans-Loss: 12.7767\n",
      "lr: 0.0005\n",
      "Epoch: 47\n",
      "Train: \n",
      "Epoch: 47.  loss: 3908.5986\n",
      "Average Train loss: 3853.3818, MSE-Loss: 1917.8000, MSE-Future-Loss 1870.9924, KL-Loss: 48.5560,  Kmeans-Loss: 16.0334, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 443.6932, MSE-Loss: 386.6742, KL-Loss: 44.1426, Kmeans-Loss: 12.8764\n",
      "lr: 0.0005\n",
      "Epoch: 48\n",
      "Train: \n",
      "Epoch: 48.  loss: 3628.5781\n",
      "Average Train loss: 3794.8863, MSE-Loss: 1900.4039, MSE-Future-Loss 1829.6615, KL-Loss: 48.7612,  Kmeans-Loss: 16.0597, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 413.7587, MSE-Loss: 356.6376, KL-Loss: 44.2711, Kmeans-Loss: 12.8500\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 49\n",
      "Train: \n",
      "Epoch: 49.  loss: 3727.1599\n",
      "Average Train loss: 3685.6974, MSE-Loss: 1846.6041, MSE-Future-Loss 1774.1364, KL-Loss: 48.8391,  Kmeans-Loss: 16.1177, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 429.4633, MSE-Loss: 372.2055, KL-Loss: 44.3740, Kmeans-Loss: 12.8838\n",
      "lr: 0.0005\n",
      "Epoch: 50\n",
      "Train: \n",
      "Epoch: 50.  loss: 3709.1177\n",
      "Average Train loss: 3668.5588, MSE-Loss: 1826.5271, MSE-Future-Loss 1776.6406, KL-Loss: 49.1953,  Kmeans-Loss: 16.1958, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 436.0199, MSE-Loss: 378.1392, KL-Loss: 44.8418, Kmeans-Loss: 13.0390\n",
      "lr: 0.0005\n",
      "Saving model snapshot!\n",
      "\n",
      "Epoch: 51\n",
      "Train: \n",
      "Epoch: 51.  loss: 3702.1682\n",
      "Average Train loss: 3644.7593, MSE-Loss: 1823.5999, MSE-Future-Loss 1755.4154, KL-Loss: 49.4718,  Kmeans-Loss: 16.2722, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 417.4071, MSE-Loss: 358.9820, KL-Loss: 45.3559, Kmeans-Loss: 13.0691\n",
      "lr: 0.0005\n",
      "Epoch: 52\n",
      "Train: \n",
      "Epoch: 52.  loss: 3714.7769\n",
      "Average Train loss: 3588.2512, MSE-Loss: 1799.1914, MSE-Future-Loss 1723.1174, KL-Loss: 49.6037,  Kmeans-Loss: 16.3387, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 427.3371, MSE-Loss: 369.0995, KL-Loss: 45.1400, Kmeans-Loss: 13.0977\n",
      "lr: 0.0005\n",
      "Epoch: 53\n",
      "Train: \n",
      "Epoch: 53.  loss: 3722.7275\n",
      "Average Train loss: 3607.3636, MSE-Loss: 1787.9760, MSE-Future-Loss 1753.2033, KL-Loss: 49.7880,  Kmeans-Loss: 16.3964, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 434.0612, MSE-Loss: 375.6976, KL-Loss: 45.2162, Kmeans-Loss: 13.1475\n",
      "lr: 0.0005\n",
      "Epoch: 54\n",
      "Train: \n",
      "Epoch: 54.  loss: 3405.0347\n",
      "Average Train loss: 3583.5722, MSE-Loss: 1846.7979, MSE-Future-Loss 1670.1576, KL-Loss: 50.1358,  Kmeans-Loss: 16.4810, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 412.9822, MSE-Loss: 353.8617, KL-Loss: 45.8716, Kmeans-Loss: 13.2490\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 55\n",
      "Train: \n",
      "Epoch: 55.  loss: 3577.6433\n",
      "Average Train loss: 3450.8595, MSE-Loss: 1765.4331, MSE-Future-Loss 1618.3010, KL-Loss: 50.5596,  Kmeans-Loss: 16.5657, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 432.0272, MSE-Loss: 372.6504, KL-Loss: 45.9803, Kmeans-Loss: 13.3964\n",
      "lr: 0.0005\n",
      "Epoch: 56\n",
      "Train: \n",
      "Epoch: 56.  loss: 3400.0305\n",
      "Average Train loss: 3410.3896, MSE-Loss: 1722.3360, MSE-Future-Loss 1620.7379, KL-Loss: 50.7123,  Kmeans-Loss: 16.6035, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 436.2133, MSE-Loss: 376.5173, KL-Loss: 46.3188, Kmeans-Loss: 13.3772\n",
      "lr: 0.0005\n",
      "Epoch: 57\n",
      "Train: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57.  loss: 3516.2605\n",
      "Average Train loss: 3326.4881, MSE-Loss: 1699.8641, MSE-Future-Loss 1558.8130, KL-Loss: 51.1378,  Kmeans-Loss: 16.6732, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 412.0988, MSE-Loss: 352.0087, KL-Loss: 46.6754, Kmeans-Loss: 13.4146\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 58\n",
      "Train: \n",
      "Epoch: 58.  loss: 3415.5298\n",
      "Average Train loss: 3387.3310, MSE-Loss: 1729.7802, MSE-Future-Loss 1589.2273, KL-Loss: 51.5485,  Kmeans-Loss: 16.7750, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 434.5777, MSE-Loss: 373.6222, KL-Loss: 47.4465, Kmeans-Loss: 13.5090\n",
      "lr: 0.0005\n",
      "Epoch: 59\n",
      "Train: \n",
      "Epoch: 59.  loss: 3178.7622\n",
      "Average Train loss: 3346.6764, MSE-Loss: 1718.7241, MSE-Future-Loss 1559.7282, KL-Loss: 51.4383,  Kmeans-Loss: 16.7859, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 406.0587, MSE-Loss: 345.5699, KL-Loss: 46.9818, Kmeans-Loss: 13.5071\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 60\n",
      "Train: \n",
      "Epoch: 60.  loss: 3411.1343\n",
      "Average Train loss: 3309.6610, MSE-Loss: 1711.3943, MSE-Future-Loss 1529.5294, KL-Loss: 51.8592,  Kmeans-Loss: 16.8782, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 430.4455, MSE-Loss: 368.7949, KL-Loss: 47.8824, Kmeans-Loss: 13.7682\n",
      "lr: 0.0005\n",
      "Epoch: 61\n",
      "Train: \n",
      "Epoch: 61.  loss: 3021.5002\n",
      "Average Train loss: 3173.8657, MSE-Loss: 1652.4184, MSE-Future-Loss 1452.4164, KL-Loss: 52.1188,  Kmeans-Loss: 16.9120, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 419.1149, MSE-Loss: 357.4698, KL-Loss: 47.9566, Kmeans-Loss: 13.6886\n",
      "lr: 0.0005\n",
      "Epoch: 62\n",
      "Train: \n",
      "Epoch: 62.  loss: 2815.2896\n",
      "Average Train loss: 3146.8583, MSE-Loss: 1637.9557, MSE-Future-Loss 1439.7772, KL-Loss: 52.1619,  Kmeans-Loss: 16.9634, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 409.9119, MSE-Loss: 348.3086, KL-Loss: 47.9049, Kmeans-Loss: 13.6984\n",
      "lr: 0.0005\n",
      "Epoch: 63\n",
      "Train: \n",
      "Epoch: 63.  loss: 3142.2944\n",
      "Average Train loss: 3226.4892, MSE-Loss: 1681.5061, MSE-Future-Loss 1475.4323, KL-Loss: 52.5134,  Kmeans-Loss: 17.0373, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 421.1488, MSE-Loss: 358.5789, KL-Loss: 48.7569, Kmeans-Loss: 13.8130\n",
      "lr: 0.0005\n",
      "Epoch: 64\n",
      "Train: \n",
      "Epoch: 64.  loss: 3120.2644\n",
      "Average Train loss: 3276.1044, MSE-Loss: 1694.3285, MSE-Future-Loss 1512.1085, KL-Loss: 52.5859,  Kmeans-Loss: 17.0814, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 429.7473, MSE-Loss: 368.4514, KL-Loss: 47.6009, Kmeans-Loss: 13.6950\n",
      "lr: 0.0005\n",
      "Epoch: 65\n",
      "Train: \n",
      "Epoch: 65.  loss: 3220.4395\n",
      "Average Train loss: 3144.2804, MSE-Loss: 1635.9259, MSE-Future-Loss 1438.6921, KL-Loss: 52.5808,  Kmeans-Loss: 17.0815, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 405.7515, MSE-Loss: 343.6437, KL-Loss: 48.2332, Kmeans-Loss: 13.8746\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 66\n",
      "Train: \n",
      "Epoch: 66.  loss: 3092.2261\n",
      "Average Train loss: 3209.3792, MSE-Loss: 1651.1892, MSE-Future-Loss 1488.3278, KL-Loss: 52.7224,  Kmeans-Loss: 17.1398, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 405.0827, MSE-Loss: 343.3283, KL-Loss: 47.9276, Kmeans-Loss: 13.8269\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 67\n",
      "Train: \n",
      "Epoch: 67.  loss: 2800.7168\n",
      "Average Train loss: 3129.0362, MSE-Loss: 1620.4742, MSE-Future-Loss 1438.4965, KL-Loss: 52.8865,  Kmeans-Loss: 17.1790, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 414.6603, MSE-Loss: 352.3539, KL-Loss: 48.3755, Kmeans-Loss: 13.9309\n",
      "lr: 0.0005\n",
      "Epoch: 68\n",
      "Train: \n",
      "Epoch: 68.  loss: 2540.3831\n",
      "Average Train loss: 3060.8816, MSE-Loss: 1594.1439, MSE-Future-Loss 1396.4278, KL-Loss: 53.0648,  Kmeans-Loss: 17.2450, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 404.0309, MSE-Loss: 341.7767, KL-Loss: 48.3606, Kmeans-Loss: 13.8936\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 69\n",
      "Train: \n",
      "Epoch: 69.  loss: 2958.9985\n",
      "Average Train loss: 3031.0288, MSE-Loss: 1580.2947, MSE-Future-Loss 1380.0670, KL-Loss: 53.3772,  Kmeans-Loss: 17.2898, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 412.8763, MSE-Loss: 350.6421, KL-Loss: 48.2339, Kmeans-Loss: 14.0002\n",
      "lr: 0.0005\n",
      "Epoch: 70\n",
      "Train: \n",
      "Epoch: 70.  loss: 2864.2178\n",
      "Average Train loss: 2979.5871, MSE-Loss: 1579.8122, MSE-Future-Loss 1328.8452, KL-Loss: 53.5968,  Kmeans-Loss: 17.3328, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 412.9525, MSE-Loss: 349.9358, KL-Loss: 48.9485, Kmeans-Loss: 14.0681\n",
      "lr: 0.0005\n",
      "Epoch: 71\n",
      "Train: \n",
      "Epoch: 71.  loss: 3143.2612\n",
      "Average Train loss: 3026.9288, MSE-Loss: 1578.1410, MSE-Future-Loss 1377.5808, KL-Loss: 53.8108,  Kmeans-Loss: 17.3962, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 421.8117, MSE-Loss: 359.3603, KL-Loss: 48.4546, Kmeans-Loss: 13.9968\n",
      "lr: 0.0005\n",
      "Epoch: 72\n",
      "Train: \n",
      "Epoch: 72.  loss: 3096.9451\n",
      "Average Train loss: 2972.2188, MSE-Loss: 1563.1343, MSE-Future-Loss 1338.0167, KL-Loss: 53.6672,  Kmeans-Loss: 17.4006, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 426.5171, MSE-Loss: 363.5953, KL-Loss: 48.8475, Kmeans-Loss: 14.0743\n",
      "lr: 0.0005\n",
      "Epoch: 73\n",
      "Train: \n",
      "Epoch: 73.  loss: 3251.5264\n",
      "Average Train loss: 2979.4871, MSE-Loss: 1579.4854, MSE-Future-Loss 1328.8546, KL-Loss: 53.7131,  Kmeans-Loss: 17.4340, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 401.9318, MSE-Loss: 339.0877, KL-Loss: 48.7201, Kmeans-Loss: 14.1240\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 74\n",
      "Train: \n",
      "Epoch: 74.  loss: 2809.2585\n",
      "Average Train loss: 2916.8402, MSE-Loss: 1542.8342, MSE-Future-Loss 1302.5289, KL-Loss: 53.9999,  Kmeans-Loss: 17.4772, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 411.9890, MSE-Loss: 348.2721, KL-Loss: 49.4694, Kmeans-Loss: 14.2474\n",
      "lr: 0.0005\n",
      "Epoch: 75\n",
      "Train: \n",
      "Epoch: 75.  loss: 2676.1238\n",
      "Average Train loss: 2919.8652, MSE-Loss: 1575.6533, MSE-Future-Loss 1272.2846, KL-Loss: 54.3469,  Kmeans-Loss: 17.5804, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 398.9981, MSE-Loss: 335.6642, KL-Loss: 49.1955, Kmeans-Loss: 14.1383\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 76\n",
      "Train: \n",
      "Epoch: 76.  loss: 2868.5442\n",
      "Average Train loss: 2874.3128, MSE-Loss: 1543.7196, MSE-Future-Loss 1258.8432, KL-Loss: 54.1825,  Kmeans-Loss: 17.5675, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 404.3125, MSE-Loss: 340.5265, KL-Loss: 49.4804, Kmeans-Loss: 14.3056\n",
      "lr: 0.0005\n",
      "Epoch: 77\n",
      "Train: \n",
      "Epoch: 77.  loss: 3050.3247\n",
      "Average Train loss: 2823.2417, MSE-Loss: 1501.2357, MSE-Future-Loss 1250.0069, KL-Loss: 54.3949,  Kmeans-Loss: 17.6043, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 426.9388, MSE-Loss: 363.5956, KL-Loss: 49.1690, Kmeans-Loss: 14.1742\n",
      "lr: 0.0005\n",
      "Epoch: 78\n",
      "Train: \n",
      "Epoch: 78.  loss: 2992.9333\n",
      "Average Train loss: 2883.2135, MSE-Loss: 1532.7064, MSE-Future-Loss 1278.2457, KL-Loss: 54.6086,  Kmeans-Loss: 17.6527, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 405.2694, MSE-Loss: 341.0040, KL-Loss: 49.9295, Kmeans-Loss: 14.3359\n",
      "lr: 0.0005\n",
      "Epoch: 79\n",
      "Train: \n",
      "Epoch: 79.  loss: 2807.2466\n",
      "Average Train loss: 2837.1054, MSE-Loss: 1525.0787, MSE-Future-Loss 1239.4996, KL-Loss: 54.8307,  Kmeans-Loss: 17.6963, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 407.7942, MSE-Loss: 343.4099, KL-Loss: 50.0305, Kmeans-Loss: 14.3538\n",
      "lr: 0.0005\n",
      "Epoch: 80\n",
      "Train: \n",
      "Epoch: 80.  loss: 2390.1853\n",
      "Average Train loss: 2821.8184, MSE-Loss: 1513.4979, MSE-Future-Loss 1235.8325, KL-Loss: 54.7748,  Kmeans-Loss: 17.7132, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 399.0504, MSE-Loss: 335.1296, KL-Loss: 49.5896, Kmeans-Loss: 14.3311\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 81\n",
      "Train: \n",
      "Epoch: 81.  loss: 2921.2292\n",
      "Average Train loss: 2762.3130, MSE-Loss: 1488.1055, MSE-Future-Loss 1201.5392, KL-Loss: 54.9079,  Kmeans-Loss: 17.7604, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 399.4139, MSE-Loss: 335.0146, KL-Loss: 50.0549, Kmeans-Loss: 14.3445\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 82\n",
      "Train: \n",
      "Epoch: 82.  loss: 3086.6001\n",
      "Average Train loss: 2799.5588, MSE-Loss: 1510.7438, MSE-Future-Loss 1215.8180, KL-Loss: 55.1782,  Kmeans-Loss: 17.8189, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 414.7072, MSE-Loss: 350.0999, KL-Loss: 50.1439, Kmeans-Loss: 14.4635\n",
      "lr: 0.0005\n",
      "Epoch: 83\n",
      "Train: \n",
      "Epoch: 83.  loss: 2786.5405\n",
      "Average Train loss: 2691.4623, MSE-Loss: 1460.3061, MSE-Future-Loss 1158.2149, KL-Loss: 55.1223,  Kmeans-Loss: 17.8190, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 413.2913, MSE-Loss: 347.9871, KL-Loss: 50.7319, Kmeans-Loss: 14.5723\n",
      "lr: 0.0005\n",
      "Epoch: 84\n",
      "Train: \n",
      "Epoch: 84.  loss: 2701.2173\n",
      "Average Train loss: 2744.8249, MSE-Loss: 1482.8144, MSE-Future-Loss 1188.8451, KL-Loss: 55.2882,  Kmeans-Loss: 17.8772, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 407.0384, MSE-Loss: 341.2634, KL-Loss: 51.1479, Kmeans-Loss: 14.6272\n",
      "lr: 0.0005\n",
      "Epoch: 85\n",
      "Train: \n",
      "Epoch: 85.  loss: 2673.5427\n",
      "Average Train loss: 2696.1028, MSE-Loss: 1471.2683, MSE-Future-Loss 1151.4590, KL-Loss: 55.4894,  Kmeans-Loss: 17.8861, weigt: 1.0000\n",
      "Test: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 419.1797, MSE-Loss: 354.0670, KL-Loss: 50.5607, Kmeans-Loss: 14.5520\n",
      "lr: 0.0005\n",
      "Epoch: 86\n",
      "Train: \n",
      "Epoch: 86.  loss: 2807.0149\n",
      "Average Train loss: 2743.0092, MSE-Loss: 1488.0176, MSE-Future-Loss 1181.3674, KL-Loss: 55.6832,  Kmeans-Loss: 17.9409, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 418.7503, MSE-Loss: 352.6570, KL-Loss: 51.3932, Kmeans-Loss: 14.7001\n",
      "lr: 0.0005\n",
      "Epoch: 87\n",
      "Train: \n",
      "Epoch: 87.  loss: 3127.7966\n",
      "Average Train loss: 2746.9772, MSE-Loss: 1488.5643, MSE-Future-Loss 1184.7971, KL-Loss: 55.6314,  Kmeans-Loss: 17.9844, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 396.7878, MSE-Loss: 331.4340, KL-Loss: 50.7230, Kmeans-Loss: 14.6309\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 88\n",
      "Train: \n",
      "Epoch: 88.  loss: 2887.8123\n",
      "Average Train loss: 2708.2909, MSE-Loss: 1465.2535, MSE-Future-Loss 1169.2014, KL-Loss: 55.8179,  Kmeans-Loss: 18.0181, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 409.2119, MSE-Loss: 343.3900, KL-Loss: 51.1505, Kmeans-Loss: 14.6715\n",
      "lr: 0.0005\n",
      "Epoch: 89\n",
      "Train: \n",
      "Epoch: 89.  loss: 2758.9690\n",
      "Average Train loss: 2625.2909, MSE-Loss: 1426.4590, MSE-Future-Loss 1125.1614, KL-Loss: 55.6578,  Kmeans-Loss: 18.0126, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 402.9739, MSE-Loss: 337.2260, KL-Loss: 51.0325, Kmeans-Loss: 14.7153\n",
      "lr: 0.0005\n",
      "Epoch: 90\n",
      "Train: \n",
      "Epoch: 90.  loss: 2622.5286\n",
      "Average Train loss: 2650.6462, MSE-Loss: 1439.0279, MSE-Future-Loss 1137.6732, KL-Loss: 55.8844,  Kmeans-Loss: 18.0607, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 404.6901, MSE-Loss: 339.6331, KL-Loss: 50.4661, Kmeans-Loss: 14.5908\n",
      "lr: 0.0005\n",
      "Epoch: 91\n",
      "Train: \n",
      "Epoch: 91.  loss: 2419.3826\n",
      "Average Train loss: 2617.8491, MSE-Loss: 1431.7902, MSE-Future-Loss 1111.8156, KL-Loss: 56.1167,  Kmeans-Loss: 18.1267, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 401.1947, MSE-Loss: 335.4589, KL-Loss: 51.0393, Kmeans-Loss: 14.6965\n",
      "lr: 0.0005\n",
      "Epoch: 92\n",
      "Train: \n",
      "Epoch: 92.  loss: 2516.5613\n",
      "Average Train loss: 2643.5544, MSE-Loss: 1440.8451, MSE-Future-Loss 1128.2081, KL-Loss: 56.3690,  Kmeans-Loss: 18.1321, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 396.5994, MSE-Loss: 329.7747, KL-Loss: 51.9910, Kmeans-Loss: 14.8338\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 93\n",
      "Train: \n",
      "Epoch: 93.  loss: 2616.7415\n",
      "Average Train loss: 2560.7008, MSE-Loss: 1405.2650, MSE-Future-Loss 1081.0701, KL-Loss: 56.2433,  Kmeans-Loss: 18.1224, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 400.8410, MSE-Loss: 334.5610, KL-Loss: 51.4311, Kmeans-Loss: 14.8489\n",
      "lr: 0.0005\n",
      "Epoch: 94\n",
      "Train: \n",
      "Epoch: 94.  loss: 2380.0554\n",
      "Average Train loss: 2628.1915, MSE-Loss: 1412.2451, MSE-Future-Loss 1141.3650, KL-Loss: 56.3865,  Kmeans-Loss: 18.1949, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 394.9194, MSE-Loss: 328.8138, KL-Loss: 51.3162, Kmeans-Loss: 14.7895\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 95\n",
      "Train: \n",
      "Epoch: 95.  loss: 2503.7539\n",
      "Average Train loss: 2583.5328, MSE-Loss: 1415.7748, MSE-Future-Loss 1093.1367, KL-Loss: 56.4058,  Kmeans-Loss: 18.2155, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 401.6186, MSE-Loss: 335.8024, KL-Loss: 51.0487, Kmeans-Loss: 14.7675\n",
      "lr: 0.0005\n",
      "Epoch: 96\n",
      "Train: \n",
      "Epoch: 96.  loss: 2949.1636\n",
      "Average Train loss: 2541.7925, MSE-Loss: 1393.6408, MSE-Future-Loss 1073.5036, KL-Loss: 56.4386,  Kmeans-Loss: 18.2096, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 406.4242, MSE-Loss: 339.3032, KL-Loss: 52.2007, Kmeans-Loss: 14.9203\n",
      "lr: 0.0005\n",
      "Epoch: 97\n",
      "Train: \n",
      "Epoch: 97.  loss: 2327.8562\n",
      "Average Train loss: 2541.3283, MSE-Loss: 1402.1059, MSE-Future-Loss 1064.2746, KL-Loss: 56.6783,  Kmeans-Loss: 18.2695, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 410.4646, MSE-Loss: 344.0151, KL-Loss: 51.5838, Kmeans-Loss: 14.8656\n",
      "lr: 0.0005\n",
      "Epoch: 98\n",
      "Train: \n",
      "Epoch: 98.  loss: 2479.0635\n",
      "Average Train loss: 2585.8577, MSE-Loss: 1404.7426, MSE-Future-Loss 1106.3016, KL-Loss: 56.5529,  Kmeans-Loss: 18.2606, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 403.5610, MSE-Loss: 337.2939, KL-Loss: 51.4534, Kmeans-Loss: 14.8137\n",
      "lr: 0.0005\n",
      "Epoch: 99\n",
      "Train: \n",
      "Epoch: 99.  loss: 2863.3264\n",
      "Average Train loss: 2531.2024, MSE-Loss: 1391.8580, MSE-Future-Loss 1064.4201, KL-Loss: 56.6133,  Kmeans-Loss: 18.3109, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 391.1913, MSE-Loss: 324.7650, KL-Loss: 51.6377, Kmeans-Loss: 14.7886\n",
      "lr: 0.0005\n",
      "Saving model!\n",
      "\n",
      "Epoch: 100\n",
      "Train: \n",
      "Epoch: 100.  loss: 2338.7866\n",
      "Average Train loss: 2550.1945, MSE-Loss: 1394.4007, MSE-Future-Loss 1080.5058, KL-Loss: 56.9285,  Kmeans-Loss: 18.3595, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 404.6115, MSE-Loss: 337.7144, KL-Loss: 51.9668, Kmeans-Loss: 14.9304\n",
      "lr: 0.0001\n",
      "Saving model snapshot!\n",
      "\n",
      "Epoch: 101\n",
      "Train: \n",
      "Epoch: 101.  loss: 2845.5305\n",
      "Average Train loss: 2361.6147, MSE-Loss: 1314.0069, MSE-Future-Loss 972.1633, KL-Loss: 57.0641,  Kmeans-Loss: 18.3805, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 392.3710, MSE-Loss: 325.3994, KL-Loss: 51.9949, Kmeans-Loss: 14.9767\n",
      "lr: 0.0001\n",
      "Epoch: 102\n",
      "Train: \n",
      "Epoch: 102.  loss: 2088.3328\n",
      "Average Train loss: 2285.6701, MSE-Loss: 1283.6287, MSE-Future-Loss 926.5085, KL-Loss: 57.1359,  Kmeans-Loss: 18.3970, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 379.6691, MSE-Loss: 312.8736, KL-Loss: 51.8461, Kmeans-Loss: 14.9493\n",
      "lr: 0.0001\n",
      "Saving model!\n",
      "\n",
      "Epoch: 103\n",
      "Train: \n",
      "Epoch: 103.  loss: 2095.9761\n",
      "Average Train loss: 2278.9461, MSE-Loss: 1283.0704, MSE-Future-Loss 920.1872, KL-Loss: 57.2606,  Kmeans-Loss: 18.4280, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 387.1287, MSE-Loss: 320.1649, KL-Loss: 51.9753, Kmeans-Loss: 14.9884\n",
      "lr: 0.0001\n",
      "Epoch: 104\n",
      "Train: \n",
      "Epoch: 104.  loss: 2314.0774\n",
      "Average Train loss: 2286.5269, MSE-Loss: 1275.6306, MSE-Future-Loss 935.2828, KL-Loss: 57.1901,  Kmeans-Loss: 18.4234, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 388.5755, MSE-Loss: 321.5788, KL-Loss: 52.0125, Kmeans-Loss: 14.9841\n",
      "lr: 0.0001\n",
      "Epoch: 105\n",
      "Train: \n",
      "Epoch: 105.  loss: 2495.1079\n",
      "Average Train loss: 2261.1645, MSE-Loss: 1269.8666, MSE-Future-Loss 915.7647, KL-Loss: 57.1296,  Kmeans-Loss: 18.4035, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 387.3103, MSE-Loss: 320.0180, KL-Loss: 52.2454, Kmeans-Loss: 15.0468\n",
      "lr: 0.0001\n",
      "Epoch: 106\n",
      "Train: \n",
      "Epoch: 106.  loss: 2056.0479\n",
      "Average Train loss: 2243.3380, MSE-Loss: 1265.0109, MSE-Future-Loss 902.4330, KL-Loss: 57.4521,  Kmeans-Loss: 18.4420, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 396.8316, MSE-Loss: 329.5274, KL-Loss: 52.2448, Kmeans-Loss: 15.0595\n",
      "lr: 0.0001\n",
      "Epoch: 107\n",
      "Train: \n",
      "Epoch: 107.  loss: 2402.4114\n",
      "Average Train loss: 2262.1308, MSE-Loss: 1275.1312, MSE-Future-Loss 911.1028, KL-Loss: 57.4379,  Kmeans-Loss: 18.4590, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 389.9219, MSE-Loss: 322.5649, KL-Loss: 52.3451, Kmeans-Loss: 15.0119\n",
      "lr: 0.0001\n",
      "Epoch: 108\n",
      "Train: \n",
      "Epoch: 108.  loss: 2174.9128\n",
      "Average Train loss: 2226.6326, MSE-Loss: 1254.0420, MSE-Future-Loss 896.8033, KL-Loss: 57.3482,  Kmeans-Loss: 18.4391, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 393.5003, MSE-Loss: 326.1367, KL-Loss: 52.3148, Kmeans-Loss: 15.0488\n",
      "lr: 0.0001\n",
      "Epoch: 109\n",
      "Train: \n",
      "Epoch: 109.  loss: 2304.7786\n",
      "Average Train loss: 2255.5118, MSE-Loss: 1274.0461, MSE-Future-Loss 905.4240, KL-Loss: 57.5748,  Kmeans-Loss: 18.4669, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 388.0013, MSE-Loss: 320.4510, KL-Loss: 52.5140, Kmeans-Loss: 15.0363\n",
      "lr: 0.0001\n",
      "Epoch: 110\n",
      "Train: \n",
      "Epoch: 110.  loss: 2413.8535\n",
      "Average Train loss: 2253.4809, MSE-Loss: 1271.5881, MSE-Future-Loss 905.7111, KL-Loss: 57.6974,  Kmeans-Loss: 18.4842, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 386.3679, MSE-Loss: 318.8914, KL-Loss: 52.4629, Kmeans-Loss: 15.0135\n",
      "lr: 0.0001\n",
      "Epoch: 111\n",
      "Train: \n",
      "Epoch: 111.  loss: 2332.9321\n",
      "Average Train loss: 2223.3267, MSE-Loss: 1254.2664, MSE-Future-Loss 892.7536, KL-Loss: 57.8027,  Kmeans-Loss: 18.5039, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 389.4705, MSE-Loss: 321.9161, KL-Loss: 52.4792, Kmeans-Loss: 15.0751\n",
      "lr: 0.0001\n",
      "Epoch: 112\n",
      "Train: \n",
      "Epoch: 112.  loss: 2388.9514\n",
      "Average Train loss: 2251.5048, MSE-Loss: 1273.8361, MSE-Future-Loss 901.2327, KL-Loss: 57.8961,  Kmeans-Loss: 18.5400, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 392.4741, MSE-Loss: 324.7621, KL-Loss: 52.6301, Kmeans-Loss: 15.0818\n",
      "lr: 0.0001\n",
      "Epoch: 113\n",
      "Train: \n",
      "Epoch: 113.  loss: 2019.8416\n",
      "Average Train loss: 2236.8711, MSE-Loss: 1265.1068, MSE-Future-Loss 895.4128, KL-Loss: 57.8463,  Kmeans-Loss: 18.5052, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 385.0195, MSE-Loss: 317.2037, KL-Loss: 52.7140, Kmeans-Loss: 15.1017\n",
      "lr: 0.0001\n",
      "Epoch: 114\n",
      "Train: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114.  loss: 2321.4226\n",
      "Average Train loss: 2227.8957, MSE-Loss: 1254.5430, MSE-Future-Loss 896.8459, KL-Loss: 57.9731,  Kmeans-Loss: 18.5338, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 390.0544, MSE-Loss: 322.0849, KL-Loss: 52.8551, Kmeans-Loss: 15.1143\n",
      "lr: 0.0001\n",
      "Epoch: 115\n",
      "Train: \n",
      "Epoch: 115.  loss: 2128.1685\n",
      "Average Train loss: 2222.4687, MSE-Loss: 1255.6202, MSE-Future-Loss 890.2999, KL-Loss: 58.0081,  Kmeans-Loss: 18.5404, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 382.1325, MSE-Loss: 314.6129, KL-Loss: 52.4428, Kmeans-Loss: 15.0768\n",
      "lr: 0.0001\n",
      "Epoch: 116\n",
      "Train: \n",
      "Epoch: 116.  loss: 2148.1655\n",
      "Average Train loss: 2237.3651, MSE-Loss: 1270.7233, MSE-Future-Loss 889.8575, KL-Loss: 58.2144,  Kmeans-Loss: 18.5699, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 394.6820, MSE-Loss: 326.5068, KL-Loss: 53.0182, Kmeans-Loss: 15.1570\n",
      "lr: 0.0001\n",
      "Epoch: 117\n",
      "Train: \n",
      "Epoch: 117.  loss: 2375.7344\n",
      "Average Train loss: 2230.8735, MSE-Loss: 1268.2462, MSE-Future-Loss 885.9209, KL-Loss: 58.1436,  Kmeans-Loss: 18.5628, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 383.4327, MSE-Loss: 315.7379, KL-Loss: 52.6276, Kmeans-Loss: 15.0671\n",
      "lr: 0.0001\n",
      "Epoch: 118\n",
      "Train: \n",
      "Epoch: 118.  loss: 2359.6147\n",
      "Average Train loss: 2196.8059, MSE-Loss: 1243.7031, MSE-Future-Loss 876.4344, KL-Loss: 58.1189,  Kmeans-Loss: 18.5495, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 387.9079, MSE-Loss: 319.8913, KL-Loss: 52.8695, Kmeans-Loss: 15.1471\n",
      "lr: 0.0001\n",
      "Epoch: 119\n",
      "Train: \n",
      "Epoch: 119.  loss: 2145.8718\n",
      "Average Train loss: 2187.3711, MSE-Loss: 1245.6707, MSE-Future-Loss 864.9808, KL-Loss: 58.1653,  Kmeans-Loss: 18.5543, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 386.4838, MSE-Loss: 318.4293, KL-Loss: 52.9232, Kmeans-Loss: 15.1314\n",
      "lr: 0.0001\n",
      "Epoch: 120\n",
      "Train: \n",
      "Epoch: 120.  loss: 2064.8645\n",
      "Average Train loss: 2222.7722, MSE-Loss: 1267.2320, MSE-Future-Loss 878.6919, KL-Loss: 58.2660,  Kmeans-Loss: 18.5823, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 380.8785, MSE-Loss: 312.9901, KL-Loss: 52.7711, Kmeans-Loss: 15.1173\n",
      "lr: 0.0001\n",
      "Epoch: 121\n",
      "Train: \n",
      "Epoch: 121.  loss: 2381.6331\n",
      "Average Train loss: 2213.5456, MSE-Loss: 1254.5905, MSE-Future-Loss 881.8089, KL-Loss: 58.5292,  Kmeans-Loss: 18.6170, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 395.0936, MSE-Loss: 326.8868, KL-Loss: 52.9881, Kmeans-Loss: 15.2188\n",
      "lr: 0.0001\n",
      "Epoch: 122\n",
      "Train: \n",
      "Epoch: 122.  loss: 1977.7795\n",
      "Average Train loss: 2200.3255, MSE-Loss: 1246.8090, MSE-Future-Loss 876.5576, KL-Loss: 58.3748,  Kmeans-Loss: 18.5842, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 387.3078, MSE-Loss: 318.7907, KL-Loss: 53.3173, Kmeans-Loss: 15.1998\n",
      "lr: 0.0001\n",
      "Epoch: 123\n",
      "Train: \n",
      "Epoch: 123.  loss: 2173.3420\n",
      "Average Train loss: 2217.5508, MSE-Loss: 1259.8656, MSE-Future-Loss 880.5597, KL-Loss: 58.5207,  Kmeans-Loss: 18.6049, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 386.7958, MSE-Loss: 318.4643, KL-Loss: 53.1373, Kmeans-Loss: 15.1941\n",
      "lr: 0.0001\n",
      "Epoch: 124\n",
      "Train: \n",
      "Epoch: 124.  loss: 2232.0886\n",
      "Average Train loss: 2189.6258, MSE-Loss: 1233.5938, MSE-Future-Loss 878.7596, KL-Loss: 58.6504,  Kmeans-Loss: 18.6220, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 385.2411, MSE-Loss: 316.6859, KL-Loss: 53.3697, Kmeans-Loss: 15.1855\n",
      "lr: 0.0001\n",
      "Epoch: 125\n",
      "Train: \n",
      "Epoch: 125.  loss: 1970.7302\n",
      "Average Train loss: 2189.0601, MSE-Loss: 1248.7752, MSE-Future-Loss 863.0137, KL-Loss: 58.6308,  Kmeans-Loss: 18.6404, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 388.0174, MSE-Loss: 319.1810, KL-Loss: 53.6211, Kmeans-Loss: 15.2153\n",
      "lr: 0.0001\n",
      "Epoch: 126\n",
      "Train: \n",
      "Epoch: 126.  loss: 2307.9194\n",
      "Average Train loss: 2211.4080, MSE-Loss: 1258.5195, MSE-Future-Loss 875.3675, KL-Loss: 58.8530,  Kmeans-Loss: 18.6681, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 393.5489, MSE-Loss: 324.6293, KL-Loss: 53.6621, Kmeans-Loss: 15.2575\n",
      "lr: 0.0001\n",
      "Epoch: 127\n",
      "Train: \n",
      "Epoch: 127.  loss: 2135.7114\n",
      "Average Train loss: 2150.4686, MSE-Loss: 1226.2262, MSE-Future-Loss 846.7244, KL-Loss: 58.8642,  Kmeans-Loss: 18.6538, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 392.3718, MSE-Loss: 323.3053, KL-Loss: 53.7814, Kmeans-Loss: 15.2851\n",
      "lr: 0.0001\n",
      "Epoch: 128\n",
      "Train: \n",
      "Epoch: 128.  loss: 2172.8816\n",
      "Average Train loss: 2159.3880, MSE-Loss: 1226.5823, MSE-Future-Loss 855.1090, KL-Loss: 59.0024,  Kmeans-Loss: 18.6943, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 398.3841, MSE-Loss: 329.4235, KL-Loss: 53.6528, Kmeans-Loss: 15.3078\n",
      "lr: 0.0001\n",
      "Epoch: 129\n",
      "Train: \n",
      "Epoch: 129.  loss: 2370.4302\n",
      "Average Train loss: 2193.7555, MSE-Loss: 1244.9131, MSE-Future-Loss 871.2349, KL-Loss: 58.9370,  Kmeans-Loss: 18.6705, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 382.3091, MSE-Loss: 313.5635, KL-Loss: 53.5371, Kmeans-Loss: 15.2085\n",
      "lr: 0.0001\n",
      "Epoch: 130\n",
      "Train: \n",
      "Epoch: 130.  loss: 2223.1787\n",
      "Average Train loss: 2182.2965, MSE-Loss: 1240.2829, MSE-Future-Loss 864.2717, KL-Loss: 59.0254,  Kmeans-Loss: 18.7165, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 382.7401, MSE-Loss: 313.8001, KL-Loss: 53.7254, Kmeans-Loss: 15.2147\n",
      "lr: 0.0001\n",
      "Epoch: 131\n",
      "Train: \n",
      "Epoch: 131.  loss: 1992.8547\n",
      "Average Train loss: 2190.8087, MSE-Loss: 1246.3761, MSE-Future-Loss 866.6981, KL-Loss: 59.0215,  Kmeans-Loss: 18.7129, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 390.2301, MSE-Loss: 321.2573, KL-Loss: 53.7191, Kmeans-Loss: 15.2537\n",
      "lr: 0.0001\n",
      "Epoch: 132\n",
      "Train: \n",
      "Epoch: 132.  loss: 2258.9648\n",
      "Average Train loss: 2183.6201, MSE-Loss: 1242.6405, MSE-Future-Loss 862.8250, KL-Loss: 59.4064,  Kmeans-Loss: 18.7482, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 391.5088, MSE-Loss: 322.8103, KL-Loss: 53.4714, Kmeans-Loss: 15.2272\n",
      "lr: 0.0001\n",
      "Epoch: 133\n",
      "Train: \n",
      "Epoch: 133.  loss: 2007.2760\n",
      "Average Train loss: 2171.5836, MSE-Loss: 1232.5218, MSE-Future-Loss 861.2141, KL-Loss: 59.1460,  Kmeans-Loss: 18.7017, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 378.3097, MSE-Loss: 309.2998, KL-Loss: 53.8125, Kmeans-Loss: 15.1974\n",
      "lr: 0.0001\n",
      "Saving model!\n",
      "\n",
      "Epoch: 134\n",
      "Train: \n",
      "Epoch: 134.  loss: 2214.5642\n",
      "Average Train loss: 2170.6549, MSE-Loss: 1234.3612, MSE-Future-Loss 858.1294, KL-Loss: 59.4181,  Kmeans-Loss: 18.7461, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 382.1732, MSE-Loss: 313.0627, KL-Loss: 53.8418, Kmeans-Loss: 15.2687\n",
      "lr: 0.0001\n",
      "Epoch: 135\n",
      "Train: \n",
      "Epoch: 135.  loss: 2375.0623\n",
      "Average Train loss: 2189.2717, MSE-Loss: 1246.8343, MSE-Future-Loss 864.2209, KL-Loss: 59.4447,  Kmeans-Loss: 18.7718, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 389.6739, MSE-Loss: 320.5904, KL-Loss: 53.8264, Kmeans-Loss: 15.2572\n",
      "lr: 0.0001\n",
      "Epoch: 136\n",
      "Train: \n",
      "Epoch: 136.  loss: 2285.5266\n",
      "Average Train loss: 2182.4524, MSE-Loss: 1234.3388, MSE-Future-Loss 869.7883, KL-Loss: 59.5560,  Kmeans-Loss: 18.7694, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 394.4496, MSE-Loss: 324.8949, KL-Loss: 54.2055, Kmeans-Loss: 15.3491\n",
      "lr: 0.0001\n",
      "Epoch: 137\n",
      "Train: \n",
      "Epoch: 137.  loss: 2527.1440\n",
      "Average Train loss: 2186.8297, MSE-Loss: 1240.9342, MSE-Future-Loss 867.5943, KL-Loss: 59.5459,  Kmeans-Loss: 18.7553, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 380.2145, MSE-Loss: 311.1233, KL-Loss: 53.8494, Kmeans-Loss: 15.2417\n",
      "lr: 0.0001\n",
      "Epoch: 138\n",
      "Train: \n",
      "Epoch: 138.  loss: 2092.6521\n",
      "Average Train loss: 2140.6852, MSE-Loss: 1218.5019, MSE-Future-Loss 843.9564, KL-Loss: 59.4488,  Kmeans-Loss: 18.7781, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 384.3068, MSE-Loss: 314.9170, KL-Loss: 54.0812, Kmeans-Loss: 15.3085\n",
      "lr: 0.0001\n",
      "Epoch: 139\n",
      "Train: \n",
      "Epoch: 139.  loss: 2388.6267\n",
      "Average Train loss: 2174.3338, MSE-Loss: 1238.1621, MSE-Future-Loss 857.7753, KL-Loss: 59.6261,  Kmeans-Loss: 18.7703, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 396.6521, MSE-Loss: 327.1597, KL-Loss: 54.1646, Kmeans-Loss: 15.3278\n",
      "lr: 0.0001\n",
      "Epoch: 140\n",
      "Train: \n",
      "Epoch: 140.  loss: 1937.8777\n",
      "Average Train loss: 2156.7707, MSE-Loss: 1233.5687, MSE-Future-Loss 844.6791, KL-Loss: 59.7144,  Kmeans-Loss: 18.8085, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 396.7655, MSE-Loss: 326.8672, KL-Loss: 54.4815, Kmeans-Loss: 15.4168\n",
      "lr: 0.0001\n",
      "Epoch: 141\n",
      "Train: \n",
      "Epoch: 141.  loss: 2115.9929\n",
      "Average Train loss: 2170.4056, MSE-Loss: 1233.4744, MSE-Future-Loss 858.3018, KL-Loss: 59.7955,  Kmeans-Loss: 18.8338, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 387.2671, MSE-Loss: 317.3962, KL-Loss: 54.5054, Kmeans-Loss: 15.3654\n",
      "lr: 0.0001\n",
      "Epoch: 142\n",
      "Train: \n",
      "Epoch: 142.  loss: 2060.2605\n",
      "Average Train loss: 2145.2607, MSE-Loss: 1219.9497, MSE-Future-Loss 846.6260, KL-Loss: 59.8814,  Kmeans-Loss: 18.8036, weigt: 1.0000\n",
      "Test: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 389.9778, MSE-Loss: 320.1211, KL-Loss: 54.5060, Kmeans-Loss: 15.3506\n",
      "lr: 0.0001\n",
      "Epoch: 143\n",
      "Train: \n",
      "Epoch: 143.  loss: 2013.9957\n",
      "Average Train loss: 2160.8534, MSE-Loss: 1230.2905, MSE-Future-Loss 851.8935, KL-Loss: 59.8373,  Kmeans-Loss: 18.8321, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 396.9677, MSE-Loss: 327.2234, KL-Loss: 54.3460, Kmeans-Loss: 15.3983\n",
      "lr: 0.0001\n",
      "Epoch: 144\n",
      "Train: \n",
      "Epoch: 144.  loss: 2121.8577\n",
      "Average Train loss: 2138.8742, MSE-Loss: 1228.5979, MSE-Future-Loss 831.3734, KL-Loss: 60.0612,  Kmeans-Loss: 18.8416, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 386.5311, MSE-Loss: 316.7349, KL-Loss: 54.4317, Kmeans-Loss: 15.3646\n",
      "lr: 0.0001\n",
      "Epoch: 145\n",
      "Train: \n",
      "Epoch: 145.  loss: 2233.4146\n",
      "Average Train loss: 2138.1698, MSE-Loss: 1217.9393, MSE-Future-Loss 841.4891, KL-Loss: 59.9059,  Kmeans-Loss: 18.8356, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 383.3712, MSE-Loss: 313.7436, KL-Loss: 54.2968, Kmeans-Loss: 15.3308\n",
      "lr: 0.0001\n",
      "Epoch: 146\n",
      "Train: \n",
      "Epoch: 146.  loss: 2084.6277\n",
      "Average Train loss: 2154.1294, MSE-Loss: 1220.9104, MSE-Future-Loss 854.3381, KL-Loss: 60.0100,  Kmeans-Loss: 18.8708, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 387.8818, MSE-Loss: 317.8942, KL-Loss: 54.6320, Kmeans-Loss: 15.3555\n",
      "lr: 0.0001\n",
      "Epoch: 147\n",
      "Train: \n",
      "Epoch: 147.  loss: 2149.6082\n",
      "Average Train loss: 2141.1117, MSE-Loss: 1225.1500, MSE-Future-Loss 836.9804, KL-Loss: 60.1263,  Kmeans-Loss: 18.8550, weigt: 1.0000\n",
      "Test: \n",
      "Average Test loss: 395.0387, MSE-Loss: 324.9984, KL-Loss: 54.5858, Kmeans-Loss: 15.4545\n",
      "lr: 0.0001\n",
      "Epoch: 148\n",
      "Train: \n",
      "Epoch: 148.  loss: 2079.4607\n"
     ]
    }
   ],
   "source": [
    "vame.rnn_model(config, model_name='VAME', pretrained_weights=False, pretrained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
